[{"title": "quantile and probability curves without crossing", "id": "0704.3649", "abstract": "this paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. the method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. we show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. we also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. we illustrate the results with an application to estimation of structural quantile functions using data on vietnam veteran status and earnings.", "categories": "stat.me econ.em math.st stat.th", "created": "2007-04-27", "updated": "2014-07-14", "authors": ["victor chernozhukov", "ivan fernandez-val", "alfred galichon"], "url": "https://arxiv.org/abs/0704.3649"}, {"title": "improving estimates of monotone functions by rearrangement", "id": "0704.3686", "abstract": "suppose that a target function is monotonic, namely, weakly increasing, and an original estimate of the target function is available, which is not weakly increasing. many common estimation methods used in statistics produce such estimates. we show that these estimates can always be improved with no harm using rearrangement techniques: the rearrangement methods, univariate and multivariate, transform the original estimate to a monotonic estimate, and the resulting estimate is closer to the true curve in common metrics than the original estimate. we illustrate the results with a computational example and an empirical example dealing with age-height growth charts.", "categories": "stat.me econ.em math.st stat.th", "created": "2007-04-27", "updated": "2010-11-03", "authors": ["victor chernozhukov", "ivan fernandez-val", "alfred galichon"], "url": "https://arxiv.org/abs/0704.3686"}, {"title": "rearranging edgeworth-cornish-fisher expansions", "id": "0708.1627", "abstract": "this paper applies a regularization procedure called increasing rearrangement to monotonize edgeworth and cornish-fisher expansions and any other related approximations of distribution and quantile functions of sample statistics. besides satisfying the logical monotonicity, required of distribution and quantile functions, the procedure often delivers strikingly better approximations to the distribution and quantile functions of the sample mean than the original edgeworth-cornish-fisher expansions.", "categories": "stat.me econ.em", "created": "2007-08-12", "updated": "2013-05-30", "authors": ["victor chernozhukov", "ivan fernandez-val", "alfred galichon"], "url": "https://arxiv.org/abs/0708.1627"}, {"title": "improving point and interval estimates of monotone functions by   rearrangement", "id": "0806.4730", "abstract": "suppose that a target function is monotonic, namely, weakly increasing, and an available original estimate of this target function is not weakly increasing. rearrangements, univariate and multivariate, transform the original estimate to a monotonic estimate that always lies closer in common metrics to the target function. furthermore, suppose an original simultaneous confidence interval, which covers the target function with probability at least $1-\\alpha$, is defined by an upper and lower end-point functions that are not weakly increasing. then the rearranged confidence interval, defined by the rearranged upper and lower end-point functions, is shorter in length in common norms than the original interval and also covers the target function with probability at least $1-\\alpha$. we demonstrate the utility of the improved point and interval estimates with an age-height growth chart example.", "categories": "math.st econ.em math.fa stat.me stat.th", "created": "2008-06-28", "updated": "2008-11-07", "authors": ["victor chernozhukov", "ivan fernandez-val", "alfred galichon"], "url": "https://arxiv.org/abs/0806.4730"}, {"title": "inference on counterfactual distributions", "id": "0904.0951", "abstract": "counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. in this article we develop modeling and inference tools for counterfactual distributions based on regression methods. the counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. for either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. these results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. these confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. we illustrate the results with an empirical application to wage decompositions using data for the united states.   as a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the \\textit{entire} conditional distribution. we show that distribution regression encompasses the cox duration regression and represents a useful alternative to quantile regression. we establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.", "categories": "stat.me econ.em stat.ap", "created": "2009-04-06", "updated": "2013-09-18", "authors": ["victor chernozhukov", "ivan fernandez-val", "blaise melly"], "url": "https://arxiv.org/abs/0904.0951"}, {"title": "average and quantile effects in nonseparable panel models", "id": "0904.1990", "abstract": "nonseparable panel models are important in a variety of economic settings, including discrete choice. this paper gives identification and estimation results for nonseparable models under time homogeneity conditions that are like \"time is randomly assigned\" or \"time is an instrument.\" partial identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. it is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. a simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. bounds for overall effects in static and dynamic models are given. the dynamic bounds provide a partial identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. the impact of $t$, the number of time periods, is shown by deriving shrinkage rates for the identified set as $t$ grows. we also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. computationally-convenient methods for semiparametric models are presented. we propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. we give empirical illustrations.", "categories": "stat.me econ.em math.st stat.ap stat.th", "created": "2009-04-13", "updated": "2013-03-26", "authors": ["victor chernozhukov", "ivan fernandez-val", "jinyong hahn", "whitney newey"], "url": "https://arxiv.org/abs/0904.1990"}, {"title": "l1-penalized quantile regression in high-dimensional sparse models", "id": "0904.2931", "abstract": "we consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. in these models the overall number of regressors $p$ is very large, possibly larger than the sample size $n$, but only $s$ of these regressors have non-zero impact on the conditional quantile of the response variable, where $s$ grows slower than $n$. we consider quantile regression penalized by the $\\ell_1$-norm of coefficients ($\\ell_1$-qr). first, we show that $\\ell_1$-qr is consistent at the rate $\\sqrt{s/n} \\sqrt{\\log p}$. the overall number of regressors $p$ affects the rate only through the $\\log p$ factor, thus allowing nearly exponential growth in the number of zero-impact regressors. the rate result holds under relatively weak conditions, requiring that $s/n$ converges to zero at a super-logarithmic speed and that regularization parameter satisfies certain theoretical constraints. second, we propose a pivotal, data-driven choice of the regularization parameter and show that it satisfies these theoretical constraints. third, we show that $\\ell_1$-qr correctly selects the true minimal model as a valid submodel, when the non-zero coefficients of the true model are well separated from zero. we also show that the number of non-zero coefficients in $\\ell_1$-qr is of same stochastic order as $s$. fourth, we analyze the rate of convergence of a two-step estimator that applies ordinary quantile regression to the selected model. fifth, we evaluate the performance of $\\ell_1$-qr in a monte-carlo experiment, and illustrate its use on an international economic growth application.", "categories": "math.st econ.em math.pr stat.me stat.th", "created": "2009-04-19", "updated": "2019-09-26", "authors": ["alexandre belloni", "victor chernozhukov"], "url": "https://arxiv.org/abs/0904.2931"}, {"title": "posterior inference in curved exponential families under increasing   dimensions", "id": "0904.3132", "abstract": "this work studies the large sample properties of the posterior-based inference in the curved exponential family under increasing dimension. the curved structure arises from the imposition of various restrictions on the model, such as moment restrictions, and plays a fundamental role in econometrics and others branches of data analysis. we establish conditions under which the posterior distribution is approximately normal, which in turn implies various good properties of estimation and inference procedures based on the posterior. in the process we also revisit and improve upon previous results for the exponential family under increasing dimension by making use of concentration of measure. we also discuss a variety of applications to high-dimensional versions of the classical econometric models including the multinomial model with moment restrictions, seemingly unrelated regression equations, and single structural equation models. in our analysis, both the parameter dimension and the number of moments are increasing with the sample size.", "categories": "math.st econ.em math.pr stat.me stat.th", "created": "2009-04-20", "updated": "2014-04-22", "authors": ["alexandre belloni", "victor chernozhukov"], "url": "https://arxiv.org/abs/0904.3132"}, {"title": "complete characterization of functions satisfying the conditions of   arrow's theorem", "id": "0910.2465", "abstract": "arrow's theorem implies that a social choice function satisfying transitivity, the pareto principle (unanimity) and independence of irrelevant alternatives (iia) must be dictatorial. when non-strict preferences are allowed, a dictatorial social choice function is defined as a function for which there exists a single voter whose strict preferences are followed. this definition allows for many different dictatorial functions. in particular, we construct examples of dictatorial functions which do not satisfy transitivity and iia. thus arrow's theorem, in the case of non-strict preferences, does not provide a complete characterization of all social choice functions satisfying transitivity, the pareto principle, and iia.   the main results of this article provide such a characterization for arrow's theorem, as well as for follow up results by wilson. in particular, we strengthen arrow's and wilson's result by giving an exact if and only if condition for a function to satisfy transitivity and iia (and the pareto principle). additionally, we derive formulas for the number of functions satisfying these conditions.", "categories": "math.co econ.th", "created": "2009-10-13", "updated": "2011-04-14", "authors": ["elchanan mossel", "omer tamuz"], "url": "https://arxiv.org/abs/0910.2465"}, {"title": "inference for extremal conditional quantile models, with an application   to market and birthweight risks", "id": "0912.5013", "abstract": "quantile regression is an increasingly important empirical tool in economics and other sciences for analyzing the impact of a set of regressors on the conditional distribution of an outcome. extremal quantile regression, or quantile regression applied to the tails, is of interest in many economic and financial applications, such as conditional value-at-risk, production efficiency, and adjustment bands in (s,s) models. in this paper we provide feasible inference tools for extremal conditional quantile models that rely upon extreme value approximations to the distribution of self-normalized quantile regression statistics. the methods are simple to implement and can be of independent interest even in the non-regression case. we illustrate the results with two empirical examples analyzing extreme fluctuations of a stock return and extremely low percentiles of live infants' birthweights in the range between 250 and 1500 grams.", "categories": "stat.me econ.em math.st q-fin.rm stat.th", "created": "2009-12-26", "updated": "", "authors": ["victor chernozhukov", "ivan fernandez-val"], "url": "https://arxiv.org/abs/0912.5013"}, {"title": "toy model for large non-symmetric random matrices", "id": "1004.4522", "abstract": "non-symmetric rectangular correlation matrices occur in many problems in economics. we test the method of extracting statistically meaningful correlations between input and output variables of large dimensionality and build a toy model for artificially included correlations in large random time series.the results are then applied to analysis of polish macroeconomic data and can be used as an alternative to classical cointegration approach.", "categories": "physics.data-an econ.gn q-fin.ec q-fin.st stat.me", "created": "2010-04-26", "updated": "", "authors": ["ma\u0142gorzata snarska"], "url": "https://arxiv.org/abs/1004.4522"}, {"title": "sparse models and methods for optimal instruments with an application to   eminent domain", "id": "1010.4345", "abstract": "we develop results for the use of lasso and post-lasso methods to form first-stage predictions and estimate optimal instruments in linear instrumental variables (iv) models with many instruments, $p$. our results apply even when $p$ is much larger than the sample size, $n$. we show that the iv estimator based on using lasso or post-lasso in the first stage is root-n consistent and asymptotically normal when the first-stage is approximately sparse; i.e. when the conditional expectation of the endogenous variables given the instruments can be well-approximated by a relatively small set of variables whose identities may be unknown. we also show the estimator is semi-parametrically efficient when the structural error is homoscedastic. notably our results allow for imperfect model selection, and do not rely upon the unrealistic \"beta-min\" conditions that are widely used to establish validity of inference following model selection. in simulation experiments, the lasso-based iv estimator with a data-driven penalty performs well compared to recently advocated many-instrument-robust procedures. in an empirical example dealing with the effect of judicial eminent domain decisions on economic outcomes, the lasso-based iv estimator outperforms an intuitive benchmark.   in developing the iv results, we establish a series of new results for lasso and post-lasso estimators of nonparametric conditional expectation functions which are of independent theoretical and practical interest. we construct a modification of lasso designed to deal with non-gaussian, heteroscedastic disturbances which uses a data-weighted $\\ell_1$-penalty function. using moderate deviation theory for self-normalized sums, we provide convergence rates for the resulting lasso and post-lasso estimators that are as sharp as the corresponding rates in the homoscedastic gaussian case under the condition that $\\log p = o(n^{1/3})$.", "categories": "stat.me econ.em math.st stat.th", "created": "2010-10-20", "updated": "2015-04-19", "authors": ["alexandre belloni", "daniel chen", "victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1010.4345"}, {"title": "lasso methods for gaussian instrumental variables models", "id": "1012.1297", "abstract": "in this note, we propose to use sparse methods (e.g. lasso, post-lasso, sqrt-lasso, and post-sqrt-lasso) to form first-stage predictions and estimate optimal instruments in linear instrumental variables (iv) models with many instruments in the canonical gaussian case. the methods apply even when the number of instruments is much larger than the sample size. we derive asymptotic distributions for the resulting iv estimators and provide conditions under which these sparsity-based iv estimators are asymptotically oracle-efficient. in simulation experiments, a sparsity-based iv estimator with a data-driven penalty performs well compared to recently advocated many-instrument-robust procedures. we illustrate the procedure in an empirical example using the angrist and krueger (1991) schooling data.", "categories": "stat.me econ.em math.st stat.ap stat.th", "created": "2010-12-06", "updated": "2011-02-23", "authors": ["alexandre belloni", "victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1012.1297"}, {"title": "quantile regression with censoring and endogeneity", "id": "1104.4580", "abstract": "in this paper, we develop a new censored quantile instrumental variable (cqiv) estimator and describe its properties and computation. the cqiv estimator combines powell (1986) censored quantile regression (cqr) to deal with censoring, with a control variable approach to incorporate endogenous regressors. the cqiv estimator is obtained in two stages that are non-additive in the unobservables. the first stage estimates a non-additive model with infinite dimensional parameters for the control variable, such as a quantile or distribution regression model. the second stage estimates a non-additive censored quantile regression model for the response variable of interest, including the estimated control variable to deal with endogeneity. for computation, we extend the algorithm for cqr developed by chernozhukov and hong (2002) to incorporate the estimation of the control variable. we give generic regularity conditions for asymptotic normality of the cqiv estimator and for the validity of resampling methods to approximate its asymptotic distribution. we verify these conditions for quantile and distribution regression estimation of the control variable. our analysis covers two-stage (uncensored) quantile regression with non-additive first stage as an important special case. we illustrate the computation and applicability of the cqiv estimator with a monte-carlo numerical example and an empirical application on estimation of engel curves for alcohol.", "categories": "stat.me econ.em", "created": "2011-04-23", "updated": "2014-03-13", "authors": ["victor chernozhukov", "ivan fernandez-val", "amanda kowalski"], "url": "https://arxiv.org/abs/1104.4580"}, {"title": "conditional quantile processes based on series or many regressors", "id": "1105.6154", "abstract": "quantile regression (qr) is a principal regression method for analyzing the impact of covariates on outcomes. the impact is described by the conditional quantile function and its functionals. in this paper we develop the nonparametric qr-series framework, covering many regressors as a special case, for performing inference on the entire conditional quantile function and its linear functionals. in this framework, we approximate the entire conditional quantile function by a linear combination of series terms with quantile-specific coefficients and estimate the function-valued coefficients from the data. we develop large sample theory for the qr-series coefficient process, namely we obtain uniform strong approximations to the qr-series coefficient process by conditionally pivotal and gaussian processes. based on these strong approximations, or couplings, we develop four resampling methods (pivotal, gradient bootstrap, gaussian, and weighted bootstrap) that can be used for inference on the entire qr-series coefficient function.   we apply these results to obtain estimation and inference methods for linear functionals of the conditional quantile function, such as the conditional quantile function itself, its partial derivatives, average partial derivatives, and conditional average partial derivatives. specifically, we obtain uniform rates of convergence and show how to use the four resampling methods mentioned above for inference on the functionals. all of the above results are for function-valued parameters, holding uniformly in both the quantile index and the covariate value, and covering the pointwise case as a by-product. we demonstrate the practical utility of these results with an example, where we estimate the price elasticity function and test the slutsky condition of the individual demand for gasoline, as indexed by the individual unobserved propensity for gasoline consumption.", "categories": "stat.me econ.em math.st stat.th", "created": "2011-05-30", "updated": "2018-08-09", "authors": ["alexandre belloni", "victor chernozhukov", "denis chetverikov", "iv\u00e1n fern\u00e1ndez-val"], "url": "https://arxiv.org/abs/1105.6154"}, {"title": "high dimensional sparse econometric models: an introduction", "id": "1106.5242", "abstract": "in this chapter we discuss conceptually high dimensional sparse econometric models as well as estimation of these models using l1-penalization and post-l1-penalization methods. focusing on linear and nonparametric regression frameworks, we discuss various econometric examples, present basic theoretical results, and illustrate the concepts and methods with monte carlo simulations and an empirical application. in the application, we examine and confirm the empirical validity of the solow-swan model for international economic growth.", "categories": "stat.ap econ.em stat.me", "created": "2011-06-26", "updated": "2011-09-01", "authors": ["alexandre belloni", "victor chernozhukov"], "url": "https://arxiv.org/abs/1106.5242"}, {"title": "inference for high-dimensional sparse econometric models", "id": "1201.0220", "abstract": "this article is about estimation and inference methods for high dimensional sparse (hds) regression models in econometrics. high dimensional sparse models arise in situations where many regressors (or series terms) are available and the regression function is well-approximated by a parsimonious, yet unknown set of regressors. the latter condition makes it possible to estimate the entire regression function effectively by searching for approximately the right set of regressors. we discuss methods for identifying this set of regressors and estimating their coefficients based on $\\ell_1$-penalization and describe key theoretical results. in order to capture realistic practical situations, we expressly allow for imperfect selection of regressors and study the impact of this imperfect selection on estimation and inference results. we focus the main part of the article on the use of hds models and methods in the instrumental variables model and the partially linear model. we present a set of novel inference results for these models and illustrate their use with applications to returns to schooling and growth regression.", "categories": "stat.me econ.em stat.ap", "created": "2011-12-30", "updated": "", "authors": ["alexandre belloni", "victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1201.0220"}, {"title": "inference on treatment effects after selection amongst high-dimensional   controls", "id": "1201.0224", "abstract": "we propose robust methods for inference on the effect of a treatment variable on a scalar outcome in the presence of very many controls. our setting is a partially linear model with possibly non-gaussian and heteroscedastic disturbances. our analysis allows the number of controls to be much larger than the sample size. to make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by conditioning on a relatively small number of controls whose identities are unknown. the latter condition makes it possible to estimate the treatment effect by selecting approximately the right set of controls. we develop a novel estimation and uniformly valid inference method for the treatment effect in this setting, called the \"post-double-selection\" method. our results apply to lasso-type methods used for covariate selection as well as to any other model selection method that is able to find a sparse model with good approximation properties.   the main attractive feature of our method is that it allows for imperfect selection of the controls and provides confidence intervals that are valid uniformly across a large class of models. in contrast, standard post-model selection estimators fail to provide uniform inference even in simple cases with a small, fixed number of controls. thus our method resolves the problem of uniform inference after model selection for a large, interesting class of models. we illustrate the use of the developed methods with numerical simulations and an application to the effect of abortion on crime rates.", "categories": "stat.me econ.em stat.ap", "created": "2011-12-30", "updated": "2012-05-09", "authors": ["alexandre belloni", "victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1201.0224"}, {"title": "panel data models with nonadditive unobserved heterogeneity: estimation   and inference", "id": "1206.2966", "abstract": "this paper considers fixed effects estimation and inference in linear and nonlinear panel data models with random coefficients and endogenous regressors. the quantities of interest -- means, variances, and other moments of the random coefficients -- are estimated by cross sectional sample moments of gmm estimators applied separately to the time series of each individual. to deal with the incidental parameter problem introduced by the noise of the within-individual estimators in short panels, we develop bias corrections. these corrections are based on higher-order asymptotic expansions of the gmm estimators and produce improved point and interval estimates in moderately long panels. under asymptotic sequences where the cross sectional and time series dimensions of the panel pass to infinity at the same rate, the uncorrected estimator has an asymptotic bias of the same order as the asymptotic variance. the bias corrections remove the bias without increasing variance. an empirical example on cigarette demand based on becker, grossman and murphy (1994) shows significant heterogeneity in the price effect across u.s. states.", "categories": "stat.me econ.em math.st stat.ap stat.th", "created": "2012-06-13", "updated": "2013-10-11", "authors": ["ivan fernandez-val", "joonhwah lee"], "url": "https://arxiv.org/abs/1206.2966"}, {"title": "social learning equilibria", "id": "1207.5895", "abstract": "we consider a large class of social learning models in which a group of agents face uncertainty regarding a state of the world, share the same utility function, observe private signals, and interact in a general dynamic setting. we introduce social learning equilibria, a static equilibrium concept that abstracts away from the details of the given extensive form, but nevertheless captures the corresponding asymptotic equilibrium behavior. we establish general conditions for agreement, herding, and information aggregation in equilibrium, highlighting a connection between agreement and information aggregation.", "categories": "math.st econ.th stat.th", "created": "2012-07-25", "updated": "2019-09-27", "authors": ["elchanan mossel", "manuel mueller-frank", "allan sly", "omer tamuz"], "url": "https://arxiv.org/abs/1207.5895"}, {"title": "strategic learning and the topology of social networks", "id": "1209.5527", "abstract": "we consider a group of strategic agents who must each repeatedly take one of two possible actions. they learn which of the two actions is preferable from initial private signals, and by observing the actions of their neighbors in a social network.   we show that the question of whether or not the agents learn efficiently depends on the topology of the social network. in particular, we identify a geometric \"egalitarianism\" condition on the social network that guarantees learning in infinite networks, or learning with high probability in large finite networks, in any equilibrium. we also give examples of non-egalitarian networks with equilibria in which learning fails.", "categories": "cs.gt econ.th math.pr", "created": "2012-09-25", "updated": "2015-05-30", "authors": ["elchanan mossel", "allan sly", "omer tamuz"], "url": "https://arxiv.org/abs/1209.5527"}, {"title": "dual regression", "id": "1210.6958", "abstract": "we propose dual regression as an alternative to the quantile regression process for the global estimation of conditional distribution functions under minimal assumptions. dual regression provides all the interpretational power of the quantile regression process while avoiding the need for repairing the intersecting conditional quantile surfaces that quantile regression often produces in practice. our approach introduces a mathematical programming characterization of conditional distribution functions which, in its simplest form, is the dual program of a simultaneous estimator for linear location-scale models. we apply our general characterization to the specification and estimation of a flexible class of conditional distribution functions, and present asymptotic theory for the corresponding empirical dual regression process.", "categories": "stat.me econ.em stat.ap", "created": "2012-10-25", "updated": "2018-09-23", "authors": ["richard spady", "sami stouli"], "url": "https://arxiv.org/abs/1210.6958"}, {"title": "some new asymptotic theory for least squares series: pointwise and   uniform results", "id": "1212.0442", "abstract": "in applications it is common that the exact form of a conditional expectation is unknown and having flexible functional forms can lead to improvements. series method offers that by approximating the unknown function based on $k$ basis functions, where $k$ is allowed to grow with the sample size $n$. we consider series estimators for the conditional mean in light of: (i) sharp llns for matrices derived from the noncommutative khinchin inequalities, (ii) bounds on the lebesgue factor that controls the ratio between the $l^\\infty$ and $l_2$-norms of approximation errors, (iii) maximal inequalities for processes whose entropy integrals diverge, and (iv) strong approximations to series-type processes.   these technical tools allow us to contribute to the series literature, specifically the seminal work of newey (1997), as follows. first, we weaken the condition on the number $k$ of approximating functions used in series estimation from the typical $k^2/n \\to 0$ to $k/n \\to 0$, up to log factors, which was available only for spline series before. second, we derive $l_2$ rates and pointwise central limit theorems results when the approximation error vanishes. under an incorrectly specified model, i.e. when the approximation error does not vanish, analogous results are also shown. third, under stronger conditions we derive uniform rates and functional central limit theorems that hold if the approximation error vanishes or not. that is, we derive the strong approximation for the entire estimate of the nonparametric function.   we derive uniform rates, gaussian approximations, and uniform confidence bands for a wide collection of linear functionals of the conditional expectation function.", "categories": "stat.me econ.em", "created": "2012-12-03", "updated": "2015-06-17", "authors": ["alexandre belloni", "victor chernozhukov", "denis chetverikov", "kengo kato"], "url": "https://arxiv.org/abs/1212.0442"}, {"title": "semi-parametric bayesian partially identified models based on support   function", "id": "1212.3267", "abstract": "we provide a comprehensive semi-parametric study of bayesian partially identified econometric models. while the existing literature on bayesian partial identification has mostly focused on the structural parameter, our primary focus is on bayesian credible sets (bcs's) of the unknown identified set and the posterior distribution of its support function. we construct a (two-sided) bcs based on the support function of the identified set. we prove the bernstein-von mises theorem for the posterior distribution of the support function. this powerful result in turn infers that, while the bcs and the frequentist confidence set for the partially identified parameter are asymptotically different, our constructed bcs for the identified set has an asymptotically correct frequentist coverage probability. importantly, we illustrate that the constructed bcs for the identified set does not require a prior on the structural parameter. it can be computed efficiently for subset inference, especially when the target of interest is a sub-vector of the partially identified parameter, where projecting to a low-dimensional subset is often required. hence, the proposed methods are useful in many applications.   the bayesian partial identification literature has been assuming a known parametric likelihood function. however, econometric models usually only identify a set of moment inequalities, and therefore using an incorrect likelihood function may result in misleading inferences. in contrast, with a nonparametric prior on the unknown likelihood function, our proposed bayesian procedure only requires a set of moment conditions, and can efficiently make inference about both the partially identified parameter and its identified set. this makes it widely applicable in general moment inequality models. finally, the proposed method is illustrated in a financial asset pricing problem.", "categories": "stat.me econ.em math.st stat.th", "created": "2012-12-13", "updated": "2013-11-22", "authors": ["yuan liao", "anna simoni"], "url": "https://arxiv.org/abs/1212.3267"}, {"title": "gaussian approximations and multiplier bootstrap for maxima of sums of   high-dimensional random vectors", "id": "1212.6906", "abstract": "we derive a gaussian approximation result for the maximum of a sum of high-dimensional random vectors. specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the gaussian random vectors with the same covariance matrices as the original vectors. this result applies when the dimension of random vectors ($p$) is large compared to the sample size ($n$); in fact, $p$ can be much larger than $n$, without restricting correlations of the coordinates of these vectors. we also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional gaussian random vectors obtained by multiplying the original vectors with i.i.d. gaussian multipliers. this is the gaussian multiplier (or wild) bootstrap procedure. here too, $p$ can be large or even much larger than $n$. these distributional approximations, either gaussian or conditional gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. we demonstrate how our gaussian approximations and the multiplier bootstrap can be used for modern high-dimensional estimation, multiple hypothesis testing, and adaptive specification testing. all these results contain nonasymptotic bounds on approximation errors.", "categories": "math.st econ.em math.pr stat.th", "created": "2012-12-31", "updated": "2018-01-22", "authors": ["victor chernozhukov", "denis chetverikov", "kengo kato"], "url": "https://arxiv.org/abs/1212.6906"}, {"title": "quantile models with endogeneity", "id": "1303.7050", "abstract": "in this article, we review quantile models with endogeneity. we focus on models that achieve identification through the use of instrumental variables and discuss conditions under which partial and point identification are obtained. we discuss key conditions, which include monotonicity and full-rank-type conditions, in detail. in providing this review, we update the identification results of chernozhukov and hansen (2005, econometrica). we illustrate the modeling assumptions through economically motivated examples. we also briefly review the literature on estimation and inference.   key words: identification, treatment effects, structural models, instrumental variables", "categories": "stat.ap econ.em", "created": "2013-03-28", "updated": "", "authors": ["victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1303.7050"}, {"title": "uniform post selection inference for lad regression and other   z-estimation problems", "id": "1304.0282", "abstract": "we develop uniformly valid confidence regions for regression coefficients in a high-dimensional sparse median regression model with homoscedastic errors. our methods are based on a moment equation that is immunized against non-regular estimation of the nuisance part of the median regression function by using neyman's orthogonalization. we establish that the resulting instrumental median regression estimator of a target regression coefficient is asymptotically normally distributed uniformly with respect to the underlying sparse model and is semi-parametrically efficient. we also generalize our method to a general non-smooth z-estimation framework with the number of target parameters $p_1$ being possibly much larger than the sample size $n$. we extend huber's results on asymptotic normality to this setting, demonstrating uniform asymptotic normality of the proposed estimators over $p_1$-dimensional rectangles, constructing simultaneous confidence bands on all of the $p_1$ target parameters, and establishing asymptotic validity of the bands uniformly over underlying approximately sparse models.   keywords: instrument; post-selection inference; sparsity; neyman's orthogonal score test; uniformly valid inference; z-estimation.", "categories": "math.st econ.em stat.me stat.th", "created": "2013-03-31", "updated": "2018-01-22", "authors": ["alexandre belloni", "victor chernozhukov", "kengo kato"], "url": "https://arxiv.org/abs/1304.0282"}, {"title": "post-selection inference for generalized linear models with many   controls", "id": "1304.3969", "abstract": "this paper considers generalized linear models in the presence of many controls. we lay out a general methodology to estimate an effect of interest based on the construction of an instrument that immunize against model selection mistakes and apply it to the case of logistic binary choice model. more specifically we propose new methods for estimating and constructing confidence regions for a regression parameter of primary interest $\\alpha_0$, a parameter in front of the regressor of interest, such as the treatment variable or a policy variable. these methods allow to estimate $\\alpha_0$ at the root-$n$ rate when the total number $p$ of other regressors, called controls, potentially exceed the sample size $n$ using sparsity assumptions. the sparsity assumption means that there is a subset of $s<n$ controls which suffices to accurately approximate the nuisance part of the regression function. importantly, the estimators and these resulting confidence regions are valid uniformly over $s$-sparse models satisfying $s^2\\log^2 p = o(n)$ and other technical conditions. these procedures do not rely on traditional consistent model selection arguments for their validity. in fact, they are robust with respect to moderate model selection mistakes in variable selection. under suitable conditions, the estimators are semi-parametrically efficient in the sense of attaining the semi-parametric efficiency bounds for the class of models in this paper.", "categories": "stat.me econ.em math.st stat.th", "created": "2013-04-14", "updated": "2016-03-21", "authors": ["alexandre belloni", "victor chernozhukov", "ying wei"], "url": "https://arxiv.org/abs/1304.3969"}, {"title": "supplementary appendix for \"inference on treatment effects after   selection amongst high-dimensional controls\"", "id": "1305.6099", "abstract": "in this supplementary appendix we provide additional results, omitted proofs and extensive simulations that complement the analysis of the main text (arxiv:1201.0224).", "categories": "math.st econ.em stat.th", "created": "2013-05-26", "updated": "2013-06-20", "authors": ["alexandre belloni", "victor chernozhukov", "christian hansen"], "url": "https://arxiv.org/abs/1305.6099"}, {"title": "periodic strategies: a new solution concept and an algorithm for   nontrivial strategic form games", "id": "1307.2035", "abstract": "we introduce a new solution concept, called periodicity, for selecting optimal strategies in strategic form games. this periodicity solution concept yields new insight into non-trivial games. in mixed strategy strategic form games, periodic solutions yield values for the utility function of each player that are equal to the nash equilibrium ones. in contrast to the nash strategies, here the payoffs of each player are robust against what the opponent plays. sometimes, periodicity strategies yield higher utilities, and sometimes the nash strategies do, but often the utilities of these two strategies coincide. we formally define and study periodic strategies in two player perfect information strategic form games with pure strategies and we prove that every non-trivial finite game has at least one periodic strategy, with non-trivial meaning non-degenerate payoffs. in some classes of games where mixed strategies are used, we identify quantitative features. particularly interesting are the implications for collective action games, since there the collective action strategy can be incorporated in a purely non-cooperative context. moreover, we address the periodicity issue when the players have a continuum set of strategies available.", "categories": "cs.gt econ.th", "created": "2013-07-08", "updated": "2018-01-24", "authors": ["v. k. oikonomou", "j. jost"], "url": "https://arxiv.org/abs/1307.2035"}, {"title": "robust inference on average treatment effects with possibly more   covariates than observations", "id": "1309.4686", "abstract": "this paper concerns robust inference on average treatment effects following model selection. in the selection on observables framework, we show how to construct confidence intervals based on a doubly-robust estimator that are robust to model selection errors and prove that they are valid uniformly over a large class of treatment effect models. the class allows for multivalued treatments with heterogeneous effects (in observables), general heteroskedasticity, and selection amongst (possibly) more covariates than observations. our estimator attains the semiparametric efficiency bound under appropriate conditions. precise conditions are given for any model selector to yield these results, and we show how to combine data-driven selection with economic theory. for implementation, we give a specific proposal for selection based on the group lasso, which is particularly well-suited to treatment effects data, and derive new results for high-dimensional, sparse multinomial logistic regression. a simulation study shows our estimator performs very well in finite samples over a wide range of models. revisiting the national supported work demonstration data, our method yields accurate estimates and tight confidence intervals.", "categories": "math.st econ.em stat.me stat.th", "created": "2013-09-18", "updated": "2018-02-01", "authors": ["max h. farrell"], "url": "https://arxiv.org/abs/1309.4686"}, {"title": "optimal uniform convergence rates for sieve nonparametric instrumental   variables regression", "id": "1311.0412", "abstract": "we study the problem of nonparametric regression when the regressor is endogenous, which is an important nonparametric instrumental variables (npiv) regression in econometrics and a difficult ill-posed inverse problem with unknown operator in statistics. we first establish a general upper bound on the sup-norm (uniform) convergence rate of a sieve estimator, allowing for endogenous regressors and weakly dependent data. this result leads to the optimal sup-norm convergence rates for spline and wavelet least squares regression estimators under weakly dependent data and heavy-tailed error terms. this upper bound also yields the sup-norm convergence rates for sieve npiv estimators under i.i.d. data: the rates coincide with the known optimal $l^2$-norm rates for severely ill-posed problems, and are power of $\\log(n)$ slower than the optimal $l^2$-norm rates for mildly ill-posed problems. we then establish the minimax risk lower bound in sup-norm loss, which coincides with our upper bounds on sup-norm rates for the spline and wavelet sieve npiv estimators. this sup-norm rate optimality provides another justification for the wide application of sieve npiv estimators. useful results on weakly-dependent random matrices are also provided.", "categories": "math.st econ.em stat.me stat.th", "created": "2013-11-02", "updated": "", "authors": ["xiaohong chen", "timothy christensen"], "url": "https://arxiv.org/abs/1311.0412"}, {"title": "program evaluation and causal inference with high-dimensional data", "id": "1311.2645", "abstract": "in this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (late) and local quantile treatment effects (lqte) in data-rich environments. we can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. in the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ate) and quantile treatment effects (qte). to make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. this assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. we show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. we illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets.", "categories": "math.st econ.em stat.me stat.ml stat.th", "created": "2013-11-11", "updated": "2018-01-05", "authors": ["alexandre belloni", "victor chernozhukov", "ivan fern\u00e1ndez-val", "christian hansen"], "url": "https://arxiv.org/abs/1311.2645"}, {"title": "individual and time effects in nonlinear panel models with large n, t", "id": "1311.7065", "abstract": "we derive fixed effects estimators of parameters and average partial effects in (possibly dynamic) nonlinear panel data models with individual and time effects. they cover logit, probit, ordered probit, poisson and tobit models that are important for many empirical applications in micro and macroeconomics. our estimators use analytical and jackknife bias corrections to deal with the incidental parameter problem, and are asymptotically unbiased under asymptotic sequences where $n/t$ converges to a constant. we develop inference methods and show that they perform well in numerical examples.", "categories": "stat.me econ.em", "created": "2013-11-27", "updated": "2018-12-18", "authors": ["ivan fernandez-val", "martin weidner"], "url": "https://arxiv.org/abs/1311.7065"}, {"title": "nonparametric identification in panels using quantiles", "id": "1312.4094", "abstract": "this paper considers identification and estimation of ceteris paribus effects of continuous regressors in nonseparable panel models with time homogeneity. the effects of interest are derivatives of the average and quantile structural functions of the model. we find that these derivatives are identified with two time periods for \"stayers\", i.e. for individuals with the same regressor values in two time periods. we show that the identification results carry over to models that allow location and scale time effects. we propose nonparametric series methods and a weighted bootstrap scheme to estimate and make inference on the identified effects. the bootstrap proposed allows uniform inference for function-valued parameters such as quantile effects uniformly over a region of quantile indices and/or regressor values. an empirical application to engel curve estimation with panel data illustrates the results.", "categories": "stat.me econ.em", "created": "2013-12-14", "updated": "2014-08-05", "authors": ["victor chernozhukov", "ivan fernandez-val", "stefan hoderlein", "hajo holzmann", "whitney newey"], "url": "https://arxiv.org/abs/1312.4094"}, {"title": "valid post-selection inference in high-dimensional approximately sparse   quantile regression models", "id": "1312.7186", "abstract": "this work proposes new inference methods for a regression coefficient of interest in a (heterogeneous) quantile regression model. we consider a high-dimensional model where the number of regressors potentially exceeds the sample size but a subset of them suffice to construct a reasonable approximation to the conditional quantile function. the proposed methods are (explicitly or implicitly) based on orthogonal score functions that protect against moderate model selection mistakes, which are often inevitable in the approximately sparse model considered in the present paper. we establish the uniform validity of the proposed confidence regions for the quantile regression coefficient. importantly, these methods directly apply to more than one variable and a continuum of quantile indices. in addition, the performance of the proposed methods is illustrated through monte-carlo experiments and an empirical example, dealing with risk factors in childhood malnutrition.", "categories": "math.st econ.em stat.th", "created": "2013-12-26", "updated": "2016-06-23", "authors": ["alexandre belloni", "victor chernozhukov", "kengo kato"], "url": "https://arxiv.org/abs/1312.7186"}, {"title": "what does the financial market pricing do? a simulation analysis with a   view to systemic volatility, exuberance and vagary", "id": "1312.7460", "abstract": "biondi et al. (2012) develop an analytical model to examine the emergent dynamic properties of share market price formation over time, capable to capture important stylized facts. these latter properties prove to be sensitive to regulatory regimes for fundamental information provision, as well as to market confidence conditions among actual and potential investors. regimes based upon mark-to-market (fair value) measurement of traded security, while generating higher linear correlation between market prices and fundamental signals, also involve higher market instability and volatility. these regimes also incur more relevant episodes of market exuberance and vagary in some regions of the market confidence space, where lower market liquidity further occurs.", "categories": "q-fin.gn econ.gn physics.soc-ph q-fin.ec q-fin.pr q-fin.tr", "created": "2013-12-28", "updated": "", "authors": ["yuri biondi", "simone righi"], "url": "https://arxiv.org/abs/1312.7460"}, {"title": "inference on causal and structural parameters using many moment   inequalities", "id": "1312.7614", "abstract": "this paper considers the problem of testing many moment inequalities where the number of moment inequalities, denoted by $p$, is possibly much larger than the sample size $n$. there is a variety of economic applications where solving this problem allows to carry out inference on causal and structural parameters, a notable example is the market structure model of ciliberto and tamer (2009) where $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter the market. we consider the test statistic given by the maximum of $p$ studentized (or $t$-type) inequality-specific statistics, and analyze various ways to compute critical values for the test statistic. specifically, we consider critical values based upon (i) the union bound combined with a moderate deviation inequality for self-normalized sums, (ii) the multiplier and empirical bootstraps, and (iii) two-step and three-step variants of (i) and (ii) by incorporating the selection of uninformative inequalities that are far from being binding and a novel selection of weakly informative inequalities that are potentially binding but do not provide first order information. we prove validity of these methods, showing that under mild conditions, they lead to tests with the error in size decreasing polynomially in $n$ while allowing for $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$ for some $c > 0$. importantly, all these results hold without any restriction on the correlation structure between $p$ studentized statistics, and also hold uniformly with respect to suitably large classes of underlying distributions. moreover, in the online supplement, we show validity of a test based on the block multiplier bootstrap in the case of dependent data under some general mixing conditions.", "categories": "math.st econ.em stat.ap stat.th", "created": "2013-12-29", "updated": "2018-10-18", "authors": ["victor chernozhukov", "denis chetverikov", "kengo kato"], "url": "https://arxiv.org/abs/1312.7614"}, {"title": "graphical potential games", "id": "1405.1481", "abstract": "we study the class of potential games that are also graphical games with respect to a given graph $g$ of connections between the players. we show that, up to strategic equivalence, this class of games can be identified with the set of markov random fields on $g$.   from this characterization, and from the hammersley-clifford theorem, it follows that the potentials of such games can be decomposed to local potentials. we use this decomposition to strongly bound the number of strategy changes of a single player along a better response path. this result extends to generalized graphical potential games, which are played on infinite graphs.", "categories": "math.pr cs.gt econ.th", "created": "2014-05-06", "updated": "2016-03-23", "authors": ["yakov babichenko", "omer tamuz"], "url": "https://arxiv.org/abs/1405.1481"}, {"title": "sieve wald and qlr inferences on semi/nonparametric conditional moment   models", "id": "1411.1144", "abstract": "this paper considers inference on functionals of semi/nonparametric conditional moment restrictions with possibly nonsmooth generalized residuals, which include all of the (nonlinear) nonparametric instrumental variables (iv) as special cases. these models are often ill-posed and hence it is difficult to verify whether a (possibly nonlinear) functional is root-$n$ estimable or not. we provide computationally simple, unified inference procedures that are asymptotically valid regardless of whether a functional is root-$n$ estimable or not. we establish the following new useful results: (1) the asymptotic normality of a plug-in penalized sieve minimum distance (psmd) estimator of a (possibly nonlinear) functional; (2) the consistency of simple sieve variance estimators for the plug-in psmd estimator, and hence the asymptotic chi-square distribution of the sieve wald statistic; (3) the asymptotic chi-square distribution of an optimally weighted sieve quasi likelihood ratio (qlr) test under the null hypothesis; (4) the asymptotic tight distribution of a non-optimally weighted sieve qlr statistic under the null; (5) the consistency of generalized residual bootstrap sieve wald and qlr tests; (6) local power properties of sieve wald and qlr tests and of their bootstrap versions; (7) asymptotic properties of sieve wald and sqlr for functionals of increasing dimension. simulation studies and an empirical illustration of a nonparametric quantile iv regression are presented.", "categories": "math.st econ.em stat.th", "created": "2014-11-04", "updated": "2015-03-20", "authors": ["xiaohong chen", "demian pouzo"], "url": "https://arxiv.org/abs/1411.1144"}, {"title": "bootstrap consistency for quadratic forms of sample averages with   increasing dimension", "id": "1411.2701", "abstract": "this paper establishes consistency of the weighted bootstrap for quadratic forms $\\left( n^{-1/2} \\sum_{i=1}^{n} z_{i,n} \\right)^{t}\\left( n^{-1/2} \\sum_{i=1}^{n} z_{i,n} \\right)$ where $(z_{i,n})_{i=1}^{n}$ are mean zero, independent $\\mathbb{r}^{d}$-valued random variables and $d=d(n)$ is allowed to grow with the sample size $n$, slower than $n^{1/4}$. the proof relies on an adaptation of lindeberg interpolation technique whereby we simplify the original problem to a gaussian approximation problem. we apply our bootstrap results to model-specification testing problems when the number of moments is allowed to grow with the sample size.", "categories": "math.st econ.em math.pr stat.th", "created": "2014-11-10", "updated": "2015-08-17", "authors": ["demian pouzo"], "url": "https://arxiv.org/abs/1411.2701"}, {"title": "inference in high dimensional panel models with an application to gun   control", "id": "1411.6507", "abstract": "we consider estimation and inference in panel data models with additive unobserved individual specific heterogeneity in a high dimensional setting. the setting allows the number of time varying regressors to be larger than the sample size. to make informative estimation and inference feasible, we require that the overall contribution of the time varying variables after eliminating the individual specific heterogeneity can be captured by a relatively small number of the available variables whose identities are unknown. this restriction allows the problem of estimation to proceed as a variable selection problem. importantly, we treat the individual specific heterogeneity as fixed effects which allows this heterogeneity to be related to the observed time varying variables in an unspecified way and allows that this heterogeneity may be non-zero for all individuals. within this framework, we provide procedures that give uniformly valid inference over a fixed subset of parameters in the canonical linear fixed effects model and over coefficients on a fixed vector of endogenous variables in panel data instrumental variables models with fixed effects and many instruments. an input to developing the properties of our proposed procedures is the use of a variant of the lasso estimator that allows for a grouped data structure where data across groups are independent and dependence within groups is unrestricted. we provide formal conditions within this structure under which the proposed lasso variant selects a sparse model with good approximation properties. we present simulation results in support of the theoretical developments and illustrate the use of the methods in an application aimed at estimating the effect of gun prevalence on crime rates.", "categories": "stat.me econ.em", "created": "2014-11-24", "updated": "", "authors": ["alexandre belloni", "victor chernozhukov", "christian hansen", "damian kozbur"], "url": "https://arxiv.org/abs/1411.6507"}, {"title": "comprehensive time-series regression models using gretl -- u.s. gdp and   government consumption expenditures & gross investment from 1980 to 2013", "id": "1412.5397", "abstract": "using gretl, i apply arma, vector arma, var, state-space model with a kalman filter, transfer-function and intervention models, unit root tests, cointegration test, volatility models (arch, garch, arch-m, garch-m, taylor-schwert garch, gjr, tarch, narch, aparch, egarch) to analyze quarterly time series of gdp and government consumption expenditures & gross investment (gcegi) from 1980 to 2013. the article is organized as: (i) definition; (ii) regression models; (iii) discussion. additionally, i discovered a unique interaction between gdp and gcegi in both the short-run and the long-run and provided policy makers with some suggestions. for example in the short run, gdp responded positively and very significantly (0.00248) to gcegi, while gcegi reacted positively but not too significantly (0.08051) to gdp. in the long run, current gdp responded negatively and permanently (0.09229) to a shock in past gcegi, while current gcegi reacted negatively yet temporarily (0.29821) to a shock in past gdp. therefore, policy makers should not adjust current gcegi based merely on the condition of current and past gdp. although increasing gcegi does help gdp in the short-term, significantly abrupt increase in gcegi might not be good to the long-term health of gdp. instead, a balanced, sustainable, and economically viable solution is recommended, so that the short-term benefits to the current economy from increasing gcegi often largely secured by the long-term loan outweigh or at least equal to the negative effect to the future economy from the long-term debt incurred by the loan. finally, i found that non-normally distributed volatility models generally perform better than normally distributed ones. more specifically, tarch-ged performs the best in the group of non-normally distributed, while garch-m does the best in the group of normally distributed.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2014-12-17", "updated": "2019-08-17", "authors": ["juehui shi"], "url": "https://arxiv.org/abs/1412.5397"}, {"title": "nonlinear factor models for network and panel data", "id": "1412.5647", "abstract": "factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. we consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and poisson specifications. we establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer of incidental parameter bias. we show how models with factor structures can also be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables and clustering. we illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a poisson model with multiple factors.", "categories": "stat.me econ.em", "created": "2014-12-17", "updated": "2019-10-15", "authors": ["mingli chen", "iv\u00e1n fern\u00e1ndez-val", "martin weidner"], "url": "https://arxiv.org/abs/1412.5647"}, {"title": "rational groupthink", "id": "1412.7172", "abstract": "we study how long-lived rational agents learn from repeatedly observing a private signal and each others' actions. with normal signals, a group of any size learns more slowly than just four agents who directly observe each others' private signals in each period. similar results apply to general signal structures. we identify rational groupthink---in which agents ignore their private signals and choose the same action for long periods of time---as the cause of this failure of information aggregation.", "categories": "cs.gt econ.th math.pr", "created": "2014-12-22", "updated": "2020-06-02", "authors": ["matan harel", "elchanan mossel", "philipp strack", "omer tamuz"], "url": "https://arxiv.org/abs/1412.7172"}, {"title": "monge-kantorovich depth, quantiles, ranks, and signs", "id": "1412.8434", "abstract": "we propose new concepts of statistical depth, multivariate quantiles, ranks and signs, based on canonical transportation maps between a distribution of interest on $r^d$ and a reference distribution on the $d$-dimensional unit ball. the new depth concept, called monge-kantorovich depth, specializes to halfspace depth in the case of spherical distributions, but, for more general distributions, differs from the latter in the ability for its contours to account for non convex features of the distribution of interest. we propose empirical counterparts to the population versions of those monge-kantorovich depth contours, quantiles, ranks and signs, and show their consistency by establishing a uniform convergence property for empirical transport maps, which is of independent interest.", "categories": "math.st econ.em stat.th", "created": "2014-12-29", "updated": "2015-09-21", "authors": ["victor chernozhukov", "alfred galichon", "marc hallin", "marc henry"], "url": "https://arxiv.org/abs/1412.8434"}, {"title": "the abc of simulation estimation with auxiliary statistics", "id": "1501.01265", "abstract": "the frequentist method of simulated minimum distance (smd) is widely used in economics to estimate complex models with an intractable likelihood. in other disciplines, a bayesian approach known as approximate bayesian computation (abc) is far more popular. this paper connects these two seemingly related approaches to likelihood-free estimation by means of a reverse sampler that uses both optimization and importance weighting to target the posterior distribution. its hybrid features enable us to analyze an abc estimate from the perspective of smd. we show that an ideal abc estimate can be obtained as a weighted average of a sequence of smd modes, each being the minimizer of the deviations between the data and the model. this contrasts with the smd, which is the mode of the average deviations. using stochastic expansions, we provide a general characterization of frequentist estimators and those based on bayesian computations including laplace-type estimators. their differences are illustrated using analytical examples and a simulation study of the dynamic panel model.", "categories": "stat.me econ.em", "created": "2015-01-06", "updated": "2017-10-11", "authors": ["jean-jacques forneron", "serena ng"], "url": "https://arxiv.org/abs/1501.01265"}, {"title": "post-selection and post-regularization inference in linear models with   many controls and instruments", "id": "1501.03185", "abstract": "in this note, we offer an approach to estimating causal/structural parameters in the presence of many instruments and controls based on methods for estimating sparse high-dimensional models. we use these high-dimensional methods to select both which instruments and which control variables to use. the approach we take extends bcch2012, which covers selection of instruments for iv models with a small number of controls, and extends bch2014, which covers selection of controls in models where the variable of interest is exogenous conditional on observables, to accommodate both a large number of controls and a large number of instruments. we illustrate the approach with a simulation and an empirical example. technical supporting material is available in a supplementary online appendix.", "categories": "stat.ap econ.em", "created": "2015-01-13", "updated": "", "authors": ["victor chernozhukov", "christian hansen", "martin spindler"], "url": "https://arxiv.org/abs/1501.03185"}, {"title": "valid post-selection and post-regularization inference: an elementary,   general approach", "id": "1501.03430", "abstract": "here we present an expository, general analysis of valid post-selection or post-regularization inference about a low-dimensional target parameter, $\\alpha$, in the presence of a very high-dimensional nuisance parameter, $\\eta$, which is estimated using modern selection or regularization methods. our analysis relies on high-level, easy-to-interpret conditions that allow one to clearly see the structures needed for achieving valid post-regularization inference. simple, readily verifiable sufficient conditions are provided for a class of affine-quadratic models. we focus our discussion on estimation and inference procedures based on using the empirical analog of theoretical equations $$m(\\alpha, \\eta)=0$$ which identify $\\alpha$. within this structure, we show that setting up such equations in a manner such that the orthogonality/immunization condition $$\\partial_\\eta m(\\alpha, \\eta) = 0$$ at the true parameter values is satisfied, coupled with plausible conditions on the smoothness of $m$ and the quality of the estimator $\\hat \\eta$, guarantees that inference on for the main parameter $\\alpha$ based on testing or point estimation methods discussed below will be regular despite selection or regularization biases occurring in estimation of $\\eta$. in particular, the estimator of $\\alpha$ will often be uniformly consistent at the root-$n$ rate and uniformly asymptotically normal even though estimators $\\hat \\eta$ will generally not be asymptotically linear and regular. the uniformity holds over large classes of models that do not impose highly implausible \"beta-min\" conditions. we also show that inference can be carried out by inverting tests formed from neyman's $c(\\alpha)$ (orthogonal score) statistics.", "categories": "math.st econ.em stat.th", "created": "2015-01-14", "updated": "2015-08-18", "authors": ["victor chernozhukov", "christian hansen", "martin spindler"], "url": "https://arxiv.org/abs/1501.03430"}, {"title": "a lava attack on the recovery of sums of dense and sparse signals", "id": "1502.03155", "abstract": "common high-dimensional methods for prediction rely on having either a sparse signal model, a model in which most parameters are zero and there are a small number of non-zero parameters that are large in magnitude, or a dense signal model, a model with no large parameters and very many small non-zero parameters. we consider a generalization of these two basic models, termed here a \"sparse+dense\" model, in which the signal is given by the sum of a sparse signal and a dense signal. such a structure poses problems for traditional sparse estimators, such as the lasso, and for traditional dense estimation methods, such as ridge estimation. we propose a new penalization-based method, called lava, which is computationally efficient. with suitable choices of penalty parameters, the proposed method strictly dominates both lasso and ridge. we derive analytic expressions for the finite-sample risk function of the lava estimator in the gaussian sequence model. we also provide an deviation bound for the prediction risk in the gaussian regression model with fixed design. in both cases, we provide stein's unbiased estimator for lava's prediction risk. a simulation example compares the performance of lava to lasso, ridge, and elastic net in a regression example using feasible, data-dependent penalty parameters and illustrates lava's improved performance relative to these benchmarks.", "categories": "stat.me cs.it econ.em math.it", "created": "2015-02-10", "updated": "2015-03-24", "authors": ["victor chernozhukov", "christian hansen", "yuan liao"], "url": "https://arxiv.org/abs/1502.03155"}, {"title": "equilibrium in misspecified markov decision processes", "id": "1502.06901", "abstract": "we study markov decision problems where the agent does not know the transition probability function mapping current states and actions to future states. the agent has a prior belief over a set of possible transition functions and updates beliefs using bayes' rule. we allow her to be misspecified in the sense that the true transition probability function is not in the support of her prior. this problem is relevant in many economic settings but is usually not amenable to analysis by the researcher. we make the problem tractable by studying asymptotic behavior. we propose an equilibrium notion and provide conditions under which it characterizes steady state behavior. in the special case where the problem is static, equilibrium coincides with the single-agent version of berk-nash equilibrium (esponda and pouzo (2016)). we also discuss subtle issues that arise exclusively in dynamic settings due to the possibility of a negative value of experimentation.", "categories": "q-fin.ec econ.em", "created": "2015-02-24", "updated": "2016-05-14", "authors": ["ignacio esponda", "demian pouzo"], "url": "https://arxiv.org/abs/1502.06901"}, {"title": "recursive partitioning for heterogeneous causal effects", "id": "1504.01132", "abstract": "in this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. in applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. for experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. in most of the literature on supervised machine learning (e.g. regression trees, random forests, lasso, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. a prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. the challenge is that the \"ground truth\" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. we propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. we then apply the method to a large-scale field experiment re-ranking results on a search engine.", "categories": "stat.ml econ.em", "created": "2015-04-05", "updated": "2015-12-30", "authors": ["susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1504.01132"}, {"title": "alternative asymptotics and the partially linear model with many   regressors", "id": "1505.08120", "abstract": "non-standard distributional approximations have received considerable attention in recent years. they often provide more accurate approximations in small samples, and theoretical improvements in some cases. this paper shows that the seemingly unrelated \"many instruments asymptotics\" and \"small bandwidth asymptotics\" share a common structure, where the object determining the limiting distribution is a v-statistic with a remainder that is an asymptotically normal degenerate u-statistic. we illustrate how this general structure can be used to derive new results by obtaining a new asymptotic distribution of a series estimator of the partially linear model when the number of terms in the series approximation possibly grows as fast as the sample size, which we call \"many terms asymptotics\".", "categories": "math.st econ.em stat.th", "created": "2015-05-28", "updated": "", "authors": ["matias d. cattaneo", "michael jansson", "whitney k. newey"], "url": "https://arxiv.org/abs/1505.08120"}, {"title": "on game-theoretic risk management (part one) -- towards a theory of   games with payoffs that are probability-distributions", "id": "1506.07368", "abstract": "optimal behavior in (competitive) situation is traditionally determined with the help of utility functions that measure the payoff of different actions. given an ordering on the space of revenues (payoffs), the classical axiomatic approach of von neumann and morgenstern establishes the existence of suitable utility functions, and yields to game-theory as the most prominent materialization of a theory to determine optimal behavior. although this appears to be a most natural approach to risk management too, applications in critical infrastructures often violate the implicit assumption of actions leading to deterministic consequences. in that sense, the gameplay in a critical infrastructure risk control competition is intrinsically random in the sense of actions having uncertain consequences. mathematically, this takes us to utility functions that are probability-distribution-valued, in which case we loose the canonic (in fact every possible) ordering on the space of payoffs, and the original techniques of von neumann and morgenstern no longer apply.   this work introduces a new kind of game in which uncertainty applies to the payoff functions rather than the player's actions (a setting that has been widely studied in the literature, yielding to celebrated notions like the trembling hands equilibrium or the purification theorem). in detail, we show how to fix the non-existence of a (canonic) ordering on the space of probability distributions by only mildly restricting the full set to a subset that can be totally ordered. our vehicle to define the ordering and establish basic game-theory is non-standard analysis and hyperreal numbers.", "categories": "econ.gn math.st q-fin.ec stat.ap stat.th", "created": "2015-06-24", "updated": "2020-04-24", "authors": ["stefan rass"], "url": "https://arxiv.org/abs/1506.07368"}, {"title": "inference in linear regression models with many covariates and   heteroskedasticity", "id": "1507.02493", "abstract": "the linear regression model is widely used in empirical work in economics, statistics, and many other disciplines. researchers often include many covariates in their linear model specification in an attempt to control for confounders. we give inference methods that allow for many covariates and heteroskedasticity. our results are obtained using high-dimensional approximations, where the number of included covariates are allowed to grow as fast as the sample size. we find that all of the usual versions of eicker-white heteroskedasticity consistent standard error estimators for linear models are inconsistent under this asymptotics. we then propose a new heteroskedasticity consistent standard error formula that is fully automatic and robust to both (conditional)\\ heteroskedasticity of unknown form and the inclusion of possibly many covariates. we apply our findings to three settings: parametric linear models with many covariates, linear panel models with many fixed effects, and semiparametric semi-linear models with many technical regressors. simulation evidence consistent with our theoretical results is also provided. the proposed methods are also illustrated with an empirical application.", "categories": "math.st econ.em stat.me stat.th", "created": "2015-07-09", "updated": "2017-01-16", "authors": ["matias d. cattaneo", "michael jansson", "whitney k. newey"], "url": "https://arxiv.org/abs/1507.02493"}, {"title": "nonparametric instrumental variable estimation under monotonicity", "id": "1507.05270", "abstract": "the ill-posedness of the inverse problem of recovering a regression function in a nonparametric instrumental variable model leads to estimators that may suffer from a very slow, logarithmic rate of convergence. in this paper, we show that restricting the problem to models with monotone regression functions and monotone instruments significantly weakens the ill-posedness of the problem. in stark contrast to the existing literature, the presence of a monotone instrument implies boundedness of our measure of ill-posedness when restricted to the space of monotone functions. based on this result we derive a novel non-asymptotic error bound for the constrained estimator that imposes monotonicity of the regression function. for a given sample size, the bound is independent of the degree of ill-posedness as long as the regression function is not too steep. as an implication, the bound allows us to show that the constrained estimator converges at a fast, polynomial rate, independently of the degree of ill-posedness, in a large, but slowly shrinking neighborhood of constant functions. our simulation study demonstrates significant finite-sample performance gains from imposing monotonicity even when the regression function is rather far from being a constant. we apply the constrained estimator to the problem of estimating gasoline demand functions from u.s. data.", "categories": "stat.ap econ.em math.st stat.th", "created": "2015-07-19", "updated": "", "authors": ["denis chetverikov", "daniel wilhelm"], "url": "https://arxiv.org/abs/1507.05270"}, {"title": "on the effect of bias estimation on coverage accuracy in nonparametric   inference", "id": "1508.02973", "abstract": "nonparametric methods play a central role in modern empirical work. while they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. we study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. this is achieved using a novel, yet simple, studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. in addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. for interior points, we show that the mse-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest coverage error decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too \"large\". finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate studentization is used; however, the mse-optimal bandwidth for the original point estimator is suboptimal. all the results are established using valid edgeworth expansions and illustrated with simulated data. our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available.", "categories": "math.st econ.em stat.th", "created": "2015-08-12", "updated": "2018-03-07", "authors": ["sebastian calonico", "matias d. cattaneo", "max h. farrell"], "url": "https://arxiv.org/abs/1508.02973"}, {"title": "optimal sup-norm rates and uniform inference on nonlinear functionals of   nonparametric iv regression", "id": "1508.03365", "abstract": "this paper makes several important contributions to the literature about nonparametric instrumental variables (npiv) estimation and inference on a structural function $h_0$ and its functionals. first, we derive sup-norm convergence rates for computationally simple sieve npiv (series 2sls) estimators of $h_0$ and its derivatives. second, we derive a lower bound that describes the best possible (minimax) sup-norm rates of estimating $h_0$ and its derivatives, and show that the sieve npiv estimator can attain the minimax rates when $h_0$ is approximated via a spline or wavelet sieve. our optimal sup-norm rates surprisingly coincide with the optimal root-mean-squared rates for severely ill-posed problems, and are only a logarithmic factor slower than the optimal root-mean-squared rates for mildly ill-posed problems. third, we use our sup-norm rates to establish the uniform gaussian process strong approximations and the score bootstrap uniform confidence bands (ucbs) for collections of nonlinear functionals of $h_0$ under primitive conditions, allowing for mildly and severely ill-posed problems. fourth, as applications, we obtain the first asymptotic pointwise and uniform inference results for plug-in sieve t-statistics of exact consumer surplus (cs) and deadweight loss (dl) welfare functionals under low-level conditions when demand is estimated via sieve npiv. empiricists could read our real data application of ucbs for exact cs and dl functionals of gasoline demand that reveals interesting patterns and is applicable to other markets.", "categories": "stat.me econ.em", "created": "2015-08-13", "updated": "2017-04-29", "authors": ["xiaohong chen", "timothy m. christensen"], "url": "https://arxiv.org/abs/1508.03365"}, {"title": "my reflections on the first man vs. machine no-limit texas hold 'em   competition", "id": "1510.08578", "abstract": "the first ever human vs. computer no-limit texas hold 'em competition took place from april 24-may 8, 2015 at river's casino in pittsburgh, pa. in this article i present my thoughts on the competition design, agent architecture, and lessons learned.", "categories": "cs.gt cs.ai cs.ma econ.th", "created": "2015-10-29", "updated": "2016-01-23", "authors": ["sam ganzfried"], "url": "https://arxiv.org/abs/1510.08578"}, {"title": "estimating the effect of treatments allocated by randomized waiting   lists", "id": "1511.01453", "abstract": "oversubscribed treatments are often allocated using randomized waiting lists. applicants are ranked randomly, and treatment offers are made following that ranking until all seats are filled. to estimate causal effects, researchers often compare applicants getting and not getting an offer. we show that those two groups are not statistically comparable. therefore, the estimator arising from that comparison is inconsistent. we propose a new estimator, and show that it is consistent. finally, we revisit an application, and we show that using our estimator can lead to sizably different results from those obtained using the commonly used estimator.", "categories": "stat.me econ.em", "created": "2015-11-03", "updated": "2018-10-22", "authors": ["clement de chaisemartin", "luc behaghel"], "url": "https://arxiv.org/abs/1511.01453"}, {"title": "nonparametric instrumental variable regression and quantile regression   with full independence", "id": "1511.03977", "abstract": "the problem of endogeneity in statistics and econometrics is often handled by introducing instrumental variables (iv) which are assumed to be mean independent of some regressors or other observables. when full independence of iv's and observables is assumed, nonparametric iv regression models and nonparametric demand models lead to nonlinear integral equations with unknown integral kernels. we prove convergence rates for the mean integrated square error of the iteratively regularized newton method applied to these problems. compared to related results we derive stronger convergence results that rely on weaker nonlinearity restrictions. we demonstrate in numerical simulations for a nonparametric iv regression that the method produces better results than the standard model.", "categories": "stat.co econ.em math.st stat.th", "created": "2015-11-12", "updated": "2020-04-14", "authors": ["fabian dunker"], "url": "https://arxiv.org/abs/1511.03977"}, {"title": "on game-theoretic risk management (part two) -- algorithms to compute   nash-equilibria in games with distributions as payoffs", "id": "1511.08591", "abstract": "the game-theoretic risk management framework put forth in the precursor work \"towards a theory of games with payoffs that are probability-distributions\" (arxiv:1506.07368 [q-fin.ec]) is herein extended by algorithmic details on how to compute equilibria in games where the payoffs are probability distributions. our approach is \"data driven\" in the sense that we assume empirical data (measurements, simulation, etc.) to be available that can be compiled into distribution models, which are suitable for efficient decisions about preferences, and setting up and solving games using these as payoffs. while preferences among distributions turn out to be quite simple if nonparametric methods (kernel density estimates) are used, computing nash-equilibria in games using such models is discovered as inefficient (if not impossible). in fact, we give a counterexample in which fictitious play fails to converge for the (specifically unfortunate) choice of payoff distributions in the game, and introduce a suitable tail approximation of the payoff densities to tackle the issue. the overall procedure is essentially a modified version of fictitious play, and is herein described for standard and multicriteria games, to iteratively deliver an (approximate) nash-equilibrium. an exact method using linear programming is also given.", "categories": "econ.gn cs.gt math.st q-fin.ec q-fin.rm stat.th", "created": "2015-11-27", "updated": "2020-04-09", "authors": ["stefan rass"], "url": "https://arxiv.org/abs/1511.08591"}, {"title": "the sorted effects method: discovering heterogeneous effects beyond   their averages", "id": "1512.05635", "abstract": "the partial (ceteris paribus) effects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial effects (or, at best, average effects for some groups). while average effects provide very convenient scalar summaries of typical effects, by definition they fail to reflect the entire variety of the heterogeneous effects. in order to discover these effects much more fully, we propose to estimate and report sorted effects -- a collection of estimated partial effects sorted in increasing order and indexed by percentiles. by construction the sorted effect curves completely represent and help visualize the range of the heterogeneous effects in one plot. they are as convenient and easy to report in practice as the conventional average partial effects. they also serve as a basis for classification analysis, where we divide the observational units into most or least affected groups and summarize their characteristics. we provide a quantification of uncertainty (standard errors and confidence bands) for the estimated sorted effects and related classification analysis, and provide confidence sets for the most and least affected groups. the derived statistical results rely on establishing key, new mathematical results on hadamard differentiability of a multivariate sorting operator and a related classification operator, which are of independent interest. we apply the sorted effects method and classification analysis to demonstrate several striking patterns in the gender wage gap.", "categories": "stat.me econ.em", "created": "2015-12-17", "updated": "2018-05-25", "authors": ["victor chernozhukov", "ivan fernandez-val", "ye luo"], "url": "https://arxiv.org/abs/1512.05635"}, {"title": "on the non-asymptotic properties of regularized m-estimators", "id": "1512.06290", "abstract": "we propose a general framework for regularization in m-estimation problems under time dependent (absolutely regular-mixing) data which encompasses many of the existing estimators. we derive non-asymptotic concentration bounds for the regularized m-estimator. our results exhibit a variance-bias trade-off, with the variance term being governed by a novel measure of the complexity of the parameter set. we also show that the mixing structure affect the variance term by scaling the number of observations; depending on the decay rate of the mixing coefficients, this scaling can even affect the asymptotic behavior. finally, we propose a data-driven method for choosing the tuning parameters of the regularized estimator which yield the same (up to constants) concentration bound as one that optimally balances the (squared) bias and variance terms. we illustrate the results with several canonical examples.", "categories": "math.st econ.em stat.th", "created": "2015-12-19", "updated": "2016-10-21", "authors": ["demian pouzo"], "url": "https://arxiv.org/abs/1512.06290"}, {"title": "confidence intervals for projections of partially identified parameters", "id": "1601.00934", "abstract": "we propose a bootstrap-based calibrated projection procedure to build confidence intervals for single components and for smooth functions of a partially identified parameter vector in moment (in)equality models. the method controls asymptotic coverage uniformly over a large class of data generating processes. the extreme points of the calibrated projection confidence interval are obtained by extremizing the value of the function of interest subject to a proper relaxation of studentized sample analogs of the moment (in)equality conditions. the degree of relaxation, or critical level, is calibrated so that the function of theta, not theta itself, is uniformly asymptotically covered with prespecified probability. this calibration is based on repeatedly checking feasibility of linear programming problems, rendering it computationally attractive.   nonetheless, the program defining an extreme point of the confidence interval is generally nonlinear and potentially intricate. we provide an algorithm, based on the response surface method for global optimization, that approximates the solution rapidly and accurately, and we establish its rate of convergence. the algorithm is of independent interest for optimization problems with simple objectives and complicated constraints. an empirical application estimating an entry game illustrates the usefulness of the method. monte carlo simulations confirm the accuracy of the solution algorithm, the good statistical as well as computational performance of calibrated projection (including in comparison to other methods), and the algorithm's potential to greatly accelerate computation of other confidence intervals.", "categories": "math.st econ.em stat.th", "created": "2016-01-05", "updated": "2019-06-05", "authors": ["hiroaki kaido", "francesca molinari", "j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1601.00934"}, {"title": "doubly robust uniform confidence band for the conditional average   treatment effect function", "id": "1601.02801", "abstract": "in this paper, we propose a doubly robust method to present the heterogeneity of the average treatment effect with respect to observed covariates of interest. we consider a situation where a large number of covariates are needed for identifying the average treatment effect but the covariates of interest for analyzing heterogeneity are of much lower dimension. our proposed estimator is doubly robust and avoids the curse of dimensionality. we propose a uniform confidence band that is easy to compute, and we illustrate its usefulness via monte carlo experiments and an application to the effects of smoking on birth weights.", "categories": "stat.me econ.em", "created": "2016-01-12", "updated": "2016-10-28", "authors": ["sokbae lee", "ryo okui", "yoon-jae whang"], "url": "https://arxiv.org/abs/1601.02801"}, {"title": "efficient bayesian inference for multivariate factor stochastic   volatility models", "id": "1602.08154", "abstract": "we discuss efficient bayesian estimation of dynamic covariance matrices in multivariate time series through a factor stochastic volatility model. in particular, we propose two interweaving strategies (yu and meng, journal of computational and graphical statistics, 20(3), 531-570, 2011) to substantially accelerate convergence and mixing of standard mcmc approaches. similar to marginal data augmentation techniques, the proposed acceleration procedures exploit non-identifiability issues which frequently arise in factor models. our new interweaving strategies are easy to implement and come at almost no extra computational cost; nevertheless, they can boost estimation efficiency by several orders of magnitude as is shown in extensive simulation studies. to conclude, the application of our algorithm to a 26-dimensional exchange rate data set illustrates the superior performance of the new approach for real-world data.", "categories": "stat.co econ.em stat.ap stat.me", "created": "2016-02-25", "updated": "2017-07-19", "authors": ["gregor kastner", "sylvia fr\u00fchwirth-schnatter", "hedibert freitas lopes"], "url": "https://arxiv.org/abs/1602.08154"}, {"title": "high-dimensional $l_2$boosting: rate of convergence", "id": "1602.08927", "abstract": "boosting is one of the most significant developments in machine learning. this paper studies the rate of convergence of $l_2$boosting, which is tailored for regression, in a high-dimensional setting. moreover, we introduce so-called \\textquotedblleft post-boosting\\textquotedblright. this is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $l_2$boosting. another variant is \\textquotedblleft orthogonal boosting\\textquotedblright\\ where after each step an orthogonal projection is conducted. we show that both post-$l_2$boosting and the orthogonal boosting achieve the same rate of convergence as lasso in a sparse, high-dimensional setting. we show that the rate of convergence of the classical $l_2$boosting depends on the design matrix described by a sparse eigenvalue constant. to show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $l_2$boosting. we also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. our results also allow a direct comparison between lasso and boosting which has been missing from the literature. finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. in these simulation studies, post-$l_2$boosting clearly outperforms lasso.", "categories": "stat.ml cs.lg econ.em math.st stat.me stat.th", "created": "2016-02-29", "updated": "2016-11-05", "authors": ["ye luo", "martin spindler"], "url": "https://arxiv.org/abs/1602.08927"}, {"title": "oracle estimation of a change point in high dimensional quantile   regression", "id": "1603.00235", "abstract": "in this paper, we consider a high-dimensional quantile regression model where the sparsity structure may differ between two sub-populations. we develop $\\ell_1$-penalized estimators of both regression coefficients and the threshold parameter. our penalized estimators not only select covariates but also discriminate between a model with homogeneous sparsity and a model with a change point. as a result, it is not necessary to know or pretest whether the change point is present, or where it occurs. our estimator of the change point achieves an oracle property in the sense that its asymptotic distribution is the same as if the unknown active sets of regression coefficients were known. importantly, we establish this oracle property without a perfect covariate selection, thereby avoiding the need for the minimum level condition on the signals of active covariates. dealing with high-dimensional quantile regression with an unknown change point calls for a new proof technique since the quantile loss function is non-smooth and furthermore the corresponding objective function is non-convex with respect to the change point. the technique developed in this paper is applicable to a general m-estimation framework with a change point, which may be of independent interest. the proposed methods are then illustrated via monte carlo experiments and an application to tipping in the dynamics of racial segregation.", "categories": "stat.me econ.em", "created": "2016-03-01", "updated": "2016-12-16", "authors": ["sokbae lee", "yuan liao", "myung hwan seo", "youngki shin"], "url": "https://arxiv.org/abs/1603.00235"}, {"title": "coordination event detection and initiator identification in time series   data", "id": "1603.01570", "abstract": "behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. in this work, we formalize the \"coordination initiator inference problem\" and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. the proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks. we demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate nasdaq index. our method is easily generalizable to any coordinated time-series data from interacting entities.", "categories": "cs.si cs.ai cs.ma econ.em physics.data-an", "created": "2016-03-04", "updated": "2019-11-23", "authors": ["chainarong amornbunchornvej", "ivan brugere", "ariana strandburg-peshkin", "damien farine", "margaret c. crofoot", "tanya y. berger-wolf"], "url": "https://arxiv.org/abs/1603.01570"}, {"title": "high-dimensional metrics in r", "id": "1603.01700", "abstract": "the package high-dimensional metrics (\\rpackage{hdm}) is an evolving collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. it focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ate) and average treatment effect for the treated (atet), as well for extensions of these parameters to the endogenous setting are provided. theory grounded, data-driven methods for selecting the penalization parameter in lasso regressions under heteroscedastic and non-gaussian errors are implemented. moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented, including a joint significance test for lasso regression. data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. \\r and the package \\rpackage{hdm} are open-source software projects and can be freely downloaded from cran: \\texttt{http://cran.r-project.org}.", "categories": "stat.ml econ.em stat.me", "created": "2016-03-05", "updated": "2016-08-01", "authors": ["victor chernozhukov", "chris hansen", "martin spindler"], "url": "https://arxiv.org/abs/1603.01700"}, {"title": "optimal data collection for randomized control trials", "id": "1603.03675", "abstract": "in a randomized control trial, the precision of an average treatment effect estimator can be improved either by collecting data on additional individuals, or by collecting additional covariates that predict the outcome variable. we propose the use of pre-experimental data such as a census, or a household survey, to inform the choice of both the sample size and the covariates to be collected. our procedure seeks to minimize the resulting average treatment effect estimator's mean squared error, subject to the researcher's budget constraint. we rely on a modification of an orthogonal greedy algorithm that is conceptually simple and easy to implement in the presence of a large number of potential covariates, and does not require any tuning parameters. in two empirical applications, we show that our procedure can lead to substantial gains of up to 58%, measured either in terms of reductions in data collection costs or in terms of improvements in the precision of the treatment effect estimator.", "categories": "stat.me econ.em", "created": "2016-03-11", "updated": "2016-08-22", "authors": ["pedro carneiro", "sokbae lee", "daniel wilhelm"], "url": "https://arxiv.org/abs/1603.03675"}, {"title": "augmented factor models with applications to validating market risk   factors and forecasting bond risk premia", "id": "1603.07041", "abstract": "we study factor models augmented by observed covariates that have explanatory powers on the unknown factors. in financial factor models, the unknown factors can be reasonably well explained by a few observable proxies, such as the fama-french factors. in diffusion index forecasts, identified factors are strongly related to several directly measurable economic variables such as consumption-wealth variable, financial ratios, and term spread. with those covariates, both the factors and loadings are identifiable up to a rotation matrix even only with a finite dimension. to incorporate the explanatory power of these covariates, we propose a smoothed principal component analysis (pca): (i) regress the data onto the observed covariates, and (ii) take the principal components of the fitted data to estimate the loadings and factors. this allows us to accurately estimate the percentage of both explained and unexplained components in factors and thus to assess the explanatory power of covariates. we show that both the estimated factors and loadings can be estimated with improved rates of convergence compared to the benchmark method. the degree of improvement depends on the strength of the signals, representing the explanatory power of the covariates on the factors. the proposed estimator is robust to possibly heavy-tailed distributions. we apply the model to forecast us bond risk premia, and find that the observed macroeconomic characteristics contain strong explanatory powers of the factors. the gain of forecast is more substantial when the characteristics are incorporated to estimate the common factors than directly used for forecasts.", "categories": "stat.me econ.em", "created": "2016-03-22", "updated": "2018-09-16", "authors": ["jianqing fan", "yuan ke", "yuan liao"], "url": "https://arxiv.org/abs/1603.07041"}, {"title": "estimating treatment effects using multiple surrogates: the role of the   surrogate score and the surrogate index", "id": "1603.09326", "abstract": "estimating the long-term effects of treatments is of interest in many fields. a common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. one approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. the validity of the surrogacy condition is often controversial. here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. we focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. we state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates.", "categories": "stat.me econ.em stat.ml", "created": "2016-03-30", "updated": "2020-02-29", "authors": ["susan athey", "raj chetty", "guido imbens", "hyunseung kang"], "url": "https://arxiv.org/abs/1603.09326"}, {"title": "the mittag-leffler fitting of the phillips curve", "id": "1604.00369", "abstract": "in this paper, a mathematical model based on the one-parameter mittag-leffler function is proposed to be used for the first time to describe the relation between unemployment rate and inflation rate, also known as the phillips curve. the phillips curve is in the literature often represented by an exponential-like shape. on the other hand, phillips in his fundamental paper used a power function in the model definition. considering that the ordinary as well as generalised mittag-leffler function behaves between a purely exponential function and a power function it is natural to implement it in the definition of the model used to describe the relation between the data representing the phillips curve. for the modelling purposes the data of two different european economies, france and switzerland, were used and an \"out-of-sample\" forecast was done to compare the performance of the mittag-leffler model to the performance of the power-type and exponential-type model. the results demonstrate that the ability of the mittag-leffler function to fit data that manifest signs of stretched exponentials, oscillations or even damped oscillations can be of use when describing economic relations and phenomenons, such as the phillips curve.", "categories": "econ.gn q-fin.ec", "created": "2016-04-01", "updated": "2019-09-30", "authors": ["tomas skovranek"], "url": "https://arxiv.org/abs/1604.00369"}, {"title": "program evaluation with right-censored data", "id": "1604.02642", "abstract": "in a unified framework, we provide estimators and confidence bands for a variety of treatment effects when the outcome of interest, typically a duration, is subjected to right censoring. our methodology accommodates average, distributional, and quantile treatment effects under different identifying assumptions including unconfoundedness, local treatment effects, and nonlinear differences-in-differences. the proposed estimators are easy to implement, have close-form representation, are fully data-driven upon estimation of nuisance parameters, and do not rely on parametric distributional assumptions, shape restrictions, or on restricting the potential treatment effect heterogeneity across different subpopulations. these treatment effects results are obtained as a consequence of more general results on two-step kaplan-meier estimators that are of independent interest: we provide conditions for applying (i) uniform law of large numbers, (ii) functional central limit theorems, and (iii) we prove the validity of the ordinary nonparametric bootstrap in a two-step estimation procedure where the outcome of interest may be randomly censored.", "categories": "stat.me econ.em", "created": "2016-04-10", "updated": "", "authors": ["pedro h. c. sant'anna"], "url": "https://arxiv.org/abs/1604.02642"}, {"title": "approximate residual balancing: de-biased inference of average treatment   effects in high dimensions", "id": "1604.07125", "abstract": "there are many settings where researchers are interested in estimating average treatment effects and are willing to rely on the unconfoundedness assumption, which requires that the treatment assignment be as good as random conditional on pre-treatment variables. the unconfoundedness assumption is often more plausible if a large number of pre-treatment variables are included in the analysis, but this can worsen the performance of standard approaches to treatment effect estimation. in this paper, we develop a method for de-biasing penalized regression adjustments to allow sparse regression methods like the lasso to be used for sqrt{n}-consistent inference of average treatment effects in high-dimensional linear models. given linearity, we do not need to assume that the treatment propensities are estimable, or that the average treatment effect is a sparse contrast of the outcome model parameters. rather, in addition standard assumptions used to make lasso regression on the outcome model consistent under 1-norm error, we only require overlap, i.e., that the propensity score be uniformly bounded away from 0 and 1. procedurally, our method combines balancing weights with a regularized regression adjustment.", "categories": "stat.me econ.em math.st stat.th", "created": "2016-04-25", "updated": "2018-01-31", "authors": ["susan athey", "guido w. imbens", "stefan wager"], "url": "https://arxiv.org/abs/1604.07125"}, {"title": "monte carlo confidence sets for identified sets", "id": "1605.00499", "abstract": "in complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. we provide computationally attractive procedures to construct confidence sets (css) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. these css are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated gmm criterions). the level sets are constructed using cutoffs that are computed via monte carlo (mc) simulations directly from the quasi-posterior distributions of the criterions. we establish new bernstein-von mises (or bayesian wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (qlr) and profile qlr in partially-identified regular models and some non-regular models. these results imply that our mc css have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. our mc css for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. we also provide results on uniform validity of our css over classes of dgps that include point and partially identified models. we demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows.", "categories": "stat.me econ.em math.st stat.th", "created": "2016-05-02", "updated": "2017-09-25", "authors": ["xiaohong chen", "timothy christensen", "elie tamer"], "url": "https://arxiv.org/abs/1605.00499"}, {"title": "economic development and inequality: a complex system analysis", "id": "1605.03133", "abstract": "by borrowing methods from complex system analysis, in this paper we analyze the features of the complex relationship that links the development and the industrialization of a country to economic inequality. in order to do this, we identify industrialization as a combination of a monetary index, the gdp per capita, and a recently introduced measure of the complexity of an economy, the fitness. at first we explore these relations on a global scale over the time period 1990--2008 focusing on two different dimensions of inequality: the capital share of income and a theil measure of wage inequality. in both cases, the movement of inequality follows a pattern similar to the one theorized by kuznets in the fifties. we then narrow down the object of study ad we concentrate on wage inequality within the united states. by employing data on wages and employment on the approximately 3100 us counties for the time interval 1990--2014, we generalize the fitness-complexity algorithm for counties and naics sectors, and we investigate wage inequality between industrial sectors within counties. at this scale, in the early nineties we recover a behavior similar to the global one. while, in more recent years, we uncover a trend reversal: wage inequality monotonically increases as industrialization levels grow. hence at a county level, at net of the social and institutional factors that differ among countries, we not only observe an upturn in inequality but also a change in the structure of the relation between wage inequality and development.", "categories": "econ.gn q-fin.ec", "created": "2016-05-10", "updated": "", "authors": ["angelica sbardella", "emanuele pugliese", "luciano pietronero"], "url": "https://arxiv.org/abs/1605.03133"}, {"title": "testing for common breaks in a multiple equations system", "id": "1606.00092", "abstract": "the issue addressed in this paper is that of testing for common breaks across or within equations of a multivariate system. our framework is very general and allows integrated regressors and trends as well as stationary regressors. the null hypothesis is that breaks in different parameters occur at common locations and are separated by some positive fraction of the sample size unless they occur across different equations. under the alternative hypothesis, the break dates across parameters are not the same and also need not be separated by a positive fraction of the sample size whether within or across equations. the test considered is the quasi-likelihood ratio test assuming normal errors, though as usual the limit distribution of the test remains valid with non-normal errors. of independent interest, we provide results about the rate of convergence of the estimates when searching over all possible partitions subject only to the requirement that each regime contains at least as many observations as some positive fraction of the sample size, allowing break dates not separated by a positive fraction of the sample size across equations. simulations show that the test has good finite sample properties. we also provide an application to issues related to level shifts and persistence for various measures of inflation to illustrate its usefulness.", "categories": "math.st econ.em q-fin.st stat.me stat.th", "created": "2016-05-31", "updated": "2018-01-10", "authors": ["tatsushi oka", "pierre perron"], "url": "https://arxiv.org/abs/1606.00092"}, {"title": "nonparametric analysis of random utility models", "id": "1606.04819", "abstract": "this paper develops and implements a nonparametric test of random utility models. the motivating application is to test the null hypothesis that a sample of cross-sectional demand distributions was generated by a population of rational consumers. we test a necessary and sufficient condition for this that does not rely on any restriction on unobserved heterogeneity or the number of goods. we also propose and implement a control function approach to account for endogenous expenditure. an econometric result of independent interest is a test for linear inequality constraints when these are represented as the vertices of a polyhedron rather than its faces. an empirical application to the u.k. household expenditure survey illustrates computational feasibility of the method in demand problems with 5 goods.", "categories": "math.st econ.em stat.th", "created": "2016-06-15", "updated": "2018-09-19", "authors": ["yuichi kitamura", "j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1606.04819"}, {"title": "quantile graphical models: prediction and conditional independence with   applications to systemic risk", "id": "1607.00286", "abstract": "we propose two types of quantile graphical models (qgms) --- conditional independence quantile graphical models (ciqgms) and prediction quantile graphical models (pqgms). ciqgms characterize the conditional independence of distributions by evaluating the distributional dependence structure at each quantile index. as such, ciqgms can be used for validation of the graph structure in the causal graphical models (\\cite{pearl2009causality, robins1986new, heckman2015causal}). one main advantage of these models is that we can apply them to large collections of variables driven by non-gaussian and non-separable shocks. pqgms characterize the statistical dependencies through the graphs of the best linear predictors under asymmetric loss functions. pqgms make weaker assumptions than ciqgms as they allow for misspecification. because of qgms' ability to handle large collections of variables and focus on specific parts of the distributions, we could apply them to quantify tail interdependence. the resulting tail risk network can be used for measuring systemic risk contributions that help make inroads in understanding international financial contagion and dependence structures of returns under downside market movements.   we develop estimation and inference methods for qgms focusing on the high-dimensional case, where the number of variables in the graph is large compared to the number of observations. for ciqgms, these methods and results include valid simultaneous choices of penalty functions, uniform rates of convergence, and confidence regions that are simultaneously valid. we also derive analogous results for pqgms, which include new results for penalized quantile regressions in high-dimensional settings to handle misspecification, many controls, and a continuum of additional conditioning events.", "categories": "math.st econ.em stat.th", "created": "2016-07-01", "updated": "2019-10-28", "authors": ["alexandre belloni", "mingli chen", "victor chernozhukov"], "url": "https://arxiv.org/abs/1607.00286"}, {"title": "frequentist size of bayesian inequality tests", "id": "1607.00393", "abstract": "bayesian and frequentist criteria are fundamentally different, but often posterior and sampling distributions are asymptotically equivalent (e.g., gaussian). for the corresponding limit experiment, we characterize the frequentist size of a certain bayesian hypothesis test of (possibly nonlinear) inequalities. if the null hypothesis is that the (possibly infinite-dimensional) parameter lies in a certain half-space, then the bayesian test's size is $\\alpha$; if the null hypothesis is a subset of a half-space, then size is above $\\alpha$ (sometimes strictly); and in other cases, size may be above, below, or equal to $\\alpha$. two examples illustrate our results: testing stochastic dominance and testing curvature of a translog cost function.", "categories": "math.st econ.em stat.me stat.th", "created": "2016-07-01", "updated": "2018-02-27", "authors": ["david m. kaplan", "longhao zhuo"], "url": "https://arxiv.org/abs/1607.00393"}, {"title": "the econometrics of randomized experiments", "id": "1607.00698", "abstract": "in this review, we present econometric and statistical methods for analyzing randomized experiments. for basic experiments we stress randomization-based inference as opposed to sampling-based inference. in randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. we show how this perspective relates to regression analyses for randomized experiments. we discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. we also discuss complications in randomized experiments such as non-compliance. in the presence of non-compliance we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. we consider in detail estimation and inference for heterogeneous treatment effects in settings with (possibly many) covariates. these methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. we also discuss optimal assignment to treatment based on covariates in such settings. finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.", "categories": "stat.me econ.em", "created": "2016-07-03", "updated": "", "authors": ["susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1607.00698"}, {"title": "the state of applied econometrics - causality and policy evaluation", "id": "1607.00699", "abstract": "in this paper we discuss recent developments in econometrics that we view as important for empirical researchers working on policy evaluation questions. we focus on three main areas, where in each case we highlight recommendations for applied work. first, we discuss new research on identification strategies in program evaluation, with particular focus on synthetic control methods, regression discontinuity, external validity, and the causal interpretation of regression methods. second, we discuss various forms of supplementary analyses to make the identification strategies more credible. these include placebo analyses as well as sensitivity and robustness analyses. third, we discuss recent advances in machine learning methods for causal effects. these advances include methods to adjust for differences between treated and control units in high-dimensional settings, and methods for identifying and estimating heterogeneous treatment effects.", "categories": "stat.me econ.em", "created": "2016-07-03", "updated": "", "authors": ["susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1607.00699"}, {"title": "should i stay or should i go? a latent threshold approach to large-scale   mixture innovation models", "id": "1607.04532", "abstract": "this paper proposes a straightforward algorithm to carry out inference in large time-varying parameter vector autoregressions (tvp-vars) with mixture innovation components for each coefficient in the system. we significantly decrease the computational burden by approximating the latent indicators that drive the time-variation in the coefficients with a latent threshold process that depends on the absolute size of the shocks. the merits of our approach are illustrated with two applications. first, we forecast the us term structure of interest rates and demonstrate forecast gains of the proposed mixture innovation model relative to other benchmark models. second, we apply our approach to us macroeconomic data and find significant evidence for time-varying effects of a monetary policy tightening.", "categories": "stat.me econ.em stat.ap stat.co", "created": "2016-07-15", "updated": "2018-07-26", "authors": ["florian huber", "gregor kastner", "martin feldkircher"], "url": "https://arxiv.org/abs/1607.04532"}, {"title": "effects of sea level rise on economy of the united states", "id": "1607.06247", "abstract": "we report the first ex post study of the economic impact of sea level rise. we apply two econometric approaches to estimate the past effects of sea level rise on the economy of the usa, viz. barro type growth regressions adjusted for spatial patterns and a matching estimator. unit of analysis is 3063 counties of the usa. we fit growth regressions for 13 time periods and we estimated numerous varieties and robustness tests for both growth regressions and matching estimator. although there is some evidence that sea level rise has a positive effect on economic growth, in most specifications the estimated effects are insignificant. we therefore conclude that there is no stable, significant effect of sea level rise on economic growth. this finding contradicts previous ex ante studies.", "categories": "q-fin.ec econ.em stat.ap", "created": "2016-07-21", "updated": "", "authors": ["monika novackova", "richard s. j. tol"], "url": "https://arxiv.org/abs/1607.06247"}, {"title": "decentralized bayesian learning in dynamic games: a framework for   studying informational cascades", "id": "1607.06847", "abstract": "we study the problem of bayesian learning in a dynamical system involving strategic agents with asymmetric information. in a series of seminal papers in the literature, this problem has been investigated under a simplifying model where myopically selfish players appear sequentially and act once in the game, based on private noisy observations of the system state and public observation of past players' actions. it has been shown that there exist information cascades where users discard their private information and mimic the action of their predecessor. in this paper, we provide a framework for studying bayesian learning dynamics in a more general setting than the one described above. in particular, our model incorporates cases where players are non-myopic and strategically participate for the whole duration of the game, and cases where an endogenous process selects which subset of players will act at each time instance. the proposed framework hinges on a sequential decomposition methodology for finding structured perfect bayesian equilibria (pbe) of a general class of dynamic games with asymmetric information, where user-specific states evolve as conditionally independent markov processes and users make independent noisy observations of their states. using this methodology, we study a specific dynamic learning model where players make decisions about public investment based on their estimates of everyone's types. we characterize a set of informational cascades for this problem where learning stops for the team as a whole. we show that in such cascades, all players' estimates of other players' types freeze even though each individual player asymptotically learns its own true type.", "categories": "cs.gt cs.sy econ.th", "created": "2016-07-22", "updated": "2018-04-08", "authors": ["deepanshu vasal", "achilleas anastasopoulos"], "url": "https://arxiv.org/abs/1607.06847"}, {"title": "locally robust semiparametric estimation", "id": "1608.00033", "abstract": "many economic and causal parameters depend on nonparametric or high dimensional first steps. we give a general construction of locally robust/orthogonal moment functions for gmm, where moment conditions have zero derivative with respect to first steps. we show that orthogonal moment functions can be constructed by adding to identifying moments the nonparametric influence function for the effect of the first step on identifying moments. orthogonal moments reduce model selection and regularization bias, as is very important in many applications, especially for machine learning first steps.   we give debiased machine learning estimators of functionals of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. we show that adding to identifying moments the nonparametric influence function provides a general construction of orthogonal moments, including regularity conditions, and show that the nonparametric influence function is robust to additional unknown functions on which it depends. we give a general approach to estimating the unknown functions in the nonparametric influence function and use it to automatically debias estimators of functionals of high dimensional conditional location learners. we give a variety of new doubly robust moment equations and characterize double robustness. we give general and simple regularity conditions and apply these for asymptotic inference on functionals of high dimensional regression quantiles and dynamic discrete choice parameters with high dimensional state variables.", "categories": "math.st econ.em stat.th", "created": "2016-07-29", "updated": "2020-08-03", "authors": ["victor chernozhukov", "juan carlos escanciano", "hidehiko ichimura", "whitney k. newey", "james m. robins"], "url": "https://arxiv.org/abs/1608.00033"}, {"title": "double/debiased machine learning for treatment and causal parameters", "id": "1608.00060", "abstract": "most modern supervised statistical/machine learning (ml) methods are explicitly designed to solve prediction problems very well. achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. in fact, estimates of such causal parameters obtained via naively plugging ml estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ml tools. specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ml predictions. the score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. the resulting method thus could be called a \"double ml\" method because it relies on estimating primary and auxiliary predictive models. in order to avoid overfitting, our construction also makes use of the k-fold sample splitting, which we call cross-fitting. this allows us to use a very broad set of ml predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.", "categories": "stat.ml econ.em", "created": "2016-07-29", "updated": "2017-12-12", "authors": ["victor chernozhukov", "denis chetverikov", "mert demirer", "esther duflo", "christian hansen", "whitney newey", "james robins"], "url": "https://arxiv.org/abs/1608.00060"}, {"title": "hdm: high-dimensional metrics", "id": "1608.00354", "abstract": "in this article the package high-dimensional metrics (\\texttt{hdm}) is introduced. it is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. it focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ate) and average treatment effect for the treated (atet), as well for extensions of these parameters to the endogenous setting are provided. theory grounded, data-driven methods for selecting the penalization parameter in lasso regressions under heteroscedastic and non-gaussian errors are implemented. moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included.", "categories": "stat.me econ.em stat.ml", "created": "2016-08-01", "updated": "", "authors": ["victor chernozhukov", "chris hansen", "martin spindler"], "url": "https://arxiv.org/abs/1608.00354"}, {"title": "fixed-effect regressions on network data", "id": "1608.01532", "abstract": "this paper considers inference on fixed effects in a linear regression model estimated from network data. an important special case of our setup is the two-way regression model. this is a workhorse technique in the analysis of matched data sets, such as employer-employee or student-teacher panel data. we formalize how the structure of the network affects the accuracy with which the fixed effects can be estimated. this allows us to derive sufficient conditions on the network for consistent estimation and asymptotically-valid inference to be possible. estimation of moments is also considered. we allow for general networks and our setup covers both the dense and sparse case. we provide numerical results for the estimation of teacher value-added models and regressions with occupational dummies.", "categories": "stat.me econ.em", "created": "2016-08-04", "updated": "2019-04-01", "authors": ["koen jochmans", "martin weidner"], "url": "https://arxiv.org/abs/1608.01532"}, {"title": "generic inference on quantile and quantile effect functions for discrete   outcomes", "id": "1608.05142", "abstract": "quantile and quantile effect functions are important tools for descriptive and causal analyses due to their natural and intuitive interpretation. existing inference methods for these functions do not apply to discrete random variables. this paper offers a simple, practical construction of simultaneous confidence bands for quantile and quantile effect functions of possibly discrete random variables. it is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. the construction is generic and does not depend on the nature of the underlying problem. it works in conjunction with parametric, semiparametric, and nonparametric modeling methods for observed and counterfactual distributions, and does not depend on the sampling scheme. we apply our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. we find that universal insurance coverage increases the number of doctor visits across the entire distribution, and that the racial test score gap is small at early ages but grows with age due to socio economic factors affecting child development especially at the top of the distribution. these are new, interesting empirical findings that complement previous analyses that focused on mean effects only. in both applications, the outcomes of interest are discrete rendering existing inference methods invalid for obtaining uniform confidence bands for observed and counterfactual quantile functions and for their difference -- the quantile effects functions.", "categories": "stat.me econ.em", "created": "2016-08-17", "updated": "2018-08-30", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "blaise melly", "kaspar w\u00fcthrich"], "url": "https://arxiv.org/abs/1608.05142"}, {"title": "the structure of the climate debate", "id": "1608.05597", "abstract": "first-best climate policy is a uniform carbon tax which gradually rises over time. civil servants have complicated climate policy to expand bureaucracies, politicians to create rents. environmentalists have exaggerated climate change to gain influence, other activists have joined the climate bandwagon. opponents to climate policy have attacked the weaknesses in climate research. the climate debate is convoluted and polarized as a result, and climate policy complex. climate policy should become easier and more rational as the paris agreement has shifted climate policy back towards national governments. changing political priorities, austerity, and a maturing bureaucracy should lead to a more constructive climate debate.", "categories": "q-fin.ec econ.em q-fin.gn", "created": "2016-08-19", "updated": "", "authors": ["richard s. j. tol"], "url": "https://arxiv.org/abs/1608.05597"}, {"title": "sparse bayesian time-varying covariance estimation in many dimensions", "id": "1608.08468", "abstract": "we address the curse of dimensionality in dynamic covariance estimation by modeling the underlying co-volatility dynamics of a time series vector through latent time-varying stochastic factors. the use of a global-local shrinkage prior for the elements of the factor loadings matrix pulls loadings on superfluous factors towards zero. to demonstrate the merits of the proposed framework, the model is applied to simulated data as well as to daily log-returns of 300 s&p 500 members. our approach yields precise correlation estimates, strong implied minimum variance portfolio performance and superior forecasting accuracy in terms of log predictive scores when compared to typical benchmarks.", "categories": "stat.me econ.em q-fin.pm stat.ap stat.co", "created": "2016-08-30", "updated": "2017-11-11", "authors": ["gregor kastner"], "url": "https://arxiv.org/abs/1608.08468"}, {"title": "model selection for treatment choice: penalized welfare maximization", "id": "1609.03167", "abstract": "this paper studies a penalized statistical decision rule for the treatment assignment problem. consider the setting of a utilitarian policy maker who must use sample data to allocate a binary treatment to members of a population, based on their observable characteristics. we model this problem as a statistical decision problem where the policy maker must choose a subset of the covariate space to assign to treatment, out of a class of potential subsets. we focus on settings in which the policy maker may want to select amongst a collection of constrained subset classes: examples include choosing the number of covariates over which to perform best-subset selection, and model selection when approximating a complicated class via a sieve. we adapt and extend results from statistical learning to develop the penalized welfare maximization (pwm) rule. we establish an oracle inequality for the regret of the pwm rule which shows that it is able to perform model selection over the collection of available classes. we then use this oracle inequality to derive relevant bounds on maximum regret for pwm. an important consequence of our results is that we are able to formalize model-selection using a \"hold-out\" procedure, where the policy maker would first estimate various policies using half of the data, and then select the policy which performs the best when evaluated on the other half of the data.", "categories": "math.st econ.em stat.th", "created": "2016-09-11", "updated": "2019-12-07", "authors": ["eric mbakop", "max tabord-meehan"], "url": "https://arxiv.org/abs/1609.03167"}, {"title": "smoothed estimating equations for instrumental variables quantile   regression", "id": "1609.09033", "abstract": "the moment conditions or estimating equations for instrumental variables quantile regression involve the discontinuous indicator function. we instead use smoothed estimating equations (see), with bandwidth $h$. we show that the mean squared error (mse) of the vector of the see is minimized for some $h>0$, leading to smaller asymptotic mse of the estimating equations and associated parameter estimators. the same mse-optimal $h$ also minimizes the higher-order type i error of a see-based $\\chi^2$ test and increases size-adjusted power in large samples. computation of the see estimator also becomes simpler and more reliable, especially with (more) endogenous regressors. monte carlo simulations demonstrate all of these superior properties in finite samples, and we apply our estimator to jtpa data. smoothing the estimating equations is not just a technical operation for establishing edgeworth expansions and bootstrap refinements; it also brings the real benefits of having more precise estimators and more powerful tests. code for the estimator, simulations, and empirical examples is available from the first author's website.", "categories": "stat.me econ.em math.st stat.ap stat.th", "created": "2016-09-28", "updated": "", "authors": ["david m. kaplan", "yixiao sun"], "url": "https://arxiv.org/abs/1609.09033"}, {"title": "fractional order statistic approximation for nonparametric conditional   quantile inference", "id": "1609.09035", "abstract": "using and extending fractional order statistic theory, we characterize the $o(n^{-1})$ coverage probability error of the previously proposed confidence intervals for population quantiles using $l$-statistics as endpoints in hutson (1999). we derive an analytic expression for the $n^{-1}$ term, which may be used to calibrate the nominal coverage level to get $o\\bigl(n^{-3/2}[\\log(n)]^3\\bigr)$ coverage error. asymptotic power is shown to be optimal. using kernel smoothing, we propose a related method for nonparametric inference on conditional quantiles. this new method compares favorably with asymptotic normality and bootstrap methods in theory and in simulations. code is available from the second author's website for both unconditional and conditional methods, simulations, and empirical examples.", "categories": "math.st econ.em stat.ap stat.me stat.th", "created": "2016-09-28", "updated": "", "authors": ["matt goldman", "david m. kaplan"], "url": "https://arxiv.org/abs/1609.09035"}, {"title": "generalized random forests", "id": "1610.01271", "abstract": "we propose generalized random forests, a method for non-parametric statistical estimation based on random forests (breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. we propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. we use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. a software implementation, grf for r and c++, is available from cran.", "categories": "stat.me econ.em stat.ml", "created": "2016-10-05", "updated": "2018-04-05", "authors": ["susan athey", "julie tibshirani", "stefan wager"], "url": "https://arxiv.org/abs/1610.01271"}, {"title": "best subset binary prediction", "id": "1610.02738", "abstract": "we consider a variable selection problem for the prediction of binary outcomes. we study the best subset selection procedure by which the covariates are chosen by maximizing manski (1975, 1985)'s maximum score objective function subject to a constraint on the maximal number of selected variables. we show that this procedure can be equivalently reformulated as solving a mixed integer optimization problem, which enables computation of the exact or an approximate solution with a definite approximation error bound. in terms of theoretical results, we obtain non-asymptotic upper and lower risk bounds when the dimension of potential covariates is possibly much larger than the sample size. our upper and lower risk bounds are minimax rate-optimal when the maximal number of selected variables is fixed and does not increase with the sample size. we illustrate usefulness of the best subset binary prediction approach via monte carlo simulations and an empirical application of the work-trip transportation mode choice.", "categories": "stat.me econ.em", "created": "2016-10-09", "updated": "2018-05-16", "authors": ["le-yu chen", "sokbae lee"], "url": "https://arxiv.org/abs/1610.02738"}, {"title": "probitfe and logitfe: bias corrections for probit and logit models with   two-way fixed effects", "id": "1610.07714", "abstract": "we present the stata commands probitfe and logitfe, which estimate probit and logit panel data models with individual and/or time unobserved effects. fixed effect panel data methods that estimate the unobserved effects can be severely biased because of the incidental parameter problem (neyman and scott, 1948). we tackle this problem by using the analytical and jackknife bias corrections derived in fernandez-val and weidner (2016) for panels where the two dimensions ($n$ and $t$) are moderately large. we illustrate the commands with an empirical application to international trade and a monte carlo simulation calibrated to this application.", "categories": "stat.me econ.em", "created": "2016-10-24", "updated": "2017-02-26", "authors": ["mario cruz-gonzalez", "ivan fernandez-val", "martin weidner"], "url": "https://arxiv.org/abs/1610.07714"}, {"title": "counterfactual: an r package for counterfactual analysis", "id": "1610.07894", "abstract": "the counterfactual package implements the estimation and inference methods of chernozhukov, fern\\'andez-val and melly (2013) for counterfactual analysis. the counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. they can be applied to estimate quantile treatment effects and wage decompositions. this paper serves as an introduction to the package and displays basic functionality of the commands contained within.", "categories": "stat.co econ.em", "created": "2016-10-25", "updated": "", "authors": ["mingli chen", "victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "blaise melly"], "url": "https://arxiv.org/abs/1610.07894"}, {"title": "quantreg.nonpar: an r package for performing nonparametric series   quantile regression", "id": "1610.08329", "abstract": "the r package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. it also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. this paper serves as an introduction to the package and displays basic functionality of the functions contained within.", "categories": "stat.co econ.em", "created": "2016-10-26", "updated": "", "authors": ["michael lipsitz", "alexandre belloni", "victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val"], "url": "https://arxiv.org/abs/1610.08329"}, {"title": "honest confidence sets in nonparametric iv regression and other   ill-posed models", "id": "1611.03015", "abstract": "this paper develops inferential methods for a very general class of ill-posed models in econometrics encompassing the nonparametric instrumental regression, various functional regressions, and the density deconvolution. we focus on uniform confidence sets for the parameter of interest estimated with tikhonov regularization, as in \\cite{darolles2011nonparametric}. since it is impossible to have inferential methods based on the central limit theorem, we develop two alternative approaches relying on the concentration inequality and bootstrap approximations. we show that expected diameters and coverage properties of resulting sets have uniform validity over a large class of models, i.e., constructed confidence sets are honest. monte carlo experiments illustrate that introduced confidence sets have reasonable width and coverage properties. using u.s. data, we provide uniform confidence sets for engel curves for various commodities.", "categories": "math.st econ.em stat.ap stat.me stat.th", "created": "2016-11-09", "updated": "2019-10-12", "authors": ["andrii babii"], "url": "https://arxiv.org/abs/1611.03015"}, {"title": "specification tests for the propensity score", "id": "1611.06217", "abstract": "this paper proposes new nonparametric diagnostic tools to assess the asymptotic validity of different treatment effects estimators that rely on the correct specification of the propensity score. we derive a particular restriction relating the propensity score distribution of treated and control groups, and develop specification tests based upon it. the resulting tests do not suffer from the \"curse of dimensionality\" when the vector of covariates is high-dimensional, are fully data-driven, do not require tuning parameters such as bandwidths, and are able to detect a broad class of local alternatives converging to the null at the parametric rate $n^{-1/2}$, with $n$ the sample size. we show that the use of an orthogonal projection on the tangent space of nuisance parameters facilitates the simulation of critical values by means of a multiplier bootstrap procedure, and can lead to power gains. the finite sample performance of the tests is examined by means of a monte carlo experiment and an empirical application. open-source software is available for implementing the proposed tests.", "categories": "stat.me econ.em", "created": "2016-11-18", "updated": "2019-02-07", "authors": ["pedro h. c. sant'anna", "xiaojun song"], "url": "https://arxiv.org/abs/1611.06217"}, {"title": "the factor-lasso and k-step bootstrap approach for inference in   high-dimensional economic applications", "id": "1611.09420", "abstract": "we consider inference about coefficients on a small number of variables of interest in a linear panel data model with additive unobserved individual and time specific effects and a large number of additional time-varying confounding variables. we allow the number of these additional confounding variables to be larger than the sample size, and suppose that, in addition to unrestricted time and individual specific effects, these confounding variables are generated by a small number of common factors and high-dimensional weakly-dependent disturbances. we allow that both the factors and the disturbances are related to the outcome variable and other variables of interest. to make informative inference feasible, we impose that the contribution of the part of the confounding variables not captured by time specific effects, individual specific effects, or the common factors can be captured by a relatively small number of terms whose identities are unknown. within this framework, we provide a convenient computational algorithm based on factor extraction followed by lasso regression for inference about parameters of interest and show that the resulting procedure has good asymptotic properties. we also provide a simple k-step bootstrap procedure that may be used to construct inferential statements about parameters of interest and prove its asymptotic validity. the proposed bootstrap may be of substantive independent interest outside of the present context as the proposed bootstrap may readily be adapted to other contexts involving inference after lasso variable selection and the proof of its validity requires some new technical arguments. we also provide simulation evidence about performance of our procedure and illustrate its use in two empirical applications.", "categories": "stat.me econ.em", "created": "2016-11-28", "updated": "2016-12-06", "authors": ["christian hansen", "yuan liao"], "url": "https://arxiv.org/abs/1611.09420"}, {"title": "nonparametric tests for treatment effect heterogeneity with duration   outcomes", "id": "1612.02090", "abstract": "this article proposes different tests for treatment effect heterogeneity when the outcome of interest, typically a duration variable, may be right-censored. the proposed tests study whether a policy 1) has zero distributional (average) effect for all subpopulations defined by covariate values, and 2) has homogeneous average effect across different subpopulations. the proposed tests are based on two-step kaplan-meier integrals and do not rely on parametric distributional assumptions, shape restrictions, or on restricting the potential treatment effect heterogeneity across different subpopulations. our framework is suitable not only to exogenous treatment allocation but can also account for treatment noncompliance - an important feature in many applications. the proposed tests are consistent against fixed alternatives, and can detect nonparametric alternatives converging to the null at the parametric $n^{-1/2}$-rate, $n$ being the sample size. critical values are computed with the assistance of a multiplier bootstrap. the finite sample properties of the proposed tests are examined by means of a monte carlo study and an application about the effect of labor market programs on unemployment duration. open-source software is available for implementing all proposed tests.", "categories": "stat.me econ.em", "created": "2016-12-06", "updated": "2020-02-17", "authors": ["pedro h. c. sant'anna"], "url": "https://arxiv.org/abs/1612.02090"}, {"title": "maximum likelihood estimation in possibly misspecified dynamic models   with time-inhomogeneous markov regimes", "id": "1612.04932", "abstract": "this paper considers maximum likelihood (ml) estimation in a large class of models with hidden markov regimes. we investigate consistency and local asymptotic normality of the ml estimator under general conditions which allow for autoregressive dynamics in the observable process, time-inhomogeneous markov regime sequences, and possible model misspecification. a monte carlo study examines the finite-sample properties of the ml estimator. an empirical application is also discussed.", "categories": "math.st econ.em stat.th", "created": "2016-12-15", "updated": "2018-05-09", "authors": ["demian pouzo", "zacharias psaradakis", "martin sola"], "url": "https://arxiv.org/abs/1612.04932"}, {"title": "extremal quantile regression: an overview", "id": "1612.06850", "abstract": "extremal quantile regression, i.e. quantile regression applied to the tails of the conditional distribution, counts with an increasing number of economic and financial applications such as value-at-risk, production frontiers, determinants of low infant birth weights, and auction models. this chapter provides an overview of recent developments in the theory and empirics of extremal quantile regression. the advances in the theory have relied on the use of extreme value approximations to the law of the koenker and bassett (1978) quantile regression estimator. extreme value laws not only have been shown to provide more accurate approximations than gaussian laws at the tails, but also have served as the basis to develop bias corrected estimators and inference methods using simulation and suitable variations of bootstrap and subsampling. the applicability of these methods is illustrated with two empirical examples on conditional value-at-risk and financial contagion.", "categories": "stat.me econ.em", "created": "2016-12-20", "updated": "2017-02-08", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "tetsuya kaji"], "url": "https://arxiv.org/abs/1612.06850"}, {"title": "population and trends in the global mean temperature", "id": "1612.09123", "abstract": "the fisher ideal index, developed to measure price inflation, is applied to define a population-weighted temperature trend. this method has the advantages that the trend is representative for the population distribution throughout the sample but without conflating the trend in the population distribution and the trend in the temperature. i show that the trend in the global area-weighted average surface air temperature is different in key details from the population-weighted trend. i extend the index to include urbanization and the urban heat island effect. this substantially changes the trend again. i further extend the index to include international migration, but this has a minor impact on the trend.", "categories": "q-fin.ec econ.em physics.ao-ph stat.ap", "created": "2016-12-27", "updated": "", "authors": ["richard s. j. tol"], "url": "https://arxiv.org/abs/1612.09123"}, {"title": "representation of i(1) and i(2) autoregressive hilbertian processes", "id": "1701.08149", "abstract": "we extend the granger-johansen representation theorems for i(1) and i(2) vector autoregressive processes to accommodate processes that take values in an arbitrary complex separable hilbert space. this more general setting is of central relevance for statistical applications involving functional time series. we first obtain a range of necessary and sufficient conditions for a pole in the inverse of a holomorphic index-zero fredholm operator pencil to be of first or second order. those conditions form the basis for our development of i(1) and i(2) representations of autoregressive hilbertian processes. cointegrating and attractor subspaces are characterized in terms of the behavior of the autoregressive operator pencil in a neighborhood of one.", "categories": "math.st econ.em stat.th", "created": "2017-01-27", "updated": "2019-09-06", "authors": ["brendan k. beare", "won-ki seo"], "url": "https://arxiv.org/abs/1701.08149"}, {"title": "estimating average treatment effects: supplementary analyses and   remaining challenges", "id": "1702.01250", "abstract": "there is a large literature on semiparametric estimation of average treatment effects under unconfounded treatment assignment in settings with a fixed number of covariates. more recently attention has focused on settings with a large number of covariates. in this paper we extend lessons from the earlier literature to this new setting. we propose that in addition to reporting point estimates and standard errors, researchers report results from a number of supplementary analyses to assist in assessing the credibility of their estimates.", "categories": "stat.me econ.em", "created": "2017-02-04", "updated": "", "authors": ["susan athey", "guido imbens", "thai pham", "stefan wager"], "url": "https://arxiv.org/abs/1702.01250"}, {"title": "learning and type compatibility in signaling games", "id": "1702.01819", "abstract": "which equilibria will arise in signaling games depends on how the receiver interprets deviations from the path of play. we develop a micro-foundation for these off-path beliefs, and an associated equilibrium refinement, in a model where equilibrium arises through non-equilibrium learning by populations of patient and long-lived senders and receivers. in our model, young senders are uncertain about the prevailing distribution of play, so they rationally send out-of-equilibrium signals as experiments to learn about the behavior of the population of receivers. differences in the payoff functions of the types of senders generate different incentives for these experiments. using the gittins index (gittins, 1979), we characterize which sender types use each signal more often, leading to a constraint on the receiver's off-path beliefs based on \"type compatibility\" and hence a learning-based equilibrium selection.", "categories": "q-fin.ec econ.th", "created": "2017-02-06", "updated": "2018-06-30", "authors": ["drew fudenberg", "kevin he"], "url": "https://arxiv.org/abs/1702.01819"}, {"title": "policy learning with observational data", "id": "1702.02896", "abstract": "in many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. for example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. we propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.", "categories": "math.st cs.lg econ.em stat.ml stat.th", "created": "2017-02-09", "updated": "2020-09-04", "authors": ["susan athey", "stefan wager"], "url": "https://arxiv.org/abs/1702.02896"}, {"title": "$l_2$boosting for economic applications", "id": "1702.03244", "abstract": "in the recent years more and more high-dimensional data sets, where the number of parameters $p$ is high compared to the number of observations $n$ or even larger, are available for applied researchers. boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. while lasso has been applied very successfully for high-dimensional data sets in economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like biostatistics and pattern recognition. we attribute this to missing theoretical results for boosting. the goal of this paper is to fill this gap and show that boosting is a competitive method for inference of a treatment effect or instrumental variable (iv) estimation in a high-dimensional setting. first, we present the $l_2$boosting with componentwise least squares algorithm and variants which are tailored for regression problems which are the workhorse for most econometric problems. then we show how $l_2$boosting can be used for estimation of treatment effects and iv estimation. we highlight the methods and illustrate them with simulations and empirical examples. for further results and technical details we refer to luo and spindler (2016, 2017) and to the online supplement of the paper.", "categories": "stat.ml econ.em stat.me", "created": "2017-02-10", "updated": "", "authors": ["ye luo", "martin spindler"], "url": "https://arxiv.org/abs/1702.03244"}, {"title": "interbank credit and the money manufacturing process. a systemic   perspective on financial stability", "id": "1702.08774", "abstract": "interbank lending and borrowing occur when financial institutions seek to settle and refinance their mutual positions over time and circumstances. this interactive process involves money creation at the aggregate level. coordination mismatch on interbank credit may trigger systemic crises. this happened when, since summer 2007, interbank credit coordination did not longer work smoothly across financial institutions, eventually requiring exceptional monetary policies by central banks, and guarantee and bailout interventions by governments. our article develops an interacting heterogeneous agents-based model of interbank credit coordination under minimal institutions. first, we explore the link between interbank credit coordination and the money generation process. contrary to received understanding, interbank credit has the capacity to make the monetary system unbound. second, we develop simulation analysis on imperfect interbank credit coordination, studying impact of interbank dynamics on financial stability and resilience at individual and aggregate levels. systemically destabilizing forces prove to be related to the working of the banking system over time, especially interbank coordination conditions and circumstances.", "categories": "q-fin.gn econ.gn physics.soc-ph q-fin.ec q-fin.rm", "created": "2017-02-28", "updated": "", "authors": ["yuri biondi", "feng zhou"], "url": "https://arxiv.org/abs/1702.08774"}, {"title": "network structure and naive sequential learning", "id": "1703.02105", "abstract": "we study a sequential-learning model featuring a network of naive agents with gaussian information structures. agents apply a heuristic rule to aggregate predecessors' actions. they weigh these actions according the strengths of their social connections to different predecessors. we show this rule arises endogenously when agents wrongly believe others act solely on private information and thus neglect redundancies among observations. we provide a simple linear formula expressing agents' actions in terms of network paths and use this formula to characterize the set of networks where naive agents eventually learn correctly. this characterization implies that, on all networks where later agents observe more than one neighbor, there exist disproportionately influential early agents who can cause herding on incorrect actions. going beyond existing social-learning results, we compute the probability of such mislearning exactly. this allows us to compare likelihoods of incorrect herding, and hence expected welfare losses, across network structures. the probability of mislearning increases when link densities are higher and when networks are more integrated. in partially segregated networks, divergent early signals can lead to persistent disagreement between groups.", "categories": "q-fin.ec cs.si econ.th", "created": "2017-02-25", "updated": "2020-05-01", "authors": ["krishna dasaratha", "kevin he"], "url": "https://arxiv.org/abs/1703.02105"}, {"title": "tests for qualitative features in the random coefficients model", "id": "1704.01066", "abstract": "the random coefficients model is an extension of the linear regression model that allows for unobserved heterogeneity in the population by modeling the regression coefficients as random variables. given data from this model, the statistical challenge is to recover information about the joint density of the random coefficients which is a multivariate and ill-posed problem. because of the curse of dimensionality and the ill-posedness, pointwise nonparametric estimation of the joint density is difficult and suffers from slow convergence rates. larger features, such as an increase of the density along some direction or a well-accentuated mode can, however, be much easier detected from data by means of statistical tests. in this article, we follow this strategy and construct tests and confidence statements for qualitative features of the joint density, such as increases, decreases and modes. we propose a multiple testing approach based on aggregating single tests which are designed to extract shape information on fixed scales and directions. using recent tools for gaussian approximations of multivariate empirical processes, we derive expressions for the critical value. we apply our method to simulated and real data.", "categories": "stat.me econ.em", "created": "2017-04-04", "updated": "2018-03-13", "authors": ["fabian dunker", "konstantin eckle", "katharina proksch", "johannes schmidt-hieber"], "url": "https://arxiv.org/abs/1704.01066"}, {"title": "consistent approval-based multi-winner rules", "id": "1704.02453", "abstract": "this paper is an axiomatic study of consistent approval-based multi-winner rules, i.e., voting rules that select a fixed-size group of candidates based on approval ballots. we introduce the class of counting rules and provide an axiomatic characterization of this class based on the consistency axiom. building upon this result, we axiomatically characterize three important consistent multi-winner rules: proportional approval voting, multi-winner approval voting and the approval chamberlin--courant rule. our results demonstrate the variety of multi-winner rules and illustrate three different, orthogonal principles that multi-winner voting rules may represent: individual excellence, diversity, and proportionality.", "categories": "cs.gt cs.dm cs.ma econ.th", "created": "2017-04-08", "updated": "2019-10-23", "authors": ["martin lackner", "piotr skowron"], "url": "https://arxiv.org/abs/1704.02453"}, {"title": "sparse bayesian vector autoregressions in huge dimensions", "id": "1704.03239", "abstract": "we develop a bayesian vector autoregressive (var) model with multivariate stochastic volatility that is capable of handling vast dimensional information sets. three features are introduced to permit reliable estimation of the model. first, we assume that the reduced-form errors in the var feature a factor stochastic volatility structure, allowing for conditional equation-by-equation estimation. second, we apply recently developed global-local shrinkage priors to the var coefficients to cure the curse of dimensionality. third, we utilize recent innovations to efficiently sample from high-dimensional multivariate gaussian distributions. this makes simulation-based fully bayesian inference feasible when the dimensionality is large but the time series length is moderate. we demonstrate the merits of our approach in an extensive simulation study and apply the model to us macroeconomic data to evaluate its forecasting capabilities.", "categories": "stat.co econ.em stat.ap stat.me", "created": "2017-04-11", "updated": "2019-12-04", "authors": ["gregor kastner", "florian huber"], "url": "https://arxiv.org/abs/1704.03239"}, {"title": "bootstrap-based inference for cube root asymptotics", "id": "1704.08066", "abstract": "this paper proposes a valid bootstrap-based distributional approximation for m-estimators exhibiting a chernoff (1964)-type limiting distribution. for estimators of this kind, the standard nonparametric bootstrap is inconsistent. the method proposed herein is based on the nonparametric bootstrap, but restores consistency by altering the shape of the criterion function defining the estimator whose distribution we seek to approximate. this modification leads to a generic and easy-to-implement resampling method for inference that is conceptually distinct from other available distributional approximations. we illustrate the applicability of our results with four examples in econometrics and machine learning.", "categories": "math.st econ.em stat.me stat.th", "created": "2017-04-26", "updated": "2020-05-29", "authors": ["matias d. cattaneo", "michael jansson", "kenichi nagasawa"], "url": "https://arxiv.org/abs/1704.08066"}, {"title": "are unobservables separable?", "id": "1705.01654", "abstract": "it is common to assume in empirical research that observables and unobservables are additively separable, especially, when the former are endogenous. this is done because it is widely recognized that identification and estimation challenges arise when interactions between the two are allowed for. starting from a nonseparable iv model, where the instrumental variable is independent of unobservables, we develop a novel nonparametric test of separability of unobservables. the large-sample distribution of the test statistics is nonstandard and relies on a novel donsker-type central limit theorem for the empirical distribution of nonparametric iv residuals. using a dataset drawn from the 2015 us consumer expenditure survey, we find that the test rejects the separability in engel curves for most of the commodities.", "categories": "math.st econ.em stat.ap stat.me stat.th", "created": "2017-05-03", "updated": "2020-01-28", "authors": ["andrii babii", "jean-pierre florens"], "url": "https://arxiv.org/abs/1705.01654"}, {"title": "inference on breakdown frontiers", "id": "1705.04765", "abstract": "given a set of baseline assumptions, a breakdown frontier is the boundary between the set of assumptions which lead to a specific conclusion and those which do not. in a potential outcomes model with a binary treatment, we consider two conclusions: first, that ate is at least a specific value (e.g., nonnegative) and second that the proportion of units who benefit from treatment is at least a specific value (e.g., at least 50\\%). for these conclusions, we derive the breakdown frontier for two kinds of assumptions: one which indexes relaxations of the baseline random assignment of treatment assumption, and one which indexes relaxations of the baseline rank invariance assumption. these classes of assumptions nest both the point identifying assumptions of random assignment and rank invariance and the opposite end of no constraints on treatment selection or the dependence structure between potential outcomes. this frontier provides a quantitative measure of robustness of conclusions to relaxations of the baseline point identifying assumptions. we derive $\\sqrt{n}$-consistent sample analog estimators for these frontiers. we then provide two asymptotically valid bootstrap procedures for constructing lower uniform confidence bands for the breakdown frontier. as a measure of robustness, estimated breakdown frontiers and their corresponding confidence bands can be presented alongside traditional point estimates and confidence intervals obtained under point identifying assumptions. we illustrate this approach in an empirical application to the effect of child soldiering on wages. we find that sufficiently weak conclusions are robust to simultaneous failures of rank invariance and random assignment, while some stronger conclusions are fairly robust to failures of rank invariance but not necessarily to relaxations of random assignment.", "categories": "stat.me econ.em", "created": "2017-05-12", "updated": "2019-02-02", "authors": ["matthew a. masten", "alexandre poirier"], "url": "https://arxiv.org/abs/1705.04765"}, {"title": "banks as tanks: a continuous-time model of financial clearing", "id": "1705.05943", "abstract": "we present a simple continuous-time model of clearing in financial networks. financial firms are represented as \"tanks\" filled with fluid (money), flowing in and out. once \"pipes\" connecting \"tanks\" are open, the system reaches the clearing payment vector in finite time. this approach provides a simple recursive solution to a classical static model of financial clearing in bankruptcy, and suggests a practical payment mechanism. with sufficient resources, a system of mutual obligations can be restructured into an equivalent system that has a cascade structure: there is a group of banks that paid off their debts, another group that owes money only to banks in the first group, and so on. technically, we use the machinery of markov chains to analyze evolution of a deterministic dynamical system.", "categories": "econ.gn q-fin.ec", "created": "2017-05-16", "updated": "2020-07-30", "authors": ["isaac m. sonin", "konstantin sonin"], "url": "https://arxiv.org/abs/1705.05943"}, {"title": "optimal sequential treatment allocation", "id": "1705.09952", "abstract": "in treatment allocation problems the individuals to be treated often arrive sequentially. we study a problem in which the policy maker is not only interested in the expected cumulative welfare but is also concerned about the uncertainty/risk of the treatment outcomes. at the outset, the total number of treatment assignments to be made may even be unknown. a sequential treatment policy which attains the minimax optimal regret is proposed. we also demonstrate that the expected number of suboptimal treatments only grows slowly in the number of treatments. finally, we study a setting where outcomes are only observed with delay.", "categories": "stat.ml econ.em", "created": "2017-05-28", "updated": "2018-08-22", "authors": ["anders bredahl kock", "martin thyrsgaard"], "url": "https://arxiv.org/abs/1705.09952"}, {"title": "sampling-based vs. design-based uncertainty in regression analysis", "id": "1706.01778", "abstract": "consider a researcher estimating the parameters of a regression function based on data for all 50 states in the united states or on data for all visits to a website. what is the interpretation of the estimated parameters and the standard errors? in practice, researchers typically assume that the sample is randomly drawn from a large population of interest and report standard errors that are designed to capture sampling variation. this is common even in applications where it is difficult to articulate what that population of interest is, and how it differs from the sample. in this article, we explore an alternative approach to inference, which is partly design-based. in a design-based setting, the values of some of the regressors can be manipulated, perhaps through a policy intervention. design-based uncertainty emanates from lack of knowledge about the values that the regression outcome would have taken under alternative interventions. we derive standard errors that account for design-based uncertainty instead of, or in addition to, sampling-based uncertainty. we show that our standard errors in general are smaller than the usual infinite-population sampling-based standard errors and provide conditions under which they coincide.", "categories": "math.st econ.em stat.th", "created": "2017-06-06", "updated": "2019-06-21", "authors": ["alberto abadie", "susan athey", "guido w. imbens", "jeffrey m. wooldridge"], "url": "https://arxiv.org/abs/1706.01778"}, {"title": "testing ambiguity and machina preferences within a quantum-theoretic   framework for decision-making", "id": "1706.02168", "abstract": "the machina thought experiments pose to major non-expected utility models challenges that are similar to those posed by the ellsberg thought experiments to subjective expected utility theory (seut). we test human choices in the `ellsberg three-color example', confirming typical ambiguity aversion patterns, and the `machina 50/51 and reflection examples', partially confirming the preferences hypothesized by machina. then, we show that a quantum-theoretic framework for decision-making under uncertainty recently elaborated by some of us allows faithful modeling of all data on the ellsberg and machina paradox situations. in the quantum-theoretic framework subjective probabilities are represented by quantum probabilities, while quantum state transformations enable representations of ambiguity aversion and subjective attitudes toward it.", "categories": "q-fin.ec econ.gn q-fin.mf", "created": "2017-06-06", "updated": "", "authors": ["diederik aerts", "suzette geriente", "catarina moreira", "sandro sozzo"], "url": "https://arxiv.org/abs/1706.02168"}, {"title": "ancillarity-sufficiency interweaving strategy (asis) for boosting mcmc   estimation of stochastic volatility models", "id": "1706.05280", "abstract": "bayesian inference for stochastic volatility models using mcmc methods highly depends on actual parameter values in terms of sampling efficiency. while draws from the posterior utilizing the standard centered parameterization break down when the volatility of volatility parameter in the latent state equation is small, non-centered versions of the model show deficiencies for highly persistent latent variable series. the novel approach of ancillarity-sufficiency interweaving has recently been shown to aid in overcoming these issues for a broad class of multilevel models. in this paper, we demonstrate how such an interweaving strategy can be applied to stochastic volatility models in order to greatly improve sampling efficiency for all parameters and throughout the entire parameter range. moreover, this method of \"combining best of different worlds\" allows for inference for parameter constellations that have previously been infeasible to estimate without the need to select a particular parameterization beforehand.", "categories": "stat.me econ.em stat.ap stat.co", "created": "2017-06-16", "updated": "", "authors": ["gregor kastner", "sylvia fr\u00fchwirth-schnatter"], "url": "https://arxiv.org/abs/1706.05280"}, {"title": "on heckits, late, and numerical equivalence", "id": "1706.05982", "abstract": "structural econometric methods are often criticized for being sensitive to functional form assumptions. we study parametric estimators of the local average treatment effect (late) derived from a widely used class of latent threshold crossing models and show they yield late estimates algebraically equivalent to the instrumental variables (iv) estimator. our leading example is heckman's (1979) two-step (\"heckit\") control function estimator which, with two-sided non-compliance, can be used to compute estimates of a variety of causal parameters. equivalence with iv is established for a semi-parametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. our results suggest differences between structural and iv estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. in cases where equivalence fails, reporting structural estimates of late alongside iv provides a simple means of assessing the credibility of structural extrapolation exercises.", "categories": "stat.me econ.em", "created": "2017-06-19", "updated": "2018-10-24", "authors": ["patrick kline", "christopher r. walters"], "url": "https://arxiv.org/abs/1706.05982"}, {"title": "nonseparable multinomial choice models in cross-section and panel data", "id": "1706.08418", "abstract": "multinomial choice models are fundamental for empirical modeling of economic choices among discrete alternatives. we analyze identification of binary and multinomial choice models when the choice utilities are nonseparable in observed attributes and multidimensional unobserved heterogeneity with cross-section and panel data. we show that derivatives of choice probabilities with respect to continuous attributes are weighted averages of utility derivatives in cross-section models with exogenous heterogeneity. in the special case of random coefficient models with an independent additive effect, we further characterize that the probability derivative at zero is proportional to the population mean of the coefficients. we extend the identification results to models with endogenous heterogeneity using either a control function or panel data. in time stationary panel models with two periods, we find that differences over time of derivatives of choice probabilities identify utility derivatives \"on the diagonal,\" i.e. when the observed attributes take the same values in the two periods. we also show that time stationarity does not identify structural derivatives \"off the diagonal\" both in continuous and multinomial choice panel models.", "categories": "stat.me econ.em stat.ap", "created": "2017-06-26", "updated": "2018-05-09", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "whitney newey"], "url": "https://arxiv.org/abs/1706.08418"}, {"title": "machine-learning tests for effects on multiple outcomes", "id": "1707.01473", "abstract": "in this paper we present tools for applied researchers that re-purpose off-the-shelf methods from the computer-science field of machine learning to create a \"discovery engine\" for data from randomized controlled trials (rcts). the applied problem we seek to solve is that economists invest vast resources into carrying out rcts, including the collection of a rich set of candidate outcome measures. but given concerns about inference in the presence of multiple testing, economists usually wind up exploring just a small subset of the hypotheses that the available data could be used to test. this prevents us from extracting as much information as possible from each rct, which in turn impairs our ability to develop new theories or strengthen the design of policy interventions. our proposed solution combines the basic intuition of reverse regression, where the dependent variable of interest now becomes treatment assignment itself, with methods from machine learning that use the data themselves to flexibly identify whether there is any function of the outcomes that predicts (or has signal about) treatment group status. this leads to correctly-sized tests with appropriate $p$-values, which also have the important virtue of being easy to implement in practice. one open challenge that remains with our work is how to meaningfully interpret the signal that these methods find.", "categories": "stat.ml econ.em stat.ap stat.me", "created": "2017-07-05", "updated": "2019-05-09", "authors": ["jens ludwig", "sendhil mullainathan", "jann spiess"], "url": "https://arxiv.org/abs/1707.01473"}, {"title": "the speed of sequential asymptotic learning", "id": "1707.02689", "abstract": "in the classical herding literature, agents receive a private signal regarding a binary state of nature, and sequentially choose an action, after observing the actions of their predecessors. when the informativeness of private signals is unbounded, it is known that agents converge to the correct action and correct belief. we study how quickly convergence occurs, and show that it happens more slowly than it does when agents observe signals. however, we also show that the speed of learning from actions can be arbitrarily close to the speed of learning from signals. in particular, the expected time until the agents stop taking the wrong action can be either finite or infinite, depending on the private signal distribution. in the canonical case of gaussian private signals we calculate the speed of convergence precisely, and show explicitly that, in this case, learning from actions is significantly slower than learning from signals.", "categories": "math.pr econ.th", "created": "2017-07-09", "updated": "2017-11-24", "authors": ["wade hann-caruthers", "vadim v. martynov", "omer tamuz"], "url": "https://arxiv.org/abs/1707.02689"}, {"title": "smoothed gmm for quantile models", "id": "1707.03436", "abstract": "this paper develops theory for feasible estimators of finite-dimensional parameters identified by general conditional quantile restrictions, under much weaker assumptions than previously seen in the literature. this includes instrumental variables nonlinear quantile regression as a special case. more specifically, we consider a set of unconditional moments implied by the conditional quantile restrictions, providing conditions for local identification. since estimators based on the sample moments are generally impossible to compute numerically in practice, we study feasible estimators based on smoothed sample moments. we propose a method of moments estimator for exactly identified models, as well as a generalized method of moments estimator for over-identified models. we establish consistency and asymptotic normality of both estimators under general conditions that allow for weakly dependent data and nonlinear structural models. simulations illustrate the finite-sample properties of the methods. our in-depth empirical application concerns the consumption euler equation derived from quantile utility maximization. advantages of the quantile euler equation include robustness to fat tails, decoupling of risk attitude from the elasticity of intertemporal substitution, and log-linearization without any approximation error. for the four countries we examine, the quantile estimates of discount factor and elasticity of intertemporal substitution are economically reasonable for a range of quantiles above the median, even when two-stage least squares estimates are not reasonable.", "categories": "math.st econ.em stat.me stat.th", "created": "2017-07-11", "updated": "2018-02-27", "authors": ["luciano de castro", "antonio f. galvao", "david m. kaplan", "xin liu"], "url": "https://arxiv.org/abs/1707.03436"}, {"title": "good signals gone bad: dynamic signalling with switching efforts", "id": "1707.04699", "abstract": "this paper examines signalling when the sender exerts effort and receives benefits over time. receivers only observe a noisy public signal about the effort, which has no intrinsic value.   the modelling of signalling in a dynamic context gives rise to novel equilibrium outcomes. in some equilibria, a sender with a higher cost of effort exerts strictly more effort than his low-cost counterpart. the low-cost type can compensate later for initial low effort, but this is not worthwhile for a high-cost type. the interpretation of a given signal switches endogenously over time, depending on which type the receivers expect to send it.   jel classification: d82, d83, c73.   keywords: dynamic games, signalling , incomplete information", "categories": "q-fin.ec econ.th", "created": "2017-07-15", "updated": "", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1707.04699"}, {"title": "unfolding the innovation system for the development of countries:   co-evolution of science, technology and production", "id": "1707.05146", "abstract": "we show that the space in which scientific, technological and economic developments interplay with each other can be mathematically shaped using pioneering multilayer network and complexity techniques. we build the tri-layered network of human activities (scientific production, patenting, and industrial production) and study the interactions among them, also taking into account the possible time delays. within this construction we can identify which capabilities and prerequisites are needed to be competitive in a given activity, and even measure how much time is needed to transform, for instance, the technological know-how into economic wealth and scientific innovation, being able to make predictions with a very long time horizon. quite unexpectedly, we find empirical evidence that the naive knowledge flow from science, to patents, to products is not supported by data, being instead technology the best predictor for industrial and scientific production for the next decades.", "categories": "econ.gn cs.si physics.soc-ph q-fin.ec", "created": "2017-07-17", "updated": "2017-12-27", "authors": ["emanuele pugliese", "giulio cimini", "aurelio patelli", "andrea zaccaria", "luciano pietronero", "andrea gabrielli"], "url": "https://arxiv.org/abs/1707.05146"}, {"title": "identification of treatment effects under conditional partial   independence", "id": "1707.09563", "abstract": "conditional independence of treatment assignment from potential outcomes is a commonly used but nonrefutable assumption. we derive identified sets for various treatment effect parameters under nonparametric deviations from this conditional independence assumption. these deviations are defined via a conditional treatment assignment probability, which makes it straightforward to interpret. our results can be used to assess the robustness of empirical conclusions obtained under the baseline conditional independence assumption.", "categories": "stat.me econ.em", "created": "2017-07-29", "updated": "", "authors": ["matthew a. masten", "alexandre poirier"], "url": "https://arxiv.org/abs/1707.09563"}, {"title": "technology networks: the autocatalytic origins of innovation", "id": "1708.03511", "abstract": "we analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. to this aim, we define a directed network of technological fields based on the international patents classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. we show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. we further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields.", "categories": "econ.gn cs.si q-fin.ec", "created": "2017-08-11", "updated": "2018-04-19", "authors": ["lorenzo napolitano", "evangelos evangelou", "emanuele pugliese", "paolo zeppini", "graham room"], "url": "https://arxiv.org/abs/1708.03511"}, {"title": "comparing distributions by multiple testing across quantiles or cdf   values", "id": "1708.04658", "abstract": "when comparing two distributions, it is often helpful to learn at which quantiles or values there is a statistically significant difference. this provides more information than the binary \"reject\" or \"do not reject\" decision of a global goodness-of-fit test. framing our question as multiple testing across the continuum of quantiles $\\tau\\in(0,1)$ or values $r\\in\\mathbb{r}$, we show that the kolmogorov--smirnov test (interpreted as a multiple testing procedure) achieves strong control of the familywise error rate. however, its well-known flaw of low sensitivity in the tails remains. we provide an alternative method that retains such strong control of familywise error rate while also having even sensitivity, i.e., equal pointwise type i error rates at each of $n\\to\\infty$ order statistics across the distribution. our one-sample method computes instantly, using our new formula that also instantly computes goodness-of-fit $p$-values and uniform confidence bands. to improve power, we also propose stepdown and pre-test procedures that maintain control of the asymptotic familywise error rate. one-sample and two-sample cases are considered, as well as extensions to regression discontinuity designs and conditional distributions. simulations, empirical examples, and code are provided.", "categories": "math.st econ.em stat.me stat.th", "created": "2017-08-15", "updated": "", "authors": ["matt goldman", "david m. kaplan"], "url": "https://arxiv.org/abs/1708.04658"}, {"title": "economic design of memory-type control charts: the fallacy of the   formula proposed by lorenzen and vance (1986)", "id": "1708.06160", "abstract": "the memory-type control charts, such as ewma and cusum, are powerful tools for detecting small quality changes in univariate and multivariate processes. many papers on economic design of these control charts use the formula proposed by lorenzen and vance (1986) [lorenzen, t. j., & vance, l. c. (1986). the economic design of control charts: a unified approach. technometrics, 28(1), 3-10, doi: 10.2307/1269598]. this paper shows that this formula is not correct for memory-type control charts and its values can significantly deviate from the original values even if the arl values used in this formula are accurately computed. consequently, the use of this formula can result in charts that are not economically optimal. the formula is corrected for memory-type control charts, but unfortunately the modified formula is not a helpful tool from a computational perspective. we show that simulation-based optimization is a possible alternative method.", "categories": "stat.ap cs.ce econ.gn math.oc q-fin.ec", "created": "2017-08-21", "updated": "", "authors": ["amir ahmadi-javid", "mohsen ebadi"], "url": "https://arxiv.org/abs/1708.06160"}, {"title": "unbiased shrinkage estimation", "id": "1708.06436", "abstract": "shrinkage estimation usually reduces variance at the cost of bias. but when we care only about some parameters of a model, i show that we can reduce variance without incurring bias if we have additional information about the distribution of covariates. in a linear regression model with homoscedastic normal noise, i consider shrinkage estimation of the nuisance parameters associated with control variables. for at least three control variables and exogenous treatment, i establish that the standard least-squares estimator is dominated with respect to squared-error loss in the treatment effect even among unbiased estimators and even when the target parameter is low-dimensional. i construct the dominating estimator by a variant of james-stein shrinkage in a high-dimensional normal-means problem. it can be interpreted as an invariant generalized bayes estimator with an uninformative (improper) jeffreys prior in the target parameter.", "categories": "math.st econ.em stat.me stat.th", "created": "2017-08-21", "updated": "2017-10-31", "authors": ["jann spiess"], "url": "https://arxiv.org/abs/1708.06436"}, {"title": "bias reduction in instrumental variable estimation through first-stage   shrinkage", "id": "1708.06443", "abstract": "the two-stage least-squares (2sls) estimator is known to be biased when its first-stage fit is poor. i show that better first-stage prediction can alleviate this bias. in a two-stage linear regression model with normal noise, i consider shrinkage in the estimation of the first-stage instrumental variable coefficients. for at least four instrumental variables and a single endogenous regressor, i establish that the standard 2sls estimator is dominated with respect to bias. the dominating iv estimator applies james-stein type shrinkage in a first-stage high-dimensional normal-means problem followed by a control-function approach in the second stage. it preserves invariances of the structural instrumental variable equations.", "categories": "math.st econ.em stat.me stat.th", "created": "2017-08-21", "updated": "2017-10-31", "authors": ["jann spiess"], "url": "https://arxiv.org/abs/1708.06443"}, {"title": "econom\\'etrie et machine learning", "id": "1708.06992", "abstract": "econometrics and machine learning seem to have one common goal: to construct a predictive model, for a variable of interest, using explanatory variables (or features). however, these two fields developed in parallel, thus creating two different cultures, to paraphrase breiman (2001). the first was to build probabilistic models to describe economic phenomena. the second uses algorithms that will learn from their mistakes, with the aim, most often to classify (sounds, images, etc.). recently, however, learning models have proven to be more effective than traditional econometric techniques (with a price to pay less explanatory power), and above all, they manage to manage much larger data. in this context, it becomes necessary for econometricians to understand what these two cultures are, what opposes them and especially what brings them closer together, in order to appropriate tools developed by the statistical learning community to integrate them into econometric models.", "categories": "stat.ot econ.em", "created": "2017-07-26", "updated": "2018-03-19", "authors": ["arthur charpentier", "emmanuel flachaire", "antoine ly"], "url": "https://arxiv.org/abs/1708.06992"}, {"title": "principal components and regularized estimation of factor models", "id": "1708.08137", "abstract": "it is known that the common factors in a large panel of data can be consistently estimated by the method of principal components, and principal components can be constructed by iterative least squares regressions. replacing least squares with ridge regressions turns out to have the effect of shrinking the singular values of the common component and possibly reducing its rank. the method is used in the machine learning literature to recover low-rank matrices. we study the procedure from the perspective of estimating a minimum-rank approximate factor model. we show that the constrained factor estimates are biased but can be more efficient in terms of mean-squared errors. rank consideration suggests a data-dependent penalty for selecting the number of factors. the new criterion is more conservative in cases when the nominal number of factors is inflated by the presence of weak factors or large measurement noise. the framework is extended to incorporate a priori linear constraints on the loadings. we provide asymptotic results that can be used to test economic hypotheses.", "categories": "stat.me econ.em", "created": "2017-08-27", "updated": "2017-11-13", "authors": ["jushan bai", "serena ng"], "url": "https://arxiv.org/abs/1708.08137"}, {"title": "payoff information and learning in signaling games", "id": "1709.01024", "abstract": "we add the assumption that players know their opponents' payoff functions and rationality to a model of non-equilibrium learning in signaling games. agents are born into player roles and play against random opponents every period. inexperienced agents are uncertain about the prevailing distribution of opponents' play, but believe that opponents never choose conditionally dominated strategies. agents engage in active learning and update beliefs based on personal observations. payoff information can refine or expand learning predictions, since patient young senders' experimentation incentives depend on which receiver responses they deem plausible. we show that with payoff knowledge, the limiting set of long-run learning outcomes is bounded above by rationality-compatible equilibria (rce), and bounded below by uniform rce. rce refine the intuitive criterion (cho and kreps, 1987) and include all divine equilibria (banks and sobel, 1987). uniform rce sometimes but not always exists, and implies universally divine equilibrium.", "categories": "econ.th", "created": "2017-08-31", "updated": "2020-01-14", "authors": ["drew fudenberg", "kevin he"], "url": "https://arxiv.org/abs/1709.01024"}, {"title": "is completeness necessary? estimation in nonidentified linear models", "id": "1709.03473", "abstract": "this paper documents the consequences of the identification failures for a class of linear ill-posed inverse models. the tikhonov-regularized estimator converges to a well-defined limit equal to the best approximation of the structural parameter in the orthogonal complement to the null space of the operator. we illustrate that in many cases the best approximation may coincide with the structural parameter or at least may reasonably approximate it. we characterize the nonasymptotic hilbert space norm and the uniform norm convergence rates for the best approximation. nonidentification has important implications for the large sample distribution of the tikhonov-regularized estimator, and we document the transition between the gaussian and the weighted chi-squared limits. the theoretical results are illustrated for the nonparametric iv and the functional linear iv regressions and are further supported by the monte carlo experiments.", "categories": "math.st econ.em stat.ml stat.th", "created": "2017-09-11", "updated": "2020-04-17", "authors": ["andrii babii", "jean-pierre florens"], "url": "https://arxiv.org/abs/1709.03473"}, {"title": "economic complexity: \"buttarla in caciara\" vs a constructive approach", "id": "1709.05272", "abstract": "this note is a contribution to the debate about the optimal algorithm for economic complexity that recently appeared on arxiv [1, 2] . the authors of [2] eventually agree that the eci+ algorithm [1] consists just in a renaming of the fitness algorithm we introduced in 2012, as we explicitly showed in [3]. however, they omit any comment on the fact that their extensive numerical tests claimed to demonstrate that the same algorithm works well if they name it eci+, but not if its name is fitness. they should realize that this eliminates any credibility to their numerical methods and therefore also to their new analysis, in which they consider many algorithms [2]. since by their own admission the best algorithm is the fitness one, their new claim became that the search for the best algorithm is pointless and all algorithms are alike. this is exactly the opposite of what they claimed a few days ago and it does not deserve much comments. after these clarifications we also present a constructive analysis of the status of economic complexity, its algorithms, its successes and its perspectives. for us the discussion closes here, we will not reply to further comments.", "categories": "econ.gn q-fin.ec", "created": "2017-09-15", "updated": "", "authors": ["luciano pietronero", "matthieu cristelli", "andrea gabrielli", "dario mazzilli", "emanuele pugliese", "andrea tacchella", "andrea zaccaria"], "url": "https://arxiv.org/abs/1709.05272"}, {"title": "counterparty credit limits: an effective tool for mitigating   counterparty risk?", "id": "1709.08238", "abstract": "a counterparty credit limit (ccl) is a limit imposed by a financial institution to cap its maximum possible exposure to a specified counterparty. although ccls are designed to help institutions mitigate counterparty risk by selective diversification of their exposures, their implementation restricts the liquidity that institutions can access in an otherwise centralized pool. we address the question of how this mechanism impacts trade prices and volatility, both empirically and via a new model of trading with ccls. we find empirically that ccls cause little impact on trade. however, our model highlights that in extreme situations, ccls could serve to destabilize prices and thereby influence systemic risk.", "categories": "q-fin.tr econ.em math.pr stat.ap stat.co", "created": "2017-09-24", "updated": "2020-02-04", "authors": ["martin d. gould", "nikolaus hautsch", "sam d. howison", "mason a. porter"], "url": "https://arxiv.org/abs/1709.08238"}, {"title": "fixed effect estimation of large t panel data models", "id": "1709.08980", "abstract": "this article reviews recent advances in fixed effect estimation of panel data models for long panels, where the number of time periods is relatively large. we focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable conditional on covariates and unobserved effects is specified parametrically, while the distribution of the unobserved effects is left unrestricted. compared to existing reviews on long panels (arellano and hahn 2007; a section in arellano and bonhomme 2011) we discuss models with both individual and time effects, split-panel jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects is our main focus, and the unifying theme is that the order of this bias is given by the simple formula p/n for all models discussed, with p the number of estimated parameters and n the total sample size.", "categories": "econ.em", "created": "2017-09-26", "updated": "2018-03-27", "authors": ["iv\u00e1n fern\u00e1ndez-val", "martin weidner"], "url": "https://arxiv.org/abs/1709.08980"}, {"title": "bounds on treatment effects on transitions", "id": "1709.08981", "abstract": "this paper considers the identification of treatment effects on conditional transition probabilities. we show that even under random assignment only the instantaneous average treatment effect is point identified. since treated and control units drop out at different rates, randomization only ensures the comparability of treatment and controls at the time of randomization, so that long-run average treatment effects are not point identified. instead we derive informative bounds on these average treatment effects. our bounds do not impose (semi)parametric restrictions, for example, proportional hazards. we also explore various assumptions such as monotone treatment response, common shocks and positively correlated outcomes that tighten the bounds.", "categories": "econ.em", "created": "2017-09-26", "updated": "", "authors": ["johan vikstr\u00f6m", "geert ridder", "martin weidner"], "url": "https://arxiv.org/abs/1709.08981"}, {"title": "inference on estimators defined by mathematical programming", "id": "1709.09115", "abstract": "we propose an inference procedure for estimators defined by mathematical programming problems, focusing on the important special cases of linear programming (lp) and quadratic programming (qp). in these settings, the coefficients in both the objective function and the constraints of the mathematical programming problem may be estimated from data and hence involve sampling error. our inference approach exploits the characterization of the solutions to these programming problems by complementarity conditions; by doing so, we can transform the problem of doing inference on the solution of a constrained optimization problem (a non-standard inference problem) into one involving inference based on a set of inequalities with pre-estimated coefficients, which is much better understood. we evaluate the performance of our procedure in several monte carlo simulations and an empirical application to the classic portfolio selection problem in finance.", "categories": "econ.em", "created": "2017-09-26", "updated": "", "authors": ["yu-wei hsieh", "xiaoxia shi", "matthew shum"], "url": "https://arxiv.org/abs/1709.09115"}, {"title": "discrete choice and rational inattention: a general equivalence result", "id": "1709.09117", "abstract": "this paper establishes a general equivalence between discrete choice and rational inattention models. matejka and mckay (2015, aer) showed that when information costs are modelled using the shannon entropy function, the resulting choice probabilities in the rational inattention model take the multinomial logit form. by exploiting convex-analytic properties of the discrete choice model, we show that when information costs are modelled using a class of generalized entropy functions, the choice probabilities in any rational inattention model are observationally equivalent to some additive random utility discrete choice model and vice versa. thus any additive random utility model can be given an interpretation in terms of boundedly rational behavior. this includes empirically relevant specifications such as the probit and nested logit models.", "categories": "econ.em cs.it math.it stat.ap", "created": "2017-09-26", "updated": "", "authors": ["mogens fosgerau", "emerson melo", "andre de palma", "matthew shum"], "url": "https://arxiv.org/abs/1709.09117"}, {"title": "sharp bounds and testability of a roy model of stem major choices", "id": "1709.09284", "abstract": "we analyze the empirical content of the roy model, stripped down to its essential features, namely sector specific unobserved heterogeneity and self-selection on the basis of potential outcomes. we characterize sharp bounds on the joint distribution of potential outcomes and testable implications of the roy self-selection model under an instrumental constraint on the joint distribution of potential outcomes we call stochastically monotone instrumental variable (smiv). we show that testing the roy model selection is equivalent to testing stochastic monotonicity of observed outcomes relative to the instrument. we apply our sharp bounds to the derivation of a measure of departure from roy self-selection to identify values of observable characteristics that induce the most costly misallocation of talent and sector and are therefore prime targets for intervention. special emphasis is put on the case of binary outcomes, which has received little attention in the literature to date. for richer sets of outcomes, we emphasize the distinction between pointwise sharp bounds and functional sharp bounds, and its importance, when constructing sharp bounds on functional features, such as inequality measures. we analyze a roy model of college major choice in canada and germany within this framework, and we take a new look at the under-representation of women in~stem.", "categories": "econ.em", "created": "2017-09-26", "updated": "2019-11-08", "authors": ["ismael mourifie", "marc henry", "romuald meango"], "url": "https://arxiv.org/abs/1709.09284"}, {"title": "zero-rating of content and its effect on the quality of service in the   internet", "id": "1709.09334", "abstract": "the ongoing net neutrality debate has generated a lot of heated discussions on whether or not monetary interactions should be regulated between content and access providers. among the several topics discussed, `differential pricing' has recently received attention due to `zero-rating' platforms proposed by some service providers. in the differential pricing scheme, internet service providers (isps) can exempt data access charges for on content from certain cps (zero-rated) while no exemption is on content from other cps. this allows the possibility for content providers (cps) to make `sponsorship' agreements to zero-rate their content and attract more user traffic. in this paper, we study the effect of differential pricing on various players in the internet. we first consider a model with a monopolistic isp and multiple cps where users select cps based on the quality of service (qos) and data access charges. we show that in a differential pricing regime 1) a cp offering low qos can make have higher surplus than a cp offering better qos through sponsorships. 2) overall qos (mean delay) for end users can degrade under differential pricing schemes. in the oligopolistic market with multiple isps, users tend to select the isp with lowest isp resulting in same type of conclusions as in the monopolistic market. we then study how differential pricing effects the revenue of isps.", "categories": "econ.em", "created": "2017-09-27", "updated": "2018-09-17", "authors": ["manjesh k. hanawal", "fehmina malik", "yezekael hayel"], "url": "https://arxiv.org/abs/1709.09334"}, {"title": "a bimodal network approach to model topic dynamics", "id": "1709.09373", "abstract": "this paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the latent dirichlet allocation (lda). the main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. to benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. applying the lda, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.", "categories": "cs.cl econ.gn q-fin.ec", "created": "2017-09-27", "updated": "", "authors": ["luigi di caro", "marco guerzoni", "massimiliano nuccio", "giovanni siragusa"], "url": "https://arxiv.org/abs/1709.09373"}, {"title": "identification of hedonic equilibrium and nonseparable simultaneous   equations", "id": "1709.09570", "abstract": "this paper derives conditions under which preferences and technology are nonparametrically identified in hedonic equilibrium models, where products are differentiated along more than one dimension and agents are characterized by several dimensions of unobserved heterogeneity. with products differentiated along a quality index and agents characterized by scalar unobserved heterogeneity, single crossing conditions on preferences and technology provide identifying restrictions in ekeland, heckman and nesheim (2004) and heckman, matzkin and nesheim (2010). we develop similar shape restrictions in the multi-attribute case. these shape restrictions, which are based on optimal transport theory and generalized convexity, allow us to identify preferences for goods differentiated along multiple dimensions, from the observation of a single market. we thereby derive nonparametric identification results for nonseparable simultaneous equations and multi-attribute hedonic equilibrium models with (possibly) multiple dimensions of unobserved heterogeneity. one of our results is a proof of absolute continuity of the distribution of endogenously traded qualities, which is of independent interest.", "categories": "econ.em", "created": "2017-09-27", "updated": "2020-07-09", "authors": ["victor chernozhukov", "alfred galichon", "marc henry", "brendan pass"], "url": "https://arxiv.org/abs/1709.09570"}, {"title": "inference for impulse responses under model uncertainty", "id": "1709.09583", "abstract": "in many macroeconomic applications, confidence intervals for impulse responses are constructed by estimating var models in levels - ignoring cointegration rank uncertainty. we investigate the consequences of ignoring this uncertainty. we adapt several methods for handling model uncertainty and highlight their shortcomings. we propose a new method - weighted-inference-by-model-plausibility (wimp) - that takes rank uncertainty into account in a data-driven way. in simulations the wimp outperforms all other methods considered, delivering intervals that are robust to rank uncertainty, yet not overly conservative. we also study potential ramifications of rank uncertainty on applied macroeconomic analysis by re-assessing the effects of fiscal policy shocks.", "categories": "econ.em stat.ap stat.me", "created": "2017-09-27", "updated": "2019-10-07", "authors": ["lenard lieb", "stephan smeekes"], "url": "https://arxiv.org/abs/1709.09583"}, {"title": "quasi-random monte carlo application in cge systematic sensitivity   analysis", "id": "1709.09755", "abstract": "the uncertainty and robustness of computable general equilibrium models can be assessed by conducting a systematic sensitivity analysis. different methods have been used in the literature for ssa of cge models such as gaussian quadrature and monte carlo methods. this paper explores the use of quasi-random monte carlo methods based on the halton and sobol' sequences as means to improve the efficiency over regular monte carlo ssa, thus reducing the computational requirements of the ssa. the findings suggest that by using low-discrepancy sequences, the number of simulations required by the regular mc ssa methods can be notably reduced, hence lowering the computational time required for ssa of cge models.", "categories": "econ.em", "created": "2017-09-27", "updated": "", "authors": ["theodoros chatzivasileiadis"], "url": "https://arxiv.org/abs/1709.09755"}, {"title": "estimation of peer effects in endogenous social networks: control   function approach", "id": "1709.10024", "abstract": "we propose a method of estimating the linear-in-means model of peer effects in which the peer group, defined by a social network, is endogenous in the outcome equation for peer effects. endogeneity is due to unobservable individual characteristics that influence both link formation in the network and the outcome of interest. we propose two estimators of the peer effect equation that control for the endogeneity of the social connections using a control function approach. we leave the functional form of the control function unspecified and treat it as unknown. to estimate the model, we use a sieve semiparametric approach, and we establish asymptotics of the semiparametric estimator.", "categories": "econ.em", "created": "2017-09-28", "updated": "2019-07-30", "authors": ["ida johnsson", "hyungsik roger moon"], "url": "https://arxiv.org/abs/1709.10024"}, {"title": "estimation of graphical models using the $l_{1,2}$ norm", "id": "1709.10038", "abstract": "gaussian graphical models are recently used in economics to obtain networks of dependence among agents. a widely-used estimator is the graphical lasso (glasso), which amounts to a maximum likelihood estimation regularized using the $l_{1,1}$ matrix norm on the precision matrix $\\omega$. the $l_{1,1}$ norm is a lasso penalty that controls for sparsity, or the number of zeros in $\\omega$. we propose a new estimator called structured graphical lasso (sglasso) that uses the $l_{1,2}$ mixed norm. the use of the $l_{1,2}$ penalty controls for the structure of the sparsity in $\\omega$. we show that when the network size is fixed, sglasso is asymptotically equivalent to an infeasible glasso problem which prioritizes the sparsity-recovery of high-degree nodes. monte carlo simulation shows that sglasso outperforms glasso in terms of estimating the overall precision matrix and in terms of estimating the structure of the graphical model. in an empirical illustration using a classic firms' investment dataset, we obtain a network of firms' dependence that exhibits the core-periphery structure, with general motors, general electric and u.s. steel forming the core group of firms.", "categories": "econ.em stat.me", "created": "2017-09-28", "updated": "2017-10-01", "authors": ["khai x. chiong", "hyungsik roger moon"], "url": "https://arxiv.org/abs/1709.10038"}, {"title": "forecasting with dynamic panel data models", "id": "1709.10193", "abstract": "this paper considers the problem of forecasting a collection of short time series using cross sectional information in panel data. we construct point predictors using tweedie's formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. this formula utilizes cross-sectional information to transform the unit-specific (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. we show that the risk of a predictor based on a non-parametric estimate of the tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated-random-effects distribution as known (ratio-optimality). our empirical bayes predictor performs well compared to various competitors in a monte carlo study. in an empirical application we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions.", "categories": "econ.em", "created": "2017-09-28", "updated": "", "authors": ["laura liu", "hyungsik roger moon", "frank schorfheide"], "url": "https://arxiv.org/abs/1709.10193"}, {"title": "inference for vars identified with sign restrictions", "id": "1709.10196", "abstract": "there is a fast growing literature that set-identifies structural vector autoregressions (svars) by imposing sign restrictions on the responses of a subset of the endogenous variables to a particular structural shock (sign-restricted svars). most methods that have been used to construct pointwise coverage bands for impulse responses of sign-restricted svars are justified only from a bayesian perspective. this paper demonstrates how to formulate the inference problem for sign-restricted svars within a moment-inequality framework. in particular, it develops methods of constructing confidence bands for impulse response functions of sign-restricted svars that are valid from a frequentist perspective. the paper also provides a comparison of frequentist and bayesian coverage bands in the context of an empirical application - the former can be substantially wider than the latter.", "categories": "econ.em", "created": "2017-09-28", "updated": "2018-02-07", "authors": ["eleonora granziera", "hyungsik roger moon", "frank schorfheide"], "url": "https://arxiv.org/abs/1709.10196"}, {"title": "a note on the multi-agent contracts in continuous time", "id": "1710.00377", "abstract": "dynamic contracts with multiple agents is a classical decentralized decision-making problem with asymmetric information. in this paper, we extend the single-agent dynamic incentive contract model in continuous-time to a multi-agent scheme in finite horizon and allow the terminal reward to be dependent on the history of actions and incentives. we first derive a set of sufficient conditions for the existence of optimal contracts in the most general setting and conditions under which they form a nash equilibrium. then we show that the principal's problem can be converted to solving hamilton-jacobi-bellman (hjb) equation requiring a static nash equilibrium. finally, we provide a framework to solve this problem by solving partial differential equations (pde) derived from backward stochastic differential equations (bsde).", "categories": "econ.em", "created": "2017-10-01", "updated": "2017-10-08", "authors": ["qi luo", "romesh saigal"], "url": "https://arxiv.org/abs/1710.00377"}, {"title": "a justification of conditional confidence intervals", "id": "1710.00643", "abstract": "to quantify uncertainty around point estimates of conditional objects such as conditional means or variances, parameter uncertainty has to be taken into account. attempts to incorporate parameter uncertainty are typically based on the unrealistic assumption of observing two independent processes, where one is used for parameter estimation, and the other for conditioning upon. such unrealistic foundation raises the question whether these intervals are theoretically justified in a realistic setting. this paper presents an asymptotic justification for this type of intervals that does not require such an unrealistic assumption, but relies on a sample-split approach instead. by showing that our sample-split intervals coincide asymptotically with the standard intervals, we provide a novel, and realistic, justification for confidence intervals of conditional objects. the analysis is carried out for a rich class of time series models.", "categories": "econ.em math.st stat.th", "created": "2017-10-02", "updated": "2019-01-19", "authors": ["eric beutner", "alexander heinemann", "stephan smeekes"], "url": "https://arxiv.org/abs/1710.00643"}, {"title": "rate-optimal estimation of the intercept in a semiparametric   sample-selection model", "id": "1710.01423", "abstract": "this paper presents a new estimator of the intercept of a linear regression model in cases where the outcome varaible is observed subject to a selection rule. the intercept is often in this context of inherent interest; for example, in a program evaluation context, the difference between the intercepts in outcome equations for participants and non-participants can be interpreted as the difference in average outcomes of participants and their counterfactual average outcomes if they had chosen not to participate. the new estimator can under mild conditions exhibit a rate of convergence in probability equal to $n^{-p/(2p+1)}$, where $p\\ge 2$ is an integer that indexes the strength of certain smoothness assumptions. this rate of convergence is shown in this context to be the optimal rate of convergence for estimation of the intercept parameter in terms of a minimax criterion. the new estimator, unlike other proposals in the literature, is under mild conditions consistent and asymptotically normal with a rate of convergence that is the same regardless of the degree to which selection depends on unobservables in the outcome equation. simulation evidence and an empirical example are included.", "categories": "econ.em", "created": "2017-10-03", "updated": "2018-09-25", "authors": ["chuan goh"], "url": "https://arxiv.org/abs/1710.01423"}, {"title": "finite time identification in unstable linear systems", "id": "1710.01852", "abstract": "identification of the parameters of stable linear dynamical systems is a well-studied problem in the literature, both in the low and high-dimensional settings. however, there are hardly any results for the unstable case, especially regarding finite time bounds. for this setting, classical results on least-squares estimation of the dynamics parameters are not applicable and therefore new concepts and technical approaches need to be developed to address the issue. unstable linear systems arise in key real applications in control theory, econometrics, and finance. this study establishes finite time bounds for the identification error of the least-squares estimates for a fairly large class of heavy-tailed noise distributions, and transition matrices of such systems. the results relate the time length (samples) required for estimation to a function of the problem dimension and key characteristics of the true underlying transition matrix and the noise distribution. to establish them, appropriate concentration inequalities for random matrices and for sequences of martingale differences are leveraged.", "categories": "cs.sy econ.em eess.sp math.st stat.th", "created": "2017-10-04", "updated": "2018-06-05", "authors": ["mohamad kazem shirani faradonbeh", "ambuj tewari", "george michailidis"], "url": "https://arxiv.org/abs/1710.01852"}, {"title": "a note on gale, kuhn, and tucker's reductions of zero-sum games", "id": "1710.02326", "abstract": "gale, kuhn and tucker (1950) introduced two ways to reduce a zero-sum game by packaging some strategies with respect to a probability distribution on them. in terms of value, they gave conditions for a desirable reduction. we show that a probability distribution for a desirable reduction relies on optimal strategies in the original game. also, we correct an improper example given by them to show that the reverse of a theorem does not hold.", "categories": "econ.em", "created": "2017-10-06", "updated": "", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1710.02326"}, {"title": "when should you adjust standard errors for clustering?", "id": "1710.02926", "abstract": "in empirical work in economics it is common to report standard errors that account for clustering of units. typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. however, because correlation may occur across more than one dimension, this motivation makes it difficult to justify why researchers use clustering in some dimensions, such as geographic, but not others, such as age cohorts or gender. it also makes it difficult to explain why one should not cluster with data from a randomized experiment. in this paper, we argue that clustering is in essence a design problem, either a sampling design or an experimental design issue. it is a sampling design issue if sampling follows a two stage process where in the first stage, a subset of clusters were sampled randomly from a population of clusters, while in the second stage, units were sampled randomly from the sampled clusters. in this case the clustering adjustment is justified by the fact that there are clusters in the population that we do not see in the sample. clustering is an experimental design issue if the assignment is correlated within the clusters. we take the view that this second perspective best fits the typical setting in economics where clustering adjustments are used. this perspective allows us to shed new light on three questions: (i) when should one adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering appropriate, and (iii) when does the conventional adjustment of the standard errors matter.", "categories": "math.st econ.em stat.th", "created": "2017-10-08", "updated": "2017-10-24", "authors": ["alberto abadie", "susan athey", "guido imbens", "jeffrey wooldridge"], "url": "https://arxiv.org/abs/1710.02926"}, {"title": "a unified approach on the local power of panel unit root tests", "id": "1710.02944", "abstract": "in this paper, a unified approach is proposed to derive the exact local asymptotic power for panel unit root tests, which is one of the most important issues in nonstationary panel data literature. two most widely used panel unit root tests known as levin-lin-chu (llc, levin, lin and chu (2002)) and im-pesaran-shin (ips, im, pesaran and shin (2003)) tests are systematically studied for various situations to illustrate our method. our approach is characteristic function based, and can be used directly in deriving the moments of the asymptotic distributions of these test statistics under the null and the local-to-unity alternatives. for the llc test, the approach provides an alternative way to obtain the results that can be derived by the existing method. for the ips test, the new results are obtained, which fills the gap in the literature where few results exist, since the ips test is non-admissible. moreover, our approach has the advantage in deriving edgeworth expansions of these tests, which are also given in the paper. the simulations are presented to illustrate our theoretical findings.", "categories": "econ.em stat.co stat.me", "created": "2017-10-09", "updated": "", "authors": ["zhongwen liang"], "url": "https://arxiv.org/abs/1710.02944"}, {"title": "forecasting across time series databases using recurrent neural networks   on groups of similar series: a clustering approach", "id": "1710.03222", "abstract": "with the advent of big data, nowadays in many applications databases containing large quantities of similar time series are available. forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. recurrent neural networks (rnns), and in particular long short-term memory (lstm) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context when trained across all available time series. however, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. to this end, we present a prediction model that can be used with different types of rnn models on subgroups of similar time series, which are identified by time series clustering techniques. we assess our proposed methodology using lstm networks, a widely popular rnn variant. our method achieves competitive results on benchmarking datasets under competition evaluation procedures. in particular, in terms of mean smape accuracy, it consistently outperforms the baseline lstm model and outperforms all other methods on the cif2016 forecasting competition dataset.", "categories": "cs.lg cs.db econ.em stat.ap stat.ml", "created": "2017-10-09", "updated": "2018-09-12", "authors": ["kasun bandara", "christoph bergmeir", "slawek smyl"], "url": "https://arxiv.org/abs/1710.03222"}, {"title": "inference on auctions with weak assumptions on information", "id": "1710.03830", "abstract": "given a sample of bids from independent auctions, this paper examines the question of inference on auction fundamentals (e.g. valuation distributions, welfare measures) under weak assumptions on information structure. the question is important as it allows us to learn about the valuation distribution in a robust way, i.e., without assuming that a particular information structure holds across observations. we leverage the recent contributions of \\cite{bergemann2013} in the robust mechanism design literature that exploit the link between bayesian correlated equilibria and bayesian nash equilibria in incomplete information games to construct an econometrics framework for learning about auction fundamentals using observed data on bids. we showcase our construction of identified sets in private value and common value auctions. our approach for constructing these sets inherits the computational simplicity of solving for correlated equilibria: checking whether a particular valuation distribution belongs to the identified set is as simple as determining whether a {\\it linear} program is feasible. a similar linear program can be used to construct the identified set on various welfare measures and counterfactual objects. for inference and to summarize statistical uncertainty, we propose novel finite sample methods using tail inequalities that are used to construct confidence regions on sets. we also highlight methods based on bayesian bootstrap and subsampling. a set of monte carlo experiments show adequate finite sample properties of our inference procedures. we illustrate our methods using data from ocs auctions.", "categories": "econ.em cs.gt cs.lg math.st stat.th", "created": "2017-10-10", "updated": "2018-03-19", "authors": ["vasilis syrgkanis", "elie tamer", "juba ziani"], "url": "https://arxiv.org/abs/1710.03830"}, {"title": "revenue-based attribution modeling for online advertising", "id": "1710.06561", "abstract": "this paper examines and proposes several attribution modeling methods that quantify how revenue should be attributed to online advertising inputs. we adopt and further develop relative importance method, which is based on regression models that have been extensively studied and utilized to investigate the relationship between advertising efforts and market reaction (revenue). relative importance method aims at decomposing and allocating marginal contributions to the coefficient of determination (r^2) of regression models as attribution values. in particular, we adopt two alternative submethods to perform this decomposition: dominance analysis and relative weight analysis. moreover, we demonstrate an extension of the decomposition methods from standard linear model to additive model. we claim that our new approaches are more flexible and accurate in modeling the underlying relationship and calculating the attribution values. we use simulation examples to demonstrate the superior performance of our new approaches over traditional methods. we further illustrate the value of our proposed approaches using a real advertising campaign dataset.", "categories": "econ.em stat.ml", "created": "2017-10-17", "updated": "", "authors": ["kaifeng zhao", "seyed hanif mahboobi", "saeed bagheri"], "url": "https://arxiv.org/abs/1710.06561"}, {"title": "minimax linear estimation at a boundary point", "id": "1710.06809", "abstract": "this paper characterizes the minimax linear estimator of the value of an unknown function at a boundary point of its domain in a gaussian white noise model under the restriction that the first-order derivative of the unknown function is lipschitz continuous (the second-order h\\\"{o}lder class). the result is then applied to construct the minimax optimal estimator for the regression discontinuity design model, where the parameter of interest involves function values at boundary points.", "categories": "econ.em math.st stat.th", "created": "2017-10-18", "updated": "", "authors": ["wayne yuan gao"], "url": "https://arxiv.org/abs/1710.06809"}, {"title": "modal regression using kernel density estimation: a review", "id": "1710.07004", "abstract": "we review recent advances in modal regression studies using kernel density estimation. modal regression is an alternative approach for investigating relationship between a response variable and its covariates. specifically, modal regression summarizes the interactions between the response variable and covariates using the conditional mode or local modes. we first describe the underlying model of modal regression and its estimators based on kernel density estimation. we then review the asymptotic properties of the estimators and strategies for choosing the smoothing bandwidth. we also discuss useful algorithms and similar alternative approaches for modal regression, and propose future direction in this field.", "categories": "stat.me econ.em", "created": "2017-10-19", "updated": "2017-12-06", "authors": ["yen-chi chen"], "url": "https://arxiv.org/abs/1710.07004"}, {"title": "electricity market theory based on continuous time commodity model", "id": "1710.07918", "abstract": "the recent research report of u.s. department of energy prompts us to re-examine the pricing theories applied in electricity market design. the theory of spot pricing is the basis of electricity market design in many countries, but it has two major drawbacks: one is that it is still based on the traditional hourly scheduling/dispatch model, ignores the crucial time continuity in electric power production and consumption and does not treat the inter-temporal constraints seriously; the second is that it assumes that the electricity products are homogeneous in the same dispatch period and cannot distinguish the base, intermediate and peak power with obviously different technical and economic characteristics. to overcome the shortcomings, this paper presents a continuous time commodity model of electricity, including spot pricing model and load duration model. the market optimization models under the two pricing mechanisms are established with the riemann and lebesgue integrals respectively and the functional optimization problem are solved by the euler-lagrange equation to obtain the market equilibria. the feasibility of pricing according to load duration is proved by strict mathematical derivation. simulation results show that load duration pricing can correctly identify and value different attributes of generators, reduce the total electricity purchasing cost, and distribute profits among the power plants more equitably. the theory and methods proposed in this paper will provide new ideas and theoretical foundation for the development of electric power markets.", "categories": "econ.em q-fin.gn", "created": "2017-10-22", "updated": "", "authors": ["haoyong chen", "lijia han"], "url": "https://arxiv.org/abs/1710.07918"}, {"title": "existence in multidimensional screening with general nonlinear   preferences", "id": "1710.08549", "abstract": "we generalize the approach of carlier (2001) and provide an existence proof for the multidimensional screening problem with general nonlinear preferences. we first formulate the principal's problem as a maximization problem with $g$-convexity constraints and then use $g$-convex analysis to prove existence.", "categories": "econ.em math.oc", "created": "2017-10-23", "updated": "2018-12-07", "authors": ["kelvin shuangjian zhang"], "url": "https://arxiv.org/abs/1710.08549"}, {"title": "propensity score matching for multiple treatment levels: a coda-based   contribution", "id": "1710.08558", "abstract": "this study proposes a simple technique for propensity score matching for multiple treatment levels under the strong unconfoundedness assumption with the help of the aitchison distance proposed in the field of compositional data analysis (coda).", "categories": "econ.em", "created": "2017-10-23", "updated": "", "authors": ["hajime seya", "takahiro yoshida"], "url": "https://arxiv.org/abs/1710.08558"}, {"title": "calibration of machine learning classifiers for probability of default   modelling", "id": "1710.08901", "abstract": "binary classification is highly used in credit scoring in the estimation of probability of default. the validation of such predictive models is based both on rank ability, and also on calibration (i.e. how accurately the probabilities output by the model map to the observed probabilities). in this study we cover the current best practices regarding calibration for binary classification, and explore how different approaches yield different results on real world credit scoring data. the limitations of evaluating credit scoring models using only rank ability metrics are explored. a benchmark is run on 18 real world datasets, and results compared. the calibration techniques used are platt scaling and isotonic regression. also, different machine learning models are used: logistic regression, random forest classifiers, and gradient boosting classifiers. results show that when the dataset is treated as a time series, the use of re-calibration with isotonic regression is able to improve the long term calibration better than the alternative methods. using re-calibration, the non-parametric models are able to outperform the logistic regression on brier score loss.", "categories": "econ.em stat.ml", "created": "2017-10-24", "updated": "", "authors": ["pedro g. fonseca", "hugo d. lopes"], "url": "https://arxiv.org/abs/1710.08901"}, {"title": "asymptotic distribution and simultaneous confidence bands for ratios of   quantile functions", "id": "1710.09009", "abstract": "ratio of medians or other suitable quantiles of two distributions is widely used in medical research to compare treatment and control groups or in economics to compare various economic variables when repeated cross-sectional data are available. inspired by the so-called growth incidence curves introduced in poverty research, we argue that the ratio of quantile functions is a more appropriate and informative tool to compare two distributions. we present an estimator for the ratio of quantile functions and develop corresponding simultaneous confidence bands, which allow to assess significance of certain features of the quantile functions ratio. derived simultaneous confidence bands rely on the asymptotic distribution of the quantile functions ratio and do not require re-sampling techniques. the performance of the simultaneous confidence bands is demonstrated in simulations. analysis of the expenditure data from uganda in years 1999, 2002 and 2005 illustrates the relevance of our approach.", "categories": "stat.me econ.em stat.ap", "created": "2017-10-24", "updated": "", "authors": ["fabian dunker", "stephan klasen", "tatyana krivobokova"], "url": "https://arxiv.org/abs/1710.09009"}, {"title": "shape-constrained density estimation via optimal transport", "id": "1710.09069", "abstract": "constraining the maximum likelihood density estimator to satisfy a sufficiently strong constraint, $\\log-$concavity being a common example, has the effect of restoring consistency without requiring additional parameters. since many results in economics require densities to satisfy a regularity condition, these estimators are also attractive for the structural estimation of economic models. in all of the examples of regularity conditions provided by bagnoli and bergstrom (2005) and ewerhart (2013), $\\log-$concavity is sufficient to ensure that the density satisfies the required conditions. however, in many cases $\\log-$concavity is far from necessary, and it has the unfortunate side effect of ruling out sub-exponential tail behavior.   in this paper, we use optimal transport to formulate a shape constrained density estimator. we initially describe the estimator using a $\\rho-$concavity constraint. in this setting we provide results on consistency, asymptotic distribution, convexity of the optimization problem defining the estimator, and formulate a test for the null hypothesis that the population density satisfies a shape constraint. afterward, we provide sufficient conditions for these results to hold using an arbitrary shape constraint. this generalization is used to explore whether the california department of transportation's decision to award construction contracts with the use of a first price auction is cost minimizing. we estimate the marginal costs of construction firms subject to myerson's (1981) regularity condition, which is a requirement for the first price reverse auction to be cost minimizing. the proposed test fails to reject that the regularity condition is satisfied.", "categories": "econ.em stat.me", "created": "2017-10-25", "updated": "2018-11-22", "authors": ["ryan cumings-menon"], "url": "https://arxiv.org/abs/1710.09069"}, {"title": "calibrated projection in matlab: users' manual", "id": "1710.09707", "abstract": "we present the calibrated-projection matlab package implementing the method to construct confidence intervals proposed by kaido, molinari and stoye (2017). this manual provides details on how to use the package for inference on projections of partially identified parameters. it also explains how to use the matlab functions we developed to compute confidence intervals on solutions of nonlinear optimization problems with estimated constraints.", "categories": "econ.em stat.co", "created": "2017-10-24", "updated": "", "authors": ["hiroaki kaido", "francesca molinari", "j\u00f6rg stoye", "matthew thirkettle"], "url": "https://arxiv.org/abs/1710.09707"}, {"title": "matrix completion methods for causal panel data models", "id": "1710.10251", "abstract": "in this paper we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. we develop a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \"missing\" elements of the control outcome matrix, corresponding to treated units/periods. the approach estimates a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. we generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure. we present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. we show that all these estimators can be viewed as focusing on the same objective function. they differ in the way they deal with lack of identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). proposed method outperforms unconfoundedness-based or synthetic control estimators.", "categories": "math.st econ.em stat.th", "created": "2017-10-27", "updated": "2020-06-23", "authors": ["susan athey", "mohsen bayati", "nikolay doudchenko", "guido imbens", "khashayar khosravi"], "url": "https://arxiv.org/abs/1710.10251"}, {"title": "artificial intelligence as structural estimation: economic   interpretations of deep blue, bonanza, and alphago", "id": "1710.10967", "abstract": "artificial intelligence (ai) has achieved superhuman performance in a growing number of tasks, but understanding and explaining ai remain challenging. this paper clarifies the connections between machine-learning algorithms to develop ais and the econometrics of dynamic structural models through the case studies of three famous game ais. chess-playing deep blue is a calibrated value function, whereas shogi-playing bonanza is an estimated value function via rust's (1987) nested fixed-point method. alphago's \"supervised-learning policy network\" is a deep neural network implementation of hotz and miller's (1993) conditional choice probability estimation; its \"reinforcement-learning value network\" is equivalent to hotz, miller, sanders, and smith's (1994) conditional choice simulation method. relaxing these ais' implicit econometric assumptions would improve their structural interpretability.", "categories": "econ.em cs.ai cs.lg", "created": "2017-10-30", "updated": "2018-03-01", "authors": ["mitsuru igami"], "url": "https://arxiv.org/abs/1710.10967"}, {"title": "nonparametric identification in index models of link formation", "id": "1710.11230", "abstract": "we consider an index model of dyadic link formation with a homophily effect index and a degree heterogeneity index. we provide nonparametric identification results in a single large network setting for the potentially nonparametric homophily effect function, the realizations of unobserved individual fixed effects and the unknown distribution of idiosyncratic pairwise shocks, up to normalization, for each possible true value of the unknown parameters. we propose a novel form of scale normalization on an arbitrary interquantile range, which is not only theoretically robust but also proves particularly convenient for the identification analysis, as quantiles provide direct linkages between the observable conditional probabilities and the unknown index values. we then use an inductive \"in-fill and out-expansion\" algorithm to establish our main results, and consider extensions to more general settings that allow nonseparable dependence between homophily and degree heterogeneity, as well as certain extents of network sparsity and weaker assumptions on the support of unobserved heterogeneity. as a byproduct, we also propose a concept called \"modeling equivalence\" as a refinement of \"observational equivalence\", and use it to provide a formal discussion about normalization, identification and their interplay with counterfactuals.", "categories": "econ.em", "created": "2017-10-30", "updated": "2018-05-15", "authors": ["wayne yuan gao"], "url": "https://arxiv.org/abs/1710.11230"}, {"title": "macroeconomics and fintech: uncovering latent macroeconomic effects on   peer-to-peer lending", "id": "1710.11283", "abstract": "peer-to-peer (p2p) lending is a fast growing financial technology (fintech) trend that is displacing traditional retail banking. studies on p2p lending have focused on predicting individual interest rates or default probabilities. however, the relationship between aggregated p2p interest rates and the general economy will be of interest to investors and borrowers as the p2p credit market matures. we show that the variation in p2p interest rates across grade types are determined by three macroeconomic latent factors formed by canonical correlation analysis (cca) - macro default, investor uncertainty, and the fundamental value of the market. however, the variation in p2p interest rates across term types cannot be explained by the general economy.", "categories": "econ.em stat.ap", "created": "2017-10-30", "updated": "", "authors": ["jessica foo", "lek-heng lim", "ken sze-wai wong"], "url": "https://arxiv.org/abs/1710.11283"}, {"title": "on some further properties and application of weibull-r family of   distributions", "id": "1711.00171", "abstract": "in this paper, we provide some new results for the weibull-r family of distributions (alzaghal, ghosh and alzaatreh (2016)). we derive some new structural properties of the weibull-r family of distributions. we provide various characterizations of the family via conditional moments, some functions of order statistics and via record values.", "categories": "math.st econ.em stat.th", "created": "2017-10-31", "updated": "", "authors": ["indranil ghosh", "saralees nadarajah"], "url": "https://arxiv.org/abs/1711.00171"}, {"title": "orthogonal machine learning: power and limitations", "id": "1711.00342", "abstract": "double machine learning provides $\\sqrt{n}$-consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance parameters are estimated at an $n^{-1/4}$ rate. the key is to employ neyman-orthogonal moment equations which are first-order insensitive to perturbations in the nuisance parameters. we show that the $n^{-1/4}$ requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order notion of orthogonality that grants robustness to more complex or higher-dimensional nuisance parameters. in the partially linear regression setting popular in causal inference, we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed. our proof relies on stein's lemma and may be of independent interest. we conclude by demonstrating the robustness benefits of an explicit doubly-orthogonal estimation procedure for treatment effect.", "categories": "cs.lg econ.em math.st stat.ml stat.th", "created": "2017-11-01", "updated": "2018-08-01", "authors": ["lester mackey", "vasilis syrgkanis", "ilias zadik"], "url": "https://arxiv.org/abs/1711.00342"}, {"title": "sophisticated and small versus simple and sizeable: when does it pay off   to introduce drifting coefficients in bayesian vars?", "id": "1711.00564", "abstract": "we assess the relationship between model size and complexity in the time-varying parameter var framework via thorough predictive exercises for the euro area, the united kingdom and the united states. it turns out that sophisticated dynamics through drifting coefficients are important in small data sets while simpler models tend to perform better in sizeable data sets. to combine best of both worlds, novel shrinkage priors help to mitigate the curse of dimensionality, resulting in competitive forecasts for all scenarios considered. furthermore, we discuss dynamic model selection to improve upon the best performing individual model for each point in time.", "categories": "stat.me econ.em stat.ap stat.co", "created": "2017-11-01", "updated": "2017-11-29", "authors": ["martin feldkircher", "florian huber", "gregor kastner"], "url": "https://arxiv.org/abs/1711.00564"}, {"title": "startups and stanford university", "id": "1711.00644", "abstract": "startups have become in less than 50 years a major component of innovation and economic growth. silicon valley has been the place where the startup phenomenon was the most obvious and stanford university was a major component of that success. companies such as google, yahoo, sun microsystems, cisco, hewlett packard had very strong links with stanford but even these vary famous success stories cannot fully describe the richness and diversity of the stanford entrepreneurial activity. this report explores the dynamics of more than 5000 companies founded by stanford university alumni and staff, through their value creation, their field of activities, their growth patterns and more. the report also explores some features of the founders of these companies such as their academic background or the number of years between their stanford experience and their company creation.", "categories": "econ.em", "created": "2017-11-02", "updated": "", "authors": ["herv\u00e9 lebret"], "url": "https://arxiv.org/abs/1711.00644"}, {"title": "equity in startups", "id": "1711.00661", "abstract": "startups have become in less than 50 years a major component of innovation and economic growth. an important feature of the startup phenomenon has been the wealth created through equity in startups to all stakeholders. these include the startup founders, the investors, and also the employees through the stock-option mechanism and universities through licenses of intellectual property. in the employee group, the allocation to important managers like the chief executive, vice-presidents and other officers, and independent board members is also analyzed. this report analyzes how equity was allocated in more than 400 startups, most of which had filed for an initial public offering. the author has the ambition of informing a general audience about best practice in equity split, in particular in silicon valley, the central place for startup innovation.", "categories": "econ.em", "created": "2017-11-02", "updated": "", "authors": ["herv\u00e9 lebret"], "url": "https://arxiv.org/abs/1711.00661"}, {"title": "identification with latent choice sets", "id": "1711.02048", "abstract": "in a common experimental format, individuals are randomly assigned to either a treatment group with access to a program or a control group without access. in such experiments, analyzing the average effects of the treatment of program access may be hindered by the problem that some control individuals do not comply with their assigned status and receive program access from outside the experiment. available tools to account for such a problem typically require the researcher to observe the receipt of program access for every individual. however, in many experiments, this is not the case as data is not collected on where any individual received access. in this paper, i develop a framework to show how data on only each individual's treatment assignment status, program participation decision and outcome can be exploited to learn about the average effects of program access. i propose a nonparametric selection model with latent choice sets to relate where access was received to the treatment assignment status, participation decision and outcome, and a linear programming procedure to compute the identified set for parameters evaluating the average effects of program access in this model. i illustrate the framework by analyzing the average effects of head start preschool access using the head start impact study. i find that the provision of head start access induces parents to enroll their child into head start and also positively impacts test scores, and that these effects heterogeneously depend on the availability of access to an alternative preschool.", "categories": "econ.em", "created": "2017-11-06", "updated": "2019-08-08", "authors": ["vishal kamat"], "url": "https://arxiv.org/abs/1711.02048"}, {"title": "semiparametric estimation of structural functions in nonseparable   triangular models", "id": "1711.02184", "abstract": "triangular systems with nonadditively separable unobserved heterogeneity provide a theoretically appealing framework for the modelling of complex structural relationships. however, they are not commonly used in practice due to the need for exogenous variables with large support for identification, the curse of dimensionality in estimation, and the lack of inferential tools. this paper introduces two classes of semiparametric nonseparable triangular models that address these limitations. they are based on distribution and quantile regression modelling of the reduced form conditional distributions of the endogenous variables. we show that average, distribution and quantile structural functions are identified in these systems through a control function approach that does not require a large support condition. we propose a computationally attractive three-stage procedure to estimate the structural functions where the first two stages consist of quantile or distribution regressions. we provide asymptotic theory and uniform inference methods for each stage. in particular, we derive functional central limit theorems and bootstrap functional central limit theorems for the distribution regression estimators of the structural functions. these results establish the validity of the bootstrap for three-stage estimators of structural functions, and lead to simple inference algorithms. we illustrate the implementation and applicability of all our methods with numerical simulations and an empirical application to demand analysis.", "categories": "econ.em stat.me", "created": "2017-11-06", "updated": "2019-10-05", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "whitney newey", "sami stouli", "francis vella"], "url": "https://arxiv.org/abs/1711.02184"}, {"title": "in search of a new economic model determined by logistic growth", "id": "1711.02625", "abstract": "in this paper we extend the work by ryuzo sato devoted to the development of economic growth models within the framework of the lie group theory. we propose a new growth model based on the assumption of logistic growth in factors. it is employed to derive new production functions and introduce a new notion of wage share. in the process it is shown that the new functions compare reasonably well against relevant economic data. the corresponding problem of maximization of profit under conditions of perfect competition is solved with the aid of one of these functions. in addition, it is explained in reasonably rigorous mathematical terms why bowley's law no longer holds true in post-1960 data.", "categories": "math.gr econ.em", "created": "2017-11-07", "updated": "2018-10-22", "authors": ["roman g. smirnov", "kunpeng wang"], "url": "https://arxiv.org/abs/1711.02625"}, {"title": "identification and estimation of spillover effects in randomized   experiments", "id": "1711.02745", "abstract": "i study identification, estimation and inference for spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group. i show that the commonly-used linear-in-means regression identifies a weighted sum of spillover effects with some negative weights, and characterize the estimand that is recovered by a simple difference in means in the presence of spillovers. my results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation. i then propose nonparametric estimators that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. i show that these results can be used to rank assignment mechanisms for experimental design. these findings are illustrated using data from a conditional cash transfer program and with simulations.", "categories": "econ.em", "created": "2017-11-07", "updated": "2020-07-23", "authors": ["gonzalo vazquez-bare"], "url": "https://arxiv.org/abs/1711.02745"}, {"title": "measuring price discovery between nearby and deferred contracts in   storable and non-storable commodity futures markets", "id": "1711.03506", "abstract": "futures market contracts with varying maturities are traded concurrently and the speed at which they process information is of value in understanding the pricing discovery process. using price discovery measures, including putnins (2013) information leadership share and intraday data, we quantify the proportional contribution of price discovery between nearby and deferred contracts in the corn and live cattle futures markets. price discovery is more systematic in the corn than in the live cattle market. on average, nearby contracts lead all deferred contracts in price discovery in the corn market, but have a relatively less dominant role in the live cattle market. in both markets, the nearby contract loses dominance when its relative volume share dips below 50%, which occurs about 2-3 weeks before expiration in corn and 5-6 weeks before expiration in live cattle. regression results indicate that the share of price discovery is most closely linked to trading volume but is also affected, to far less degree, by time to expiration, backwardation, usda announcements and market crashes. the effects of these other factors vary between the markets which likely reflect the difference in storability as well as other market-related characteristics.", "categories": "econ.em", "created": "2017-11-09", "updated": "", "authors": ["zhepeng hu", "mindy mallory", "teresa serra", "philip garcia"], "url": "https://arxiv.org/abs/1711.03506"}, {"title": "shopper: a probabilistic model of consumer choice with substitutes and   complements", "id": "1711.03560", "abstract": "we develop shopper, a sequential probabilistic model of shopping data. shopper uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed shopper to capture how items interact with other items. we develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. we are interested in answering counterfactual queries about changes in prices. we found that shopper provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products.", "categories": "stat.ml cs.lg econ.em", "created": "2017-11-09", "updated": "2019-06-09", "authors": ["francisco j. r. ruiz", "susan athey", "david m. blei"], "url": "https://arxiv.org/abs/1711.03560"}, {"title": "testing for observation-dependent regime switching in mixture   autoregressive models", "id": "1711.03959", "abstract": "testing for regime switching when the regime switching probabilities are specified either as constants (`mixture models') or are governed by a finite-state markov chain (`markov switching models') are long-standing problems that have also attracted recent interest. this paper considers testing for regime switching when the regime switching probabilities are time-varying and depend on observed data (`observation-dependent regime switching'). specifically, we consider the likelihood ratio test for observation-dependent regime switching in mixture autoregressive models. the testing problem is highly nonstandard, involving unidentified nuisance parameters under the null, parameters on the boundary, singular information matrices, and higher-order approximations of the log-likelihood. we derive the asymptotic null distribution of the likelihood ratio test statistic in a general mixture autoregressive setting using high-level conditions that allow for various forms of dependence of the regime switching probabilities on past observations, and we illustrate the theory using two particular mixture autoregressive models. the likelihood ratio test has a nonstandard asymptotic distribution that can easily be simulated, and monte carlo studies show the test to have satisfactory finite sample size and power properties.", "categories": "econ.em math.st stat.me stat.th", "created": "2017-11-10", "updated": "", "authors": ["mika meitz", "pentti saikkonen"], "url": "https://arxiv.org/abs/1711.03959"}, {"title": "how fragile are information cascades?", "id": "1711.04024", "abstract": "it is well known that sequential decision making may lead to information cascades. that is, when agents make decisions based on their private information, as well as observing the actions of those before them, then it might be rational to ignore their private signal and imitate the action of previous individuals. if the individuals are choosing between a right and a wrong state, and the initial actions are wrong, then the whole cascade will be wrong. this issue is due to the fact that cascades can be based on very little information.   we show that if agents occasionally disregard the actions of others and base their action only on their private information, then wrong cascades can be avoided. moreover, we study the optimal asymptotic rate at which the error probability at time $t$ can go to zero. the optimal policy is for the player at time $t$ to follow their private information with probability $p_{t} = c/t$, leading to a learning rate of $c'/t$, where the constants $c$ and $c'$ are explicit.", "categories": "math.pr cs.gt cs.si econ.em", "created": "2017-11-10", "updated": "2018-02-21", "authors": ["yuval peres", "miklos z. racz", "allan sly", "izabella stuhl"], "url": "https://arxiv.org/abs/1711.04024"}, {"title": "uniform inference for characteristic effects of large continuous-time   linear models", "id": "1711.04392", "abstract": "we consider continuous-time models with a large panel of moment conditions, where the structural parameter depends on a set of characteristics, whose effects are of interest. the leading example is the linear factor model in financial economics where factor betas depend on observed characteristics such as firm specific instruments and macroeconomic variables, and their effects pick up long-run time-varying beta fluctuations. we specify the factor betas as the sum of characteristic effects and an orthogonal idiosyncratic parameter that captures high-frequency movements. it is often the case that researchers do not know whether or not the latter exists, or its strengths, and thus the inference about the characteristic effects should be valid uniformly over a broad class of data generating processes for idiosyncratic parameters. we construct our estimation and inference in a two-step continuous-time gmm framework. it is found that the limiting distribution of the estimated characteristic effects has a discontinuity when the variance of the idiosyncratic parameter is near the boundary (zero), which makes the usual \"plug-in\" method using the estimated asymptotic variance only valid pointwise and may produce either over- or under- coveraging probabilities. we show that the uniformity can be achieved by cross-sectional bootstrap. our procedure allows both known and estimated factors, and also features a bias correction for the effect of estimating unknown factors.", "categories": "econ.em stat.me", "created": "2017-11-12", "updated": "2018-12-02", "authors": ["yuan liao", "xiye yang"], "url": "https://arxiv.org/abs/1711.04392"}, {"title": "improved density and distribution function estimation", "id": "1711.04793", "abstract": "given additional distributional information in the form of moment restrictions, kernel density and distribution function estimators with implied generalised empirical likelihood probabilities as weights achieve a reduction in variance due to the systematic use of this extra information. the particular interest here is the estimation of densities or distributions of (generalised) residuals in semi-parametric models defined by a finite number of moment restrictions. such estimates are of great practical interest, being potentially of use for diagnostic purposes, including tests of parametric assumptions on an error distribution, goodness-of-fit tests or tests of overidentifying moment restrictions. the paper gives conditions for the consistency and describes the asymptotic mean squared error properties of the kernel density and distribution estimators proposed in the paper. a simulation study evaluates the small sample performance of these estimators. supplements provide analytic examples to illustrate situations where kernel weighting provides a reduction in variance together with proofs of the results in the paper.", "categories": "stat.me econ.em", "created": "2017-11-13", "updated": "2018-06-20", "authors": ["vitaliy oryshchenko", "richard j. smith"], "url": "https://arxiv.org/abs/1711.04793"}, {"title": "calibration of distributionally robust empirical optimization models", "id": "1711.06565", "abstract": "we study the out-of-sample properties of robust empirical optimization problems with smooth $\\phi$-divergence penalties and smooth concave objective functions, and develop a theory for data-driven calibration of the non-negative \"robustness parameter\" $\\delta$ that controls the size of the deviations from the nominal model. building on the intuition that robust optimization reduces the sensitivity of the expected reward to errors in the model by controlling the spread of the reward distribution, we show that the first-order benefit of ``little bit of robustness\" (i.e., $\\delta$ small, positive) is a significant reduction in the variance of the out-of-sample reward while the corresponding impact on the mean is almost an order of magnitude smaller. one implication is that substantial variance (sensitivity) reduction is possible at little cost if the robustness parameter is properly calibrated. to this end, we introduce the notion of a robust mean-variance frontier to select the robustness parameter and show that it can be approximated using resampling methods like the bootstrap. our examples show that robust solutions resulting from \"open loop\" calibration methods (e.g., selecting a $90\\%$ confidence level regardless of the data and objective function) can be very conservative out-of-sample, while those corresponding to the robustness parameter that optimizes an estimate of the out-of-sample expected reward (e.g., via the bootstrap) with no regard for the variance are often insufficiently robust.", "categories": "stat.ml cs.sy econ.em eess.sy q-fin.pm", "created": "2017-11-17", "updated": "2020-05-18", "authors": ["jun-ya gotoh", "michael jong kim", "andrew e. b. lim"], "url": "https://arxiv.org/abs/1711.06565"}, {"title": "robust synthetic control", "id": "1711.06940", "abstract": "we present a robust generalization of the synthetic control method for comparative case studies. like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. a distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. to begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. we provide the first finite sample analysis for a broader class of models, the latent variable model, in contrast to factor models previously considered in the literature. further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \\omega( t^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $t$ is the time interval of interest. under the same setting, we prove that the mean-squared-error (mse) in our prediction estimation scales as $o(\\sigma^2/p + 1/\\sqrt{t})$, where $\\sigma^2$ is the noise variance. using a data aggregation method, we show that the mse can be made as small as $o(t^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to a consistent estimator. we also introduce a bayesian framework to quantify the model uncertainty through posterior probabilities. our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.", "categories": "econ.em stat.ap stat.ml", "created": "2017-11-18", "updated": "", "authors": ["muhammad jehangir amjad", "devavrat shah", "dennis shen"], "url": "https://arxiv.org/abs/1711.06940"}, {"title": "estimation considerations in contextual bandits", "id": "1711.07077", "abstract": "contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. we study a consideration for the exploration vs. exploitation framework that does not arise in multi-armed bandits but is crucial in contextual bandits; the way exploration and exploitation is conducted in the present affects the bias and variance in the potential outcome model estimation in subsequent stages of learning. we develop parametric and non-parametric contextual bandits that integrate balancing methods from the causal inference literature in their estimation to make it less prone to problems of estimation bias. we provide the first regret bound analyses for contextual bandits with balancing in the domain of linear contextual bandits that match the state of the art regret bounds. we demonstrate the strong practical advantage of balanced contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model mis-specification and prejudice in the initial training data. additionally, we develop contextual bandits with simpler assignment policies by leveraging sparse model estimation methods from the econometrics literature and demonstrate empirically that in the early stages they can improve the rate of learning and decrease regret.", "categories": "stat.ml cs.lg econ.em", "created": "2017-11-19", "updated": "2018-12-16", "authors": ["maria dimakopoulou", "zhengyuan zhou", "susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1711.07077"}, {"title": "economic complexity unfolded: interpretable model for the productive   structure of economies", "id": "1711.07327", "abstract": "economic complexity reflects the amount of knowledge that is embedded in the productive structure of an economy. it resides on the premise of hidden capabilities - fundamental endowments underlying the productive structure. in general, measuring the capabilities behind economic complexity directly is difficult, and indirect measures have been suggested which exploit the fact that the presence of the capabilities is expressed in a country's mix of products. we complement these studies by introducing a probabilistic framework which leverages bayesian non-parametric techniques to extract the dominant features behind the comparative advantage in exported products. based on economic evidence and trade data, we place a restricted indian buffet process on the distribution of countries' capability endowment, appealing to a culinary metaphor to model the process of capability acquisition. the approach comes with a unique level of interpretability, as it produces a concise and economically plausible description of the instantiated capabilities.", "categories": "econ.em", "created": "2017-11-17", "updated": "2018-07-17", "authors": ["zoran utkovski", "melanie f. pradier", "viktor stojkoski", "fernando perez-cruz", "ljupco kocarev"], "url": "https://arxiv.org/abs/1711.07327"}, {"title": "the research on the stagnant development of shantou special economic   zone under reform and opening-up policy", "id": "1711.08877", "abstract": "this study briefly introduces the development of shantou special economic zone under reform and opening-up policy from 1980 through 2016 with a focus on policy making issues and its influences on local economy. this paper is divided into two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of the original shantou district into three cities: shantou, chaozhou and jieyang in the end of 1991. this study analyzes the policy making issues in the separation of the original shantou district, the influences of the policy on shantou's economy after separation, the possibility of merging the three cities into one big new economic district in the future and reasons that lead to the stagnant development of shantou in recent 20 years. this paper uses statistical longitudinal analysis in analyzing economic problems with applications of non-parametric statistics through generalized additive model and time series forecasting methods. the paper is authored by bowen cai solely, who is the graduate student in the phd program of applied and computational mathematics and statistics at the university of notre dame with concentration in big data analysis.", "categories": "econ.em", "created": "2017-11-24", "updated": "", "authors": ["bowen cai"], "url": "https://arxiv.org/abs/1711.08877"}, {"title": "constructive identification of heterogeneous elasticities in the   cobb-douglas production function", "id": "1711.10031", "abstract": "this paper presents the identification of heterogeneous elasticities in the cobb-douglas production function. the identification is constructive with closed-form formulas for the elasticity with respect to each input for each firm. we propose that the flexible input cost ratio plays the role of a control function under \"non-collinear heterogeneity\" between elasticities with respect to two flexible inputs. the ex ante flexible input cost share can be used to identify the elasticities with respect to flexible inputs for each firm. the elasticities with respect to labor and capital can be subsequently identified for each firm under the timing assumption admitting the functional independence.", "categories": "econ.em", "created": "2017-11-27", "updated": "", "authors": ["tong li", "yuya sasaki"], "url": "https://arxiv.org/abs/1711.10031"}, {"title": "identification of and correction for publication bias", "id": "1711.10527", "abstract": "some empirical results are more likely to be published than others. such selective publication leads to biased estimates and distorted inference. this paper proposes two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second based on meta-studies. for known conditional publication probabilities, we propose median-unbiased estimators and associated confidence sets that correct for selective publication. we apply our methods to recent large-scale replication studies in experimental economics and psychology, and to meta-studies of the effects of minimum wages and de-worming programs.", "categories": "econ.em", "created": "2017-11-28", "updated": "", "authors": ["isaiah andrews", "maximilian kasy"], "url": "https://arxiv.org/abs/1711.10527"}, {"title": "the effect of partisanship and political advertising on close family   ties", "id": "1711.10602", "abstract": "research on growing american political polarization and antipathy primarily studies public institutions and political processes, ignoring private effects including strained family ties. using anonymized smartphone-location data and precinct-level voting, we show that thanksgiving dinners attended by opposing-party precinct residents were 30-50 minutes shorter than same-party dinners. this decline from a mean of 257 minutes survives extensive spatial and demographic controls. dinner reductions in 2016 tripled for travelers from media markets with heavy political advertising --- an effect not observed in 2015 --- implying a relationship to election-related behavior. effects appear asymmetric: while fewer democratic-precinct residents traveled in 2016 than 2015, political differences shortened thanksgiving dinners more among republican-precinct residents. nationwide, 34 million person-hours of cross-partisan thanksgiving discourse were lost in 2016 to partisan effects.", "categories": "econ.em", "created": "2017-11-28", "updated": "2018-06-03", "authors": ["m. keith chen", "ryne rohla"], "url": "https://arxiv.org/abs/1711.10602"}, {"title": "geometrically stopped markovian random growth processes and pareto tails", "id": "1712.01431", "abstract": "many empirical studies document power law behavior in size distributions of economic interest such as cities, firms, income, and wealth. one mechanism for generating such behavior combines independent and identically distributed gaussian additive shocks to log-size with a geometric age distribution. we generalize this mechanism by allowing the shocks to be non-gaussian (but light-tailed) and dependent upon a markov state variable. our main results provide sharp bounds on tail probabilities, a simple equation determining pareto exponents, and comparative statics. we present two applications: we show that (i) the tails of the wealth distribution in a heterogeneous-agent dynamic general equilibrium model with idiosyncratic investment risk are paretian, and (ii) a random growth model for the population dynamics of japanese municipalities is consistent with the observed pareto exponent but only after allowing for markovian dynamics.", "categories": "econ.em math.st stat.th", "created": "2017-12-04", "updated": "2019-12-31", "authors": ["brendan k. beare", "alexis akira toda"], "url": "https://arxiv.org/abs/1712.01431"}, {"title": "on monitoring development indicators using high resolution satellite   images", "id": "1712.02282", "abstract": "we develop a machine learning based tool for accurate prediction of socio-economic indicators from daytime satellite imagery. the diverse set of indicators are often not intuitively related to observable features in satellite images, and are not even always well correlated with each other. our predictive tool is more accurate than using night light as a proxy, and can be used to predict missing data, smooth out noise in surveys, monitor development progress of a region, and flag potential anomalies. finally, we use predicted variables to do robustness analysis of a regression study of high rate of stunting in india.", "categories": "econ.em cs.cy", "created": "2017-12-06", "updated": "2018-06-25", "authors": ["potnuru kishen suraj", "ankesh gupta", "makkunda sharma", "sourabh bikas paul", "subhashis banerjee"], "url": "https://arxiv.org/abs/1712.02282"}, {"title": "online red packets: a large-scale empirical study of gift giving on   wechat", "id": "1712.02926", "abstract": "gift giving is a ubiquitous social phenomenon, and red packets have been used as monetary gifts in asian countries for thousands of years. in recent years, online red packets have become widespread in china through the wechat platform. exploiting a unique dataset consisting of 61 million group red packets and seven million users, we conduct a large-scale, data-driven study to understand the spread of red packets and the effect of red packets on group activity. we find that the cash flows between provinces are largely consistent with provincial gdp rankings, e.g., red packets are sent from users in the south to those in the north. by distinguishing spontaneous from reciprocal red packets, we reveal the behavioral patterns in sending red packets: males, seniors, and people with more in-group friends are more inclined to spontaneously send red packets, while red packets from females, youths, and people with less in-group friends are more reciprocal. furthermore, we use propensity score matching to study the external effects of red packets on group dynamics. we show that red packets increase group participation and strengthen in-group relationships, which partly explain the benefits and motivations for sending red packets.", "categories": "cs.si cs.cy cs.hc cs.mm econ.em", "created": "2017-12-07", "updated": "", "authors": ["yuan yuan", "tracy xiao liu", "chenhao tan", "jie tang"], "url": "https://arxiv.org/abs/1712.02926"}, {"title": "on metropolis growth", "id": "1712.02937", "abstract": "we consider the scaling laws, second-order statistics and entropy of the consumed energy of metropolis cities which are hybrid complex systems comprising social networks, engineering systems, agricultural output, economic activity and energy components. we abstract a city in terms of two fundamental variables; $s$ resource cells (of unit area) that represent energy-consuming geographic or spatial zones (e.g. land, housing or infrastructure etc.) and a population comprising $n$ mobile units that can migrate between these cells. we show that with a constant metropolis area (fixed $s$), the variance and entropy of consumed energy initially increase with $n$, reach a maximum and then eventually diminish to zero as saturation is reached. these metrics are indicators of the spatial mobility of the population. under certain situations, the variance is bounded as a quadratic function of the mean consumed energy of the metropolis. however, when population and metropolis area are endogenous, growth in the latter is arrested when $n\\leq\\frac{s}{2}\\log(s)$ due to diminished population density. conversely, the population growth reaches equilibrium when $n\\geq {s}\\log{n}$ or equivalently when the aggregate of both over-populated and under-populated areas is large. moreover, we also draw the relationship between our approach and multi-scalar information, when economic dependency between a metropolis's sub-regions is based on the entropy of consumed energy. finally, if the city's economic size (domestic product etc.) is proportional to the consumed energy, then for a constant population density, we show that the economy scales linearly with the surface area (or $s$).", "categories": "physics.soc-ph econ.em", "created": "2017-12-07", "updated": "2017-12-15", "authors": ["syed amaar ahmad"], "url": "https://arxiv.org/abs/1712.02937"}, {"title": "aggregating google trends: multivariate testing and analysis", "id": "1712.03152", "abstract": "web search data are a valuable source of business and economic information. previous studies have utilized google trends web search data for economic forecasting. we expand this work by providing algorithms to combine and aggregate search volume data, so that the resulting data is both consistent over time and consistent between data series. we give a brand equity example, where google trends is used to analyze shopping data for 100 top ranked brands and these data are used to nowcast economic variables. we describe the importance of out of sample prediction and show how principal component analysis (pca) can be used to improve the signal to noise ratio and prevent overfitting in nowcasting models. we give a finance example, where exploratory data analysis and classification is used to analyze the relationship between google trends searches and stock prices.", "categories": "econ.em", "created": "2017-12-08", "updated": "2018-03-24", "authors": ["stephen l. france", "yuying shi"], "url": "https://arxiv.org/abs/1712.03152"}, {"title": "a random attention model", "id": "1712.03448", "abstract": "this paper illustrates how one can deduce preference from observed choices when attention is not only limited but also random. in contrast to earlier approaches, we introduce a random attention model (ram) where we abstain from any particular attention formation, and instead consider a large class of nonparametric random attention rules. our model imposes one intuitive condition, termed monotonic attention, which captures the idea that each consideration set competes for the decision-maker's attention. we then develop revealed preference theory within ram and obtain precise testable implications for observable choice probabilities. based on these theoretical findings, we propose econometric methods for identification, estimation, and inference of the decision maker's preferences. to illustrate the applicability of our results and their concrete empirical content in specific settings, we also develop revealed preference theory and accompanying econometric methods under additional nonparametric assumptions on the consideration set for binary choice problems. finally, we provide general purpose software implementation of our estimation and inference results, and showcase their performance using simulations.", "categories": "econ.em econ.th stat.me", "created": "2017-12-09", "updated": "2019-08-29", "authors": ["matias d. cattaneo", "xinwei ma", "yusufcan masatlioglu", "elchin suleymanov"], "url": "https://arxiv.org/abs/1712.03448"}, {"title": "rnn-based counterfactual prediction, with an application to homestead   policy and public schooling", "id": "1712.03553", "abstract": "this paper proposes a method for estimating the effect of a policy intervention on an outcome over time. we train recurrent neural networks (rnns) on the history of control unit outcomes to learn a useful representation for predicting future outcomes. the learned representation of control units is then applied to the treated units for predicting counterfactual outcomes. rnns are specifically structured to exploit temporal dependencies in panel data, and are able to learn negative and nonlinear interactions between control unit outcomes. we apply the method to the problem of estimating the long-run impact of u.s. homestead policy on public school spending.", "categories": "stat.ml econ.em stat.ap", "created": "2017-12-10", "updated": "2020-09-30", "authors": ["jason poulos", "shuxi zeng"], "url": "https://arxiv.org/abs/1712.03553"}, {"title": "set identified dynamic economies and robustness to misspecification", "id": "1712.03675", "abstract": "we propose a new inferential methodology for dynamic economies that is robust to misspecification of the mechanism generating frictions. economies with frictions are treated as perturbations of a frictionless economy that are consistent with a variety of mechanisms. we derive a representation for the law of motion for such economies and we characterize parameter set identification. we derive a link from model aggregate predictions to distributional information contained in qualitative survey data and specify conditions under which the identified set is refined. the latter is used to semi-parametrically estimate distortions due to frictions in macroeconomic variables. based on these estimates, we propose a novel test for complete models. using consumer and business survey data collected by the european commission, we apply our method to estimate distortions due to financial frictions in the spanish economy. we investigate the implications of these estimates for the adequacy of the standard model of financial frictions sw-bgg (smets and wouters (2007), bernanke, gertler, and gilchrist (1999)).", "categories": "econ.em", "created": "2017-12-11", "updated": "2018-01-04", "authors": ["andreas tryphonides"], "url": "https://arxiv.org/abs/1712.03675"}, {"title": "forecasting of a hierarchical functional time series on example of   macromodel for day and night air pollution in silesia region: a critical   overview", "id": "1712.03797", "abstract": "in economics we often face a system, which intrinsically imposes a structure of hierarchy of its components, i.e., in modelling trade accounts related to foreign exchange or in optimization of regional air protection policy.   a problem of reconciliation of forecasts obtained on different levels of hierarchy has been addressed in the statistical and econometric literature for many times and concerns bringing together forecasts obtained independently at different levels of hierarchy.   this paper deals with this issue in case of a hierarchical functional time series. we present and critically discuss a state of art and indicate opportunities of an application of these methods to a certain environment protection problem. we critically compare the best predictor known from the literature with our own original proposal. within the paper we study a macromodel describing a day and night air pollution in silesia region divided into five subregions.", "categories": "econ.em stat.ap", "created": "2017-11-25", "updated": "", "authors": ["daniel kosiorowski", "dominik mielczarek", "jerzy. p. rydlewski"], "url": "https://arxiv.org/abs/1712.03797"}, {"title": "the calculus of democratization and development", "id": "1712.04117", "abstract": "in accordance with \"democracy's effect on development: more questions than answers\", we seek to carry out a study in following the description in the 'questions for further study.' to that end, we studied 33 countries in the sub-saharan africa region, who all went through an election which should signal a \"step-up\" for their democracy, one in which previously homogenous regimes transfer power to an opposition party that fairly won the election. after doing so, liberal-democracy indicators and democracy indicators were evaluated in the five years prior to and after the election took place, and over that ten-year period, we examine the data for trends. if we see positive or negative trends over this time horizon, we are able to conclude that it was the recent increase in the quality of their democracy which led to it. having investigated examples of this in depth, there seem to be three main archetypes which drive the results. countries with positive results to their democracy from the election have generally positive effects on their development, countries with more \"plateau\" like results also did well, but countries for whom the descent to authoritarianism was continued by this election found more negative results.", "categories": "econ.em q-fin.gn", "created": "2017-12-11", "updated": "", "authors": ["jacob ferguson"], "url": "https://arxiv.org/abs/1712.04117"}, {"title": "finite-sample optimal estimation and inference on average treatment   effects under unconfoundedness", "id": "1712.04594", "abstract": "we consider estimation and inference on average treatment effects under unconfoundedness conditional on the realizations of the treatment variable and covariates. given nonparametric smoothness and/or shape restrictions on the conditional mean of the outcome variable, we derive estimators and confidence intervals (cis) that are optimal in finite samples when the regression errors are normal with known variance. in contrast to conventional cis, our cis use a larger critical value that explicitly takes into account the potential bias of the estimator. when the error distribution is unknown, feasible versions of our cis are valid asymptotically, even when $\\sqrt{n}$-inference is not possible due to lack of overlap, or low smoothness of the conditional mean. we also derive the minimum smoothness conditions on the conditional mean that are necessary for $\\sqrt{n}$-inference. when the conditional mean is restricted to be lipschitz with a large enough bound on the lipschitz constant, the optimal estimator reduces to a matching estimator with the number of matches set to one. we illustrate our methods in an application to the national supported work demonstration.", "categories": "stat.ap econ.em stat.me", "created": "2017-12-12", "updated": "2020-07-20", "authors": ["timothy b. armstrong", "michal koles\u00e1r"], "url": "https://arxiv.org/abs/1712.04594"}, {"title": "generic machine learning inference on heterogenous treatment effects in   randomized experiments", "id": "1712.04802", "abstract": "we propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. these key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. the approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. we post-process these proxies into the estimates of the key features. our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. it does not rely on strong assumptions. in particular, we don't require conditions for consistency of the machine learning methods. estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. for inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. this variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. we illustrate the use of the approach with two randomized experiments in development on the effects of microcredit and nudges to stimulate immunization demand.", "categories": "stat.ml econ.em math.st stat.th", "created": "2017-12-13", "updated": "2019-09-03", "authors": ["victor chernozhukov", "mert demirer", "esther duflo", "iv\u00e1n fern\u00e1ndez-val"], "url": "https://arxiv.org/abs/1712.04802"}, {"title": "quasi-oracle estimation of heterogeneous treatment effects", "id": "1712.04912", "abstract": "flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. in this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. we first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. then, we optimize this data-adaptive objective function. our approach has several advantages over existing methods. from a practical perspective, our method is flexible and easy to use: in both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. we implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.", "categories": "stat.ml econ.em math.st stat.th", "created": "2017-12-13", "updated": "2020-08-06", "authors": ["xinkun nie", "stefan wager"], "url": "https://arxiv.org/abs/1712.04912"}, {"title": "assessment voting in large electorates", "id": "1712.05470", "abstract": "we analyze assessment voting, a new two-round voting procedure that can be applied to binary decisions in democratic societies. in the first round, a randomly-selected number of citizens cast their vote on one of the two alternatives at hand, thereby irrevocably exercising their right to vote. in the second round, after the results of the first round have been published, the remaining citizens decide whether to vote for one alternative or to ab- stain. the votes from both rounds are aggregated, and the final outcome is obtained by applying the majority rule, with ties being broken by fair randomization. within a costly voting framework, we show that large elec- torates will choose the preferred alternative of the majority with high prob- ability, and that average costs will be low. this result is in contrast with the literature on one-round voting, which predicts either higher voting costs (when voting is compulsory) or decisions that often do not represent the preferences of the majority (when voting is voluntary).", "categories": "econ.em math.pr", "created": "2017-12-14", "updated": "2018-02-03", "authors": ["hans gersbach", "akaki mamageishvili", "oriol tejada"], "url": "https://arxiv.org/abs/1712.05470"}, {"title": "towards a general large sample theory for regularized estimators", "id": "1712.07248", "abstract": "we present a general framework for studying regularized estimators; such estimators are pervasive in estimation problems wherein \"plug-in\" type estimators are either ill-defined or ill-behaved. within this framework, we derive, under primitive conditions, consistency and a generalization of the asymptotic linearity property. we also provide data-driven methods for choosing tuning parameters that, under some conditions, achieve the aforementioned properties. we illustrate the scope of our approach by presenting a wide range of applications.", "categories": "math.st econ.em stat.th", "created": "2017-12-19", "updated": "2020-07-13", "authors": ["michael jansson", "demian pouzo"], "url": "https://arxiv.org/abs/1712.07248"}, {"title": "transformation models in high-dimensions", "id": "1712.07364", "abstract": "transformation models are a very important tool for applied statisticians and econometricians. in many applications, the dependent variable is transformed so that homogeneity or normal distribution of the error holds. in this paper, we analyze transformation models in a high-dimensional setting, where the set of potential covariates is large. we propose an estimator for the transformation parameter and we show that it is asymptotically normally distributed using an orthogonalized moment condition where the nuisance functions depend on the target parameter. in a simulation study, we show that the proposed estimator works well in small samples. a common practice in labor economics is to transform wage with the log-function. in this study, we test if this transformation holds in cps data from the united states.", "categories": "stat.me econ.em math.st stat.ml stat.th", "created": "2017-12-20", "updated": "", "authors": ["sven klaassen", "jannis kueck", "martin spindler"], "url": "https://arxiv.org/abs/1712.07364"}, {"title": "cointegration in functional autoregressive processes", "id": "1712.07522", "abstract": "this paper defines the class of $\\mathcal{h}$-valued autoregressive (ar) processes with a unit root of finite type, where $\\mathcal{h}$ is an infinite dimensional separable hilbert space, and derives a generalization of the granger-johansen representation theorem valid for any integration order $d=1,2,\\dots$. an existence theorem shows that the solution of an ar with a unit root of finite type is necessarily integrated of some finite integer $d$ and displays a common trends representation with a finite number of common stochastic trends of the type of (cumulated) bilateral random walks and an infinite dimensional cointegrating space. a characterization theorem clarifies the connections between the structure of the ar operators and $(i)$ the order of integration, $(ii)$ the structure of the attractor space and the cointegrating space, $(iii)$ the expression of the cointegrating relations, and $(iv)$ the triangular representation of the process. except for the fact that the number of cointegrating relations that are integrated of order 0 is infinite, the representation of $\\mathcal{h}$-valued ars with a unit root of finite type coincides with that of usual finite dimensional vars, which corresponds to the special case $\\mathcal{h}=\\mathbb{r}^p$.", "categories": "econ.em", "created": "2017-12-20", "updated": "2018-10-29", "authors": ["massimo franchi", "paolo paruolo"], "url": "https://arxiv.org/abs/1712.07522"}, {"title": "on long memory origins and forecast horizons", "id": "1712.08057", "abstract": "most long memory forecasting studies assume that the memory is generated by the fractional difference operator. we argue that the most cited theoretical arguments for the presence of long memory do not imply the fractional difference operator, and assess the performance of the autoregressive fractionally integrated moving average $(arfima)$ model when forecasting series with long memory generated by nonfractional processes. we find that high-order autoregressive $(ar)$ models produce similar or superior forecast performance than $arfima$ models at short horizons. nonetheless, as the forecast horizon increases, the $arfima$ models tend to dominate in forecast performance. hence, $arfima$ models are well suited for forecasts of long memory processes regardless of the long memory generating mechanism, particularly for medium and long forecast horizons. additionally, we analyse the forecasting performance of the heterogeneous autoregressive ($har$) model which imposes restrictions on high-order $ar$ models. we find that the structure imposed by the $har$ model produces better long horizon forecasts than $ar$ models of the same order, at the price of inferior short horizon forecasts in some cases. our results have implications for, among others, climate econometrics and financial econometrics models dealing with long memory series at different forecast horizons. we show in an example that while a short memory autoregressive moving average $(arma)$ model gives the best performance when forecasting the realized variance of the s\\&p 500 up to a month ahead, the $arfima$ model gives the best performance for longer forecast horizons.", "categories": "econ.em math.st stat.th", "created": "2017-12-21", "updated": "", "authors": ["j. eduardo vera-vald\u00e9s"], "url": "https://arxiv.org/abs/1712.08057"}, {"title": "simultaneous confidence intervals for high-dimensional linear models   with many endogenous variables", "id": "1712.08102", "abstract": "high-dimensional linear models with endogenous variables play an increasingly important role in recent econometric literature. in this work we allow for models with many endogenous variables and many instrument variables to achieve identification. because of the high-dimensionality in the second stage, constructing honest confidence regions with asymptotically correct coverage is non-trivial. our main contribution is to propose estimators and confidence regions that would achieve that. the approach relies on moment conditions that have an additional orthogonal property with respect to nuisance parameters. moreover, estimation of high-dimension nuisance parameters is carried out via new pivotal procedures. in order to achieve simultaneously valid confidence regions we use a multiplier bootstrap procedure to compute critical values and establish its validity.", "categories": "econ.em", "created": "2017-12-21", "updated": "2019-08-28", "authors": ["alexandre belloni", "christian hansen", "whitney newey"], "url": "https://arxiv.org/abs/1712.08102"}, {"title": "player-compatible learning and player-compatible equilibrium", "id": "1712.08954", "abstract": "player-compatible equilibrium (pce) imposes cross-player restrictions on the magnitudes of the players' \"trembles\" onto different strategies. these restrictions capture the idea that trembles correspond to deliberate experiments by agents who are unsure of the prevailing distribution of play. pce selects intuitive equilibria in a number of examples where trembling-hand perfect equilibrium (selten, 1975) and proper equilibrium (myerson, 1978) have no bite. we show that rational learning and weighted fictitious play imply our compatibility restrictions in a steady-state setting.", "categories": "econ.th", "created": "2017-12-24", "updated": "2020-05-27", "authors": ["drew fudenberg", "kevin he"], "url": "https://arxiv.org/abs/1712.08954"}, {"title": "an exact and robust conformal inference method for counterfactual and   synthetic controls", "id": "1712.09089", "abstract": "we introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. we recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. this allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution.", "categories": "econ.em stat.me", "created": "2017-12-25", "updated": "2020-09-01", "authors": ["victor chernozhukov", "kaspar wuthrich", "yinchu zhu"], "url": "https://arxiv.org/abs/1712.09089"}, {"title": "variational bayes estimation of discrete-margined copula models with   application to time series", "id": "1712.09150", "abstract": "we propose a new variational bayes estimator for high-dimensional copulas with discrete, or a combination of discrete and continuous, margins. the method is based on a variational approximation to a tractable augmented posterior, and is faster than previous likelihood-based approaches. we use it to estimate drawable vine copulas for univariate and multivariate markov ordinal and mixed time series. these have dimension $rt$, where $t$ is the number of observations and $r$ is the number of series, and are difficult to estimate using previous methods. the vine pair-copulas are carefully selected to allow for heteroskedasticity, which is a feature of most ordinal time series data. when combined with flexible margins, the resulting time series models also allow for other common features of ordinal data, such as zero inflation, multiple modes and under- or over-dispersion. using six example series, we illustrate both the flexibility of the time series copula models, and the efficacy of the variational bayes estimator for copulas of up to 792 dimensions and 60 parameters. this far exceeds the size and complexity of copula models for discrete data that can be estimated using previous methods.", "categories": "stat.me econ.em stat.ml", "created": "2017-12-25", "updated": "2018-07-20", "authors": ["ruben loaiza-maya", "michael stanley smith"], "url": "https://arxiv.org/abs/1712.09150"}, {"title": "machine learning for set-identified linear models", "id": "1712.10024", "abstract": "this paper provides estimation and inference methods for an identified set where the selection among a very large number of covariates is based on modern machine learning tools. i characterize the boundary of the identified set (i.e., support function) using a semiparametric moment condition. combining neyman-orthogonality and sample splitting ideas, i construct a root-n consistent, uniformly asymptotically gaussian estimator of the support function and propose a weighted bootstrap procedure to conduct inference about the identified set. i provide a general method to construct a neyman-orthogonal moment condition for the support function. applying my method to lee (2008)'s endogenous selection model, i provide the asymptotic theory for the sharp (i.e., the tightest possible) bounds on the average treatment effect in the presence of high-dimensional covariates. furthermore, i relax the conventional monotonicity assumption and allow the sign of the treatment effect on the selection (e.g., employment) to be determined by covariates. using jobcorps data set with very rich baseline characteristics, i substantially tighten the bounds on the jobcorps effect on wages under weakened monotonicity assumption.", "categories": "stat.ml cs.lg econ.em", "created": "2017-12-28", "updated": "2019-12-06", "authors": ["vira semenova"], "url": "https://arxiv.org/abs/1712.10024"}, {"title": "confidence set for group membership", "id": "1801.00332", "abstract": "we develop new procedures to quantify the statistical uncertainty of data-driven clustering algorithms. in our panel setting, each unit belongs to one of a finite number of latent groups with group-specific regression curves. we propose methods for computing unit-wise and joint confidence sets for group membership. the unit-wise sets give possible group memberships for a given unit and the joint sets give possible vectors of group memberships for all units. we also propose an algorithm that can improve the power of our procedures by detecting units that are easy to classify. the confidence sets invert a test for group membership that is based on a characterization of the true group memberships by a system of moment inequalities. to construct the joint confidence, we solve a high-dimensional testing problem that tests group membership simultaneously for all units. we justify this procedure under $n, t \\to \\infty$ asymptotics where we allow $t$ to be much smaller than $n$. as part of our theoretical arguments, we develop new simultaneous anti-concentration inequalities for the max and the qlr statistics. monte carlo results indicate that our confidence sets have adequate coverage and are informative. we illustrate the practical relevance of our confidence sets in two applications.", "categories": "econ.em stat.me", "created": "2017-12-31", "updated": "2018-12-12", "authors": ["andreas dzemski", "ryo okui"], "url": "https://arxiv.org/abs/1801.00332"}, {"title": "estimation and inference of treatment effects with $l_2$-boosting in   high-dimensional settings", "id": "1801.00364", "abstract": "boosting algorithms are very popular in machine learning and have proven very useful for prediction and variable selection. nevertheless in many applications the researcher is interested in inference on treatment effects or policy variables in a high-dimensional setting. empirical researchers are more and more faced with rich datasets containing very many controls or instrumental variables, where variable selection is challenging. in this paper we give results for the valid inference of a treatment effect after selecting from among very many control variables and the estimation of instrumental variables with potentially very many instruments when post- or orthogonal $l_2$-boosting is used for the variable selection. this setting allows for valid inference on low-dimensional components in a regression estimated with $l_2$-boosting. we give simulation results for the proposed methods and an empirical application, in which we analyze the effectiveness of a pulmonary artery catheter.", "categories": "stat.ml econ.em stat.me", "created": "2017-12-31", "updated": "", "authors": ["ye luo", "martin spindler"], "url": "https://arxiv.org/abs/1801.00364"}, {"title": "resource abundance and life expectancy", "id": "1801.00369", "abstract": "this paper investigates the impacts of major natural resource discoveries since 1960 on life expectancy in the nations that they were resource poor prior to the discoveries. previous literature explains the relation between nations wealth and life expectancy, but it has been silent about the impacts of resource discoveries on life expectancy. we attempt to fill this gap in this study. an important advantage of this study is that as the previous researchers argued resource discovery could be an exogenous variable. we use longitudinal data from 1960 to 2014 and we apply three modern empirical methods including difference-in-differences, event studies, and synthetic control approach, to investigate the main question of the research which is 'how resource discoveries affect life expectancy?'. the findings show that resource discoveries in ecuador, yemen, oman, and equatorial guinea have positive and significant impacts on life expectancy, but the effects for the european countries are mostly negative.", "categories": "econ.em", "created": "2017-12-31", "updated": "", "authors": ["bahram sanginabadi"], "url": "https://arxiv.org/abs/1801.00369"}, {"title": "complexity theory, game theory, and economics: the barbados lectures", "id": "1801.00734", "abstract": "this document collects the lecture notes from my mini-course \"complexity theory, game theory, and economics,\" taught at the bellairs research institute of mcgill university, holetown, barbados, february 19--23, 2017, as the 29th mcgill invitational workshop on computational complexity.   the goal of this mini-course is twofold: (i) to explain how complexity theory has helped illuminate several barriers in economics and game theory; and (ii) to illustrate how game-theoretic questions have led to new and interesting complexity theory, including recent several breakthroughs. it consists of two five-lecture sequences: the solar lectures, focusing on the communication and computational complexity of computing equilibria; and the lunar lectures, focusing on applications of complexity theory in game theory and economics. no background in game theory is assumed.", "categories": "cs.cc cs.gt econ.em", "created": "2018-01-02", "updated": "2020-02-08", "authors": ["tim roughgarden"], "url": "https://arxiv.org/abs/1801.00734"}, {"title": "a new wald test for hypothesis testing based on mcmc outputs", "id": "1801.00973", "abstract": "in this paper, a new and convenient $\\chi^2$ wald test based on mcmc outputs is proposed for hypothesis testing. the new statistic can be explained as mcmc version of wald test and has several important advantages that make it very convenient in practical applications. first, it is well-defined under improper prior distributions and avoids jeffrey-lindley's paradox. second, it's asymptotic distribution can be proved to follow the $\\chi^2$ distribution so that the threshold values can be easily calibrated from this distribution. third, it's statistical error can be derived using the markov chain monte carlo (mcmc) approach. fourth, most importantly, it is only based on the posterior mcmc random samples drawn from the posterior distribution. hence, it is only the by-product of the posterior outputs and very easy to compute. in addition, when the prior information is available, the finite sample theory is derived for the proposed test statistic. at last, the usefulness of the test is illustrated with several applications to latent variable models widely used in economics and finance.", "categories": "econ.em stat.me", "created": "2018-01-03", "updated": "", "authors": ["yong li", "xiaobin liu", "jun yu", "tao zeng"], "url": "https://arxiv.org/abs/1801.00973"}, {"title": "comparing the forecasting performances of linear models for electricity   prices with high res penetration", "id": "1801.01093", "abstract": "this paper compares alternative univariate versus multivariate models, frequentist versus bayesian autoregressive and vector autoregressive specifications, for hourly day-ahead electricity prices, both with and without renewable energy sources. the accuracy of point and density forecasts are inspected in four main european markets (germany, denmark, italy and spain) characterized by different levels of renewable energy power generation. our results show that the bayesian var specifications with exogenous variables dominate other multivariate and univariate specifications, in terms of both point and density forecasting.", "categories": "econ.em q-fin.ec", "created": "2018-01-03", "updated": "2019-11-12", "authors": ["angelica gianfreda", "francesco ravazzolo", "luca rossini"], "url": "https://arxiv.org/abs/1801.01093"}, {"title": "dynamic and granular loss reserving with copulae", "id": "1801.01792", "abstract": "an intensive research sprang up for stochastic methods in insurance during the past years. to meet all future claims rising from policies, it is requisite to quantify the outstanding loss liabilities. loss reserving methods based on aggregated data from run-off triangles are predominantly used to calculate the claims reserves. conventional reserving techniques have some disadvantages: loss of information from the policy and the claim's development due to the aggregation, zero or negative cells in the triangle; usually small number of observations in the triangle; only few observations for recent accident years; and sensitivity to the most recent paid claims.   to overcome these dilemmas, granular loss reserving methods for individual claim-by-claim data will be derived. reserves' estimation is a crucial part of the risk valuation process, which is now a front burner in economics. since there is a growing demand for prediction of total reserves for different types of claims or even multiple lines of business, a time-varying copula framework for granular reserving will be established.", "categories": "econ.em stat.ap", "created": "2018-01-05", "updated": "", "authors": ["mat\u00fa\u0161 maciak", "ostap okhrin", "michal pe\u0161ta"], "url": "https://arxiv.org/abs/1801.01792"}, {"title": "sabcemm-a simulator for agent-based computational economic market models", "id": "1801.01811", "abstract": "we introduce the simulation tool sabcemm (simulator for agent-based computational economic market models) for agent-based computational economic market (abcem) models. our simulation tool is implemented in c++ and we can easily run abcem models with several million agents. the object-oriented software design enables the isolated implementation of building blocks for abcem models, such as agent types and market mechanisms. the user can design and compare abcem models in a unified environment by recombining existing building blocks using the xml-based sabcemm configuration file. we introduce an abstract abcem model class which our simulation tool is built upon. furthermore, we present the software architecture as well as computational aspects of sabcemm. here, we focus on the efficiency of sabcemm with respect to the run time of our simulations. we show the great impact of different random number generators on the run time of abcem models. the code and documentation is published on github at https://github.com/sabcemm/sabcemm, such that all results can be reproduced by the reader.", "categories": "q-fin.cp econ.em q-fin.gn q-fin.tr", "created": "2018-01-05", "updated": "2018-10-09", "authors": ["torsten trimborn", "philipp otte", "simon cramer", "max beikirch", "emma pabich", "martin frank"], "url": "https://arxiv.org/abs/1801.01811"}, {"title": "why markets are inefficient: a gambling \"theory\" of financial markets   for practitioners and theorists", "id": "1801.01948", "abstract": "the purpose of this article is to propose a new \"theory,\" the strategic analysis of financial markets (safm) theory, that explains the operation of financial markets using the analytical perspective of an enlightened gambler. the gambler understands that all opportunities for superior performance arise from suboptimal decisions by humans, but understands also that knowledge of human decision making alone is not enough to understand market behavior --- one must still model how those decisions lead to market prices. thus are there three parts to the model: gambling theory, human decision making, and strategic problem solving. a new theory is necessary because at this writing in 2017, there is no theory of financial markets acceptable to both practitioners and theorists. theorists' efficient market theory, for example, cannot explain bubbles and crashes nor the exceptional returns of famous investors and speculators such as warren buffett and george soros. at the same time, a new theory must be sufficiently quantitative, explain market \"anomalies\" and provide predictions in order to satisfy theorists. it is hoped that the safm framework will meet these requirements.", "categories": "econ.em q-fin.gn", "created": "2018-01-05", "updated": "", "authors": ["steven d. moffitt"], "url": "https://arxiv.org/abs/1801.01948"}, {"title": "learning from neighbors about a changing state", "id": "1801.02042", "abstract": "agents learn about a changing state using private signals and past actions of neighbors in a network. we characterize equilibrium learning and social influence in this setting. we then examine when agents can aggregate information well, responding quickly to recent changes. a key sufficient condition for good aggregation is that each individual's neighbors have sufficiently different types of private information. in contrast, when signals are homogeneous, aggregation is suboptimal on any network. we also examine behavioral versions of the model, and show that achieving good aggregation requires a sophisticated understanding of correlations in neighbors' actions. the model provides a bayesian foundation for a tractable learning dynamic in networks, closely related to the degroot model, and offers new tools for counterfactual and welfare analyses.", "categories": "econ.th cs.gt cs.si", "created": "2018-01-06", "updated": "2020-01-20", "authors": ["krishna dasaratha", "benjamin golub", "nir hak"], "url": "https://arxiv.org/abs/1801.02042"}, {"title": "stochastic dynamic pricing for ev charging stations with renewables   integration and energy storage", "id": "1801.02128", "abstract": "this paper studies the problem of stochastic dynamic pricing and energy management policy for electric vehicle (ev) charging service providers. in the presence of renewable energy integration and energy storage system, ev charging service providers must deal with multiple uncertainties --- charging demand volatility, inherent intermittency of renewable energy generation, and wholesale electricity price fluctuation. the motivation behind our work is to offer guidelines for charging service providers to determine proper charging prices and manage electricity to balance the competing objectives of improving profitability, enhancing customer satisfaction, and reducing impact on power grid in spite of these uncertainties. we propose a new metric to assess the impact on power grid without solving complete power flow equations. to protect service providers from severe financial losses, a safeguard of profit is incorporated in the model. two algorithms --- stochastic dynamic programming (sdp) algorithm and greedy algorithm (benchmark algorithm) --- are applied to derive the pricing and electricity procurement policy. a pareto front of the multiobjective optimization is derived. simulation results show that using sdp algorithm can achieve up to 7% profit gain over using greedy algorithm. additionally, we observe that the charging service provider is able to reshape spatial-temporal charging demands to reduce the impact on power grid via pricing signals.", "categories": "eess.sp econ.em math.oc", "created": "2018-01-06", "updated": "", "authors": ["chao luo", "yih-fang huang", "vijay gupta"], "url": "https://arxiv.org/abs/1801.02128"}, {"title": "placement of ev charging stations --- balancing benefits among multiple   entities", "id": "1801.02129", "abstract": "this paper studies the problem of multi-stage placement of electric vehicle (ev) charging stations with incremental ev penetration rates. a nested logit model is employed to analyze the charging preference of the individual consumer (ev owner), and predict the aggregated charging demand at the charging stations. the ev charging industry is modeled as an oligopoly where the entire market is dominated by a few charging service providers (oligopolists). at the beginning of each planning stage, an optimal placement policy for each service provider is obtained through analyzing strategic interactions in a bayesian game. to derive the optimal placement policy, we consider both the transportation network graph and the electric power network graph. a simulation software --- the ev virtual city 1.0 --- is developed using java to investigate the interactions among the consumers (ev owner), the transportation network graph, the electric power network graph, and the charging stations. through a series of experiments using the geographic and demographic data from the city of san pedro district of los angeles, we show that the charging station placement is highly consistent with the heatmap of the traffic flow. in addition, we observe a spatial economic phenomenon that service providers prefer clustering instead of separation in the ev charging market.", "categories": "eess.sp cs.gt econ.em math.oc", "created": "2018-01-06", "updated": "", "authors": ["chao luo", "yih-fang huang", "vijay gupta"], "url": "https://arxiv.org/abs/1801.02129"}, {"title": "a consumer behavior based approach to multi-stage ev charging station   placement", "id": "1801.02135", "abstract": "this paper presents a multi-stage approach to the placement of charging stations under the scenarios of different electric vehicle (ev) penetration rates. the ev charging market is modeled as the oligopoly. a consumer behavior based approach is applied to forecast the charging demand of the charging stations using a nested logit model. the impacts of both the urban road network and the power grid network on charging station planning are also considered. at each planning stage, the optimal station placement strategy is derived through solving a bayesian game among the service providers. to investigate the interplay of the travel pattern, the consumer behavior, urban road network, power grid network, and the charging station placement, a simulation platform (the ev virtual city 1.0) is developed using java on repast.we conduct a case study in the san pedro district of los angeles by importing the geographic and demographic data of that region into the platform. the simulation results demonstrate a strong consistency between the charging station placement and the traffic flow of evs. the results also reveal an interesting phenomenon that service providers prefer clustering instead of spatial separation in this oligopoly market.", "categories": "eess.sp cs.gt econ.em math.oc", "created": "2018-01-07", "updated": "", "authors": ["chao luo", "yih-fang huang", "vijay gupta"], "url": "https://arxiv.org/abs/1801.02135"}, {"title": "revealed price preference: theory and empirical analysis", "id": "1801.02702", "abstract": "with the aim of determining the welfare implications of price change in consumption data, we introduce a revealed preference relation over prices. we show that an absence of cycles in this preference relation characterizes a model of demand where consumers trade-off the utility of consumption against the disutility of expenditure. this model is appropriate whenever a consumer's demand over a {\\em strict} subset of all available goods is being analyzed. for the random utility extension of the model, we devise nonparametric statistical procedures for testing and welfare comparisons. the latter requires the development of novel tests of linear hypotheses for partially identified parameters. in doing so, we provide new algorithms for the calculation and statistical inference in nonparametric counterfactual analysis for a general partially identified model. our applications on national household expenditure data provide support for the model and yield informative bounds concerning welfare rankings across different prices.", "categories": "econ.em econ.th", "created": "2018-01-08", "updated": "2018-09-26", "authors": ["rahul deb", "yuichi kitamura", "john k. -h. quah", "j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1801.02702"}, {"title": "dynamic pricing and energy management strategy for ev charging stations   under uncertainties", "id": "1801.02783", "abstract": "this paper presents a dynamic pricing and energy management framework for electric vehicle (ev) charging service providers. to set the charging prices, the service providers faces three uncertainties: the volatility of wholesale electricity price, intermittent renewable energy generation, and spatial-temporal ev charging demand. the main objective of our work here is to help charging service providers to improve their total profits while enhancing customer satisfaction and maintaining power grid stability, taking into account those uncertainties. we employ a linear regression model to estimate the ev charging demand at each charging station, and introduce a quantitative measure for customer satisfaction. both the greedy algorithm and the dynamic programming (dp) algorithm are employed to derive the optimal charging prices and determine how much electricity to be purchased from the wholesale market in each planning horizon. simulation results show that dp algorithm achieves an increased profit (up to 9%) compared to the greedy algorithm (the benchmark algorithm) under certain scenarios. additionally, we observe that the integration of a low-cost energy storage into the system can not only improve the profit, but also smooth out the charging price fluctuation, protecting the end customers from the volatile wholesale market.", "categories": "eess.sp econ.em math.oc", "created": "2018-01-08", "updated": "", "authors": ["chao luo", "yih-fang huang", "vijay gupta"], "url": "https://arxiv.org/abs/1801.02783"}, {"title": "implications of macroeconomic volatility in the euro area", "id": "1801.02925", "abstract": "in this paper we estimate a bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the euro area. this allows us to treat macroeconomic uncertainty as a latent quantity during estimation. only a limited number of contributions to the literature estimate uncertainty and its macroeconomic consequences jointly, and most are based on single country models. we analyze the special case of a shock restricted to the euro area, where member states are highly related by construction. we find significant results of a decrease in real activity for all countries over a period of roughly a year following an uncertainty shock. moreover, equity prices, short-term interest rates and exports tend to decline, while unemployment levels increase. dynamic responses across countries differ slightly in magnitude and duration, with ireland, slovakia and greece exhibiting different reactions for some macroeconomic fundamentals.", "categories": "econ.em", "created": "2018-01-09", "updated": "2018-06-28", "authors": ["niko hauzenberger", "maximilian b\u00f6ck", "michael pfarrhofer", "anna stelzer", "gregor zens"], "url": "https://arxiv.org/abs/1801.02925"}, {"title": "a method for winning at lotteries", "id": "1801.02958", "abstract": "we report a new result on lotteries --- that a well-funded syndicate has a purely mechanical strategy to achieve expected returns of 10\\% to 25\\% in an equiprobable lottery with no take and no carryover pool. we prove that an optimal strategy (nash equilibrium) in a game between the syndicate and other players consists of betting one of each ticket (the \"trump ticket\"), and extend that result to proportional ticket selection in non-equiprobable lotteries. the strategy can be adjusted to accommodate lottery taxes and carryover pools. no \"irrationality\" need be involved for the strategy to succeed --- it requires only that a large group of non-syndicate bettors each choose a few tickets independently.", "categories": "econ.em", "created": "2018-01-05", "updated": "", "authors": ["steven d. moffitt", "william t. ziemba"], "url": "https://arxiv.org/abs/1801.02958"}, {"title": "does it pay to buy the pot in the canadian 6/49 lotto? implications for   lottery design", "id": "1801.02959", "abstract": "despite its unusual payout structure, the canadian 6/49 lotto is one of the few government sponsored lotteries that has the potential for a favorable strategy we call \"buying the pot.\" by buying the pot we mean that a syndicate buys each ticket in the lottery, ensuring that it holds a jackpot winner. we assume that the other bettors independently buy small numbers of tickets. this paper presents (1) a formula for the syndicate's expected return, (2) conditions under which buying the pot produces a significant positive expected return, and (3) the implications of these findings for lottery design.", "categories": "econ.em", "created": "2018-01-05", "updated": "", "authors": ["steven d. moffitt", "william t. ziemba"], "url": "https://arxiv.org/abs/1801.02959"}, {"title": "on a constructive theory of markets", "id": "1801.02994", "abstract": "this article is a prologue to the article \"why markets are inefficient: a gambling 'theory' of financial markets for practitioners and theorists.\" it presents important background for that article --- why gambling is important, even necessary, for real-world traders --- the reason for the superiority of the strategic/gambling approach to the competing market ideologies of market fundamentalism and the scientific approach --- and its potential to uncover profitable trading systems. much of this article was drawn from chapter 1 of the book \"the strategic analysis of financial markets (in 2 volumes)\" world scientific, 2017.", "categories": "econ.em q-fin.gn", "created": "2018-01-07", "updated": "", "authors": ["steven d. moffitt"], "url": "https://arxiv.org/abs/1801.02994"}, {"title": "assessing the effect of advertising expenditures upon sales: a bayesian   structural time series model", "id": "1801.03050", "abstract": "we propose a robust implementation of the nerlove--arrow model using a bayesian structural time series model to explain the relationship between advertising expenditures of a country-wide fast-food franchise network with its weekly sales. thanks to the flexibility and modularity of the model, it is well suited to generalization to other markets or situations. its bayesian nature facilitates incorporating \\emph{a priori} information (the manager's views), which can be updated with relevant data. this aspect of the model will be used to present a strategy of budget scheduling across time and channels.", "categories": "stat.ml econ.em q-fin.rm stat.ap", "created": "2018-01-09", "updated": "2019-05-29", "authors": ["v\u00edctor gallego", "pablo su\u00e1rez-garc\u00eda", "pablo angulo", "david g\u00f3mez-ullate"], "url": "https://arxiv.org/abs/1801.03050"}, {"title": "solving dynamic discrete choice models: integrated or expected value   function?", "id": "1801.03978", "abstract": "dynamic discrete choice models (ddcms) are important in the structural estimation literature. since the structural errors are practically always continuous and unbounded in nature, researchers often use the expected value function. the idea to solve for the expected value function made solution more practical and estimation feasible. however, as we show in this paper, the expected value function is impractical compared to an alternative: the integrated (ex ante) value function. we provide brief descriptions of the inefficacy of the former, and benchmarks on actual problems with varying cardinality of the state space and number of decisions. though the two approaches solve the same problem in theory, the benchmarks support the claim that the integrated value function is preferred in practice.", "categories": "econ.em", "created": "2018-01-11", "updated": "", "authors": ["patrick kofod mogensen"], "url": "https://arxiv.org/abs/1801.03978"}, {"title": "optimal contracts under competition when uncertainty from adverse   selection and moral hazard are present", "id": "1801.04080", "abstract": "in a continuous-time setting where a risk-averse agent controls the drift of an output process driven by a brownian motion, optimal contracts are linear in the terminal output; this result is well-known in a setting with moral hazard and -under stronger assumptions - adverse selection. we show that this result continues to hold when in addition reservation utilities are type-dependent. this type of problem occurs in the study of optimal compensation problems involving competing principals.", "categories": "q-fin.pm econ.th", "created": "2018-01-12", "updated": "", "authors": ["n. packham"], "url": "https://arxiv.org/abs/1801.04080"}, {"title": "heterogeneous structural breaks in panel data models", "id": "1801.04672", "abstract": "this paper develops a new model and estimation procedure for panel data that allows us to identify heterogeneous structural breaks. we model individual heterogeneity using a grouped pattern. for each group, we allow common structural breaks in the coefficients. however, the number, timing, and size of these breaks can differ across groups. we develop a hybrid estimation procedure of the grouped fixed effects approach and adaptive group fused lasso. we show that our method can consistently identify the latent group structure, detect structural breaks, and estimate the regression parameters. monte carlo results demonstrate the good performance of the proposed method in finite samples. an empirical application to the relationship between income and democracy illustrates the importance of considering heterogeneous structural breaks.", "categories": "econ.em", "created": "2018-01-15", "updated": "2018-11-24", "authors": ["ryo okui", "wendun wang"], "url": "https://arxiv.org/abs/1801.04672"}, {"title": "characterizing assumption of rationality by incomplete information", "id": "1801.04714", "abstract": "we characterize common assumption of rationality of 2-person games within an incomplete information framework. we use the lexicographic model with incomplete information and show that a belief hierarchy expresses common assumption of rationality within a complete information framework if and only if there is a belief hierarchy within the corresponding incomplete information framework that expresses common full belief in caution, rationality, every good choice is supported, and prior belief in the original utility functions.", "categories": "econ.em", "created": "2018-01-15", "updated": "", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1801.04714"}, {"title": "panel data quantile regression with grouped fixed effects", "id": "1801.05041", "abstract": "this paper introduces estimation methods for grouped latent heterogeneity in panel data quantile regression. we assume that the observed individuals come from a heterogeneous population with a finite number of types. the number of types and group membership is not assumed to be known in advance and is estimated by means of a convex optimization problem. we provide conditions under which group membership is estimated consistently and establish asymptotic normality of the resulting estimators. simulations show that the method works well in finite samples when t is reasonably large. to illustrate the proposed methodology we study the effects of the adoption of right-to-carry concealed weapon laws on violent crime rates using panel data of 51 u.s. states from 1977 - 2010.", "categories": "econ.em stat.me", "created": "2018-01-15", "updated": "2018-08-05", "authors": ["jiaying gu", "stanislav volgushev"], "url": "https://arxiv.org/abs/1801.05041"}, {"title": "censored quantile instrumental variable estimation with stata", "id": "1801.05305", "abstract": "many applications involve a censored dependent variable and an endogenous independent variable. chernozhukov et al. (2015) introduced a censored quantile instrumental variable estimator (cqiv) for use in those applications, which has been applied by kowalski (2016), among others. in this article, we introduce a stata command, cqiv, that simplifes application of the cqiv estimator in stata. we summarize the cqiv estimator and algorithm, we describe the use of the cqiv command, and we provide empirical examples.", "categories": "econ.em stat.co", "created": "2018-01-13", "updated": "2019-09-24", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "sukjin han", "amanda kowalski"], "url": "https://arxiv.org/abs/1801.05305"}, {"title": "eliminating the effect of rating bias on reputation systems", "id": "1801.05734", "abstract": "the ongoing rapid development of the e-commercial and interest-base websites make it more pressing to evaluate objects' accurate quality before recommendation by employing an effective reputation system. the objects' quality are often calculated based on their historical information, such as selected records or rating scores, to help visitors to make decisions before watching, reading or buying. usually high quality products obtain a higher average ratings than low quality products regardless of rating biases or errors. however many empirical cases demonstrate that consumers may be misled by rating scores added by unreliable users or deliberate tampering. in this case, users' reputation, i.e., the ability to rating trustily and precisely, make a big difference during the evaluating process. thus, one of the main challenges in designing reputation systems is eliminating the effects of users' rating bias on the evaluation results. to give an objective evaluation of each user's reputation and uncover an object's intrinsic quality, we propose an iterative balance (ib) method to correct users' rating biases. experiments on two online video-provided web sites, namely movielens and netflix datasets, show that the ib method is a highly self-consistent and robust algorithm and it can accurately quantify movies' actual quality and users' stability of rating. compared with existing methods, the ib method has higher ability to find the \"dark horses\", i.e., not so popular yet good movies, in the academy awards.", "categories": "physics.soc-ph cs.cy cs.ir cs.si econ.th", "created": "2018-01-17", "updated": "", "authors": ["leilei wu", "zhuoming ren", "xiao-long ren", "jianlin zhang", "linyuan l\u00fc"], "url": "https://arxiv.org/abs/1801.05734"}, {"title": "a dirichlet process mixture model of discrete choice", "id": "1801.06296", "abstract": "we present a mixed multinomial logit (mnl) model, which leverages the truncated stick-breaking process representation of the dirichlet process as a flexible nonparametric mixing distribution. the proposed model is a dirichlet process mixture model and accommodates discrete representations of heterogeneity, like a latent class mnl model. yet, unlike a latent class mnl model, the proposed discrete choice model does not require the analyst to fix the number of mixture components prior to estimation, as the complexity of the discrete mixing distribution is inferred from the evidence. for posterior inference in the proposed dirichlet process mixture model of discrete choice, we derive an expectation maximisation algorithm. in a simulation study, we demonstrate that the proposed model framework can flexibly capture differently-shaped taste parameter distributions. furthermore, we empirically validate the model framework in a case study on motorists' route choice preferences and find that the proposed dirichlet process mixture model of discrete choice outperforms a latent class mnl model and mixed mnl models with common parametric mixing distributions in terms of both in-sample fit and out-of-sample predictive ability. compared to extant modelling approaches, the proposed discrete choice model substantially abbreviates specification searches, as it relies on less restrictive parametric assumptions and does not require the analyst to specify the complexity of the discrete mixing distribution prior to estimation.", "categories": "stat.ap econ.em stat.co stat.me stat.ml", "created": "2018-01-19", "updated": "", "authors": ["rico krueger", "akshay vij", "taha h. rashidi"], "url": "https://arxiv.org/abs/1801.06296"}, {"title": "predicting crypto-currencies using sparse non-gaussian state space   models", "id": "1801.06373", "abstract": "in this paper we forecast daily returns of crypto-currencies using a wide variety of different econometric models. to capture salient features commonly observed in financial time series like rapid changes in the conditional variance, non-normality of the measurement errors and sharply increasing trends, we develop a time-varying parameter var with t-distributed measurement errors and stochastic volatility. to control for overparameterization, we rely on the bayesian literature on shrinkage priors that enables us to shrink coefficients associated with irrelevant predictors and/or perform model specification in a flexible manner. using around one year of daily data we perform a real-time forecasting exercise and investigate whether any of the proposed models is able to outperform the naive random walk benchmark. to assess the economic relevance of the forecasting gains produced by the proposed models we moreover run a simple trading exercise.", "categories": "econ.em q-fin.st", "created": "2018-01-19", "updated": "2018-02-13", "authors": ["christian hotz-behofsits", "florian huber", "thomas o. z\u00f6rner"], "url": "https://arxiv.org/abs/1801.06373"}, {"title": "usda forecasts: a meta-analysis study", "id": "1801.06575", "abstract": "the primary goal of this study is doing a meta-analysis research on two groups of published studies. first, the ones that focus on the evaluation of the united states department of agriculture (usda) forecasts and second, the ones that evaluate the market reactions to the usda forecasts. we investigate four questions. 1) how the studies evaluate the accuracy of the usda forecasts? 2) how they evaluate the market reactions to the usda forecasts? 3) is there any heterogeneity in the results of the mentioned studies? 4) is there any publication bias? about the first question, while some researchers argue that the forecasts are unbiased, most of them maintain that they are biased, inefficient, not optimal, or not rational. about the second question, while a few studies claim that the forecasts are not newsworthy, most of them maintain that they are newsworthy, provide useful information, and cause market reactions. about the third and the fourth questions, based on our findings, there are some clues that the results of the studies are heterogeneous, but we didn't find enough evidences of publication bias.", "categories": "econ.em q-fin.gn", "created": "2018-01-19", "updated": "", "authors": ["bahram sanginabadi"], "url": "https://arxiv.org/abs/1801.06575"}, {"title": "capital structure in u.s., a quantile regression approach with   macroeconomic impacts", "id": "1801.06651", "abstract": "the major perspective of this paper is to provide more evidence into the empirical determinants of capital structure adjustment in different macroeconomics states by focusing and discussing the relative importance of firm-specific and macroeconomic characteristics from an alternative scope in u.s. this study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic variables across all quantiles of distribution of leverage (total debt, long-terms debt and short-terms debt). thus, based on a partial adjustment model, we find that long-term and short-term debt ratios varying regarding their partial adjustment speeds; the short-term debt raises up while the long-term debt ratio slows down for same periods.", "categories": "q-fin.ec econ.em q-fin.gn", "created": "2018-01-20", "updated": "", "authors": ["andreas kaloudis", "dimitrios tsolis"], "url": "https://arxiv.org/abs/1801.06651"}, {"title": "nonfractional memory: filtering, antipersistence, and forecasting", "id": "1801.06677", "abstract": "the fractional difference operator remains to be the most popular mechanism to generate long memory due to the existence of efficient algorithms for their simulation and forecasting. nonetheless, there is no theoretical argument linking the fractional difference operator with the presence of long memory in real data. in this regard, one of the most predominant theoretical explanations for the presence of long memory is cross-sectional aggregation of persistent micro units. yet, the type of processes obtained by cross-sectional aggregation differs from the one due to fractional differencing. thus, this paper develops fast algorithms to generate and forecast long memory by cross-sectional aggregation. moreover, it is shown that the antipersistent phenomenon that arises for negative degrees of memory in the fractional difference literature is not present for cross-sectionally aggregated processes. pointedly, while the autocorrelations for the fractional difference operator are negative for negative degrees of memory by construction, this restriction does not apply to the cross-sectional aggregated scheme. we show that this has implications for long memory tests in the frequency domain, which will be misspecified for cross-sectionally aggregated processes with negative degrees of memory. finally, we assess the forecast performance of high-order $ar$ and $arfima$ models when the long memory series are generated by cross-sectional aggregation. our results are of interest to practitioners developing forecasts of long memory variables like inflation, volatility, and climate data, where aggregation may be the source of long memory.", "categories": "math.st econ.em stat.th", "created": "2018-01-20", "updated": "", "authors": ["j. eduardo vera-vald\u00e9s"], "url": "https://arxiv.org/abs/1801.06677"}, {"title": "testing the number of regimes in markov regime switching models", "id": "1801.06862", "abstract": "markov regime switching models have been used in numerous empirical studies in economics and finance. however, the asymptotic distribution of the likelihood ratio test statistic for testing the number of regimes in markov regime switching models has been an unresolved problem. this paper derives the asymptotic distribution of the likelihood ratio test statistic for testing the null hypothesis of $m_0$ regimes against the alternative hypothesis of $m_0 + 1$ regimes for any $m_0 \\geq 1$ both under the null hypothesis and under local alternatives. we show that the contiguous alternatives converge to the null hypothesis at a rate of $n^{-1/8}$ in regime switching models with normal density. the asymptotic validity of the parametric bootstrap is also established.", "categories": "econ.em math.st q-fin.mf stat.th", "created": "2018-01-21", "updated": "2018-01-30", "authors": ["hiroyuki kasahara", "katsumi shimotsu"], "url": "https://arxiv.org/abs/1801.06862"}, {"title": "evolution of regional innovation with spatial knowledge spillovers:   convergence or divergence?", "id": "1801.06936", "abstract": "this paper extends endogenous economic growth models to incorporate knowledge externality. we explores whether spatial knowledge spillovers among regions exist, whether spatial knowledge spillovers promote regional innovative activities, and whether external knowledge spillovers affect the evolution of regional innovations in the long run. we empirically verify the theoretical results through applying spatial statistics and econometric model in the analysis of panel data of 31 regions in china. an accurate estimate of the range of knowledge spillovers is achieved and the convergence of regional knowledge growth rate is found, with clear evidences that developing regions benefit more from external knowledge spillovers than developed regions.", "categories": "econ.em stat.ap", "created": "2018-01-21", "updated": "2018-03-06", "authors": ["jinwen qiu", "wenjian liu", "ning ning"], "url": "https://arxiv.org/abs/1801.06936"}, {"title": "accurate evaluation of asset pricing under uncertainty and ambiguity of   information", "id": "1801.06966", "abstract": "since exchange economy considerably varies in the market assets, asset prices have become an attractive research area for investigating and modeling ambiguous and uncertain information in today markets. this paper proposes a new generative uncertainty mechanism based on the bayesian inference and correntropy (bic) technique for accurately evaluating asset pricing in markets. this technique examines the potential processes of risk, ambiguity, and variations of market information in a controllable manner. we apply the new bic technique to a consumption asset-pricing model in which the consumption variations are modeled using the bayesian network model with observing the dynamics of asset pricing phenomena in the data. these dynamics include the procyclical deviations of price, the countercyclical deviations of equity premia and equity volatility, the leverage impact and the mean reversion of excess returns. the key findings reveal that the precise modeling of asset information can estimate price changes in the market effectively.", "categories": "q-fin.gn econ.em", "created": "2018-01-22", "updated": "2018-03-26", "authors": ["farouq abdulaziz masoudy"], "url": "https://arxiv.org/abs/1801.06966"}, {"title": "estimating heterogeneous consumer preferences for restaurants and travel   time using mobile location data", "id": "1801.07826", "abstract": "this paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the san francisco bay area. the data is used to identify users' approximate typical morning location, as well as their choices of lunchtime restaurants. we build a model where restaurants have latent characteristics (whose distribution may depend on restaurant observables, such as star ratings, food category, and price range), each user has preferences for these latent characteristics, and these preferences are heterogeneous across users. similarly, each item has latent characteristics that describe users' willingness to travel to the restaurant, and each user has individual-specific preferences for those latent characteristics. thus, both users' willingness to travel and their base utility for each restaurant vary across user-restaurant pairs. we use a bayesian approach to estimation. to make the estimation computationally feasible, we rely on variational inference to approximate the posterior distribution, as well as stochastic gradient descent as a computational approach. our model performs better than more standard competing models such as multinomial logit and nested logit models, in part due to the personalization of the estimates. we analyze how consumers re-allocate their demand after a restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics, and we compare our predictions to actual outcomes. finally, we show how the model can be used to analyze counterfactual questions such as what type of restaurant would attract the most consumers in a given location.", "categories": "econ.em cs.ai stat.ap stat.ml", "created": "2018-01-22", "updated": "", "authors": ["susan athey", "david blei", "robert donnelly", "francisco ruiz", "tobias schmidt"], "url": "https://arxiv.org/abs/1801.07826"}, {"title": "quantifying health shocks over the life cycle", "id": "1801.08746", "abstract": "we first show (1) the importance of investigating health expenditure process using the order two markov chain model, rather than the standard order one model, which is widely used in the literature. markov chain of order two is the minimal framework that is capable of distinguishing those who experience a certain health expenditure level for the first time from those who have been experiencing that or other levels for some time. in addition, using the model we show (2) that the probability of encountering a health shock first de- creases until around age 10, and then increases with age, particularly, after age 40, (3) that health shock distributions among different age groups do not differ until their percentiles reach the median range, but that above the median the health shock distributions of older age groups gradually start to first-order dominate those of younger groups, and (4) that the persistency of health shocks also shows a u-shape in relation to age.", "categories": "econ.em", "created": "2018-01-26", "updated": "", "authors": ["taiyo fukai", "hidehiko ichimura", "kyogo kanazawa"], "url": "https://arxiv.org/abs/1801.08746"}, {"title": "ordered kripke model, permissibility, and convergence of probabilistic   kripke model", "id": "1801.08767", "abstract": "we define a modification of the standard kripke model, called the ordered kripke model, by introducing a linear order on the set of accessible states of each state. we first show this model can be used to describe the lexicographic belief hierarchy in epistemic game theory, and perfect rationalizability can be characterized within this model. then we show that each ordered kripke model is the limit of a sequence of standard probabilistic kripke models with a modified (common) belief operator, in the senses of structure and the (epsilon-)permissibilities characterized within them.", "categories": "econ.em", "created": "2018-01-26", "updated": "", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1801.08767"}, {"title": "nonseparable sample selection models with censored selection rules", "id": "1801.08961", "abstract": "we consider identification and estimation of nonseparable sample selection models with censored selection rules. we employ a control function approach and discuss different objects of interest based on (1) local effects conditional on the control function, and (2) global effects obtained from integration over ranges of values of the control function. we derive the conditions for the identification of these different objects and suggest strategies for estimation. moreover, we provide the associated asymptotic theory. these strategies are illustrated in an empirical investigation of the determinants of female wages in the united kingdom.", "categories": "econ.em stat.me", "created": "2018-01-26", "updated": "2020-09-29", "authors": ["iv\u00e1n fern\u00e1ndez-val", "aico van vuuren", "francis vella"], "url": "https://arxiv.org/abs/1801.08961"}, {"title": "greedy algorithms for maximizing nash social welfare", "id": "1801.09046", "abstract": "we study the problem of fairly allocating a set of indivisible goods among agents with additive valuations. the extent of fairness of an allocation is measured by its nash social welfare, which is the geometric mean of the valuations of the agents for their bundles. while the problem of maximizing nash social welfare is known to be apx-hard in general, we study the effectiveness of simple, greedy algorithms in solving this problem in two interesting special cases.   first, we show that a simple, greedy algorithm provides a 1.061-approximation guarantee when agents have identical valuations, even though the problem of maximizing nash social welfare remains np-hard for this setting. second, we show that when agents have binary valuations over the goods, an exact solution (i.e., a nash optimal allocation) can be found in polynomial time via a greedy algorithm. our results in the binary setting extend to provide novel, exact algorithms for optimizing nash social welfare under concave valuations. notably, for the above mentioned scenarios, our techniques provide a simple alternative to several of the existing, more sophisticated techniques for this problem such as constructing equilibria of fisher markets or using real stable polynomials.", "categories": "cs.gt econ.th", "created": "2018-01-27", "updated": "", "authors": ["siddharth barman", "sanath kumar krishnamurthy", "rohit vaish"], "url": "https://arxiv.org/abs/1801.09046"}, {"title": "are `water smart landscapes' contagious? an epidemic approach on   networks to study peer effects", "id": "1801.10516", "abstract": "we test the existence of a neighborhood based peer effect around participation in an incentive based conservation program called `water smart landscapes' (wsl) in the city of las vegas, nevada. we use 15 years of geo-coded daily records of wsl program applications and approvals compiled by the southern nevada water authority and clark county tax assessors rolls for home characteristics. we use this data to test whether a spatially mediated peer effect can be observed in wsl participation likelihood at the household level. we show that epidemic spreading models provide more flexibility in modeling assumptions, and also provide one mechanism for addressing problems associated with correlated unobservables than hazards models which can also be applied to address the same questions. we build networks of neighborhood based peers for 16 randomly selected neighborhoods in las vegas and test for the existence of a peer based influence on wsl participation by using a susceptible-exposed-infected-recovered epidemic spreading model (seir), in which a home can become infected via autoinfection or through contagion from its infected neighbors. we show that this type of epidemic model can be directly recast to an additive-multiplicative hazard model, but not to purely multiplicative one. using both inference and prediction approaches we find evidence of peer effects in several las vegas neighborhoods.", "categories": "econ.em physics.data-an physics.soc-ph stat.ap", "created": "2018-01-29", "updated": "", "authors": ["christa brelsford", "caterina de bacco"], "url": "https://arxiv.org/abs/1801.10516"}, {"title": "how can we induce more women to competitions?", "id": "1801.10518", "abstract": "why women avoid participating in a competition and how can we encourage them to participate in it? in this paper, we investigate how social image concerns affect women's decision to compete. we first construct a theoretical model and show that participating in a competition, even under affirmative action policies favoring women, is costly for women under public observability since it deviates from traditional female gender norms, resulting in women's low appearance in competitive environments. we propose and theoretically show that introducing prosocial incentives in the competitive environment is effective and robust to public observability since (i) it induces women who are intrinsically motivated by prosocial incentives to the competitive environment and (ii) it makes participating in a competition not costly for women from social image point of view. we conduct a laboratory experiment where we randomly manipulate the public observability of decisions to compete and test our theoretical predictions. the results of the experiment are fairly consistent with our theoretical predictions. we suggest that when designing policies to promote gender equality in competitive environments, using prosocial incentives through company philanthropy or other social responsibility policies, either as substitutes or as complements to traditional affirmative action policies, could be promising.", "categories": "econ.em", "created": "2018-01-27", "updated": "", "authors": ["masayuki yagasaki", "mitsunosuke morishita"], "url": "https://arxiv.org/abs/1801.10518"}, {"title": "hyper-rational choice theory", "id": "1801.10520", "abstract": "the rational choice theory is based on this idea that people rationally pursue goals for increasing their personal interests. in most conditions, the behavior of an actor is not independent of the person and others' behavior. here, we present a new concept of rational choice as a hyper-rational choice which in this concept, the actor thinks about profit or loss of other actors in addition to his personal profit or loss and then will choose an action which is desirable to him. we implement the hyper-rational choice to generalize and expand the game theory. results of this study will help to model the behavior of people considering environmental conditions, the kind of behavior interactive, valuation system of itself and others and system of beliefs and internal values of societies. hyper-rationality helps us understand how human decision makers behave in interactive decisions.", "categories": "econ.em", "created": "2018-01-11", "updated": "2018-02-26", "authors": ["madjid eshaghi gordji", "gholamreza askari"], "url": "https://arxiv.org/abs/1801.10520"}, {"title": "structural analysis with mixed-frequency data: a midas-svar model of us   capital flows", "id": "1802.00793", "abstract": "we develop a new var model for structural analysis with mixed-frequency data. the midas-svar model allows to identify structural dynamic links exploiting the information contained in variables sampled at different frequencies. it also provides a general framework to test homogeneous frequency-based representations versus mixed-frequency data models. a set of monte carlo experiments suggests that the test performs well both in terms of size and power. the midas-svar is then used to study how monetary policy and financial market volatility impact on the dynamics of gross capital inflows to the us. while no relation is found when using standard quarterly data, exploiting the variability present in the series within the quarter shows that the effect of an interest rate shock is greater the longer the time lag between the month of the shock and the end of the quarter", "categories": "econ.em", "created": "2018-02-02", "updated": "", "authors": ["emanuele bacchiocchi", "andrea bastianin", "alessandro missale", "eduardo rossi"], "url": "https://arxiv.org/abs/1802.00793"}, {"title": "voting patterns in 2016: exploration using multilevel regression and   poststratification (mrp) on pre-election polls", "id": "1802.00842", "abstract": "we analyzed 2012 and 2016 yougov pre-election polls in order to understand how different population groups voted in the 2012 and 2016 elections. we broke the data down by demographics and state. we display our findings with a series of graphs and maps. the r code associated with this project is available at https://github.com/rtrangucci/mrp_2016_election/.", "categories": "stat.ap econ.em", "created": "2018-02-02", "updated": "2018-03-14", "authors": ["rob trangucci", "imad ali", "andrew gelman", "doug rivers"], "url": "https://arxiv.org/abs/1802.00842"}, {"title": "promoting cooperation by reputation-driven group formation", "id": "1802.01253", "abstract": "in previous studies of spatial public goods game, each player is able to establish a group. however, in real life, some players cannot successfully organize groups for various reasons. in this paper, we propose a mechanism of reputation-driven group formation, in which groups can only be organized by players whose reputation reaches or exceeds a threshold. we define a player's reputation as the frequency of cooperation in the last $t$ time steps. we find that the highest cooperation level can be obtained when groups are only established by pure cooperators who always cooperate in the last $t$ time steps. effects of the memory length $t$ on cooperation are also studied.", "categories": "physics.soc-ph econ.th", "created": "2018-02-04", "updated": "", "authors": ["han-xin yang", "zhen wang"], "url": "https://arxiv.org/abs/1802.01253"}, {"title": "an experimental investigation of preference misrepresentation in the   residency match", "id": "1802.01990", "abstract": "the development and deployment of matching procedures that incentivize truthful preference reporting is considered one of the major successes of market design research. in this study, we test the degree to which these procedures succeed in eliminating preference misrepresentation. we administered an online experiment to 1,714 medical students immediately after their participation in the medical residency match--a leading field application of strategy-proof market design. when placed in an analogous, incentivized matching task, we find that 23% of participants misrepresent their preferences. we explore the factors that predict preference misrepresentation, including cognitive ability, strategic positioning, overconfidence, expectations, advice, and trust. we discuss the implications of this behavior for the design of allocation mechanisms and the social welfare in markets that use them.", "categories": "econ.em", "created": "2018-02-05", "updated": "2018-08-30", "authors": ["alex rees-jones", "samuel skowronek"], "url": "https://arxiv.org/abs/1802.01990"}, {"title": "random taste heterogeneity in discrete choice models: flexible   nonparametric finite mixture distributions", "id": "1802.02299", "abstract": "this study proposes a mixed logit model with multivariate nonparametric finite mixture distributions. the support of the distribution is specified as a high-dimensional grid over the coefficient space, with equal or unequal intervals between successive points along the same dimension; the location of each point on the grid and the probability mass at that point are model parameters that need to be estimated. the framework does not require the analyst to specify the shape of the distribution prior to model estimation, but can approximate any multivariate probability distribution function to any arbitrary degree of accuracy. the grid with unequal intervals, in particular, offers greater flexibility than existing multivariate nonparametric specifications, while requiring the estimation of a small number of additional parameters. an expectation maximization algorithm is developed for the estimation of these models. multiple synthetic datasets and a case study on travel mode choice behavior are used to demonstrate the value of the model framework and estimation algorithm. compared to extant models that incorporate random taste heterogeneity through continuous mixture distributions, the proposed model provides better out-of-sample predictive ability. findings reveal significant differences in willingness to pay measures between the proposed model and extant specifications. the case study further demonstrates the ability of the proposed model to endogenously recover patterns of attribute non-attendance and choice set formation.", "categories": "econ.em stat.ap", "created": "2018-02-06", "updated": "", "authors": ["akshay vij", "rico krueger"], "url": "https://arxiv.org/abs/1802.02299"}, {"title": "prediction of shared bicycle demand with wavelet thresholding", "id": "1802.02683", "abstract": "consumers are creatures of habit, often periodic, tied to work, shopping and other schedules. we analyzed one month of data from the world's largest bike-sharing company to elicit demand behavioral cycles, initially using models from animal tracking that showed large customers fit an ornstein-uhlenbeck model with demand peaks at periodicities of 7, 12, 24 hour and 7-days. lorenz curves of bicycle demand showed that the majority of customer usage was infrequent, and demand cycles from time-series models would strongly overfit the data yielding unreliable models. analysis of thresholded wavelets for the space-time tensor of bike-sharing contracts was able to compress the data into a 56-coefficient model with little loss of information, suggesting that bike-sharing demand behavior is exceptionally strong and regular. improvements to predicted demand could be made by adjusting for 'noise' filtered by our model from air quality and weather information and demand from infrequent riders.", "categories": "econ.em", "created": "2018-02-07", "updated": "", "authors": ["j. christopher westland", "jian mou", "dafei yin"], "url": "https://arxiv.org/abs/1802.02683"}, {"title": "long-term unemployed hirings: should targeted or untargeted policies be   preferred?", "id": "1802.03343", "abstract": "to what extent, hiring incentives targeting a specific group of vulnerable unemployed (i.e. long term unemployed) are more effective, with respect to generalised incentives (without a definite target), to increase hirings of the targeted group? are generalized incentives able to influence hirings of the vulnerable group? do targeted policies have negative side effects too important to accept them? even though there is a huge literature on hiring subsidies, these questions remained unresolved. we tried to answer them, comparing the impact of two similar hiring policies, one oriented towards a target group and one generalised, implemented on the italian labour market. we used administrative data on job contracts, and counterfactual analysis methods. the targeted policy had a positive and significant impact, while the generalized policy didn't have a significant impact on the vulnerable group. moreover, we concluded the targeted policy didn't have any indirect negative side effect.", "categories": "econ.em stat.ap", "created": "2018-02-09", "updated": "2018-05-02", "authors": ["alessandra pasquini", "marco centra", "guido pellegrini"], "url": "https://arxiv.org/abs/1802.03343"}, {"title": "structural estimation of behavioral heterogeneity", "id": "1802.03735", "abstract": "we develop a behavioral asset pricing model in which agents trade in a market with information friction. profit-maximizing agents switch between trading strategies in response to dynamic market conditions. due to noisy private information about the fundamental value, the agents form different evaluations about heterogeneous strategies. we exploit a thin set---a small sub-population---to pointly identify this nonlinear model, and estimate the structural parameters using extended method of moments. based on the estimated parameters, the model produces return time series that emulate the moments of the real data. these results are robust across different sample periods and estimation methods.", "categories": "q-fin.tr econ.em", "created": "2018-02-11", "updated": "2018-06-11", "authors": ["zhentao shi", "huanhuan zheng"], "url": "https://arxiv.org/abs/1802.03735"}, {"title": "a general method for demand inversion", "id": "1802.04444", "abstract": "this paper describes a numerical method to solve for mean product qualities which equates the real market share to the market share predicted by a discrete choice model. the method covers a general class of discrete choice model, including the pure characteristics model in berry and pakes(2007) and the random coefficient logit model in berry et al.(1995) (hereafter blp). the method transforms the original market share inversion problem to an unconstrained convex minimization problem, so that any convex programming algorithm can be used to solve the inversion. moreover, such results also imply that the computational complexity of inverting a demand model should be no more than that of a convex programming problem. in simulation examples, i show the method outperforms the contraction mapping algorithm in blp. i also find the method remains robust in pure characteristics models with near-zero market shares.", "categories": "econ.em", "created": "2018-02-12", "updated": "2018-02-26", "authors": ["lixiong li"], "url": "https://arxiv.org/abs/1802.04444"}, {"title": "knowledge and unanimous acceptance of core payoffs: an epistemic   foundation for cooperative game theory", "id": "1802.04595", "abstract": "we provide an epistemic foundation for cooperative games by proof theory via studying the knowledge for players unanimously accepting only core payoffs. we first transform each cooperative game into a decision problem where a player can accept or reject any payoff vector offered to her based on her knowledge about available cooperation. then we use a modified kd-system in epistemic logic, which can be regarded as a counterpart of the model for non-cooperative games in bonanno (2008), (2015), to describe a player's knowledge, decision-making criterion, and reasoning process; especially, a formula called c-acceptability is defined to capture the criterion for accepting a core payoff vector. within this syntactical framework, we characterize the core of a cooperative game in terms of players' knowledge. based on that result, we discuss an epistemic inconsistency behind debreu-scarf theorem, that is, the increase of the number of replicas has invariant requirement on each participant's knowledge from the aspect of competitive market, while requires unbounded epistemic ability players from the aspect of cooperative game.", "categories": "econ.em", "created": "2018-02-13", "updated": "2019-01-02", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1802.04595"}, {"title": "analysis of financial credit risk using machine learning", "id": "1802.05326", "abstract": "corporate insolvency can have a devastating effect on the economy. with an increasing number of companies making expansion overseas to capitalize on foreign resources, a multinational corporate bankruptcy can disrupt the world's financial ecosystem. corporations do not fail instantaneously; objective measures and rigorous analysis of qualitative (e.g. brand) and quantitative (e.g. econometric factors) data can help identify a company's financial risk. gathering and storage of data about a corporation has become less difficult with recent advancements in communication and information technologies. the remaining challenge lies in mining relevant information about a company's health hidden under the vast amounts of data, and using it to forecast insolvency so that managers and stakeholders have time to react. in recent years, machine learning has become a popular field in big data analytics because of its success in learning complicated models. methods such as support vector machines, adaptive boosting, artificial neural networks, and gaussian processes can be used for recognizing patterns in the data (with a high degree of accuracy) that may not be apparent to human analysts. this thesis studied corporate bankruptcy of manufacturing companies in korea and poland using experts' opinions and financial measures, respectively. using publicly available datasets, several machine learning methods were applied to learn the relationship between the company's current state and its fate in the near future. results showed that predictions with accuracy greater than 95% were achievable using any machine learning technique when informative features like experts' assessment were used. however, when using purely financial factors to predict whether or not a company will go bankrupt, the correlation is not as strong.", "categories": "q-fin.st econ.em q-fin.mf", "created": "2018-02-14", "updated": "", "authors": ["jacky c. k. chow"], "url": "https://arxiv.org/abs/1802.05326"}, {"title": "bootstrap-assisted unit root testing with piecewise locally stationary   errors", "id": "1802.05333", "abstract": "in unit root testing, a piecewise locally stationary process is adopted to accommodate nonstationary errors that can have both smooth and abrupt changes in second- or higher-order properties. under this framework, the limiting null distributions of the conventional unit root test statistics are derived and shown to contain a number of unknown parameters. to circumvent the difficulty of direct consistent estimation, we propose to use the dependent wild bootstrap to approximate the non-pivotal limiting null distributions and provide a rigorous theoretical justification for bootstrap consistency. the proposed method is compared through finite sample simulations with the recolored wild bootstrap procedure, which was developed for errors that follow a heteroscedastic linear process. further, a combination of autoregressive sieve recoloring with the dependent wild bootstrap is shown to perform well. the validity of the dependent wild bootstrap in a nonstationary setting is demonstrated for the first time, showing the possibility of extensions to other inference problems associated with locally stationary processes.", "categories": "econ.em math.st stat.th", "created": "2018-02-14", "updated": "", "authors": ["yeonwoo rho", "xiaofeng shao"], "url": "https://arxiv.org/abs/1802.05333"}, {"title": "the dynamic impact of monetary policy on regional housing prices in the   us: evidence based on factor-augmented vector autoregressions", "id": "1802.05870", "abstract": "in this study interest centers on regional differences in the response of housing prices to monetary policy shocks in the us. we address this issue by analyzing monthly home price data for metropolitan regions using a factor-augmented vector autoregression (favar) model. bayesian model estimation is based on gibbs sampling with normal-gamma shrinkage priors for the autoregressive coefficients and factor loadings, while monetary policy shocks are identified using high-frequency surprises around policy announcements as external instruments. the empirical results indicate that monetary policy actions typically have sizeable and significant positive effects on regional housing prices, revealing differences in magnitude and duration. the largest effects are observed in regions located in states on both the east and west coasts, notably california, arizona and florida.", "categories": "econ.em", "created": "2018-02-16", "updated": "", "authors": ["manfred m. fischer", "florian huber", "michael pfarrhofer", "petra staufer-steinnocher"], "url": "https://arxiv.org/abs/1802.05870"}, {"title": "on the iterated estimation of dynamic discrete choice games", "id": "1802.06665", "abstract": "we study the asymptotic properties of a class of estimators of the structural parameters in dynamic discrete choice games. we consider k-stage policy iteration (pi) estimators, where k denotes the number of policy iterations employed in the estimation. this class nests several estimators proposed in the literature such as those in aguirregabiria and mira (2002, 2007), pesendorfer and schmidt-dengler (2008), and pakes et al. (2007). first, we establish that the k-pml estimator is consistent and asymptotically normal for all k. this complements findings in aguirregabiria and mira (2007), who focus on k=1 and k large enough to induce convergence of the estimator. furthermore, we show under certain conditions that the asymptotic variance of the k-pml estimator can exhibit arbitrary patterns as a function of k. second, we establish that the k-md estimator is consistent and asymptotically normal for all k. for a specific weight matrix, the k-md estimator has the same asymptotic distribution as the k-pml estimator. our main result provides an optimal sequence of weight matrices for the k-md estimator and shows that the optimally weighted k-md estimator has an asymptotic distribution that is invariant to k. the invariance result is especially unexpected given the findings in aguirregabiria and mira (2007) for k-pml estimators. our main result implies two new corollaries about the optimal 1-md estimator (derived by pesendorfer and schmidt-dengler (2008)). first, the optimal 1-md estimator is optimal in the class of k-md estimators. in other words, additional policy iterations do not provide asymptotic efficiency gains relative to the optimal 1-md estimator. second, the optimal 1-md estimator is more or equally asymptotically efficient than any k-pml estimator for all k. finally, the appendix provides appropriate conditions under which the optimal 1-md estimator is asymptotically efficient.", "categories": "econ.em", "created": "2018-02-19", "updated": "2020-05-24", "authors": ["federico a. bugni", "jackson bunting"], "url": "https://arxiv.org/abs/1802.06665"}, {"title": "achieving perfect coordination amongst agents in the co-action minority   game", "id": "1802.06770", "abstract": "we discuss the strategy that rational agents can use to maximize their expected long-term payoff in the co-action minority game. we argue that the agents will try to get into a cyclic state, where each of the $(2n +1)$ agent wins exactly $n$ times in any continuous stretch of $(2n+1)$ days. we propose and analyse a strategy for reaching such a cyclic state quickly, when any direct communication between agents is not allowed, and only the publicly available common information is the record of total number of people choosing the first restaurant in the past. we determine exactly the average time required to reach the periodic state for this strategy. we show that it varies as $(n/\\ln 2) [1 + \\alpha \\cos (2 \\pi \\log_2 n)$], for large $n$, where the amplitude $\\alpha$ of the leading term in the log-periodic oscillations is found be $\\frac{8 \\pi^2}{(\\ln 2)^2} \\exp{(- 2 \\pi^2/\\ln 2)} \\approx {\\color{blue}7 \\times 10^{-11}}$.", "categories": "econ.em q-fin.ec", "created": "2018-02-17", "updated": "2018-05-24", "authors": ["hardik rajpal", "deepak dhar"], "url": "https://arxiv.org/abs/1802.06770"}, {"title": "the allen--uzawa elasticity of substitution for nonhomogeneous   production functions", "id": "1802.06885", "abstract": "this note proves that the representation of the allen elasticity of substitution obtained by uzawa for linear homogeneous functions holds true for nonhomogeneous functions. it is shown that the criticism of the allen-uzawa elasticity of substitution in the works of blackorby, primont, russell is based on an incorrect example.", "categories": "econ.em math.oc", "created": "2018-02-09", "updated": "", "authors": ["elena burmistrova", "sergey lobanov"], "url": "https://arxiv.org/abs/1802.06885"}, {"title": "the security of the united kingdom electricity imports under conditions   of high european demand", "id": "1802.07457", "abstract": "energy policy in europe has been driven by the three goals of security of supply, economic competitiveness and environmental sustainability, referred to as the energy trilemma. although there are clear conflicts within the trilemma, member countries have acted to facilitate a fully integrated european electricity market. interconnection and cross-border electricity trade has been a fundamental part of such market liberalisation. however, it has been suggested that consumers are exposed to a higher price volatility as a consequence of interconnection. furthermore, during times of energy shortages and high demand, issues of national sovereignty take precedence over cooperation. in this article, the unique and somewhat peculiar conditions of early 2017 within france, germany and the united kingdom have been studied to understand how the existing integration arrangements address the energy trilemma. it is concluded that the dominant interests are economic and national security; issues of environmental sustainability are neglected or overridden. although the optimisation of european electricity generation to achieve a lower overall carbon emission is possible, such a goal is far from being realised. furthermore, it is apparent that the united kingdom, and other countries, cannot rely upon imports from other countries during periods of high demand and/or limited supply.", "categories": "econ.em q-fin.gn", "created": "2018-02-21", "updated": "", "authors": ["anthony d stephens", "david r walwyn"], "url": "https://arxiv.org/abs/1802.07457"}, {"title": "algorithmic collusion in cournot duopoly market: evidence from   experimental economics", "id": "1802.08061", "abstract": "algorithmic collusion is an emerging concept in current artificial intelligence age. whether algorithmic collusion is a creditable threat remains as an argument. in this paper, we propose an algorithm which can extort its human rival to collude in a cournot duopoly competing market. in experiments, we show that, the algorithm can successfully extorted its human rival and gets higher profit in long run, meanwhile the human rival will fully collude with the algorithm. as a result, the social welfare declines rapidly and stably. both in theory and in experiment, our work confirms that, algorithmic collusion can be a creditable threat. in application, we hope, the frameworks, the algorithm design as well as the experiment environment illustrated in this work, can be an incubator or a test bed for researchers and policymakers to handle the emerging algorithmic collusion.", "categories": "econ.em cs.gt stat.ap stat.ml", "created": "2018-02-21", "updated": "", "authors": ["nan zhou", "li zhang", "shijian li", "zhijian wang"], "url": "https://arxiv.org/abs/1802.08061"}, {"title": "de-biased machine learning of global and local parameters using   regularized riesz representers", "id": "1802.08667", "abstract": "we provide novel adaptive inference methods, based on $\\ell_1$ regularization, for regular (semi-parametric) and non-regular (nonparametric) linear functionals of the conditional expectation function. examples of regular functionals include average treatment effects, policy effects, and derivatives. examples of non-regular functionals include localized quantities: average treatment effects, policy effects, and derivatives conditional on a covariate subvector fixed at a point. we construct a neyman orthogonal equation for the target parameter that is approximately invariant to small perturbations of the nuisance parameters. to achieve this property, we include the linear riesz representer for the functional as an additional nuisance parameter.   under weak assumptions, the estimator of the functional concentrates in a $l/\\sqrt{n}$ neighborhood of the target with deviations controlled by the gaussian law. we require that the operator norm $l$ of the functional is small compared to $\\sqrt{n}$; the functional does not lose regularity too fast. our analysis yields weak ``double sparsity robustness'': either the approximation to the regression or the approximation to the representer can be ``completely dense'' as long as the other is sufficiently ``sparse''. our main results are non-asymptotic and imply asymptotic uniform validity over large classes of models, translating into honest confidence bands for both global and local parameters.", "categories": "stat.ml econ.em math.st stat.th", "created": "2018-02-23", "updated": "2020-07-10", "authors": ["victor chernozhukov", "whitney newey", "rahul singh"], "url": "https://arxiv.org/abs/1802.08667"}, {"title": "measuring the demand effects of formal and informal communication :   evidence from online markets for illicit drugs", "id": "1802.08778", "abstract": "i present evidence that communication between marketplace participants is an important influence on market demand. i find that consumer demand is approximately equally influenced by communication on both formal and informal networks- namely, product reviews and community forums. in addition, i find empirical evidence of a vendor's ability to commit to disclosure dampening the effect of communication on demand. i also find that product demand is more responsive to average customer sentiment as the number of messages grows, as may be expected in a bayesian updating framework.", "categories": "stat.ap econ.em", "created": "2018-02-23", "updated": "", "authors": ["luis armona"], "url": "https://arxiv.org/abs/1802.08778"}, {"title": "kernel estimation for panel data with heterogeneous dynamics", "id": "1802.08825", "abstract": "this paper proposes nonparametric kernel-smoothing estimation for panel data to examine the degree of heterogeneity across cross-sectional units. we first estimate the sample mean, autocovariances, and autocorrelations for each unit and then apply kernel smoothing to compute their density functions. the dependence of the kernel estimator on bandwidth makes asymptotic bias of very high order affect the required condition on the relative magnitudes of the cross-sectional sample size (n) and the time-series length (t). in particular, it makes the condition on n and t stronger and more complicated than those typically observed in the long-panel literature without kernel smoothing. we also consider a split-panel jackknife method to correct bias and construction of confidence intervals. an empirical application and monte carlo simulations illustrate our procedure in finite samples.", "categories": "econ.em", "created": "2018-02-24", "updated": "2019-05-27", "authors": ["ryo okui", "takahide yanagi"], "url": "https://arxiv.org/abs/1802.08825"}, {"title": "identifying the occurrence or non occurrence of cognitive bias in   situations resembling the monty hall problem", "id": "1802.08935", "abstract": "people reason heuristically in situations resembling inferential puzzles such as bertrand's box paradox and the monty hall problem. the practical significance of that fact for economic decision making is uncertain because a departure from sound reasoning may, but does not necessarily, result in a \"cognitively biased\" outcome different from what sound reasoning would have produced. criteria are derived here, applicable to both experimental and non-experimental situations, for heuristic reasoning in an inferential-puzzle situations to result, or not to result, in cognitively bias. in some situations, neither of these criteria is satisfied, and whether or not agents' posterior probability assessments or choices are cognitively biased cannot be determined.", "categories": "econ.em", "created": "2018-02-24", "updated": "", "authors": ["fatemeh borhani", "edward j. green"], "url": "https://arxiv.org/abs/1802.08935"}, {"title": "persuading a consumer to visit", "id": "1802.09396", "abstract": "we consider a variation on the classic weitzman search problem: competing firms with products of unknown quality may design how much information a consumer's visit will glean. after observing these information structures, the consumer then decides how to search sequentially across the firms for a high value product. if there are no search frictions, then there is a unique symmetric equilibrium in pure strategies, and the firms are not fully informative. with search frictions, the information a visit will reveal depends in a systematic way on the ex-ante probability that a firm's product is high quality. when the expected quality of the product is sufficiently high, there is a unique symmetric equilibrium in which firms are fully informative. there, a small search cost leads to the perfect competition level of information provision--consumers gain when firms are forced to compete on information. conversely, in the low and medium expected quality cases there are no pure strategy equilibria. instead, firms mix over a continuum of levels of information: in the low expected quality case they provide full information with probability zero; and in the medium expected quality case they provide full information with positive probability. in both cases there are mixed strategy equilibria in which the firms' realized information structures are blackwell comparable. moreover, though recall is permitted, in each case there are equilibria in which the consumer never returns.", "categories": "math.pr cs.gt econ.th q-fin.ec", "created": "2018-02-26", "updated": "2020-03-04", "authors": ["mark whitmeyer"], "url": "https://arxiv.org/abs/1802.09396"}, {"title": "forecasting the impact of state pension reforms in post-brexit england   and wales using microsimulation and deep learning", "id": "1802.09427", "abstract": "we employ stochastic dynamic microsimulations to analyse and forecast the pension cost dependency ratio for england and wales from 1991 to 2061, evaluating the impact of the ongoing state pension reforms and changes in international migration patterns under different brexit scenarios. to fully account for the recently observed volatility in life expectancies, we propose mortality rate model based on deep learning techniques, which discovers complex patterns in data and extrapolated trends. our results show that the recent reforms can effectively stave off the \"pension crisis\" and bring back the system on a sounder fiscal footing. at the same time, increasingly more workers can expect to spend greater share of their lifespan in retirement, despite the eligibility age rises. the population ageing due to the observed postponement of death until senectitude often occurs with the compression of morbidity, and thus will not, perforce, intrinsically strain healthcare costs. to a lesser degree, the future pension cost dependency ratio will depend on the post-brexit relations between the uk and the eu, with \"soft\" alignment on the free movement lowering the relative cost of the pension system compared to the \"hard\" one. in the long term, however, the ratio has a rising tendency.", "categories": "econ.em q-fin.gn", "created": "2018-02-06", "updated": "2018-04-21", "authors": ["agnieszka werpachowska"], "url": "https://arxiv.org/abs/1802.09427"}, {"title": "on the solution of the variational optimisation in the rational   inattention framework", "id": "1802.09869", "abstract": "i analyse the solution method for the variational optimisation problem in the rational inattention framework proposed by christopher a. sims. the solution, in general, does not exist, although it may exist in exceptional cases. i show that the solution does not exist for the quadratic and the logarithmic objective functions analysed by sims (2003, 2006). for a linear-quadratic objective function a solution can be constructed under restrictions on all but one of its parameters. this approach is, therefore, unlikely to be applicable to a wider set of economic models.", "categories": "econ.em", "created": "2018-02-27", "updated": "2018-07-18", "authors": ["nigar hashimzade"], "url": "https://arxiv.org/abs/1802.09869"}, {"title": "partial identification of expectations with interval data", "id": "1802.10490", "abstract": "a conditional expectation function (cef) can at best be partially identified when the conditioning variable is interval censored. when the number of bins is small, existing methods often yield minimally informative bounds. we propose three innovations that make meaningful inference possible in interval data contexts. first, we prove novel nonparametric bounds for contexts where the distribution of the censored variable is known. second, we show that a class of measures that describe the conditional mean across a fixed interval of the conditioning space can often be bounded tightly even when the cef itself cannot. third, we show that a constraint on cef curvature can either tighten bounds or can substitute for the monotonicity assumption often made in interval data applications. we derive analytical bounds that use the first two innovations, and develop a numerical method to calculate bounds under the third. we show the performance of the method in simulations and then present two applications. first, we resolve a known problem in the estimation of mortality as a function of education: because individuals with high school or less are a smaller and thus more negatively selected group over time, estimates of their mortality change are likely to be biased. our method makes it possible to hold education rank bins constant over time, revealing that current estimates of rising mortality for less educated women are biased upward in some cases by a factor of three. second, we apply the method to the estimation of intergenerational mobility, where researchers frequently use coarsely measured education data in the many contexts where matched parent-child income data are unavailable. conventional measures like the rank-rank correlation may be uninformative once interval censoring is taken into account; cef interval-based measures of mobility are bounded tightly.", "categories": "econ.em stat.ap stat.me", "created": "2018-02-28", "updated": "", "authors": ["sam asher", "paul novosad", "charlie rafkin"], "url": "https://arxiv.org/abs/1802.10490"}, {"title": "dimensional analysis in economics: a study of the neoclassical economic   growth model", "id": "1802.10528", "abstract": "the fundamental purpose of the present research article is to introduce the basic principles of dimensional analysis in the context of the neoclassical economic theory, in order to apply such principles to the fundamental relations that underlay most models of economic growth. in particular, basic instruments from dimensional analysis are used to evaluate the analytical consistency of the neoclassical economic growth model. the analysis shows that an adjustment to the model is required in such a way that the principle of dimensional homogeneity is satisfied.", "categories": "econ.em physics.soc-ph q-fin.gn", "created": "2018-02-28", "updated": "", "authors": ["miguel alvarez texocotitla", "m. david alvarez hernandez", "shani alvarez hernandez"], "url": "https://arxiv.org/abs/1802.10528"}, {"title": "synthetic control methods and big data", "id": "1803.00096", "abstract": "many macroeconomic policy questions may be assessed in a case study framework, where the time series of a treated unit is compared to a counterfactual constructed from a large pool of control units. i provide a general framework for this setting, tailored to predict the counterfactual by minimizing a tradeoff between underfitting (bias) and overfitting (variance). the framework nests recently proposed structural and reduced form machine learning approaches as special cases. furthermore, difference-in-differences with matching and the original synthetic control are restrictive cases of the framework, in general not minimizing the bias-variance objective. using simulation studies i find that machine learning methods outperform traditional methods when the number of potential controls is large or the treated unit is substantially different from the controls. equipped with a toolbox of approaches, i revisit a study on the effect of economic liberalisation on economic growth. i find effects for several countries where no effect was found in the original study. furthermore, i inspect how a systematically important bank respond to increasing capital requirements by using a large pool of banks to estimate the counterfactual. finally, i assess the effect of a changing product price on product sales using a novel scanner dataset.", "categories": "econ.em", "created": "2018-02-28", "updated": "", "authors": ["daniel kinn"], "url": "https://arxiv.org/abs/1803.00096"}, {"title": "deep learning for causal inference", "id": "1803.00149", "abstract": "in this paper, we propose deep learning techniques for econometrics, specifically for causal inference and for estimating individual as well as average treatment effects. the contribution of this paper is twofold: 1. for generalized neighbor matching to estimate individual and average treatment effects, we analyze the use of autoencoders for dimensionality reduction while maintaining the local neighborhood structure among the data points in the embedding space. this deep learning based technique is shown to perform better than simple k nearest neighbor matching for estimating treatment effects, especially when the data points have several features/covariates but reside in a low dimensional manifold in high dimensional space. we also observe better performance than manifold learning methods for neighbor matching. 2. propensity score matching is one specific and popular way to perform matching in order to estimate average and individual treatment effects. we propose the use of deep neural networks (dnns) for propensity score matching, and present a network called propensitynet for this. this is a generalization of the logistic regression technique traditionally used to estimate propensity scores and we show empirically that dnns perform better than logistic regression at propensity score matching. code for both methods will be made available shortly on github at: https://github.com/vikas84bf", "categories": "econ.em cs.lg stat.ml", "created": "2018-02-28", "updated": "", "authors": ["vikas ramachandra"], "url": "https://arxiv.org/abs/1803.00149"}, {"title": "optimization-based algorithm for evolutionarily stable strategies   against pure mutations", "id": "1803.00607", "abstract": "evolutionarily stable strategy (ess) is an important solution concept in game theory which has been applied frequently to biological models. informally an ess is a strategy that if followed by the population cannot be taken over by a mutation strategy that is initially rare. finding such a strategy has been shown to be difficult from a theoretical complexity perspective. we present an algorithm for the case where mutations are restricted to pure strategies, and present experiments on several game classes including random and a recently-proposed cancer model. our algorithm is based on a mixed-integer non-convex feasibility program formulation, which constitutes the first general optimization formulation for this problem. it turns out that the vast majority of the games included in the experiments contain ess with small support, and our algorithm is outperformed by a support-enumeration based approach. however we suspect our algorithm may be useful in the future as games are studied that have ess with potentially larger and unknown support size.", "categories": "cs.gt cs.ai econ.th math.oc q-bio.pe", "created": "2018-03-01", "updated": "2019-01-13", "authors": ["sam ganzfried"], "url": "https://arxiv.org/abs/1803.00607"}, {"title": "permutation tests for equality of distributions of functional data", "id": "1803.00798", "abstract": "economic data are often generated by stochastic processes that take place in continuous time, though observations may occur only at discrete times. for example, electricity and gas consumption take place in continuous time. data generated by a continuous time stochastic process are called functional data. this paper is concerned with comparing two or more stochastic processes that generate functional data. the data may be produced by a randomized experiment in which there are multiple treatments. the paper presents a method for testing the hypothesis that the same stochastic process generates all the functional data. the test described here applies to both functional data and multiple treatments. it is implemented as a combination of two permutation tests. this ensures that in finite samples, the true and nominal probabilities that each test rejects a correct null hypothesis are equal. the paper presents upper and lower bounds on the asymptotic power of the test under alternative hypotheses. the results of monte carlo experiments and an application to an experiment on billing and pricing of natural gas illustrate the usefulness of the test.", "categories": "econ.em stat.me", "created": "2018-03-02", "updated": "2019-12-26", "authors": ["federico a. bugni", "joel l. horowitz"], "url": "https://arxiv.org/abs/1803.00798"}, {"title": "an note on why geographically weighted regression overcomes   multidimensional-kernel-based varying-coefficient model", "id": "1803.01402", "abstract": "it is widely known that geographically weighted regression(gwr) is essentially same as varying-coefficient model. in the former research about varying-coefficient model, scholars tend to use multidimensional-kernel-based locally weighted estimation(mlwe) so that information of both distance and direction is considered. however, when we construct the local weight matrix of geographically weighted estimation, distance among the locations in the neighbor is the only factor controlling the value of entries of weight matrix. in other word, estimation of gwr is distance-kernel-based. thus, in this paper, under stationary and limited dependent data with multidimensional subscripts, we analyze the local mean squared properties of without any assumption of the form of coefficient functions and compare it with mlwe. according to the theoretical and simulation results, geographically-weighted locally linear estimation(gwle) is asymptotically more efficient than mlwe. furthermore, a relationship between optimal bandwith selection and design of scale parameters is also obtained.", "categories": "econ.em", "created": "2018-03-04", "updated": "2018-04-12", "authors": ["zihao yuan"], "url": "https://arxiv.org/abs/1803.01402"}, {"title": "a comment on 'testing goodwin: growth cycles in ten oecd countries'", "id": "1803.01527", "abstract": "we revisit the results of harvie (2000) and show how correcting for a reporting mistake in some of the estimated parameter values leads to significantly different conclusions, including realistic parameter values for the philips curve and estimated equilibrium employment rates exhibiting on average one tenth of the relative error of those obtained in harvie (2000).", "categories": "econ.em q-fin.ec", "created": "2018-03-05", "updated": "", "authors": ["matheus r. grasselli", "aditya maheshwari"], "url": "https://arxiv.org/abs/1803.01527"}, {"title": "pricing mechanism in information goods", "id": "1803.01530", "abstract": "we study three pricing mechanisms' performance and their effects on the participants in the data industry from the data supply chain perspective. a win-win pricing strategy for the players in the data supply chain is proposed. we obtain analytical solutions in each pricing mechanism, including the decentralized and centralized pricing, nash bargaining pricing, and revenue sharing mechanism.", "categories": "econ.em", "created": "2018-03-05", "updated": "", "authors": ["xinming li", "huaqing wang"], "url": "https://arxiv.org/abs/1803.01530"}, {"title": "testing a goodwin model with general capital accumulation rate", "id": "1803.01536", "abstract": "we perform econometric tests on a modified goodwin model where the capital accumulation rate is constant but not necessarily equal to one as in the original model (goodwin, 1967). in addition to this modification, we find that addressing the methodological and reporting issues in harvie (2000) leads to remarkably better results, with near perfect agreement between the estimates of equilibrium employment rates and the corresponding empirical averages, as well as significantly improved estimates of equilibrium wage shares. despite its simplicity and obvious limitations, the performance of the modified goodwin model implied by our results show that it can be used as a starting point for more sophisticated models for endogenous growth cycles.", "categories": "econ.em q-fin.ec", "created": "2018-03-05", "updated": "", "authors": ["matheus r. grasselli", "aditya maheshwari"], "url": "https://arxiv.org/abs/1803.01536"}, {"title": "an online algorithm for learning buyer behavior under realistic pricing   restrictions", "id": "1803.01968", "abstract": "we propose a new efficient online algorithm to learn the parameters governing the purchasing behavior of a utility maximizing buyer, who responds to prices, in a repeated interaction setting. the key feature of our algorithm is that it can learn even non-linear buyer utility while working with arbitrary price constraints that the seller may impose. this overcomes a major shortcoming of previous approaches, which use unrealistic prices to learn these parameters making them unsuitable in practice.", "categories": "stat.ml cs.lg econ.em math.oc", "created": "2018-03-05", "updated": "", "authors": ["debjyoti saharoy", "theja tulabandhula"], "url": "https://arxiv.org/abs/1803.01968"}, {"title": "kinetic models for optimal control of wealth inequalities", "id": "1803.02171", "abstract": "we introduce and discuss optimal control strategies for kinetic models for wealth distribution in a simple market economy, acting to minimize the variance of the wealth density among the population. our analysis is based on a finite time horizon approximation, or model predictive control, of the corresponding control problem for the microscopic agents' dynamic and results in an alternative theoretical approach to the taxation and redistribution policy at a global level. it is shown that in general the control is able to modify the pareto index of the stationary solution of the corresponding boltzmann kinetic equation, and that this modification can be exactly quantified. connections between previous fokker-planck based models and taxation-redistribution policies and the present approach are also discussed.", "categories": "physics.soc-ph econ.gn math.oc q-fin.ec q-fin.gn", "created": "2018-03-06", "updated": "2018-07-27", "authors": ["bertram d\u00fcring", "lorenzo pareschi", "giuseppe toscani"], "url": "https://arxiv.org/abs/1803.02171"}, {"title": "a nonparametric approach to measure the heterogeneous spatial   association: under spatial temporal data", "id": "1803.02334", "abstract": "spatial association and heterogeneity are two critical areas in the research about spatial analysis, geography, statistics and so on. though large amounts of outstanding methods has been proposed and studied, there are few of them tend to study spatial association under heterogeneous environment. additionally, most of the traditional methods are based on distance statistic and spatial weighted matrix. however, in some abstract spatial situations, distance statistic can not be applied since we can not even observe the geographical locations directly. meanwhile, under these circumstances, due to invisibility of spatial positions, designing of weight matrix can not absolutely avoid subjectivity. in this paper, a new entropy-based method, which is data-driven and distribution-free, has been proposed to help us investigate spatial association while fully taking the fact that heterogeneity widely exist. specifically, this method is not bounded with distance statistic or weight matrix. asymmetrical dependence is adopted to reflect the heterogeneity in spatial association for each individual and the whole discussion in this paper is performed on spatio-temporal data with only assuming stationary m-dependent over time.", "categories": "econ.em", "created": "2018-03-06", "updated": "2018-03-22", "authors": ["zihao yuan"], "url": "https://arxiv.org/abs/1803.02334"}, {"title": "almost sure uniqueness of a global minimum without convexity", "id": "1803.02415", "abstract": "this paper establishes the argmin of a random objective function to be unique almost surely. this paper first formulates a general result that proves almost sure uniqueness without convexity of the objective function. the general result is then applied to a variety of applications in statistics. four applications are discussed, including uniqueness of m-estimators, both classical likelihood and penalized likelihood estimators, and two applications of the argmin theorem, threshold regression and weak identification.", "categories": "econ.em math.st stat.th", "created": "2018-03-06", "updated": "2019-02-19", "authors": ["gregory cox"], "url": "https://arxiv.org/abs/1803.02415"}, {"title": "a first look at browser-based cryptojacking", "id": "1803.02887", "abstract": "in this paper, we examine the recent trend towards in-browser mining of cryptocurrencies; in particular, the mining of monero through coinhive and similar code- bases. in this model, a user visiting a website will download a javascript code that executes client-side in her browser, mines a cryptocurrency, typically without her consent or knowledge, and pays out the seigniorage to the website. websites may consciously employ this as an alternative or to supplement advertisement revenue, may offer premium content in exchange for mining, or may be unwittingly serving the code as a result of a breach (in which case the seigniorage is collected by the attacker). the cryptocurrency monero is preferred seemingly for its unfriendliness to large-scale asic mining that would drive browser-based efforts out of the market, as well as for its purported privacy features. in this paper, we survey this landscape, conduct some measurements to establish its prevalence and profitability, outline an ethical framework for considering whether it should be classified as an attack or business opportunity, and make suggestions for the detection, mitigation and/or prevention of browser-based mining for non- consenting users.", "categories": "cs.cr cs.cy cs.hc econ.em", "created": "2018-03-07", "updated": "", "authors": ["shayan eskandari", "andreas leoutsarakos", "troy mursch", "jeremy clark"], "url": "https://arxiv.org/abs/1803.02887"}, {"title": "does the time horizon of the return predictive effect of investor   sentiment vary with stock characteristics? a granger causality analysis in   the frequency domain", "id": "1803.02962", "abstract": "behavioral theories posit that investor sentiment exhibits predictive power for stock returns, whereas there is little study have investigated the relationship between the time horizon of the predictive effect of investor sentiment and the firm characteristics. to this end, by using a granger causality analysis in the frequency domain proposed by lemmens et al. (2008), this paper examine whether the time horizon of the predictive effect of investor sentiment on the u.s. returns of stocks vary with different firm characteristics (e.g., firm size (size), book-to-market equity (b/m) rate, operating profitability (op) and investment (inv)). the empirical results indicate that investor sentiment has a long-term (more than 12 months) or short-term (less than 12 months) predictive effect on stock returns with different firm characteristics. specifically, the investor sentiment has strong predictability in the stock returns for smaller size stocks, lower b/m stocks and lower op stocks, both in the short term and long term, but only has a short-term predictability for higher quantile ones. the investor sentiment merely has predictability for the returns of smaller inv stocks in the short term, but has a strong short-term and long-term predictability for larger inv stocks. these results have important implications for the investors for the planning of the short and the long run stock investment strategy.", "categories": "econ.em q-fin.st", "created": "2018-03-07", "updated": "", "authors": ["yong jiang", "zhongbao zhou"], "url": "https://arxiv.org/abs/1803.02962"}, {"title": "a study of strategy to the remove and ease tbt for increasing export in   gcc6 countries", "id": "1803.03394", "abstract": "the last technical barriers to trade(tbt) between countries are non-tariff barriers(ntbs), meaning all trade barriers are possible other than tariff barriers. and the most typical examples are (tbt), which refer to measure technical regulation, standards, procedure for conformity assessment, test & certification etc. therefore, in order to eliminate tbt, wto has made all membership countries automatically enter into an agreement on tbt", "categories": "econ.em", "created": "2018-03-09", "updated": "2018-03-23", "authors": ["yongjae kim"], "url": "https://arxiv.org/abs/1803.03394"}, {"title": "how smart are `water smart landscapes'?", "id": "1803.04593", "abstract": "understanding the effectiveness of alternative approaches to water conservation is crucially important for ensuring the security and reliability of water services for urban residents. we analyze data from one of the longest-running \"cash for grass\" policies - the southern nevada water authority's water smart landscapes program, where homeowners are paid to replace grass with xeric landscaping. we use a twelve year long panel dataset of monthly water consumption records for 300,000 households in las vegas, nevada. utilizing a panel difference-in-differences approach, we estimate the average water savings per square meter of turf removed. we find that participation in this program reduced the average treated household's consumption by 18 percent. we find no evidence that water savings degrade as the landscape ages, or that water savings per unit area are influenced by the value of the rebate. depending on the assumed time horizon of benefits from turf removal, we find that the wsl program cost the water authority about $1.62 per thousand gallons of water saved, which compares favorably to alternative means of water conservation or supply augmentation.", "categories": "econ.em", "created": "2018-03-12", "updated": "", "authors": ["christa brelsford", "joshua k. abbott"], "url": "https://arxiv.org/abs/1803.04593"}, {"title": "inference on a distribution from noisy draws", "id": "1803.04991", "abstract": "we consider a situation where the distribution of a random variable is being estimated by the empirical distribution of noisy measurements of that variable. this is common practice in, for example, teacher value-added models and other fixed-effect models for panel data. we use an asymptotic embedding where the noise shrinks with the sample size to calculate the leading bias in the empirical distribution arising from the presence of noise. the leading bias in the empirical quantile function is equally obtained. these calculations are new in the literature, where only results on smooth functionals such as the mean and variance have been derived. given a closed-form expression for the bias, bias-corrected estimator of the distribution function and quantile function can be constructed. we provide both analytical and jackknife corrections that recenter the limit distribution and yield confidence intervals with correct coverage in large samples. these corrections are non-parametric and easy to implement. our approach can be connected to corrections for selection bias and shrinkage estimation and is to be contrasted with deconvolution. simulation results confirm the much-improved sampling behavior of the corrected estimators.", "categories": "econ.em stat.me", "created": "2018-03-13", "updated": "2019-09-10", "authors": ["koen jochmans", "martin weidner"], "url": "https://arxiv.org/abs/1803.04991"}, {"title": "limitations of p-values and $r^2$ for stepwise regression building: a   fairness demonstration in health policy risk adjustment", "id": "1803.05513", "abstract": "stepwise regression building procedures are commonly used applied statistical tools, despite their well-known drawbacks. while many of their limitations have been widely discussed in the literature, other aspects of the use of individual statistical fit measures, especially in high-dimensional stepwise regression settings, have not. giving primacy to individual fit, as is done with p-values and $r^2$, when group fit may be the larger concern, can lead to misguided decision making. one of the most consequential uses of stepwise regression is in health care, where these tools allocate hundreds of billions of dollars to health plans enrolling individuals with different predicted health care costs. the main goal of this \"risk adjustment\" system is to convey incentives to health plans such that they provide health care services fairly, a component of which is not to discriminate in access or care for persons or groups likely to be expensive. we address some specific limitations of p-values and $r^2$ for high-dimensional stepwise regression in this policy problem through an illustrated example by additionally considering a group-level fairness metric.", "categories": "econ.em cs.cy", "created": "2018-03-14", "updated": "2018-08-06", "authors": ["sherri rose", "thomas g. mcguire"], "url": "https://arxiv.org/abs/1803.05513"}, {"title": "does agricultural subsidies foster italian southern farms? a spatial   quantile regression approach", "id": "1803.05659", "abstract": "during the last decades, public policies become a central pillar in supporting and stabilising agricultural sector. in 1962, eu policy-makers developed the so-called common agricultural policy (cap) to ensure competitiveness and a common market organisation for agricultural products, while 2003 reform decouple the cap from the production to focus only on income stabilization and the sustainability of agricultural sector. notwithstanding farmers are highly dependent to public support, literature on the role played by the cap in fostering agricultural performances is still scarce and fragmented. actual cap policies increases performance differentials between northern central eu countries and peripheral regions. this paper aims to evaluate the effectiveness of cap in stimulate performances by focusing on italian lagged regions. moreover, agricultural sector is deeply rooted in place-based production processes. in this sense, economic analysis which omit the presence of spatial dependence produce biased estimates of the performances. therefore, this paper, using data on subsidies and economic results of farms from the rica dataset which is part of the farm accountancy data network (fadn), proposes a spatial augmented cobb-douglas production function to evaluate the effects of subsidies on farm's performances. the major innovation in this paper is the implementation of a micro-founded quantile version of a spatial lag model to examine how the impact of the subsidies may vary across the conditional distribution of agricultural performances. results show an increasing shape which switch from negative to positive at the median and becomes statistical significant for higher quantiles. additionally, spatial autocorrelation parameter is positive and significant across all the conditional distribution, suggesting the presence of significant spatial spillovers in agricultural performances.", "categories": "econ.em stat.ap", "created": "2018-03-15", "updated": "", "authors": ["marusca de castris", "daniele di gennaro"], "url": "https://arxiv.org/abs/1803.05659"}, {"title": "are bitcoin bubbles predictable? combining a generalized metcalfe's law   and the lppls model", "id": "1803.05663", "abstract": "we develop a strong diagnostic for bubbles and crashes in bitcoin, by analyzing the coincidence (and its absence) of fundamental and technical indicators. using a generalized metcalfe's law based on network properties, a fundamental value is quantified and shown to be heavily exceeded, on at least four occasions, by bubbles that grow and burst. in these bubbles, we detect a universal super-exponential unsustainable growth. we model this universal pattern with the log-periodic power law singularity (lppls) model, which parsimoniously captures diverse positive feedback phenomena, such as herding and imitation. the lppls model is shown to provide an ex-ante warning of market instabilities, quantifying a high crash hazard and probabilistic bracket of the crash time consistent with the actual corrections; although, as always, the precise time and trigger (which straw breaks the camel's back) being exogenous and unpredictable. looking forward, our analysis identifies a substantial but not unprecedented overvaluation in the price of bitcoin, suggesting many months of volatile sideways bitcoin prices ahead (from the time of writing, march 2018).", "categories": "econ.em q-fin.gn", "created": "2018-03-15", "updated": "", "authors": ["spencer wheatley", "didier sornette", "tobias huber", "max reppen", "robert n. gantner"], "url": "https://arxiv.org/abs/1803.05663"}, {"title": "practical volume computation of structured convex bodies, and an   application to modeling portfolio dependencies and financial crises", "id": "1803.05861", "abstract": "we examine volume computation of general-dimensional polytopes and more general convex bodies, defined as the intersection of a simplex by a family of parallel hyperplanes, and another family of parallel hyperplanes or a family of concentric ellipsoids. such convex bodies appear in modeling and predicting financial crises. the impact of crises on the economy (labor, income, etc.) makes its detection of prime interest. certain features of dependencies in the markets clearly identify times of turmoil. we describe the relationship between asset characteristics by means of a copula; each characteristic is either a linear or quadratic form of the portfolio components, hence the copula can be constructed by computing volumes of convex bodies. we design and implement practical algorithms in the exact and approximate setting, we experimentally juxtapose them and study the tradeoff of exactness and accuracy for speed. we analyze the following methods in order of increasing generality: rejection sampling relying on uniformly sampling the simplex, which is the fastest approach, but inaccurate for small volumes; exact formulae based on the computation of integrals of probability distribution functions; an optimized lawrence sign decomposition method, since the polytopes at hand are shown to be simple; markov chain monte carlo algorithms using random walks based on the hit-and-run paradigm generalized to nonlinear convex bodies and relying on new methods for computing a ball enclosed; the latter is experimentally extended to non-convex bodies with very encouraging results. our c++ software, based on cgal and eigen and available on github, is shown to be very effective in up to 100 dimensions. our results offer novel, effective means of computing portfolio dependencies and an indicator of financial crises, which is shown to correctly identify past crises.", "categories": "cs.cg econ.em q-fin.gn", "created": "2018-03-15", "updated": "", "authors": ["ludovic cales", "apostolos chalkis", "ioannis z. emiris", "vissarion fisikopoulos"], "url": "https://arxiv.org/abs/1803.05861"}, {"title": "reputation is required for cooperation to emerge in dynamic networks", "id": "1803.06035", "abstract": "melamed, harrell, and simpson have recently reported on an experiment which appears to show that cooperation can arise in a dynamic network without reputational knowledge, i.e., purely via dynamics [1]. we believe that their experimental design is actually not testing this, in so far as players do know the last action of their current partners before making a choice on their own next action and subsequently deciding which link to cut. had the authors given no information at all, the result would be a decline in cooperation as shown in [2].", "categories": "physics.soc-ph cs.si econ.th", "created": "2018-03-15", "updated": "", "authors": ["jose a. cuesta", "carlos gracia-l\u00e1zaro", "yamir moreno", "angel s\u00e1nchez"], "url": "https://arxiv.org/abs/1803.06035"}, {"title": "business cycles in economics", "id": "1803.06108", "abstract": "the business cycles are generated by the oscillating macro-/micro-/nano- economic output variables in the economy of the scale and the scope in the amplitude/frequency/phase/time domains in the economics. the accurate forward looking assumptions on the business cycles oscillation dynamics can optimize the financial capital investing and/or borrowing by the economic agents in the capital markets. the book's main objective is to study the business cycles in the economy of the scale and the scope, formulating the ledenyov unified business cycles theory in the ledenyov classic and quantum econodynamics.", "categories": "econ.em", "created": "2018-03-16", "updated": "", "authors": ["viktor o. ledenyov", "dimitri o. ledenyov"], "url": "https://arxiv.org/abs/1803.06108"}, {"title": "evaluating conditional cash transfer policies with machine learning   methods", "id": "1803.06401", "abstract": "this paper presents an out-of-sample prediction comparison between major machine learning models and the structural econometric model. over the past decade, machine learning has established itself as a powerful tool in many prediction applications, but this approach is still not widely adopted in empirical economic studies. to evaluate the benefits of this approach, i use the most common machine learning algorithms, cart, c4.5, lasso, random forest, and adaboost, to construct prediction models for a cash transfer experiment conducted by the progresa program in mexico, and i compare the prediction results with those of a previous structural econometric study. two prediction tasks are performed in this paper: the out-of-sample forecast and the long-term within-sample simulation. for the out-of-sample forecast, both the mean absolute error and the root mean square error of the school attendance rates found by all machine learning models are smaller than those found by the structural model. random forest and adaboost have the highest accuracy for the individual outcomes of all subgroups. for the long-term within-sample simulation, the structural model has better performance than do all of the machine learning models. the poor within-sample fitness of the machine learning model results from the inaccuracy of the income and pregnancy prediction models. the result shows that the machine learning model performs better than does the structural model when there are many data to learn; however, when the data are limited, the structural model offers a more sensible prediction. the findings of this paper show promise for adopting machine learning in economic policy analyses in the era of big data.", "categories": "econ.em stat.ml", "created": "2018-03-16", "updated": "", "authors": ["tzai-shuen chen"], "url": "https://arxiv.org/abs/1803.06401"}, {"title": "large-scale dynamic predictive regressions", "id": "1803.06738", "abstract": "we develop a novel \"decouple-recouple\" dynamic predictive strategy and contribute to the literature on forecasting and economic decision making in a data-rich environment. under this framework, clusters of predictors generate different latent states in the form of predictive densities that are later synthesized within an implied time-varying latent factor model. as a result, the latent inter-dependencies across predictive densities and biases are sequentially learned and corrected. unlike sparse modeling and variable selection procedures, we do not assume a priori that there is a given subset of active predictors, which characterize the predictive density of a quantity of interest. we test our procedure by investigating the predictive content of a large set of financial ratios and macroeconomic variables on both the equity premium across different industries and the inflation rate in the u.s., two contexts of topical interest in finance and macroeconomics. we find that our predictive synthesis framework generates both statistically and economically significant out-of-sample benefits while maintaining interpretability of the forecasting variables. in addition, the main empirical results highlight that our proposed framework outperforms both lasso-type shrinkage regressions, factor based dimension reduction, sequential variable selection, and equal-weighted linear pooling methodologies.", "categories": "stat.me econ.em q-fin.st", "created": "2018-03-18", "updated": "", "authors": ["daniele bianchi", "kenichiro mcalinn"], "url": "https://arxiv.org/abs/1803.06738"}, {"title": "adversarial generalized method of moments", "id": "1803.07164", "abstract": "we provide an approach for learning deep neural net representations of models described via conditional moment restrictions. conditional moment restrictions are widely used, as they are the language by which social scientists describe the assumptions they make to enable causal inference. we formulate the problem of estimating the underling model as a zero-sum game between a modeler and an adversary and apply adversarial training. our approach is similar in nature to generative adversarial networks (gan), though here the modeler is learning a representation of a function that satisfies a continuum of moment conditions and the adversary is identifying violating moments. we outline ways of constructing effective adversaries in practice, including kernels centered by k-means clustering, and random forests. we examine the practical performance of our approach in the setting of non-parametric instrumental variable regression.", "categories": "econ.em cs.gt cs.lg math.st stat.ml stat.th", "created": "2018-03-19", "updated": "2018-04-24", "authors": ["greg lewis", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/1803.07164"}, {"title": "testing for unobserved heterogeneous treatment effects in a nonseparable   model with endogenous selection", "id": "1803.07514", "abstract": "unobserved heterogeneous treatment effects have been emphasized in the policy evaluation literature. this paper proposes a nonparametric test for unobserved heterogeneous treatment effects in a general framework, allowing for self-selection to the treatment. the proposed modified kolmogorov-smirnov-type test is consistent and simple to implement. monte carlo simulations show that our test performs well in finite samples. for illustration, we apply our test to study heterogeneous treatment effects of the job training partnership act on earnings and the impacts of fertility on family income.", "categories": "econ.em", "created": "2018-03-20", "updated": "", "authors": ["yu-chin hsu", "ta-cheng huang", "haiqing xu"], "url": "https://arxiv.org/abs/1803.07514"}, {"title": "testing continuity of a density via g-order statistics in the regression   discontinuity design", "id": "1803.07951", "abstract": "in the regression discontinuity design (rdd), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., mccrary (2008). in this paper we propose an approximate sign test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under two complementary asymptotic frameworks. in the first asymptotic framework, the number q of observations local to the cut-off is fixed as the sample size n diverges to infinity, while in the second framework q diverges to infinity slowly as n diverges to infinity. under both of these frameworks, we show that the test we propose is asymptotically valid in the sense that it has limiting rejection probability under the null hypothesis not exceeding the nominal level. more importantly, the test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, and exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity. in a simulation study, we find that the approximate sign test provides good control of the rejection probability under the null hypothesis while remaining competitive under the alternative hypothesis. we finally apply our test to the design in lee (2008), a well-known application of the rdd to study incumbency advantage.", "categories": "econ.em", "created": "2018-03-21", "updated": "2020-02-11", "authors": ["federico a. bugni", "ivan a. canay"], "url": "https://arxiv.org/abs/1803.07951"}, {"title": "network and panel quantile effects via distribution regression", "id": "1803.08154", "abstract": "this paper provides a method to construct simultaneous confidence bands for quantile functions and quantile effects in nonlinear network and panel models with unobserved two-way effects, strictly exogenous covariates, and possibly discrete outcome variables. the method is based upon projection of simultaneous confidence bands for distribution functions constructed from fixed effects distribution regression estimators. these fixed effects estimators are debiased to deal with the incidental parameter problem. under asymptotic sequences where both dimensions of the data set grow at the same rate, the confidence bands for the quantile functions and effects have correct joint coverage in large samples. an empirical application to gravity models of trade illustrates the applicability of the methods to network data.", "categories": "econ.em stat.me", "created": "2018-03-21", "updated": "2020-06-08", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "martin weidner"], "url": "https://arxiv.org/abs/1803.08154"}, {"title": "mislearning from censored data: the gambler's fallacy in   optimal-stopping problems", "id": "1803.08170", "abstract": "i study endogenous learning dynamics for people expecting systematic reversals from random sequences - the \"gambler's fallacy.\" biased agents face an optimal-stopping problem. they are uncertain about the underlying distribution and learn its parameters from predecessors. agents stop when early draws are \"good enough,\" so predecessors' experience contain negative streaks but not positive streaks. since biased agents understate the likelihood of consecutive below-average draws, society converges to over-pessimistic beliefs about the distribution's mean and stops too early. agents uncertain about the distribution's variance overestimate it to an extent that depends on predecessors' stopping thresholds. subsidizing search partially mitigates long-run belief distortions.", "categories": "q-fin.ec cs.gt econ.th", "created": "2018-03-21", "updated": "2020-03-21", "authors": ["kevin he"], "url": "https://arxiv.org/abs/1803.08170"}, {"title": "causal inference for survival analysis", "id": "1803.08218", "abstract": "in this paper, we propose the use of causal inference techniques for survival function estimation and prediction for subgroups of the data, upto individual units. tree ensemble methods, specifically random forests were modified for this purpose. a real world healthcare dataset was used with about 1800 patients with breast cancer, which has multiple patient covariates as well as disease free survival days (dfs) and a death event binary indicator (y). we use the type of cancer curative intervention as the treatment variable (t=0 or 1, binary treatment case in our example). the algorithm is a 2 step approach. in step 1, we estimate heterogeneous treatment effects using a causaltree with the dfs as the dependent variable. next, in step 2, for each selected leaf of the causaltree with distinctly different average treatment effect (with respect to survival), we fit a survival forest to all the patients in that leaf, one forest each for treatment t=0 as well as t=1 to get estimated patient level survival curves for each treatment (more generally, any model can be used at this step). then, we subtract the patient level survival curves to get the differential survival curve for a given patient, to compare the survival function as a result of the 2 treatments. the path to a selected leaf also gives us the combination of patient features and their values which are causally important for the treatment effect difference at the leaf.", "categories": "econ.em", "created": "2018-03-21", "updated": "", "authors": ["vikas ramachandra"], "url": "https://arxiv.org/abs/1803.08218"}, {"title": "two-way fixed effects estimators with heterogeneous treatment effects", "id": "1803.08807", "abstract": "linear regressions with period and group fixed effects are widely used to estimate treatment effects. we show that they estimate weighted sums of the average treatment effects (ate) in each group and period, with weights that may be negative. due to the negative weights, the linear regression coefficient may for instance be negative while all the ates are positive. we propose another estimator that solves this issue. in the two applications we revisit, it is significantly different from the linear regression estimator.", "categories": "econ.em", "created": "2018-03-21", "updated": "2020-03-05", "authors": ["cl\u00e9ment de chaisemartin", "xavier d'haultf\u0153uille"], "url": "https://arxiv.org/abs/1803.08807"}, {"title": "edgeworth trading on networks", "id": "1803.08836", "abstract": "we define a class of pure exchange edgeworth trading processes that under minimal assumptions converge to a stable set in the space of allocations, and characterise the pareto set of these processes. choosing a specific process belonging to this class, that we define fair trading, we analyse the trade dynamics between agents located on a weighted network. we determine the conditions under which there always exists a one-to-one map between the set of networks and the set of stable equilibria. this result is used to understand what is the effect of the network topology on the trade dynamics and on the final allocation. we find that the positions in the network affect the distribution of the utility gains, given the initial allocations", "categories": "econ.em cs.gt", "created": "2018-03-23", "updated": "2019-01-08", "authors": ["daniele cassese", "paolo pin"], "url": "https://arxiv.org/abs/1803.08836"}, {"title": "how does monetary policy affect income inequality in japan? evidence   from grouped data", "id": "1803.08868", "abstract": "we examine the effects of monetary policy on income inequality in japan using a novel econometric approach that jointly estimates the gini coefficient based on micro-level grouped data of households and the dynamics of macroeconomic quantities. our results indicate different effects on income inequality for different types of households: a monetary tightening increases inequality when income data is based on households whose head is employed (workers' households), while the effect reverses over the medium term when considering a broader definition of households. differences in the relative strength of the transmission channels can account for this finding. finally we demonstrate that the proposed joint estimation strategy leads to more informative inference while results based on the frequently used two-step estimation approach yields inconclusive results.", "categories": "econ.em", "created": "2018-03-23", "updated": "", "authors": ["martin feldkircher", "kazuhiko kakamu"], "url": "https://arxiv.org/abs/1803.08868"}, {"title": "difference-in-differences with multiple time periods", "id": "1803.09015", "abstract": "in this article, we consider identification, estimation, and inference procedures for treatment effect parameters using difference-in-differences (did) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. we show that a family of causal effect parameters are identified in staggered did setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. we also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. we establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001--2007. open-source software is available for implementing the proposed methods.", "categories": "econ.em math.st stat.ap stat.th", "created": "2018-03-23", "updated": "2020-08-17", "authors": ["brantly callaway", "pedro h. c. sant'anna"], "url": "https://arxiv.org/abs/1803.09015"}, {"title": "schooling choice, labour market matching, and wages", "id": "1803.09020", "abstract": "we develop inference for a two-sided matching model where the characteristics of agents on one side of the market are endogenous due to pre-matching investments. the model can be used to measure the impact of frictions in labour markets using a single cross-section of matched employer-employee data. the observed matching of workers to firms is the outcome of a discrete, two-sided matching process where firms with heterogeneous preferences over education sequentially choose workers according to an index correlated with worker preferences over firms. the distribution of education arises in equilibrium from a bayesian game: workers, knowing the distribution of worker and firm types, invest in education prior to the matching process. although the observed matching exhibits strong cross-sectional dependence due to the matching process, we propose an asymptotically valid inference procedure that combines discrete choice methods with simulation.", "categories": "econ.em", "created": "2018-03-23", "updated": "2019-08-27", "authors": ["jacob schwartz"], "url": "https://arxiv.org/abs/1803.09020"}, {"title": "efficient discovery of heterogeneous treatment effects in randomized   experiments via anomalous pattern detection", "id": "1803.09159", "abstract": "in the recent literature on estimating heterogeneous treatment effects, each proposed method makes its own set of restrictive assumptions about the intervention's effects and which subpopulations to explicitly estimate. moreover, the majority of the literature provides no mechanism to identify which subpopulations are the most affected--beyond manual inspection--and provides little guarantee on the correctness of the identified subpopulations. therefore, we propose treatment effect subset scan (tess), a new method for discovering which subpopulation in a randomized experiment is most significantly affected by a treatment. we frame this challenge as a pattern detection problem where we efficiently maximize a nonparametric scan statistic over subpopulations. furthermore, we identify the subpopulation which experiences the largest distributional change as a result of the intervention, while making minimal assumptions about the intervention's effects or the underlying data generating process. in addition to the algorithm, we demonstrate that the asymptotic type i and ii error can be controlled, and provide sufficient conditions for detection consistency--i.e., exact identification of the affected subpopulation. finally, we validate the efficacy of the method by discovering heterogeneous treatment effects in simulations and in real-world data from a well-known program evaluation study.", "categories": "stat.me econ.em stat.ml", "created": "2018-03-24", "updated": "2018-06-07", "authors": ["edward mcfowland", "sriram somanchi", "daniel b. neill"], "url": "https://arxiv.org/abs/1803.09159"}, {"title": "panel data analysis with heterogeneous dynamics", "id": "1803.09452", "abstract": "this paper proposes a model-free approach to analyze panel data with heterogeneous dynamic structures across observational units. we first compute the sample mean, autocovariances, and autocorrelations for each unit, and then estimate the parameters of interest based on their empirical distributions. we then investigate the asymptotic properties of our estimators using double asymptotics and propose split-panel jackknife bias correction and inference based on the cross-sectional bootstrap. we illustrate the usefulness of our procedures by studying the deviation dynamics of the law of one price. monte carlo simulations confirm that the proposed bias correction is effective and yields valid inference in small samples.", "categories": "econ.em", "created": "2018-03-26", "updated": "2019-01-15", "authors": ["ryo okui", "takahide yanagi"], "url": "https://arxiv.org/abs/1803.09452"}, {"title": "a perfect specialization model for gravity equation in bilateral trade   based on production structure", "id": "1803.09935", "abstract": "although initially originated as a totally empirical relationship to explain the volume of trade between two partners, gravity equation has been the focus of several theoretic models that try to explain it. specialization models are of great importance in providing a solid theoretic ground for gravity equation in bilateral trade. some research papers try to improve specialization models by adding imperfect specialization to model, but we believe it is unnecessary complication. we provide a perfect specialization model based on the phenomenon we call tradability, which overcomes the problems with simpler initial. we provide empirical evidence using estimates on panel data of bilateral trade of 40 countries over 10 years that support the theoretical model. the empirical results have implied that tradability is the only reason for deviations of data from basic perfect specialization models.", "categories": "econ.em q-fin.gn", "created": "2018-03-27", "updated": "", "authors": ["majid einian", "farshad ranjbar ravasan"], "url": "https://arxiv.org/abs/1803.09935"}, {"title": "emergence of cooperation in the thermodynamic limit", "id": "1803.10083", "abstract": "predicting how cooperative behavior arises in the thermodynamic limit is one of the outstanding problems in evolutionary game theory. for two player games, cooperation is seldom the nash equilibrium. however, in the thermodynamic limit cooperation is the natural recourse regardless of whether we are dealing with humans or animals. in this work, we use the analogy with the ising model to predict how cooperation arises in the thermodynamic limit.", "categories": "physics.soc-ph cond-mat.stat-mech cs.gt econ.em math-ph math.mp q-bio.pe", "created": "2018-03-27", "updated": "2020-03-13", "authors": ["colin benjamin", "shubhayan sarkar"], "url": "https://arxiv.org/abs/1803.10083"}, {"title": "generalized laplace inference in multiple change-points models", "id": "1803.10871", "abstract": "under the classical long-span asymptotic framework we develop a class of generalized laplace (gl) inference methods for the change-point dates in a linear time series regression model with multiple structural changes analyzed in, e.g., bai and perron (1998). the gl estimator is defined by an integration rather than optimization-based method and relies on the least-squares criterion function. it is interpreted as a classical (non-bayesian) estimator and the inference methods proposed retain a frequentist interpretation. this approach provides a better approximation about the uncertainty in the data of the change-points relative to existing methods. on the theoretical side, depending on some input (smoothing) parameter, the class of gl estimators exhibits a dual limiting distribution; namely, the classical shrinkage asymptotic distribution, or a bayes-type asymptotic distribution. we propose an inference method based on highest density regions using the latter distribution. we show that it has attractive theoretical properties not shared by the other popular alternatives, i.e., it is bet-proof. simulations confirm that these theoretical properties translate to better finite-sample performance.", "categories": "math.st econ.em stat.th", "created": "2018-03-28", "updated": "2020-03-02", "authors": ["alessandro casini", "pierre perron"], "url": "https://arxiv.org/abs/1803.10871"}, {"title": "continuous record asymptotics for structural change models", "id": "1803.10881", "abstract": "for a partial structural change in a linear regression model with a single break, we develop a continuous record asymptotic framework to build inference methods for the break date. we have t observations with a sampling frequency h over a fixed time horizon [0, n] , and let t with h 0 while keeping the time span n fixed. we impose very mild regularity conditions on an underlying continuous-time model assumed to generate the data. we consider the least-squares estimate of the break date and establish consistency and convergence rate. we provide a limit theory for shrinking magnitudes of shifts and locally increasing variances. the asymptotic distribution corresponds to the location of the extremum of a function of the quadratic variation of the regressors and of a gaussian centered martingale process over a certain time interval. we can account for the asymmetric informational content provided by the pre- and post-break regimes and show how the location of the break and shift magnitude are key ingredients in shaping the distribution. we consider a feasible version based on plug-in estimates, which provides a very good approximation to the finite sample distribution. we use the concept of highest density region to construct confidence sets. overall, our method is reliable and delivers accurate coverage probabilities and relatively short average length of the confidence sets. importantly, it does so irrespective of the size of the break.", "categories": "math.st econ.em stat.th", "created": "2018-03-28", "updated": "2019-10-05", "authors": ["alessandro casini", "pierre perron"], "url": "https://arxiv.org/abs/1803.10881"}, {"title": "tests for forecast instability and forecast failure under a continuous   record asymptotic framework", "id": "1803.10883", "abstract": "we develop a novel continuous-time asymptotic framework for inference on whether the predictive ability of a given forecast model remains stable over time. we formally define forecast instability from the economic forecaster's perspective and highlight that the time duration of the instability bears no relationship with stable period. our approach is applicable in forecasting environment involving low-frequency as well as high-frequency macroeconomic and financial variables. as the sampling interval between observations shrinks to zero the sequence of forecast losses is approximated by a continuous-time stochastic process (i.e., an ito semimartingale) possessing certain pathwise properties. we build an hypotheses testing problem based on the local properties of the continuous-time limit counterpart of the sequence of losses. the null distribution follows an extreme value distribution. while controlling the statistical size well, our class of test statistics feature uniform power over the location of the forecast failure in the sample. the test statistics are designed to have power against general form of insatiability and are robust to common forms of non-stationarity such as heteroskedasticty and serial correlation. the gains in power are substantial relative to extant methods, especially when the instability is short-lasting and when occurs toward the tail of the sample.", "categories": "econ.em stat.ap", "created": "2018-03-28", "updated": "2018-12-02", "authors": ["alessandro casini"], "url": "https://arxiv.org/abs/1803.10883"}, {"title": "bi-demographic changes and current account using svar modeling", "id": "1803.11161", "abstract": "the paper aims to explore the impacts of bi-demographic structure on the current account and growth. using a svar modeling, we track the dynamic impacts between these underlying variables. new insights have been developed about the dynamic interrelation between population growth, current account and economic growth. the long-run net impact on economic growth of the domestic working population growth and demand labor for emigrants is positive, due to the predominant contribution of skilled emigrant workers. besides, the positive long-run contribution of emigrant workers to the current account growth largely compensates the negative contribution from the native population, because of the predominance of skilled compared to unskilled workforce. we find that a positive shock in demand labor for emigrant workers leads to an increasing effect on native active age ratio. thus, the emigrants appear to be more complements than substitutes for native workers.", "categories": "econ.em q-fin.gn", "created": "2018-03-29", "updated": "2019-03-28", "authors": ["hassan b. ghassan", "hassan r. al-hajhoj", "faruk balli"], "url": "https://arxiv.org/abs/1803.11161"}, {"title": "mortality in a heterogeneous population - lee-carter's methodology", "id": "1803.11233", "abstract": "the eu solvency ii directive recommends insurance companies to pay more attention to the risk management methods. the sense of risk management is the ability to quantify risk and apply methods that reduce uncertainty. in life insurance, the risk is a consequence of the random variable describing the life expectancy. the article will present a proposal for stochastic mortality modeling based on the lee and carter methodology. the maximum likelihood method is often used to estimate parameters in mortality models. this method assumes that the population is homogeneous and the number of deaths has the poisson distribution. the aim of this article is to change assumptions about the distribution of the number of deaths. the results indicate that the model can get a better match to historical data, when the number of deaths has a negative binomial distribution.", "categories": "econ.em stat.ap", "created": "2018-03-29", "updated": "", "authors": ["kamil jod\u017a"], "url": "https://arxiv.org/abs/1803.11233"}, {"title": "continuous record laplace-based inference about the break date in   structural change models", "id": "1804.00232", "abstract": "building upon the continuous record asymptotic framework recently introduced by casini and perron (2018a) for inference in structural change models, we propose a laplace-based (quasi-bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. it is defined by an integration rather than an optimization-based method. a transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the quasi-posterior. for a given choice of a loss function, the laplace-type estimator is the minimizer of the expected risk with the expectation taken under the quasi-posterior. besides providing an alternative estimate that is more precise|lower mean absolute error (mae) and lower root-mean squared error (rmse)|than the usual least-squares one, the quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of highest density region. the resulting laplace-based inferential procedure is shown to have lower mae and rmse, and the confidence sets strike the best balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large.", "categories": "econ.em math.st stat.th", "created": "2018-03-31", "updated": "2020-05-11", "authors": ["alessandro casini", "pierre perron"], "url": "https://arxiv.org/abs/1804.00232"}, {"title": "should we adjust for the test for pre-trends in difference-in-difference   designs?", "id": "1804.01208", "abstract": "the common practice in difference-in-difference (did) designs is to check for parallel trends prior to treatment assignment, yet typical estimation and inference does not account for the fact that this test has occurred. i analyze the properties of the traditional did estimator conditional on having passed (i.e. not rejected) the test for parallel pre-trends. when the did design is valid and the test for pre-trends confirms it, the typical did estimator is unbiased, but traditional standard errors are overly conservative. additionally, there exists an alternative unbiased estimator that is more efficient than the traditional did estimator under parallel trends. however, when in population there is a non-zero pre-trend but we fail to reject the hypothesis of parallel pre-trends, the did estimator is generally biased relative to the population did coefficient. moreover, if the trend is monotone, then under reasonable assumptions the bias from conditioning exacerbates the bias relative to the true treatment effect. i propose new estimation and inference procedures that account for the test for parallel trends, and compare their performance to that of the traditional estimator in a monte carlo simulation.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-04-03", "updated": "2018-05-01", "authors": ["jonathan roth"], "url": "https://arxiv.org/abs/1804.01208"}, {"title": "dealing with cross-country heterogeneity in panel vars using finite   mixture models", "id": "1804.01554", "abstract": "in this paper, we provide a parsimonious means to estimate panel vars with stochastic volatility. we assume that coefficients associated with domestic lagged endogenous variables arise from a gaussian mixture model. shrinkage on the cluster size is introduced through suitable priors on the component weights and cluster-relevant quantities are identified through novel shrinkage priors. to assess whether dynamic interdependencies between economies are needed, we moreover impose shrinkage priors on the coefficients related to other countries' endogenous variables. finally, our model controls for static interdependencies by assuming that the reduced form shocks of the model feature a factor stochastic volatility structure. we assess the merits of the proposed approach by using synthetic data as well as a real data application. in the empirical application, we forecast eurozone unemployment rates and show that our proposed approach works well in terms of predictions.", "categories": "econ.em", "created": "2018-04-04", "updated": "2019-03-12", "authors": ["florian huber", "michael pfarrhofer"], "url": "https://arxiv.org/abs/1804.01554"}, {"title": "simultaneous mean-variance regression", "id": "1804.01631", "abstract": "we propose simultaneous mean-variance regression for the linear estimation and approximation of conditional mean functions. in the presence of heteroskedasticity of unknown form, our method accounts for varying dispersion in the regression outcome across the support of conditioning variables by using weights that are jointly determined with the mean regression parameters. simultaneity generates outcome predictions that are guaranteed to improve over ordinary least-squares prediction error, with corresponding parameter standard errors that are automatically valid. under shape misspecification of the conditional mean and variance functions, we establish existence and uniqueness of the resulting approximations and characterize their formal interpretation and robustness properties. in particular, we show that the corresponding mean-variance regression location-scale model weakly dominates the ordinary least-squares location model under a kullback-leibler measure of divergence, with strict improvement in the presence of heteroskedasticity. the simultaneous mean-variance regression loss function is globally convex and the corresponding estimator is easy to implement. we establish its consistency and asymptotic normality under misspecification, provide robust inference methods, and present numerical simulations that show large improvements over ordinary and weighted least-squares in terms of estimation and inference in finite samples. we further illustrate our method with two empirical applications to the estimation of the relationship between economic prosperity in 1500 and today, and demand for gasoline in the united states.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-04-04", "updated": "2019-01-02", "authors": ["richard spady", "sami stouli"], "url": "https://arxiv.org/abs/1804.01631"}, {"title": "forex trading and twitter: spam, bots, and reputation manipulation", "id": "1804.02233", "abstract": "currency trading (forex) is the largest world market in terms of volume. we analyze trading and tweeting about the eur-usd currency pair over a period of three years. first, a large number of tweets were manually labeled, and a twitter stance classification model is constructed. the model then classifies all the tweets by the trading stance signal: buy, hold, or sell (eur vs. usd). the twitter stance is compared to the actual currency rates by applying the event study methodology, well-known in financial economics. it turns out that there are large differences in twitter stance distribution and potential trading returns between the four groups of twitter users: trading robots, spammers, trading companies, and individual traders. additionally, we observe attempts of reputation manipulation by post festum removal of tweets with poor predictions, and deleting/reposting of identical tweets to increase the visibility without tainting one's twitter timeline.", "categories": "cs.si cs.cl cs.cy econ.th", "created": "2018-04-06", "updated": "2018-04-16", "authors": ["igor mozeti\u010d", "peter gabrov\u0161ek", "petra kralj novak"], "url": "https://arxiv.org/abs/1804.02233"}, {"title": "statistical inference for autoregressive models under heteroscedasticity   of unknown form", "id": "1804.02348", "abstract": "this paper provides an entire inference procedure for the autoregressive model under (conditional) heteroscedasticity of unknown form with a finite variance. we first establish the asymptotic normality of the weighted least absolute deviations estimator (lade) for the model. second, we develop the random weighting (rw) method to estimate its asymptotic covariance matrix, leading to the implementation of the wald test. third, we construct a portmanteau test for model checking, and use the rw method to obtain its critical values. as a special weighted lade, the feasible adaptive lade (alade) is proposed and proved to have the same efficiency as its infeasible counterpart. the importance of our entire methodology based on the feasible alade is illustrated by simulation results and the real data analysis on three u.s. economic data sets.", "categories": "stat.me econ.em", "created": "2018-04-06", "updated": "2018-08-08", "authors": ["ke zhu"], "url": "https://arxiv.org/abs/1804.02348"}, {"title": "varying random coefficient models", "id": "1804.03110", "abstract": "this paper provides a new methodology to analyze unobserved heterogeneity when observed characteristics are modeled nonlinearly. the proposed model builds on varying random coefficients (vrc) that are determined by nonlinear functions of observed regressors and additively separable unobservables. this paper proposes a novel estimator of the vrc density based on weighted sieve minimum distance. the main example of sieve bases are hermite functions which yield a numerically stable estimation procedure. this paper shows inference results that go beyond what has been shown in ordinary rc models. we provide in each case rates of convergence and also establish pointwise limit theory of linear functionals, where a prominent example is the density of potential outcomes. in addition, a multiplier bootstrap procedure is proposed to construct uniform confidence bands. a monte carlo study examines finite sample properties of the estimator and shows that it performs well even when the regressors associated to rc are far from being heavy tailed. finally, the methodology is applied to analyze heterogeneity in income elasticity of demand for housing.", "categories": "econ.em", "created": "2018-04-09", "updated": "2020-08-03", "authors": ["christoph breunig"], "url": "https://arxiv.org/abs/1804.03110"}, {"title": "inference on local average treatment effects for misclassified treatment", "id": "1804.03349", "abstract": "we develop point-identification for the local average treatment effect when the binary treatment contains a measurement error. the standard instrumental variable estimator is inconsistent for the parameter since the measurement error is non-classical by construction. we correct the problem by identifying the distribution of the measurement error based on the use of an exogenous variable that can even be a binary covariate. the moment conditions derived from the identification lead to generalized method of moments estimation with asymptotically valid inferences. monte carlo simulations and an empirical illustration demonstrate the usefulness of the proposed procedure.", "categories": "econ.em", "created": "2018-04-10", "updated": "", "authors": ["takahide yanagi"], "url": "https://arxiv.org/abs/1804.03349"}, {"title": "moment inequalities in the context of simulated and predicted variables", "id": "1804.03674", "abstract": "this paper explores the effects of simulated moments on the performance of inference methods based on moment inequalities. commonly used confidence sets for parameters are level sets of criterion functions whose boundary points may depend on sample moments in an irregular manner. due to this feature, simulation errors can affect the performance of inference in non-standard ways. in particular, a (first-order) bias due to the simulation errors may remain in the estimated boundary of the confidence set. we demonstrate, through monte carlo experiments, that simulation errors can significantly reduce the coverage probabilities of confidence sets in small samples. the size distortion is particularly severe when the number of inequality restrictions is large. these results highlight the danger of ignoring the sampling variations due to the simulation errors in moment inequality models. similar issues arise when using predicted variables in moment inequalities models. we propose a method for properly correcting for these variations based on regularizing the intersection of moments in parameter space, and we show that our proposed method performs well theoretically and in practice.", "categories": "econ.em stat.co", "created": "2018-04-10", "updated": "", "authors": ["hiroaki kaido", "jiaxuan li", "marc rysman"], "url": "https://arxiv.org/abs/1804.03674"}, {"title": "successful nash equilibrium agent for a 3-player imperfect-information   game", "id": "1804.04789", "abstract": "creating strong agents for games with more than two players is a major open problem in ai. common approaches are based on approximating game-theoretic solution concepts such as nash equilibrium, which have strong theoretical guarantees in two-player zero-sum games, but no guarantees in non-zero-sum games or in games with more than two players. we describe an agent that is able to defeat a variety of realistic opponents using an exact nash equilibrium strategy in a 3-player imperfect-information game. this shows that, despite a lack of theoretical guarantees, agents based on nash equilibrium strategies can be successful in multiplayer games after all.", "categories": "cs.gt cs.ai cs.ma econ.th", "created": "2018-04-13", "updated": "", "authors": ["sam ganzfried", "austin nowak", "joannier pinales"], "url": "https://arxiv.org/abs/1804.04789"}, {"title": "large sample properties of partitioning-based series estimators", "id": "1804.04916", "abstract": "we present large sample results for partitioning-based least squares nonparametric regression, a popular method for approximating conditional expectation functions in statistics, econometrics, and machine learning. first, we obtain a general characterization of their leading asymptotic bias. second, we establish integrated mean squared error approximations for the point estimator and propose feasible tuning parameter selection. third, we develop pointwise inference methods based on undersmoothing and robust bias correction. fourth, employing different coupling approaches, we develop uniform distributional approximations for the undersmoothed and robust bias-corrected t-statistic processes and construct valid confidence bands. in the univariate case, our uniform distributional approximations require seemingly minimal rate restrictions and improve on approximation rates known in the literature. finally, we apply our general results to three partitioning-based estimators: splines, wavelets, and piecewise polynomials. the supplemental appendix includes several other general and example-specific technical and methodological results. a companion r package is provided.", "categories": "math.st econ.em stat.th", "created": "2018-04-13", "updated": "2019-06-01", "authors": ["matias d. cattaneo", "max h. farrell", "yingjie feng"], "url": "https://arxiv.org/abs/1804.04916"}, {"title": "shapley value methods for attribution modeling in online advertising", "id": "1804.05327", "abstract": "this paper re-examines the shapley value methods for attribution analysis in the area of online advertising. as a credit allocation solution in cooperative game theory, shapley value method directly quantifies the contribution of online advertising inputs to the advertising key performance indicator (kpi) across multiple channels. we simplify its calculation by developing an alternative mathematical formulation. the new formula significantly improves the computational efficiency and therefore extends the scope of applicability. based on the simplified formula, we further develop the ordered shapley value method. the proposed method is able to take into account the order of channels visited by users. we claim that it provides a more comprehensive insight by evaluating the attribution of channels at different stages of user conversion journeys. the proposed approaches are illustrated using a real-world online advertising campaign dataset.", "categories": "econ.em", "created": "2018-04-15", "updated": "", "authors": ["kaifeng zhao", "seyed hanif mahboobi", "saeed r. bagheri"], "url": "https://arxiv.org/abs/1804.05327"}, {"title": "estimating dynamic treatment effects in event studies with heterogeneous   treatment effects", "id": "1804.05785", "abstract": "to estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. we show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. we propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.", "categories": "econ.em", "created": "2018-04-16", "updated": "2020-09-22", "authors": ["liyang sun", "sarah abraham"], "url": "https://arxiv.org/abs/1804.05785"}, {"title": "bitcoin market route to maturity? evidence from return fluctuations,   temporal correlations and multiscaling effects", "id": "1804.05916", "abstract": "based on 1-minute price changes recorded since year 2012, the fluctuation properties of the rapidly-emerging bitcoin (btc) market are assessed over chosen sub-periods, in terms of return distributions, volatility autocorrelation, hurst exponents and multiscaling effects. the findings are compared to the stylized facts of mature world markets. while early trading was affected by system-specific irregularities, it is found that over the months preceding apr 2018 all these statistical indicators approach the features hallmarking maturity. this can be taken as an indication that the bitcoin market, and possibly other cryptocurrencies, carry concrete potential of imminently becoming a regular market, alternative to the foreign exchange (forex). since high-frequency price data are available since the beginning of trading, the bitcoin offers a unique window into the statistical characteristics of a market maturation trajectory.", "categories": "q-fin.st cs.ce econ.em", "created": "2018-04-16", "updated": "2018-07-19", "authors": ["stanis\u0142aw dro\u017cd\u017c", "robert g\u0119barowski", "ludovico minati", "pawe\u0142 o\u015bwi\u0119cimka", "marcin w\u0105torek"], "url": "https://arxiv.org/abs/1804.05916"}, {"title": "quantifying the economic case for electric semi-trucks", "id": "1804.05974", "abstract": "there has been considerable interest in the electrification of freight transport, particularly heavy-duty trucks to downscale the greenhouse-gas (ghg) emissions from the transportation sector. however, the economic competitiveness of electric semi-trucks is uncertain as there are substantial additional initial costs associated with the large battery packs required. in this work, we analyze the trade-off between the initial investment and the operating cost for realistic usage scenarios to compare a fleet of electric semi-trucks with a range of 500 miles with a fleet of diesel trucks. for the baseline case with 30% of fleet requiring battery pack replacements and a price differential of us\\$50,000, we find a payback period of about 3 years. based on sensitivity analysis, we find that the fraction of the fleet that requires battery pack replacements is a major factor. for the case with 100% replacement fraction, the payback period could be as high as 5-6 years. we identify the price of electricity as the second most important variable, where a price of us$0.14/kwh, the payback period could go up to 5 years. electric semi-trucks are expected to lead to savings due to reduced repairs and magnitude of these savings could play a crucial role in the payback period as well. with increased penetration of autonomous vehicles, the annual mileage of semi-trucks could substantially increase and this heavily sways in favor of electric semi-trucks, bringing down the payback period to around 2 years at an annual mileage of 120,000 miles. there is an undeniable economic case for electric semi-trucks and developing battery packs with longer cycle life and higher specific energy would make this case even stronger.", "categories": "econ.em cs.sy", "created": "2018-04-16", "updated": "", "authors": ["shashank sripad", "venkatasubramanian viswanathan"], "url": "https://arxiv.org/abs/1804.05974"}, {"title": "dissection of bitcoin's multiscale bubble history from january 2012 to   february 2018", "id": "1804.06261", "abstract": "we present a detailed bubble analysis of the bitcoin to us dollar price dynamics from january 2012 to february 2018. we introduce a robust automatic peak detection method that classifies price time series into periods of uninterrupted market growth (drawups) and regimes of uninterrupted market decrease (drawdowns). in combination with the lagrange regularisation method for detecting the beginning of a new market regime, we identify 3 major peaks and 10 additional smaller peaks, that have punctuated the dynamics of bitcoin price during the analyzed time period. we explain this classification of long and short bubbles by a number of quantitative metrics and graphs to understand the main socio-economic drivers behind the ascent of bitcoin over this period. then, a detailed analysis of the growing risks associated with the three long bubbles using the log-periodic power law singularity (lppls) model is based on the lppls confidence indicators, defined as the fraction of qualified fits of the lppls model over multiple time windows. furthermore, for various fictitious 'present' times $t_2$ before the crashes, we employ a clustering method to group the predicted critical times $t_c$ of the lppls fits over different time scales, where $t_c$ is the most probable time for the ending of the bubble. each cluster is proposed as a plausible scenario for the subsequent bitcoin price evolution. we present these predictions for the three long bubbles and the four short bubbles that our time scale of analysis was able to resolve. overall, our predictive scheme provides useful information to warn of an imminent crash risk.", "categories": "econ.em q-fin.st", "created": "2018-04-17", "updated": "2019-05-30", "authors": ["jan-christian gerlach", "guilherme demos", "didier sornette"], "url": "https://arxiv.org/abs/1804.06261"}, {"title": "revisiting the thermal and superthermal two-class distribution of   incomes: a critical perspective", "id": "1804.06341", "abstract": "this paper offers a two-pronged critique of the empirical investigation of the income distribution performed by physicists over the past decade. their finding rely on the graphical analysis of the observed distribution of normalized incomes. two central observations lead to the conclusion that the majority of incomes are exponentially distributed, but neither each individual piece of evidence nor their concurrent observation robustly proves that the thermal and superthermal mixture fits the observed distribution of incomes better than reasonable alternatives. a formal analysis using popular measures of fit shows that while an exponential distribution with a power-law tail provides a better fit of the irs income data than the log-normal distribution (often assumed by economists), the thermal and superthermal mixture's fit can be improved upon further by adding a log-normal component. the economic implications of the thermal and superthermal distribution of incomes, and the expanded mixture are explored in the paper.", "categories": "econ.em", "created": "2018-04-17", "updated": "", "authors": ["markus p. a. schneider"], "url": "https://arxiv.org/abs/1804.06341"}, {"title": "triggers for cooperative behavior in the thermodynamic limit: a case   study in public goods game", "id": "1804.06465", "abstract": "in this work, we aim to answer the question: what triggers cooperative behavior in the thermodynamic limit by taking recourse to the public goods game. using the idea of mapping the 1d ising model hamiltonian with nearest neighbor coupling to payoffs in the game theory we calculate the magnetisation of the game in the thermodynamic limit. we see a phase transition in the thermodynamic limit of the two player public goods game. we observe that punishment acts as an external field for the two player public goods game triggering cooperation or provide strategy, while cost can be a trigger for suppressing cooperation or free riding. finally, reward also acts as a trigger for providing while the role of inverse temperature (fluctuations in choices) is to introduce randomness in strategic choices.", "categories": "physics.soc-ph cond-mat.stat-mech cs.gt econ.em q-bio.pe", "created": "2018-04-16", "updated": "2019-04-08", "authors": ["colin benjamin", "shubhayan sarkar"], "url": "https://arxiv.org/abs/1804.06465"}, {"title": "estimating treatment effects in mover designs", "id": "1804.06721", "abstract": "researchers increasingly leverage movement across multiple treatments to estimate causal effects. while these \"mover regressions\" are often motivated by a linear constant-effects model, it is not clear what they capture under weaker quasi-experimental assumptions. i show that binary treatment mover regressions recover a convex average of four difference-in-difference comparisons and are thus causally interpretable under a standard parallel trends assumption. estimates from multiple-treatment models, however, need not be causal without stronger restrictions on the heterogeneity of treatment effects and time-varying shocks. i propose a class of two-step estimators to isolate and combine the large set of difference-in-difference quasi-experiments generated by a mover design, identifying mover average treatment effects under conditional-on-covariate parallel trends and effect homogeneity restrictions. i characterize the efficient estimators in this class and derive specification tests based on the model's overidentifying restrictions. future drafts will apply the theory to the finkelstein et al. (2016) movers design, analyzing the causal effects of geography on healthcare utilization.", "categories": "econ.em", "created": "2018-04-18", "updated": "", "authors": ["peter hull"], "url": "https://arxiv.org/abs/1804.06721"}, {"title": "transaction costs in collective waste recovery systems in the eu", "id": "1804.06792", "abstract": "the study aims to identify the institutional flaws of the current eu waste management model by analysing the economic model of extended producer responsibility and collective waste management systems and to create a model for measuring the transaction costs borne by waste recovery organizations. the model was approbated by analysing the bulgarian collective waste management systems that have been complying with the eu legislation for the last 10 years. the analysis focuses on waste oils because of their economic importance and the limited number of studies and analyses in this field as the predominant body of research to date has mainly addressed packaging waste, mixed household waste or discarded electrical and electronic equipment. the study aims to support the process of establishing a circular economy in the eu, which was initiated in 2015.", "categories": "econ.em", "created": "2018-04-18", "updated": "", "authors": ["shteryo nozharov"], "url": "https://arxiv.org/abs/1804.06792"}, {"title": "truthful fair division without free disposal", "id": "1804.06923", "abstract": "we study the problem of fairly dividing a heterogeneous resource, commonly known as cake cutting and chore division, in the presence of strategic agents. while a number of results in this setting have been established in previous works, they rely crucially on the free disposal assumption, meaning that the mechanism is allowed to throw away part of the resource at no cost. in the present work, we remove this assumption and focus on mechanisms that always allocate the entire resource. we exhibit a truthful and envy-free mechanism for cake cutting and chore division for two agents with piecewise uniform valuations, and we complement our result by showing that such a mechanism does not exist when certain additional constraints are imposed on the mechanisms. moreover, we provide bounds on the efficiency of mechanisms satisfying various properties, and give truthful mechanisms for multiple agents with restricted classes of valuations.", "categories": "cs.gt econ.th", "created": "2018-04-18", "updated": "2020-04-15", "authors": ["xiaohui bei", "guangda huzhang", "warut suksompong"], "url": "https://arxiv.org/abs/1804.06923"}, {"title": "empirical equilibrium", "id": "1804.07986", "abstract": "we study the foundations of empirical equilibrium, a refinement of nash equilibrium that is based on a non-parametric characterization of empirical distributions of behavior in games (velez and brown,2020b arxiv:1907.12408). the refinement can be alternatively defined as those nash equilibria that do not refute the regular qre theory of goeree, holt, and palfrey (2005). by contrast, some empirical equilibria may refute monotone additive randomly disturbed payoff models. as a by product, we show that empirical equilibrium does not coincide with refinements based on approximation by monotone additive randomly disturbed payoff models, and further our understanding of the empirical content of these models.", "categories": "econ.em", "created": "2018-04-21", "updated": "2020-07-13", "authors": ["rodrigo a. velez", "alexander l. brown"], "url": "https://arxiv.org/abs/1804.07986"}, {"title": "price competition with geometric brownian motion in exchange rate   uncertainty", "id": "1804.08153", "abstract": "we analyze an operational policy for a multinational manufacturer to hedge against exchange rate uncertainties and competition. we consider a single product and single period. because of long-lead times, the capacity investment must done before the selling season begins when the exchange rate between the two countries is uncertain. we consider a duopoly competition in the foreign country. we model the exchange rate as a random variable. we investigate the impact of competition and exchange rate on optimal capacities and optimal prices. we show how competition can impact the decision of the home manufacturer to enter the foreign market.", "categories": "econ.em", "created": "2018-04-22", "updated": "", "authors": ["murat erkoc", "huaqing wang", "anas ahmed"], "url": "https://arxiv.org/abs/1804.08153"}, {"title": "econometric modeling of regional electricity spot prices in the   australian market", "id": "1804.08218", "abstract": "wholesale electricity markets are increasingly integrated via high voltage interconnectors, and inter-regional trade in electricity is growing. to model this, we consider a spatial equilibrium model of price formation, where constraints on inter-regional flows result in three distinct equilibria in prices. we use this to motivate an econometric model for the distribution of observed electricity spot prices that captures many of their unique empirical characteristics. the econometric model features supply and inter-regional trade cost functions, which are estimated using bayesian monotonic regression smoothing methodology. a copula multivariate time series model is employed to capture additional dependence -- both cross-sectional and serial-- in regional prices. the marginal distributions are nonparametric, with means given by the regression means. the model has the advantage of preserving the heavy right-hand tail in the predictive densities of price. we fit the model to half-hourly spot price data in the five interconnected regions of the australian national electricity market. the fitted model is then used to measure how both supply and price shocks in one region are transmitted to the distribution of prices in all regions in subsequent periods. finally, to validate our econometric model, we show that prices forecast using the proposed model compare favorably with those from some benchmark alternatives.", "categories": "econ.em stat.ap", "created": "2018-04-22", "updated": "", "authors": ["michael stanley smith", "thomas s. shively"], "url": "https://arxiv.org/abs/1804.08218"}, {"title": "statistical and economic evaluation of time series models for   forecasting arrivals at call centers", "id": "1804.08315", "abstract": "call centers' managers are interested in obtaining accurate point and distributional forecasts of call arrivals in order to achieve an optimal balance between service quality and operating costs. we present a strategy for selecting forecast models of call arrivals which is based on three pillars: (i) flexibility of the loss function; (ii) statistical evaluation of forecast accuracy; (iii) economic evaluation of forecast performance using money metrics. we implement fourteen time series models and seven forecast combination schemes on three series of daily call arrivals. although we focus mainly on point forecasts, we also analyze density forecast evaluation. we show that second moments modeling is important both for point and density forecasting and that the simple seasonal random walk model is always outperformed by more general specifications. our results suggest that call center managers should invest in the use of forecast models which describe both first and second moments of call arrivals.", "categories": "econ.em", "created": "2018-04-23", "updated": "", "authors": ["andrea bastianin", "marzio galeotti", "matteo manera"], "url": "https://arxiv.org/abs/1804.08315"}, {"title": "economic inequality and islamic charity: an exploratory agent-based   modeling approach", "id": "1804.09284", "abstract": "economic inequality is one of the pivotal issues for most of economic and social policy makers across the world to insure the sustainable economic growth and justice. in the mainstream school of economics, namely neoclassical theories, economic issues are dealt with in a mechanistic manner. such a mainstream framework is majorly focused on investigating a socio-economic system based on an axiomatic scheme where reductionism approach plays a vital role. the major limitations of such theories include unbounded rationality of economic agents, reducing the economic aggregates to a set of predictable factors and lack of attention to adaptability and the evolutionary nature of economic agents. in tackling deficiencies of conventional economic models, in the past two decades, some new approaches have been recruited. one of those novel approaches is the complex adaptive systems (cas) framework which has shown a very promising performance in action. in contrast to mainstream school, under this framework, the economic phenomena are studied in an organic manner where the economic agents are supposed to be both boundedly rational and adaptive. according to it, the economic aggregates emerge out of the ways agents of a system decide and interact. as a powerful way of modeling cass, agent-based models (abms) has found a growing application among academicians and practitioners. abms show that how simple behavioral rules of agents and local interactions among them at micro-scale can generate surprisingly complex patterns at macro-scale. in this paper, abms have been used to show (1) how an economic inequality emerges in a system and to explain (2) how sadaqah as an islamic charity rule can majorly help alleviating the inequality and how resource allocation strategies taken by charity entities can accelerate this alleviation.", "categories": "econ.em", "created": "2018-04-24", "updated": "", "authors": ["hossein sabzian", "alireza aliahmadi", "adel azar", "madjid mirzaee"], "url": "https://arxiv.org/abs/1804.09284"}, {"title": "deep learning for predicting asset returns", "id": "1804.09314", "abstract": "deep learning searches for nonlinear factors for predicting asset returns. predictability is achieved via multiple layers of composite factors as opposed to additive ones. viewed in this way, asset pricing studies can be revisited using multi-layer deep learners, such as rectified linear units (relu) or long-short-term-memory (lstm) for time-series effects. state-of-the-art algorithms including stochastic gradient descent (sgd), tensorflow and dropout design provide imple- mentation and efficient factor exploration. to illustrate our methodology, we revisit the equity market risk premium dataset of welch and goyal (2008). we find the existence of nonlinear factors which explain predictability of returns, in particular at the extremes of the characteristic space. finally, we conclude with directions for future research.", "categories": "stat.ml cs.lg econ.em", "created": "2018-04-24", "updated": "2018-04-26", "authors": ["guanhao feng", "jingyu he", "nicholas g. polson"], "url": "https://arxiv.org/abs/1804.09314"}, {"title": "new hsic-based tests for independence between two stationary   multivariate time series", "id": "1804.09866", "abstract": "this paper proposes some novel one-sided omnibus tests for independence between two multivariate stationary time series. these new tests apply the hilbert-schmidt independence criterion (hsic) to test the independence between the innovations of both time series. under regular conditions, the limiting null distributions of our hsic-based tests are established. next, our hsic-based tests are shown to be consistent. moreover, a residual bootstrap method is used to obtain the critical values for our hsic-based tests, and its validity is justified. compared with the existing cross-correlation-based tests for linear dependence, our tests examine the general (including both linear and non-linear) dependence to give investigators more complete information on the causal relationship between two multivariate time series. the merits of our tests are illustrated by some simulation results and a real example.", "categories": "stat.me econ.em", "created": "2018-04-25", "updated": "", "authors": ["guochang wang", "wai keung li", "ke zhu"], "url": "https://arxiv.org/abs/1804.09866"}, {"title": "interpreting quantile independence", "id": "1804.10957", "abstract": "how should one assess the credibility of assumptions weaker than statistical independence, like quantile independence? in the context of identifying causal effects of a treatment variable, we argue that such deviations should be chosen based on the form of selection on unobservables they allow. for quantile independence, we characterize this form of treatment selection. specifically, we show that quantile independence is equivalent to a constraint on the average value of either a latent propensity score (for a binary treatment) or the cdf of treatment given the unobservables (for a continuous treatment). in both cases, this average value constraint requires a kind of non-monotonic treatment selection. using these results, we show that several common treatment selection models are incompatible with quantile independence. we introduce a class of assumptions which weakens quantile independence by removing the average value constraint, and therefore allows for monotonic treatment selection. in a potential outcomes model with a binary treatment, we derive identified sets for the att and qtt under both classes of assumptions. in a numerical example we show that the average value constraint inherent in quantile independence has substantial identifying power. our results suggest that researchers should carefully consider the credibility of this non-monotonicity property when using quantile independence to weaken full independence.", "categories": "econ.em stat.me", "created": "2018-04-29", "updated": "", "authors": ["matthew a. masten", "alexandre poirier"], "url": "https://arxiv.org/abs/1804.10957"}, {"title": "identifying effects of multivalued treatments", "id": "1805.00057", "abstract": "multivalued treatment models have typically been studied under restrictive assumptions: ordered choice, and more recently unordered monotonicity. we show how treatment effects can be identified in a more general class of models that allows for multidimensional unobserved heterogeneity. our results rely on two main assumptions: treatment assignment must be a measurable function of threshold-crossing rules, and enough continuous instruments must be available. we illustrate our approach for several classes of models.", "categories": "econ.em stat.me", "created": "2018-04-30", "updated": "", "authors": ["sokbae lee", "bernard salani\u00e9"], "url": "https://arxiv.org/abs/1805.00057"}, {"title": "aide et croissance dans les pays de l'union economique et mon{\\'e}taire   ouest africaine (uemoa) : retour sur une relation controvers{\\'e}e", "id": "1805.00435", "abstract": "the main purpose of this paper is to analyze threshold effects of official development assistance (oda) on economic growth in waemu zone countries. to achieve this, the study is based on oecd and wdi data covering the period 1980-2015 and used hansen's panel threshold regression (ptr) model to \"bootstrap\" aid threshold above which its effectiveness is effective. the evidence strongly supports the view that the relationship between aid and economic growth is non-linear with a unique threshold which is 12.74% gdp. above this value, the marginal effect of aid is 0.69 points, \"all things being equal to otherwise\". one of the main contribution of this paper is to show that waemu countries need investments that could be covered by the foreign aid. this later one should be considered just as a complementary resource. thus, weamu countries should continue to strengthen their efforts in internal resource mobilization in order to fulfil this need.", "categories": "econ.em", "created": "2018-04-13", "updated": "", "authors": ["nimonka bayale"], "url": "https://arxiv.org/abs/1805.00435"}, {"title": "endogenous growth - a dynamic technology augmentation of the solow model", "id": "1805.00668", "abstract": "in this paper, i endeavour to construct a new model, by extending the classic exogenous economic growth model by including a measurement which tries to explain and quantify the size of technological innovation ( a ) endogenously. i do not agree technology is a \"constant\" exogenous variable, because it is humans who create all technological innovations, and it depends on how much human and physical capital is allocated for its research. i inspect several possible approaches to do this, and then i test my model both against sample and real world evidence data. i call this method \"dynamic\" because it tries to model the details in resource allocations between research, labor and capital, by affecting each other interactively. in the end, i point out which is the new residual and the parts of the economic growth model which can be further improved.", "categories": "econ.em", "created": "2018-05-02", "updated": "", "authors": ["murad kasim"], "url": "https://arxiv.org/abs/1805.00668"}, {"title": "when panic makes you blind: a chaotic route to systemic risk", "id": "1805.00785", "abstract": "we present an analytical model to study the role of expectation feedbacks and overlapping portfolios on systemic stability of financial systems. building on [corsi et al., 2016], we model a set of financial institutions having value at risk capital requirements and investing in a portfolio of risky assets, whose prices evolve stochastically in time and are endogenously driven by the trading decisions of financial institutions. assuming that they use adaptive expectations of risk, we show that the evolution of the system is described by a slow-fast random dynamical system, which can be studied analytically in some regimes. the model shows how the risk expectations play a central role in determining the systemic stability of the financial system and how wrong risk expectations may create panic-induced reduction or over-optimistic expansion of balance sheets. specifically, when investors are myopic in estimating the risk, the fixed point equilibrium of the system breaks into leverage cycles and financial variables display a bifurcation cascade eventually leading to chaos. we discuss the role of financial policy and the effects of some market frictions, as the cost of diversification and financial transaction taxes, in determining the stability of the system in the presence of adaptive expectations of risk.", "categories": "econ.gn math.ds q-fin.ec", "created": "2018-05-02", "updated": "", "authors": ["piero mazzarisi", "fabrizio lillo", "stefano marmi"], "url": "https://arxiv.org/abs/1805.00785"}, {"title": "chain effects of clean water: the mills-reincke phenomenon in early   twentieth-century japan", "id": "1805.00875", "abstract": "this study explores the validity of chain effects of clean water, which are known as the \"mills-reincke phenomenon,\" in early twentieth-century japan. recent studies have reported that water purifications systems are responsible for huge contributions to human capital. although some studies have investigated the instantaneous effects of water-supply systems in pre-war japan, little is known about the chain effects of these systems. by analyzing city-level cause-specific mortality data from 1922-1940, we find that a decline in typhoid deaths by one per 1,000 people decreased the risk of death due to non-waterborne diseases such as tuberculosis and pneumonia by 0.742-2.942 per 1,000 people. our finding suggests that the observed mills-reincke phenomenon could have resulted in the relatively rapid decline in the mortality rate in early twentieth-century japan.", "categories": "stat.ap econ.em q-bio.pe", "created": "2018-04-26", "updated": "2019-08-25", "authors": ["tatsuki inoue", "kota ogasawara"], "url": "https://arxiv.org/abs/1805.00875"}, {"title": "when a `rat race' implies an intergenerational wealth trap", "id": "1805.01019", "abstract": "two critical questions about intergenerational outcomes are: one, whether significant barriers or traps exist between different social or economic strata; and two, the extent to which intergenerational outcomes do (or can be used to) affect individual investment and consumption decisions. we develop a model to explicitly relate these two questions, and prove the first such `rat race' theorem, showing that a fundamental relationship exists between high levels of individual investment and the existence of a wealth trap, which traps otherwise identical agents at a lower level of wealth. our simple model of intergenerational wealth dynamics involves agents which balance current consumption with investment in a single descendant. investments then determine descendant wealth via a potentially nonlinear and discontinuous competitiveness function about which we do not make concavity assumptions. from this model we demonstrate how to infer such a competitiveness function from investments, along with geometric criteria to determine individual decisions. additionally we investigate the stability of a wealth distribution, both to local perturbations and to the introduction of new agents with no wealth.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2018-04-30", "updated": "", "authors": ["joel nishimura"], "url": "https://arxiv.org/abs/1805.01019"}, {"title": "optimal linear instrumental variables approximations", "id": "1805.03275", "abstract": "this paper studies the identification and estimation of the optimal linear approximation of a structural regression function. the parameter in the linear approximation is called the optimal linear instrumental variables approximation (oliva). this paper shows that a necessary condition for standard inference on the oliva is also sufficient for the existence of an iv estimand in a linear model. the instrument in the iv estimand is unknown and may not be identified. a two-step iv (tsiv) estimator based on tikhonov regularization is proposed, which can be implemented by standard regression routines. we establish the asymptotic normality of the tsiv estimator assuming neither completeness nor identification of the instrument. as an important application of our analysis, we robustify the classical hausman test for exogeneity against misspecification of the linear structural model. we also discuss extensions to weighted least squares criteria. monte carlo simulations suggest an excellent finite sample performance for the proposed inferences. finally, in an empirical application estimating the elasticity of intertemporal substitution (eis) with us data, we obtain tsiv estimates that are much larger than their standard iv counterparts, with our robust hausman test failing to reject the null hypothesis of exogeneity of real interest rates.", "categories": "econ.em", "created": "2018-05-08", "updated": "2020-02-05", "authors": ["juan carlos escanciano", "wei li"], "url": "https://arxiv.org/abs/1805.03275"}, {"title": "the laws of the evolution of research fields", "id": "1805.03492", "abstract": "a fundamental question in the field of social studies of science is how research fields emerge, grow and decline over time and space. this study confronts this question here by developing an inductive analysis of emerging research fields represented by human microbiome, evolutionary robotics and astrobiology. in particular, number of papers from starting years to 2017 of each emerging research field is analyzed considering the subject areas (i.e., disciplines) of authors. findings suggest some empirical laws of the evolution of research fields: the first law states that the evolution of a specific research field is driven by few scientific disciplines (3- 5) that generate more than 80% of documents (concentration of the scientific production); the second law states that the evolution of research fields is path-dependent of a critical discipline (it can be a native discipline that has originated the research field or a new discipline emerged during the social dynamics of science); the third law states that a research field can be driven during its evolution by a new discipline originated by a process of specialization within science. the findings here can explain and generalize, whenever possible some properties of the evolution of scientific fields that are due to interaction between disciplines, convergence between basic and applied research fields and interdisciplinary in scientific research. overall, then, this study begins the process of clarifying and generalizing, as far as possible, the properties of the social construction and evolution of science to lay a foundation for the development of sophisticated theories.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2018-05-08", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1805.03492"}, {"title": "structural breaks in time series", "id": "1805.03807", "abstract": "this chapter covers methodological issues related to estimation, testing and computation for models involving structural changes. our aim is to review developments as they relate to econometric applications based on linear models. substantial advances have been made to cover models at a level of generality that allow a host of interesting practical applications. these include models with general stationary regressors and errors that can exhibit temporal dependence and heteroskedasticity, models with trending variables and possible unit roots and cointegrated models, among others. advances have been made pertaining to computational aspects of constructing estimates, their limit distributions, tests for structural changes, and methods to determine the number of changes present. a variety of topics are covered. the first part summarizes and updates developments described in an earlier review, perron (2006), with the exposition following heavily that of perron (2008). additions are included for recent developments: testing for common breaks, models with endogenous regressors (emphasizing that simply using least-squares is preferable over instrumental variables methods), quantile regressions, methods based on lasso, panel data models, testing for changes in forecast accuracy, factors models and methods of inference based on a continuous records asymptotic framework. our focus is on the so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. the aim is to provide the readers with an overview of methods that are of direct usefulness in practice as opposed to issues that are mostly of theoretical interest.", "categories": "econ.em stat.me", "created": "2018-05-10", "updated": "", "authors": ["alessandro casini", "pierre perron"], "url": "https://arxiv.org/abs/1805.03807"}, {"title": "a mixture autoregressive model based on student's $t$-distribution", "id": "1805.04010", "abstract": "a new mixture autoregressive model based on student's $t$-distribution is proposed. a key feature of our model is that the conditional $t$-distributions of the component models are based on autoregressions that have multivariate $t$-distributions as their (low-dimensional) stationary distributions. that autoregressions with such stationary distributions exist is not immediate. our formulation implies that the conditional mean of each component model is a linear function of past observations and the conditional variance is also time varying. compared to previous mixture autoregressive models our model may therefore be useful in applications where the data exhibits rather strong conditional heteroskedasticity. our formulation also has the theoretical advantage that conditions for stationarity and ergodicity are always met and these properties are much more straightforward to establish than is common in nonlinear autoregressive models. an empirical example employing a realized kernel series based on s&p 500 high-frequency data shows that the proposed model performs well in volatility forecasting.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-05-10", "updated": "", "authors": ["mika meitz", "daniel preve", "pentti saikkonen"], "url": "https://arxiv.org/abs/1805.04010"}, {"title": "sufficient statistics for unobserved heterogeneity in structural dynamic   logit models", "id": "1805.04048", "abstract": "we study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. we consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. this class of models includes as particular cases important economic applications such as models of market entry-exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. the identification of structural parameters requires a sufficient statistic that controls for unobserved heterogeneity not only in current utility but also in the continuation value of the forward-looking decision problem. we obtain the minimal sufficient statistic and prove identification of some structural parameters using a conditional likelihood approach. we apply this estimator to a machine replacement model.", "categories": "econ.em", "created": "2018-05-10", "updated": "", "authors": ["victor aguirregabiria", "jiaying gu", "yao luo"], "url": "https://arxiv.org/abs/1805.04048"}, {"title": "news sentiment as leading indicators for recessions", "id": "1805.04160", "abstract": "in the following paper, we use a topic modeling algorithm and sentiment scoring methods to construct a novel metric that serves as a leading indicator in recession prediction models. we hypothesize that the inclusion of such a sentiment indicator, derived purely from unstructured news data, will improve our capabilities to forecast future recessions because it provides a direct measure of the polarity of the information consumers and producers are exposed to. we go on to show that the inclusion of our proposed news sentiment indicator, with traditional sentiment data, such as the michigan index of consumer sentiment and the purchasing manager's index, and common factors derived from a large panel of economic and financial indicators helps improve model performance significantly.", "categories": "stat.ap econ.em", "created": "2018-05-10", "updated": "2018-05-31", "authors": ["melody y. huang", "randall r. rojas", "patrick d. convery"], "url": "https://arxiv.org/abs/1805.04160"}, {"title": "density forecasts in panel data models: a semiparametric bayesian   perspective", "id": "1805.04178", "abstract": "this paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients and cross-sectional heteroskedasticity. the panel considered in this paper features a large cross-sectional dimension n but short time series t. due to the short t, traditional methods have difficulty in disentangling the heterogeneous parameters from the shocks, which contaminates the estimates of the heterogeneous parameters. to tackle this problem, i assume that there is an underlying distribution of heterogeneous parameters, model this distribution nonparametrically allowing for correlation between heterogeneous parameters and initial conditions as well as individual-specific regressors, and then estimate this distribution by pooling the information from the whole cross-section together. theoretically, i prove that both the estimated common parameters and the estimated distribution of the heterogeneous parameters achieve posterior consistency, and that the density forecasts asymptotically converge to the oracle forecast. methodologically, i develop a simulation-based posterior sampling algorithm specifically addressing the nonparametric density estimation of unobserved heterogeneous parameters. monte carlo simulations and an application to young firm dynamics demonstrate improvements in density forecasts relative to alternative approaches.", "categories": "econ.em", "created": "2018-05-10", "updated": "2020-02-27", "authors": ["laura liu"], "url": "https://arxiv.org/abs/1805.04178"}, {"title": "efficiency in micro-behaviors and fl bias", "id": "1805.04225", "abstract": "in this paper, we propose a model which simulates odds distributions of pari-mutuel betting system under two hypotheses on the behavior of bettors: 1. the amount of bets increases very rapidly as the deadline for betting comes near. 2. each bettor bets on a horse which gives the largest expectation value of the benefit. the results can be interpreted as such efficient behaviors do not serve to extinguish the fl bias but even produce stronger fl bias.", "categories": "econ.em", "created": "2018-05-10", "updated": "", "authors": ["kurihara kazutaka", "yohei tutiya"], "url": "https://arxiv.org/abs/1805.04225"}, {"title": "a dynamic analysis of nash equilibria in search models with fiat money", "id": "1805.04733", "abstract": "we study the rise in the acceptability fiat money in a kiyotaki-wright economy by developing a method that can determine dynamic nash equilibria for a class of search models with genuine heterogenous agents. we also address open issues regarding the stability properties of pure strategies equilibria and the presence of multiple equilibria. experiments illustrate the liquidity conditions that favor the transition from partial to full acceptance of fiat money, and the effects of inflationary shocks on production, liquidity, and trade.", "categories": "econ.em econ.th math.ds nlin.ao", "created": "2018-05-12", "updated": "", "authors": ["federico bonetto", "maurizio iacopetta"], "url": "https://arxiv.org/abs/1805.04733"}, {"title": "distributional stability and deterministic equilibrium selection under   heterogeneous evolutionary dynamics", "id": "1805.04895", "abstract": "in the presence of persistent payoff heterogeneity, the evolution of the aggregate strategy hugely depends on the underlying strategy composition under many evolutionary dynamics, while the aggregate dynamic under the standard brd reduces to a homogenized smooth brd, where persistent payoff heterogeneity averages to homogeneous transitory payoff shocks. in this paper, we consider deterministic evolutionary dynamics in heterogeneous population and develop the stronger concept of local stability by imposing robustness to persistent payoff heterogeneity. it is known that nonaggregability holds generically if the switching rate in a given evolutionary dynamic correlates with the payoff gain from a switch. to parameterize the payoff sensitivity of an evolutionary dynamic, we propose to use tempered best response dynamics with bounded support of switching costs.", "categories": "cs.gt econ.th", "created": "2018-05-13", "updated": "", "authors": ["dai zusai"], "url": "https://arxiv.org/abs/1805.04895"}, {"title": "evolutionary dynamics in heterogeneous populations: a general framework   for an arbitrary type distribution", "id": "1805.04897", "abstract": "a general framework of evolutionary dynamics under heterogeneous populations is presented. the framework allows continuously many types of heterogeneous agents, heterogeneity both in payoff functions and in revision protocols and the entire joint distribution of strategies and types to influence the payoffs of agents. we clarify regularity conditions for the unique existence of a solution trajectory and for the existence of equilibrium. we confirm that equilibrium stationarity in general and equilibrium stability in potential games are extended from the homogeneous setting to the heterogeneous setting. in particular, a wide class of admissible dynamics share the same set of locally stable equilibria in a potential game through local maximization of the potential.", "categories": "cs.gt econ.th", "created": "2018-05-13", "updated": "2019-05-03", "authors": ["dai zusai"], "url": "https://arxiv.org/abs/1805.04897"}, {"title": "gains in evolutionary dynamics: a unifying and intuitive approach to   linking static and dynamic stability", "id": "1805.04898", "abstract": "static stability in an economic model means negative incentives for deviation from equilibrium strategies, which we expect to assure a return to equilibrium, i.e., dynamic stability, as long as agents respond to incentives. there have been many attempts to prove this link, especially in evolutionary game theory, yielding both negative and positive results. this paper offers a universal and intuitive approach to this link. we prove static stability assures dynamic stability as long as agents' decisions of switching strategies are rationalizable by revealing costs and constraints behind distortions from exact optimization. this idea guides us to track the remaining expected maximal payoff gain from switches, after deducting the costs and to be maximized subject to the constraints, as a disequilibrium index, namely, a lyapunov function. while our analysis here is confined to myopic evolutionary dynamics in population games, our approach is applicable to more complex situations.", "categories": "math.oc cs.gt econ.th", "created": "2018-05-13", "updated": "2020-06-08", "authors": ["dai zusai"], "url": "https://arxiv.org/abs/1805.04898"}, {"title": "the finite sample performance of treatment effects estimators based on   the lasso", "id": "1805.05067", "abstract": "this paper contributes to the literature on treatment effects estimation with machine learning inspired methods by studying the performance of different estimators based on the lasso. building on recent work in the field of high-dimensional statistics, we use the semiparametric efficient score estimation structure to compare different estimators. alternative weighting schemes are considered and their suitability for the incorporation of machine learning estimators is assessed using theoretical arguments and various monte carlo experiments. additionally we propose an own estimator based on doubly robust kernel matching that is argued to be more robust to nuisance parameter misspecification. in the simulation study we verify theory based intuition and find good finite sample properties of alternative weighting scheme estimators like the one we propose.", "categories": "econ.em", "created": "2018-05-14", "updated": "", "authors": ["michael zimmert"], "url": "https://arxiv.org/abs/1805.05067"}, {"title": "data-driven investment decision-making: applying moore's law and   s-curves to business strategies", "id": "1805.06339", "abstract": "this paper introduces a method for linking technological improvement rates (i.e. moore's law) and technology adoption curves (i.e. s-curves). there has been considerable research surrounding moore's law and the generalized versions applied to the time dependence of performance for other technologies. the prior work has culminated with methodology for quantitative estimation of technological improvement rates for nearly any technology. this paper examines the implications of such regular time dependence for performance upon the timing of key events in the technological adoption process. we propose a simple crossover point in performance which is based upon the technological improvement rates and current level differences for target and replacement technologies. the timing for the cross-over is hypothesized as corresponding to the first 'knee'? in the technology adoption \"s-curve\" and signals when the market for a given technology will start to be rewarding for innovators. this is also when potential entrants are likely to intensely experiment with product-market fit and when the competition to achieve a dominant design begins. this conceptual framework is then back-tested by examining two technological changes brought about by the internet, namely music and video transmission. the uncertainty analysis around the cases highlight opportunities for organizations to reduce future technological uncertainty. overall, the results from the case studies support the reliability and utility of the conceptual framework in strategic business decision-making with the caveat that while technical uncertainty is reduced, it is not eliminated.", "categories": "econ.em", "created": "2018-05-16", "updated": "", "authors": ["christopher l. benson", "christopher l. magee"], "url": "https://arxiv.org/abs/1805.06339"}, {"title": "happy family of stable marriages", "id": "1805.06687", "abstract": "some aspects of the problem of stable marriage are discussed. there are two distinguished marriage plans: the fully transferable case, where money can be transferred between the participants, and the fully non transferable case where each participant has its own rigid preference list regarding the other gender. we continue to discuss intermediate partial transferable cases. partial transferable plans can be approached as either special cases of cooperative games using the notion of a core, or as a generalization of the cyclical monotonicity property of the fully transferable case (fake promises). we shall introduced these two approaches, and prove the existence of stable marriage for the fully transferable and non-transferable plans.", "categories": "econ.em", "created": "2018-05-17", "updated": "", "authors": ["gershon wolansky"], "url": "https://arxiv.org/abs/1805.06687"}, {"title": "multi-layered network structure: relationship between financial and   macroeconomic dynamics", "id": "1805.06829", "abstract": "we demonstrate using multi-layered networks, the existence of an empirical linkage between the dynamics of the financial network constructed from the market indices and the macroeconomic networks constructed from macroeconomic variables such as trade, foreign direct investments, etc. for several countries across the globe. the temporal scales of the dynamics of the financial variables and the macroeconomic fundamentals are very different, which make the empirical linkage even more interesting and significant. also, we find that there exist in the respective networks, core-periphery structures (determined through centrality measures) that are composed of the similar set of countries -- a result that may be related through the `gravity model' of the country-level macroeconomic networks. thus, from a multi-lateral openness perspective, we elucidate that for individual countries, larger trade connectivity is positively associated with higher financial return correlations. furthermore, we show that the economic complexity index and the equity markets have a positive relationship among themselves, as is the case for gross domestic product. the data science methodology using network theory, coupled with standard econometric techniques constitute a new approach to studying multi-level economic phenomena in a comprehensive manner.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2018-05-17", "updated": "2019-03-15", "authors": ["kiran sharma", "anindya s. chakrabarti", "anirban chakraborti"], "url": "https://arxiv.org/abs/1805.06829"}, {"title": "learning non-smooth models: instrumental variable quantile regressions   and related problems", "id": "1805.06855", "abstract": "this paper proposes computationally efficient methods that can be used for instrumental variable quantile regressions (ivqr) and related methods with statistical guarantees. this is much needed when we investigate heterogenous treatment effects since interactions between the endogenous treatment and control variables lead to an increased number of endogenous covariates. we prove that the gmm formulation of ivqr is np-hard and finding an approximate solution is also np-hard. hence, solving the problem from a purely computational perspective seems unlikely. instead, we aim to obtain an estimate that has good statistical properties and is not necessarily the global solution of any optimization problem.   the proposal consists of employing $k$-step correction on an initial estimate. the initial estimate exploits the latest advances in mixed integer linear programming and can be computed within seconds. one theoretical contribution is that such initial estimators and jacobian of the moment condition used in the k-step correction need not be even consistent and merely $k=4\\log n$ fast iterations are needed to obtain an efficient estimator. the overall proposal scales well to handle extremely large sample sizes because lack of consistency requirement allows one to use a very small subsample to obtain the initial estimate and the k-step iterations on the full sample can be implemented efficiently. another contribution that is of independent interest is to propose a tuning-free estimation for the jacobian matrix, whose definition nvolves conditional densities. this jacobian estimator generalizes bootstrap quantile standard errors and can be efficiently computed via closed-end solutions. we evaluate the performance of the proposal in simulations and an empirical example on the heterogeneous treatment effect of job training partnership act.", "categories": "econ.em stat.co stat.me", "created": "2018-05-17", "updated": "2019-09-05", "authors": ["yinchu zhu"], "url": "https://arxiv.org/abs/1805.06855"}, {"title": "bitcoin price and its marginal cost of production: support for a   fundamental value", "id": "1805.07610", "abstract": "this study back-tests a marginal cost of production model proposed to value the digital currency bitcoin. results from both conventional regression and vector autoregression (var) models show that the marginal cost of production plays an important role in explaining bitcoin prices, challenging recent allegations that bitcoins are essentially worthless. even with markets pricing bitcoin in the thousands of dollars each, the valuation model seems robust. the data show that a price bubble that began in the fall of 2017 resolved itself in early 2018, converging with the marginal cost model. this suggests that while bubbles may appear in the bitcoin market, prices will tend to this bound and not collapse to zero.", "categories": "econ.em", "created": "2018-05-19", "updated": "", "authors": ["adam hayes"], "url": "https://arxiv.org/abs/1805.07610"}, {"title": "on testing substitutability", "id": "1805.07642", "abstract": "the papers~\\cite{hatfimmokomi11} and~\\cite{azizbrilharr13} propose algorithms for testing whether the choice function induced by a (strict) preference list of length $n$ over a universe $u$ is substitutable. the running time of these algorithms is $o(|u|^3\\cdot n^3)$, respectively $o(|u|^2\\cdot n^3)$. in this note we present an algorithm with running time $o(|u|^2\\cdot n^2)$. note that $n$ may be exponential in the size $|u|$ of the universe.", "categories": "cs.ds econ.em", "created": "2018-05-19", "updated": "", "authors": ["cosmina croitoru", "kurt mehlhorn"], "url": "https://arxiv.org/abs/1805.07642"}, {"title": "multiple treatments with strategic interaction", "id": "1805.08275", "abstract": "we develop an empirical framework to identify and estimate the effects of treatments on outcomes of interest when the treatments are the result of strategic interaction (e.g., bargaining, oligopolistic entry, peer effects). we consider a model where agents play a discrete game with complete information whose equilibrium actions (i.e., binary treatments) determine a post-game outcome in a nonseparable model with endogeneity. due to the simultaneity in the first stage, the model as a whole is incomplete and the selection process fails to exhibit the conventional monotonicity. without imposing parametric restrictions or large support assumptions, this poses challenges in recovering treatment parameters. to address these challenges, we first establish a monotonic pattern of the equilibria in the first-stage game in terms of the number of treatments selected. based on this finding, we derive bounds on the average treatment effects (ates) under nonparametric shape restrictions and the existence of excluded exogenous variables. we show that instrument variation that compensates strategic substitution helps solve the multiple equilibria problem. we apply our method to data on airlines and air pollution in cities in the u.s. we find that (i) the causal effect of each airline on pollution is positive, and (ii) the effect is increasing in the number of firms but at a decreasing rate.", "categories": "econ.em stat.ap stat.me", "created": "2018-05-21", "updated": "2019-09-02", "authors": ["jorge balat", "sukjin han"], "url": "https://arxiv.org/abs/1805.08275"}, {"title": "sensitivity of regular estimators", "id": "1805.08883", "abstract": "this paper studies local asymptotic relationship between two scalar estimates. we define sensitivity of a target estimate to a control estimate to be the directional derivative of the target functional with respect to the gradient direction of the control functional. sensitivity according to the information metric on the model manifold is the asymptotic covariance of regular efficient estimators. sensitivity according to a general policy metric on the model manifold can be obtained from influence functions of regular efficient estimators. policy sensitivity has a local counterfactual interpretation, where the ceteris paribus change to a counterfactual distribution is specified by the combination of a control parameter and a riemannian metric on the model manifold.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-05-22", "updated": "", "authors": ["yaroslav mukhin"], "url": "https://arxiv.org/abs/1805.08883"}, {"title": "model selection in time series analysis: using information criteria as   an alternative to hypothesis testing", "id": "1805.08991", "abstract": "the issue of model selection in applied research is of vital importance. since the true model in such research is not known, which model should be used from among various potential ones is an empirical question. there might exist several competitive models. a typical approach to dealing with this is classic hypothesis testing using an arbitrarily chosen significance level based on the underlying assumption that a true null hypothesis exists. in this paper we investigate how successful this approach is in determining the correct model for different data generating processes using time series data. an alternative approach based on more formal model selection techniques using an information criterion or cross-validation is suggested and evaluated in the time series environment via monte carlo experiments. this paper also explores the effectiveness of deciding what type of general relation exists between two variables (e.g. relation in levels or relation in first differences) using various strategies based on hypothesis testing and on information criteria with the presence or absence of unit roots.", "categories": "econ.em", "created": "2018-05-23", "updated": "", "authors": ["r. scott hacker", "abdulnasser hatemi-j"], "url": "https://arxiv.org/abs/1805.08991"}, {"title": "identification in nonparametric models for dynamic treatment effects", "id": "1805.09397", "abstract": "this paper develops a nonparametric model that represents how sequences of outcomes and treatment choices influence one another in a dynamic manner. in this setting, we are interested in identifying the average outcome for individuals in each period, had a particular treatment sequence been assigned. the identification of this quantity allows us to identify the average treatment effects (ate's) and the ate's on transitions, as well as the optimal treatment regimes, namely, the regimes that maximize the (weighted) sum of the average potential outcomes, possibly less the cost of the treatments. the main contribution of this paper is to relax the sequential randomization assumption widely used in the biostatistics literature by introducing a flexible choice-theoretic framework for a sequence of endogenous treatments. we show that the parameters of interest are identified under each period's two-way exclusion restriction, i.e., with instruments excluded from the outcome-determining process and other exogenous variables excluded from the treatment-selection process. we also consider partial identification in the case where the latter variables are not available. lastly, we extend our results to a setting where treatments do not appear in every period.", "categories": "econ.em stat.ap stat.me", "created": "2018-05-23", "updated": "2019-01-14", "authors": ["sukjin han"], "url": "https://arxiv.org/abs/1805.09397"}, {"title": "forecasting the sustainable status of the labor market in agriculture", "id": "1805.09686", "abstract": "in this article, a game-theoretic model is constructed that is related to the problem of optimal assignments. examples are considered. a compromise point is found, the nash equilibriums and the decision of the nash arbitration scheme are constructed.", "categories": "econ.gn q-fin.ec", "created": "2018-05-23", "updated": "", "authors": ["o. a. malafeyev", "v. e. onishenko", "i. v. zaytseva"], "url": "https://arxiv.org/abs/1805.09686"}, {"title": "inference related to common breaks in a multivariate system with joined   segmented trends with applications to global and hemispheric temperatures", "id": "1805.09937", "abstract": "what transpires from recent research is that temperatures and radiative forcing seem to be characterized by a linear trend with two changes in the rate of growth. the first occurs in the early 60s and indicates a very large increase in the rate of growth of both temperature and radiative forcing series. this was termed as the \"onset of sustained global warming\". the second is related to the more recent so-called hiatus period, which suggests that temperatures and total radiative forcing have increased less rapidly since the mid-90s compared to the larger rate of increase from 1960 to 1990. there are two issues that remain unresolved. the first is whether the breaks in the slope of the trend functions of temperatures and radiative forcing are common. this is important because common breaks coupled with the basic science of climate change would strongly suggest a causal effect from anthropogenic factors to temperatures. the second issue relates to establishing formally via a proper testing procedure that takes into account the noise in the series, whether there was indeed a `hiatus period' for temperatures since the mid 90s. this is important because such a test would counter the widely held view that the hiatus is the product of natural internal variability. our paper provides tests related to both issues. the results show that the breaks in temperatures and radiative forcing are common and that the hiatus is characterized by a significant decrease in their rate of growth. the statistical results are of independent interest and applicable more generally.", "categories": "econ.em math.st stat.ap stat.me stat.th", "created": "2018-05-24", "updated": "", "authors": ["dukpa kim", "tatsushi oka", "francisco estrada", "pierre perron"], "url": "https://arxiv.org/abs/1805.09937"}, {"title": "a double machine learning approach to estimate the effects of musical   practice on student's skills", "id": "1805.10300", "abstract": "this study investigates the dose-response effects of making music on youth development. identification is based on the conditional independence assumption and estimation is implemented using a recent double machine learning estimator. the study proposes solutions to two highly practically relevant questions that arise for these new methods: (i) how to investigate sensitivity of estimates to tuning parameter choices in the machine learning part? (ii) how to assess covariate balancing in high-dimensional settings? the results show that improvements in objectively measured cognitive skills require at least medium intensity, while improvements in school grades are already observed for low intensity of practice.", "categories": "econ.em", "created": "2018-05-23", "updated": "2019-01-09", "authors": ["michael c. knaus"], "url": "https://arxiv.org/abs/1805.10300"}, {"title": "flexible shrinkage in high-dimensional bayesian spatial autoregressive   models", "id": "1805.10822", "abstract": "this article introduces two absolutely continuous global-local shrinkage priors to enable stochastic variable selection in the context of high-dimensional matrix exponential spatial specifications. existing approaches as a means to dealing with overparameterization problems in spatial autoregressive specifications typically rely on computationally demanding bayesian model-averaging techniques. the proposed shrinkage priors can be implemented using markov chain monte carlo methods in a flexible and efficient way. a simulation study is conducted to evaluate the performance of each of the shrinkage priors. results suggest that they perform particularly well in high-dimensional environments, especially when the number of parameters to estimate exceeds the number of observations. for an empirical illustration we use pan-european regional economic growth data.", "categories": "econ.em", "created": "2018-05-28", "updated": "", "authors": ["michael pfarrhofer", "philipp piribauer"], "url": "https://arxiv.org/abs/1805.10822"}, {"title": "equilibrium restrictions and approximate models -- with an application   to pricing macroeconomic risk", "id": "1805.10869", "abstract": "this paper proposes a reconciliation between two approaches to structural estimation and inference: using a complete - yet approximate model versus imposing a set of credible behavioral conditions. this is done by distorting the approximate model to satisfy these conditions. we provide the asymptotic theory and monte carlo evidence, and illustrate that counterfactual experiments are possible. we apply the methodology to the model of long run risks in aggregate consumption (bansal and yaron, 2004), where the complete model is generated using the campbell and shiller (1988) approximation. using us data, we investigate the empirical importance of the neglected non-linearity. we find that distorting the model to satisfy the original equilibrium condition is strongly preferred by the data and substantially improves the identification of the structural parameters. tilting also restores key qualitative predictions of the non-linear model such as the endogenous time variation in risk premia, a property which is lost with linearization.", "categories": "econ.em", "created": "2018-05-28", "updated": "2019-09-12", "authors": ["andreas tryphonides"], "url": "https://arxiv.org/abs/1805.10869"}, {"title": "modeling the residential electricity consumption within a restructured   power market", "id": "1805.11138", "abstract": "the united states' power market is featured by the lack of judicial power at the federal level. the market thus provides a unique testing environment for the market organization structure. at the same time, the econometric modeling and forecasting of electricity market consumption become more challenging. import and export, which generally follow simple rules in european countries, can be a result of direct market behaviors. this paper seeks to build a general model for power consumption and using the model to test several hypotheses.", "categories": "econ.em", "created": "2018-05-28", "updated": "2018-06-11", "authors": ["chelsea sun"], "url": "https://arxiv.org/abs/1805.11138"}, {"title": "stationarity and ergodicity of vector star models", "id": "1805.11311", "abstract": "smooth transition autoregressive models are widely used to capture nonlinearities in univariate and multivariate time series. existence of stationary solution is typically assumed, implicitly or explicitly. in this paper we describe conditions for stationarity and ergodicity of vector star models. the key condition is that the joint spectral radius of certain matrices is below 1, which is not guaranteed if only separate spectral radii are below 1. our result allows to use recently introduced toolboxes from computational mathematics to verify the stationarity and ergodicity of vector star models.", "categories": "math.st econ.em stat.th", "created": "2018-05-29", "updated": "2019-08-08", "authors": ["igor l. kheifets", "pentti j. saikkonen"], "url": "https://arxiv.org/abs/1805.11311"}, {"title": "estimation and inference for policy relevant treatment effects", "id": "1805.11503", "abstract": "the policy relevant treatment effect (prte) measures the average effect of switching from a status-quo policy to a counterfactual policy. estimation of the prte involves estimation of multiple preliminary parameters, including propensity scores, conditional expectation functions of the outcome and covariates given the propensity score, and marginal treatment effects. these preliminary estimators can affect the asymptotic distribution of the prte estimator in complicated and intractable manners. in this light, we propose an orthogonal score for double debiased estimation of the prte, whereby the asymptotic distribution of the prte estimator is obtained without any influence of preliminary parameter estimators as far as they satisfy mild requirements of convergence rates. to our knowledge, this paper is the first to develop limit distribution theories for inference about the prte.", "categories": "econ.em", "created": "2018-05-29", "updated": "2020-07-16", "authors": ["yuya sasaki", "takuya ura"], "url": "https://arxiv.org/abs/1805.11503"}, {"title": "justifying the adoption and relevance of inflation targeting framework:   a time-varying evidence from ghana", "id": "1805.11562", "abstract": "this paper scrutinizes the rationale for the adoption of inflation targeting (it) by bank of ghana in 2002. in this case, we determine the stability or otherwise of the relationship between money supply and inflation in ghana over the period 1970m1-2016m3 using battery of econometric methods. the empirical results show an unstable link between inflation and monetary growth in ghana, while the final state coefficient of inflation elasticity to money growth is positive but statistically insignificant. we find that inflation elasticity to monetary growth has continued to decline since the 1970s, showing a waning impact of money growth on inflation in ghana. notably, there is also evidence of negative inflation elasticity to monetary growth between 2001 and 2004, lending support to the adoption of it framework in ghana in 2002. we emphasized that the unprecedented 31-months of single-digit inflation (june 2010-december 2012), despite the observed inflationary shocks in 2010 and 2012, reinforces the immense contribution of the it framework in anchoring inflation expectations, with better inflation outcomes and inflation variability in ghana. the paper therefore recommends the continuous pursuance and strengthening of the it framework in ghana, as it embodies a more eclectic approach to policy formulation and implementation.", "categories": "econ.gn q-fin.ec", "created": "2018-05-29", "updated": "", "authors": ["nana kwame akosah", "francis w. loloh", "maurice omane-adjepong"], "url": "https://arxiv.org/abs/1805.11562"}, {"title": "how do public research labs use funding for research? a case study", "id": "1805.11932", "abstract": "this paper discusses how public research organizations consume funding for research, applying a new approach based on economic metabolism of research labs, in a broad analogy with biology. this approach is applied to a case study in europe represented by one of the biggest european public research organizations, the national research council of italy. results suggest that funding for research (state subsidy and public contracts) of this public research organization is mainly consumed for the cost of personnel. in addition, the analysis shows a disproportionate growth of the cost of personnel in public research labs in comparison with total revenue from government. in the presence of shrinking public research lab budgets, this organizational behavior generates inefficiencies and stress. r&d management and public policy implications are suggested for improving economic performance of public research organizations in turbulent markets.", "categories": "econ.gn q-fin.ec", "created": "2018-05-25", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1805.11932"}, {"title": "a physical review on currency", "id": "1805.12102", "abstract": "a theoretical self-sustainable economic model is established based on the fundamental factors of production, consumption, reservation and reinvestment, where currency is set as a unconditional credit symbol serving as transaction equivalent and stock means. principle properties of currency are explored in this ideal economic system. physical analysis reveals some facts that were not addressed by traditional monetary theory, and several basic principles of ideal currency are concluded: 1. the saving-replacement is a more primary function of currency than the transaction equivalents; 2. the ideal efficiency of currency corresponds to the least practical value; 3. the contradiction between constant face value of currency and depreciable goods leads to intrinsic inflation.", "categories": "econ.gn q-fin.ec", "created": "2018-04-17", "updated": "", "authors": ["ran huang"], "url": "https://arxiv.org/abs/1805.12102"}, {"title": "information technologies in public administration", "id": "1805.12107", "abstract": "there are visible changes in the world organization, environment and health of national conscience that create a background for discussion on possible redefinition of global, state and regional management goals. the author applies the sustainable development criteria to a hierarchical management scheme that is to lead the world community to non-contradictory growth. concrete definitions are discussed in respect of decision-making process representing the state mostly. with the help of systems analysis it is highlighted how to understand who would carry the distinctive sign of world leadership in the nearest future.", "categories": "econ.gn q-fin.ec", "created": "2018-04-05", "updated": "", "authors": ["v. i. gorelov"], "url": "https://arxiv.org/abs/1805.12107"}, {"title": "lessons from the history of european emu", "id": "1805.12112", "abstract": "this paper examines the history of previous examples of emu from the viewpoint that state actors make decisions about whether to participate in a monetary union based on rational self-interest concerning costs and benefits to their national economies. illustrative examples are taken from nineteenth century german, italian and japanese attempts at monetary integration with early twentieth century ones from the latin monetary union and the scandinavian monetary union and contemporary ones from the west african monetary union and the european monetary system. lessons learned from the historical examples will be used to identify issues that could arise with the move towards closer emu in europe.", "categories": "econ.gn q-fin.ec", "created": "2018-05-25", "updated": "", "authors": ["chris kirrane"], "url": "https://arxiv.org/abs/1805.12112"}, {"title": "implications of emu for the european community", "id": "1805.12113", "abstract": "monetary integration has both costs and benefits. europeans have a strong aversion to exchange rate instability. from this perspective, the ems has shown its limits and full monetary union involving a single currency appears to be a necessity. this is the goal of the emu project contained in the maastricht treaty. this paper examines the pertinent choices: independence of the central bank, budgetary discipline and economic policy coordination. therefore, the implications of emu for the economic policy of france will be examined. if the external force disappears, the public sector still cannot circumvent its solvency constraint. the instrument of national monetary policy will not be available so the absorption of asymmetric shocks will require greater wage flexibility and fiscal policy will play a greater role. the paper includes three parts. the first concerns the economic foundations of monetary union and the costs it entails. the second is devoted to the institutional arrangements under the treaty of maastricht. the third examines the consequences of monetary union for the economy and the economic policy of france.", "categories": "econ.gn q-fin.ec", "created": "2018-05-25", "updated": "", "authors": ["chris kirrane"], "url": "https://arxiv.org/abs/1805.12113"}, {"title": "introducing shrinkage in heavy-tailed state space models to predict   equity excess returns", "id": "1805.12217", "abstract": "we forecast s&p 500 excess returns using a flexible bayesian econometric state space model with non-gaussian features at several levels. more precisely, we control for overparameterization via novel global-local shrinkage priors on the state innovation variances as well as the time-invariant part of the state space model. the shrinkage priors are complemented by heavy tailed state innovations that cater for potential large breaks in the latent states. moreover, we allow for leptokurtic stochastic volatility in the observation equation. the empirical findings indicate that several variants of the proposed approach outperform typical competitors frequently used in the literature, both in terms of point and density forecasts.", "categories": "econ.em q-fin.st stat.ap", "created": "2018-05-30", "updated": "2019-07-26", "authors": ["florian huber", "gregor kastner", "michael pfarrhofer"], "url": "https://arxiv.org/abs/1805.12217"}, {"title": "ill-posed estimation in high-dimensional models with instrumental   variables", "id": "1806.00666", "abstract": "this paper is concerned with inference about low-dimensional components of a high-dimensional parameter vector $\\beta^0$ which is identified through instrumental variables. we allow for eigenvalues of the expected outer product of included and excluded covariates, denoted by $m$, to shrink to zero as the sample size increases. we propose a novel estimator based on desparsification of an instrumental variable lasso estimator, which is a regularized version of 2sls with an additional correction term. this estimator converges to $\\beta^0$ at a rate depending on the mapping properties of $m$ captured by a sparse link condition. linear combinations of our estimator of $\\beta^0$ are shown to be asymptotically normally distributed. based on consistent covariance estimation, our method allows for constructing confidence intervals and statistical tests for single or low-dimensional components of $\\beta^0$. in monte-carlo simulations we analyze the finite sample behavior of our estimator.", "categories": "econ.em", "created": "2018-06-02", "updated": "2020-08-03", "authors": ["christoph breunig", "enno mammen", "anna simoni"], "url": "https://arxiv.org/abs/1806.00666"}, {"title": "identification of conduit countries and community structures in the   withholding tax networks", "id": "1806.00799", "abstract": "due to economic globalization, each country's economic law, including tax laws and tax treaties, has been forced to work as a single network. however, each jurisdiction (country or region) has not made its economic law under the assumption that its law functions as an element of one network, so it has brought unexpected results. we thought that the results are exactly international tax avoidance. to contribute to the solution of international tax avoidance, we tried to investigate which part of the network is vulnerable. specifically, focusing on treaty shopping, which is one of international tax avoidance methods, we attempt to identified which jurisdiction are likely to be used for treaty shopping from tax liabilities and the relationship between jurisdictions which are likely to be used for treaty shopping and others. for that purpose, based on withholding tax rates imposed on dividends, interest, and royalties by jurisdictions, we produced weighted multiple directed graphs, computed the centralities and detected the communities. as a result, we clarified the jurisdictions that are likely to be used for treaty shopping and pointed out that there are community structures. the results of this study suggested that fewer jurisdictions need to introduce more regulations for prevention of treaty abuse worldwide.", "categories": "econ.em q-fin.gn", "created": "2018-06-03", "updated": "", "authors": ["tembo nakamoto", "yuichi ikeda"], "url": "https://arxiv.org/abs/1806.00799"}, {"title": "competitive pricing despite search costs if lower price signals quality", "id": "1806.00898", "abstract": "i show that firms price almost competitively and consumers can infer product quality from prices in markets where firms differ in quality and production cost, and learning prices is costly. bankruptcy risk or regulation links higher quality to lower cost. if high-quality firms have lower cost, then they can signal quality by cutting prices. then the low-quality firms must cut prices to retain customers. this price-cutting race to the bottom ends in a separating equilibrium in which the low-quality firms charge their competitive price and the high-quality firms charge slightly less.", "categories": "econ.gn econ.th q-fin.ec", "created": "2018-06-03", "updated": "", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1806.00898"}, {"title": "asymptotic refinements of a misspecification-robust bootstrap for   generalized empirical likelihood estimators", "id": "1806.00953", "abstract": "i propose a nonparametric iid bootstrap procedure for the empirical likelihood, the exponential tilting, and the exponentially tilted empirical likelihood estimators that achieves asymptotic refinements for t tests and confidence intervals, and wald tests and confidence regions based on such estimators. furthermore, the proposed bootstrap is robust to model misspecification, i.e., it achieves asymptotic refinements regardless of whether the assumed moment condition model is correctly specified or not. this result is new, because asymptotic refinements of the bootstrap based on these estimators have not been established in the literature even under correct model specification. monte carlo experiments are conducted in dynamic panel data setting to support the theoretical finding. as an application, bootstrap confidence intervals for the returns to schooling of hellerstein and imbens (1999) are calculated. the result suggests that the returns to schooling may be higher.", "categories": "econ.em", "created": "2018-06-04", "updated": "2018-06-04", "authors": ["seojeong lee"], "url": "https://arxiv.org/abs/1806.00953"}, {"title": "a rational decentralized generalized nash equilibrium seeking for energy   markets", "id": "1806.01072", "abstract": "we propose a method to design a decentralized energy market which guarantees individual rationality (ir) in expectation, in the presence of system-level grid constraints. we formulate the market as a welfare maximization problem subject to ir constraints, and we make use of lagrangian duality to model the problem as a n-person non-cooperative game with a unique generalized nash equilibrium (gne). we provide a distributed algorithm which converges to the gne. the convergence and properties of the algorithm are investigated by means of numerical simulations.", "categories": "cs.ce cs.gt econ.th", "created": "2018-06-04", "updated": "", "authors": ["lorenzo nespoli", "matteo salani", "vasco medici"], "url": "https://arxiv.org/abs/1806.01072"}, {"title": "quasi-experimental shift-share research designs", "id": "1806.01221", "abstract": "many studies use shift-share (or \"bartik\") instruments, which average a set of shocks with exposure share weights. we provide a new econometric framework for shift-share instrumental variable (ssiv) regressions in which identification follows from the quasi-random assignment of shocks, while exposure shares are allowed to be endogenous. the framework is motivated by an equivalence result: the orthogonality between a shift-share instrument and an unobserved residual can be represented as the orthogonality between the underlying shocks and a shock-level unobservable. ssiv regression coefficients can similarly be obtained from an equivalent shock-level regression, motivating shock-level conditions for their consistency. we discuss and illustrate several practical insights delivered by this framework in the setting of autor et al. (2013).", "categories": "econ.em", "created": "2018-06-04", "updated": "2020-08-26", "authors": ["kirill borusyak", "peter hull", "xavier jaravel"], "url": "https://arxiv.org/abs/1806.01221"}, {"title": "the impact of supervision and incentive process in explaining wage   profile and variance", "id": "1806.01332", "abstract": "the implementation of a supervision and incentive process for identical workers may lead to wage variance that stems from employer and employee optimization. the harder it is to assess the nature of the labor output, the more important such a process becomes, and the influence of such a process on wage development growth. the dynamic model presented in this paper shows that an employer will choose to pay a worker a starting wage that is less than what he deserves, resulting in a wage profile that fits the classic profile in the human-capital literature. the wage profile and wage variance rise at times of technological advancements, which leads to increased turnover as older workers are replaced by younger workers due to a rise in the relative marginal cost of the former.", "categories": "econ.em", "created": "2018-06-04", "updated": "", "authors": ["nitsa kasir", "idit sohlberg"], "url": "https://arxiv.org/abs/1806.01332"}, {"title": "asymptotic refinements of a misspecification-robust bootstrap for   generalized method of moments estimators", "id": "1806.01450", "abstract": "i propose a nonparametric iid bootstrap that achieves asymptotic refinements for t tests and confidence intervals based on gmm estimators even when the model is misspecified. in addition, my bootstrap does not require recentering the moment function, which has been considered as critical for gmm. regardless of model misspecification, the proposed bootstrap achieves the same sharp magnitude of refinements as the conventional bootstrap methods which establish asymptotic refinements by recentering in the absence of misspecification. the key idea is to link the misspecified bootstrap moment condition to the large sample theory of gmm under misspecification of hall and inoue (2003). two examples are provided: combining data sets and invalid instrumental variables.", "categories": "econ.em", "created": "2018-06-04", "updated": "", "authors": ["seojeong lee"], "url": "https://arxiv.org/abs/1806.01450"}, {"title": "a consistent variance estimator for 2sls when instruments identify   different lates", "id": "1806.01457", "abstract": "under treatment effect heterogeneity, an instrument identifies the instrument-specific local average treatment effect (late). with multiple instruments, two-stage least squares (2sls) estimand is a weighted average of different lates. what is often overlooked in the literature is that the postulated moment condition evaluated at the 2sls estimand does not hold unless those lates are the same. if so, the conventional heteroskedasticity-robust variance estimator would be inconsistent, and 2sls standard errors based on such estimators would be incorrect. i derive the correct asymptotic distribution, and propose a consistent asymptotic variance estimator by using the result of hall and inoue (2003, journal of econometrics) on misspecified moment condition models. this can be used to correctly calculate the standard errors regardless of whether there is more than one late or not.", "categories": "econ.em", "created": "2018-06-04", "updated": "", "authors": ["seojeong lee"], "url": "https://arxiv.org/abs/1806.01457"}, {"title": "leave-out estimation of variance components", "id": "1806.01494", "abstract": "we propose leave-out estimators of quadratic forms designed for the study of linear models with unrestricted heteroscedasticity. applications include analysis of variance and tests of linear restrictions in models with many regressors. an approximation algorithm is provided that enables accurate computation of the estimator in very large datasets. we study the large sample properties of our estimator allowing the number of regressors to grow in proportion to the number of observations. consistency is established in a variety of settings where plug-in methods and estimators predicated on homoscedasticity exhibit first-order biases. for quadratic forms of increasing rank, the limiting distribution can be represented by a linear combination of normal and non-central $\\chi^2$ random variables, with normality ensuing under strong identification. standard error estimators are proposed that enable tests of linear restrictions and the construction of uniformly valid confidence intervals for quadratic forms of interest. we find in italian social security records that leave-out estimates of a variance decomposition in a two-way fixed effects model of wage determination yield substantially different conclusions regarding the relative contribution of workers, firms, and worker-firm sorting to wage inequality than conventional methods. monte carlo exercises corroborate the accuracy of our asymptotic approximations, with clear evidence of non-normality emerging when worker mobility between blocks of firms is limited.", "categories": "econ.em", "created": "2018-06-05", "updated": "2019-08-26", "authors": ["patrick kline", "raffaele saggio", "mikkel s\u00f8lvsten"], "url": "https://arxiv.org/abs/1806.01494"}, {"title": "a quantitative analysis of possible futures of autonomous transport", "id": "1806.01696", "abstract": "autonomous ships (as) used for cargo transport have gained a considerable amount of attention in recent years. they promise benefits such as reduced crew costs, increased safety and increased flexibility. this paper explores the effects of a faster increase in technological performance in maritime shipping achieved by leveraging fast-improving technological domains such as computer processors, and advanced energy storage. based on historical improvement rates of several modes of transport (cargo ships, air, rail, trucking) a simplified markov-chain monte-carlo (mcmc) simulation of an intermodal transport model (imtm) is used to explore the effects of differing technological improvement rates for as. the results show that the annual improvement rates of traditional shipping (ocean cargo ships = 2.6%, air cargo = 5.5%, trucking = 0.6%, rail = 1.9%, inland water transport = 0.4%) improve at lower rates than technologies associated with automation such as computer processors (35.6%), fuel cells (14.7%) and automotive autonomous hardware (27.9%). the imtm simulations up to the year 2050 show that the introduction of any mode of autonomous transport will increase competition in lower cost shipping options, but is unlikely to significantly alter the overall distribution of transport mode costs. secondly, if all forms of transport end up converting to autonomous systems, then the uncertainty surrounding the improvement rates yields a complex intermodal transport solution involving several options, all at a much lower cost over time. ultimately, the research shows a need for more accurate measurement of current autonomous transport costs and how they are changing over time.", "categories": "econ.em", "created": "2018-06-05", "updated": "", "authors": ["christopher l. benson", "pranav d sumanth", "alina p colling"], "url": "https://arxiv.org/abs/1806.01696"}, {"title": "high-dimensional econometrics and regularized gmm", "id": "1806.01888", "abstract": "this chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. high-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. we first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. we also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. we then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. the presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results.", "categories": "math.st econ.em stat.th", "created": "2018-06-05", "updated": "2018-06-10", "authors": ["alexandre belloni", "victor chernozhukov", "denis chetverikov", "christian hansen", "kengo kato"], "url": "https://arxiv.org/abs/1806.01888"}, {"title": "dark markets with multiple assets: segmentation, asymptotic stability,   and equilibrium prices", "id": "1806.01924", "abstract": "we study a generalization of the model of a dark market due to duffie-g\\^arleanu- pedersen [6]. our market is segmented and involves multiple assets. we show that this market has a unique asymptotically stable equilibrium. in order to establish this result, we use a novel approach inspired by a theory due to mckenzie and hawkins-simon. moreover, we obtain a closed form solution for the price of each asset at which investors trade at equilibrium. we conduct a comparative statics analysis which shows, among other sensitivities, how equilibrium prices respond to the level of interactions between investors.", "categories": "econ.gn q-fin.ec", "created": "2018-06-05", "updated": "", "authors": ["alain b\u00e9langer", "ndoun\u00e9 ndoun\u00e9", "roland pongou"], "url": "https://arxiv.org/abs/1806.01924"}, {"title": "role of symmetry in irrational choice", "id": "1806.02627", "abstract": "symmetry is a fundamental concept in modern physics and other related sciences. being such a powerful tool, almost all physical theories can be derived from symmetry, and the effectiveness of such an approach is astonishing. since many physicists do not actually believe that symmetry is a fundamental feature of nature, it seems more likely it is a fundamental feature of human cognition. according to evolutionary psychologists, humans have a sensory bias for symmetry. the unconscious quest for symmetrical patterns has developed as a solution to specific adaptive problems related to survival and reproduction. therefore, it comes as no surprise that some fundamental concepts in psychology and behavioral economics necessarily involve symmetry. the purpose of this paper is to draw attention to the role of symmetry in decision-making and to illustrate how it can be algebraically operationalized through the use of mathematical group theory.", "categories": "physics.pop-ph econ.em physics.soc-ph", "created": "2018-06-07", "updated": "2020-07-14", "authors": ["ivan kozic"], "url": "https://arxiv.org/abs/1806.02627"}, {"title": "driving by the elderly and their awareness of their driving difficulties   (hebrew)", "id": "1806.03254", "abstract": "in the past twenty years the number of elderly drivers has increased for two reasons. one is the higher proportion of elderly in the population, and the other is the rise in the share of the elderly who drive. this paper examines the features of their driving and the level of their awareness of problems relating to it, by analysis preference survey that included interviews with 205 drivers aged between 70 and 80. the interviewees exhibited a level of optimism and self confidence in their driving that is out of line with the real situation. there is also a discrepancy between how their driving is viewed by others and their own assessment, and between their self assessment and their assessment of the driving of other elderly drivers, which they rate lower than their own. they attributed great importance to safety feature in cars, although they did not think that they themselves needed them, and most elderly drivers did not think there was any reason that they should stop driving, despite suggestions from family members and others that they should do so. a declared preference survey was undertaken to assess the degree of difficulty elderly drivers attribute to driving conditions. it was found that they are concerned mainly about weather condition, driving at night, and long journeys. worry about night driving was most marked among women, the oldest drivers, and those who drove less frequently. in light of the findings, imposing greater responsibility on the health system should be considered. consideration should also be given to issuing partial licenses to the elderly for daytime driving only, or restricted to certain weather conditions, dependent on their medical condition. such flexibility will enable the elderly to maintain their life style and independence for a longer period on the one hand, and on the other, will minimize the risks to themselves and other.", "categories": "cs.cy econ.em", "created": "2018-06-04", "updated": "", "authors": ["idit sohlberg"], "url": "https://arxiv.org/abs/1806.03254"}, {"title": "pricing engine: estimating causal impacts in real world business   settings", "id": "1806.03285", "abstract": "we introduce the pricing engine package to enable the use of double ml estimation techniques in general panel data settings. customization allows the user to specify first-stage models, first-stage featurization, second stage treatment selection and second stage causal-modeling. we also introduce a dynamicdml class that allows the user to generate dynamic treatment-aware forecasts at a range of leads and to understand how the forecasts will vary as a function of causally estimated treatment parameters. the pricing engine is built on python 3.5 and can be run on an azure ml workbench environment with the addition of only a few python packages. this note provides high-level discussion of the double ml method, describes the packages intended use and includes an example jupyter notebook demonstrating application to some publicly available data. installation of the package and additional technical documentation is available at $\\href{https://github.com/bquistorff/pricingengine}{github.com/bquistorff/pricingengine}$.", "categories": "econ.em stat.ml", "created": "2018-06-08", "updated": "2018-06-12", "authors": ["matt goldman", "brian quistorff"], "url": "https://arxiv.org/abs/1806.03285"}, {"title": "orthogonal random forest for causal inference", "id": "1806.03467", "abstract": "we propose the orthogonal random forest, an algorithm that combines neyman-orthogonality to reduce sensitivity with respect to estimation error of nuisance parameters with generalized random forests (athey et al., 2017)--a flexible non-parametric method for statistical estimation of conditional moment models using random forests. we provide a consistency rate and establish asymptotic normality for our estimator. we show that under mild assumptions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a priori knowledge of these nuisance parameters. we show that when the nuisance functions have a locally sparse parametrization, then a local $\\ell_1$-penalized regression achieves the required rate. we apply our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike prior work, our method provably allows to control for a high-dimensional set of variables under standard sparsity conditions. we also provide a comprehensive empirical evaluation of our algorithm on both synthetic and real data.", "categories": "cs.lg econ.em math.st stat.ml stat.th", "created": "2018-06-09", "updated": "2019-09-25", "authors": ["miruna oprescu", "vasilis syrgkanis", "zhiwei steven wu"], "url": "https://arxiv.org/abs/1806.03467"}, {"title": "determining the dimension of factor structures in non-stationary large   datasets", "id": "1806.03647", "abstract": "we propose a procedure to determine the dimension of the common factor space in a large, possibly non-stationary, dataset. our procedure is designed to determine whether there are (and how many) common factors (i) with linear trends, (ii) with stochastic trends, (iii) with no trends, i.e. stationary. our analysis is based on the fact that the largest eigenvalues of a suitably scaled covariance matrix of the data (corresponding to the common factor part) diverge, as the dimension $n$ of the dataset diverges, whilst the others stay bounded. therefore, we propose a class of randomised test statistics for the null that the $p$-th eigenvalue diverges, based directly on the estimated eigenvalue. the tests only requires minimal assumptions on the data, and no restrictions on the relative rates of divergence of $n$ and $t$ are imposed. monte carlo evidence shows that our procedure has very good finite sample properties, clearly dominating competing approaches when no common factors are present. we illustrate our methodology through an application to us bond yields with different maturities observed over the last 30 years. a common linear trend and two common stochastic trends are found and identified as the classical level, slope and curvature factors.", "categories": "stat.me econ.em", "created": "2018-06-10", "updated": "", "authors": ["matteo barigozzi", "lorenzo trapani"], "url": "https://arxiv.org/abs/1806.03647"}, {"title": "inference under covariate-adaptive randomization with multiple   treatments", "id": "1806.04206", "abstract": "this paper studies inference in randomized controlled trials with covariate-adaptive randomization when there are multiple treatments. more specifically, we study inference about the average effect of one or more treatments relative to other treatments or a control. as in bugni et al. (2018), covariate-adaptive randomization refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve balance within each stratum. in contrast to bugni et al. (2018), we not only allow for multiple treatments, but further allow for the proportion of units being assigned to each of the treatments to vary across strata. we first study the properties of estimators derived from a fully saturated linear regression, i.e., a linear regression of the outcome on all interactions between indicators for each of the treatments and indicators for each of the strata. we show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are invalid; on the other hand, tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact. for the special case in which the target proportion of units being assigned to each of the treatments does not vary across strata, we additionally consider tests based on estimators derived from a linear regression with strata fixed effects, i.e., a linear regression of the outcome on indicators for each of the treatments and indicators for each of the strata. we show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are conservative, but tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact. a simulation study illustrates the practical relevance of our theoretical results.", "categories": "econ.em stat.me", "created": "2018-06-11", "updated": "2019-01-17", "authors": ["federico a. bugni", "ivan a. canay", "azeem m. shaikh"], "url": "https://arxiv.org/abs/1806.04206"}, {"title": "a growth model with unemployment", "id": "1806.04228", "abstract": "a standard growth model is modified in a straightforward way to incorporate what keynes (1936) suggests in the \"essence\" of his general theory. the theoretical essence is the idea that exogenous changes in investment cause changes in employment and unemployment. we implement this idea by assuming the path for capital growth rate is exogenous in the growth model. the result is a growth model that can explain both long term trends and fluctuations around the trend. the modified growth model was tested using the u.s. economic data from 1947 to 2014. the hypothesized inverse relationship between the capital growth and changes in unemployment was confirmed, and the structurally estimated model fits fluctuations in unemployment reasonably well.", "categories": "econ.em", "created": "2018-06-11", "updated": "", "authors": ["mina mahmoudi", "mark pingle"], "url": "https://arxiv.org/abs/1806.04228"}, {"title": "the role of agricultural sector productivity in economic growth: the   case of iran's economic development plan", "id": "1806.04235", "abstract": "this study provides the theoretical framework and empirical model for productivity growth evaluations in agricultural sector as one of the most important sectors in iran's economic development plan. we use the solow residual model to measure the productivity growth share in the value-added growth of the agricultural sector. our time series data includes value-added per worker, employment, and capital in this sector. the results show that the average total factor productivity growth rate in the agricultural sector is -0.72% during 1991-2010. also, during this period, the share of total factor productivity growth in the value-added growth is -19.6%, while it has been forecasted to be 33.8% in the fourth development plan. considering the effective role of capital in the agricultural low productivity, we suggest applying productivity management plans (especially in regards of capital productivity) to achieve future growth goals.", "categories": "econ.em", "created": "2018-06-11", "updated": "", "authors": ["morteza tahamipour", "mina mahmoudi"], "url": "https://arxiv.org/abs/1806.04235"}, {"title": "estimating trade-related adjustment costs in the agricultural sector in   iran", "id": "1806.04238", "abstract": "tariff liberalization and its impact on tax revenue is an important consideration for developing countries, because they are increasingly facing the difficult task of implementing and harmonizing regional and international trade commitments. the tariff reform and its costs for iranian government is one of the issues that are examined in this study. another goal of this paper is, estimating the cost of trade liberalization. on this regard, imports value of agricultural sector in iran in 2010 was analyzed according to two scenarios. for reforming nuisance tariff, a vat policy is used in both scenarios. in this study, trist method is used. in the first scenario, imports' value decreased to a level equal to the second scenario and higher tariff revenue will be created. the results show that reducing the average tariff rate does not always result in the loss of tariff revenue. this paper is a witness that different forms of tariff can generate different amount of income when they have same level of liberalization and equal effect on producers. therefore, using a good tariff regime can help a government to generate income when increases social welfare by liberalization.", "categories": "econ.em", "created": "2018-06-11", "updated": "", "authors": ["omid karami", "mina mahmoudi"], "url": "https://arxiv.org/abs/1806.04238"}, {"title": "asymmetric response to pmi announcements in china's stock returns", "id": "1806.04347", "abstract": "considered an important macroeconomic indicator, the purchasing managers' index (pmi) on manufacturing generally assumes that pmi announcements will produce an impact on stock markets. international experience suggests that stock markets react to negative pmi news. in this research, we empirically investigate the stock market reaction towards pmi in china. the asymmetric effects of pmi announcements on the stock market are observed: no market reaction is generated towards negative pmi announcements, while a positive reaction is generally generated for positive pmi news. we further find that the positive reaction towards the positive pmi news occurs 1 day before the announcement and lasts for nearly 3 days, and the positive reaction is observed in the context of expanding economic conditions. by contrast, the negative reaction towards negative pmi news is prevalent during downward economic conditions for stocks with low market value, low institutional shareholding ratios or high price earnings. our study implies that china's stock market favors risk to a certain extent given the vast number of individual investors in the country, and there may exist information leakage in the market.", "categories": "q-fin.st econ.em", "created": "2018-06-12", "updated": "", "authors": ["yingli wang", "xiaoguang yang"], "url": "https://arxiv.org/abs/1806.04347"}, {"title": "a hybrid econometric-machine learning approach for relative importance   analysis: prioritizing food policy", "id": "1806.04517", "abstract": "a measure of relative importance of variables is often desired by researchers when the explanatory aspects of econometric methods are of interest. to this end, the author briefly reviews the limitations of conventional econometrics in constructing a reliable measure of variable importance. the author highlights the relative stature of explanatory and predictive analysis in economics and the emergence of fruitful collaborations between econometrics and computer science. learning lessons from both, the author proposes a hybrid approach based on conventional econometrics and advanced machine learning (ml) algorithms, which are otherwise, used in predictive analytics. the purpose of this article is two-fold, to propose a hybrid approach to assess relative importance and demonstrate its applicability in addressing policy priority issues with an example of food inflation in india, followed by a broader aim to introduce the possibility of conflation of ml and conventional econometrics to an audience of researchers in economics and social sciences, in general.", "categories": "econ.em stat.ml", "created": "2018-06-09", "updated": "2020-08-22", "authors": ["akash malhotra"], "url": "https://arxiv.org/abs/1806.04517"}, {"title": "regularized orthogonal machine learning for nonlinear semiparametric   models", "id": "1806.04823", "abstract": "this paper contributes to the literature on high-dimensional sparse m-estimation by allowing the loss function to depend on a functional nuisance parameter, which we estimate by modern machine learning tools. for a class of single-index conditional moment restrictions (cmrs), we explicitly derive the loss function. we first adjust the moment function so that the gradient of the future m-estimator loss is insensitive (formally, neyman-orthogonal) with respect to the first-stage regularization bias. we then take the loss function to be an indefinite integral of the adjusted moment function with respect to the single-index. the proposed l1-regularized m-estimator achieves the oracle convergence rate, where the oracle knows the nuisance parameter and solves only the parametric problem. our framework nests a novel approach to modeling heterogeneous treatment effects with a binary dependent variable. in addition, we apply our results to conditional moment models with missing data and static games of incomplete information. finally, we generalize our results to generic extremum estimation with a nuisance component.", "categories": "math.st cs.lg econ.em stat.ml stat.th", "created": "2018-06-12", "updated": "2020-09-29", "authors": ["denis nekipelov", "vira semenova", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/1806.04823"}, {"title": "lasso-driven inference in time and space", "id": "1806.05081", "abstract": "we consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak temporal dependence. a sequence of regressions with many regressors using lasso (least absolute shrinkage and selection operator) is applied for variable selection purpose, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. correspondingly, oracle properties with a jointly selected tuning parameter are derived. we further provide high-quality de-biased simultaneous inference on the many target parameters of the system. we provide bootstrap consistency results of the test procedure, which are based on a general bahadur representation for the $z$-estimators with dependent data. simulations demonstrate good performance of the proposed inference procedure. finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors.", "categories": "econ.em stat.me", "created": "2018-06-13", "updated": "2020-05-15", "authors": ["victor chernozhukov", "wolfgang k. h\u00e4rdle", "chen huang", "weining wang"], "url": "https://arxiv.org/abs/1806.05081"}, {"title": "stratification trees for adaptive randomization in randomized controlled   trials", "id": "1806.05127", "abstract": "this paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. the method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ate). we consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. by using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, as well as the assignment probabilities within these strata. our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. moreover, the results we present are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. in a simulation study, we find that our method, paired with an appropriate cross-validation procedure ,can improve on ad-hoc choices of stratification. we conclude by applying our method to the study in karlan and wood (2017), where we estimate stratification trees using the first wave of their experiment.", "categories": "econ.em stat.me", "created": "2018-06-13", "updated": "2020-06-11", "authors": ["max tabord-meehan"], "url": "https://arxiv.org/abs/1806.05127"}, {"title": "a profit optimization approach based on the use of pumped-hydro energy   storage unit and dynamic pricing", "id": "1806.05211", "abstract": "in this study, an optimization problem is proposed in order to obtain the maximum economic benefit from wind farms with variable and intermittent energy generation in the day ahead and balancing electricity markets. this method, which is based on the use of pumped-hydro energy storage unit and wind farm together, increases the profit from the power plant by taking advantage of the price changes in the markets and at the same time supports the power system by supplying a portion of the peak load demand in the system to which the plant is connected. with the objective of examining the effectiveness of the proposed method, detailed simulation studies are carried out by making use of actual wind and price data, and the results are compared to those obtained for the various cases in which the storage unit is not available and/or the proposed price-based energy management method is not applied. as a consequence, it is demonstrated that the pumped-hydro energy storage units are the storage systems capable of being used effectively for high-power levels and that the proposed optimization problem is quite successful in the cost-effective implementation of these systems.", "categories": "econ.em eess.sp", "created": "2018-06-07", "updated": "", "authors": ["ak\u0131n ta\u015fcikarao\u011flu", "ozan erdin\u00e7"], "url": "https://arxiv.org/abs/1806.05211"}, {"title": "how much income inequality is fair? nash bargaining solution and its   connection to entropy", "id": "1806.05262", "abstract": "the question about fair income inequality has been an important open question in economics and in political philosophy for over two centuries with only qualitative answers such as the ones suggested by rawls, nozick, and dworkin. we provided a quantitative answer recently, for an ideal free-market society, by developing a game-theoretic framework that proved that the ideal inequality is a lognormal distribution of income at equilibrium. in this paper, we develop another approach, using the nash bargaining solution (nbs) framework, which also leads to the same conclusion. even though the conclusion is the same, the new approach, however, reveals the true nature of nbs, which has been of considerable interest for several decades. economists have wondered about the economic meaning or purpose of the nbs. while some have alluded to its fairness property, we show more conclusively that it is all about fairness. since the essence of entropy is also fairness, we see an interesting connection between the nash product and entropy for a large population of rational economic agents.", "categories": "econ.gn q-fin.ec", "created": "2018-06-13", "updated": "", "authors": ["venkat venkatasubramanian", "yu luo"], "url": "https://arxiv.org/abs/1806.05262"}, {"title": "generalized log-normal chain-ladder", "id": "1806.05939", "abstract": "we propose an asymptotic theory for distribution forecasting from the log normal chain-ladder model. the theory overcomes the difficulty of convoluting log normal variables and takes estimation error into account. the results differ from that of the over-dispersed poisson model and from the chain-ladder based bootstrap. we embed the log normal chain-ladder model in a class of infinitely divisible distributions called the generalized log normal chain-ladder model. the asymptotic theory uses small $\\sigma$ asymptotics where the dimension of the reserving triangle is kept fixed while the standard deviation is assumed to decrease. the resulting asymptotic forecast distributions follow t distributions. the theory is supported by simulations and an empirical application.", "categories": "stat.me econ.em", "created": "2018-06-15", "updated": "", "authors": ["d. kuang", "b. nielsen"], "url": "https://arxiv.org/abs/1806.05939"}, {"title": "explicit solutions for optimal resource extraction problems under regime   switching l\\'evy models", "id": "1806.06105", "abstract": "this paper studies the problem of optimally extracting nonrenewable natural resources. taking into account the fact that the market values of the main natural resources i.e. oil, natural gas, copper,..., etc, fluctuate randomly following global and seasonal macroeconomic parameters, the prices of natural resources are modeled using markov switching l\\'evy processes. we formulate this optimal extraction problem as an infinite-time horizon optimal control problem. we derive closed-form solutions for the value function as well as the optimal extraction policy. numerical examples are presented to illustrate these results.", "categories": "econ.gn q-fin.ec", "created": "2018-06-15", "updated": "", "authors": ["moustapha pemy"], "url": "https://arxiv.org/abs/1806.06105"}, {"title": "effect of climate and geography on worldwide fine resolution economic   activity", "id": "1806.06358", "abstract": "geography, including climatic factors, have long been considered potentially important elements in shaping socio-economic activities, alongside other determinants, such as institutions. here we demonstrate that geography and climate satisfactorily explain worldwide economic activity as measured by the per capita gross cell product (gcp-pc) at a fine geographical resolution, typically much higher than country average. a 1{\\deg} by 1{\\deg} gcp-pc dataset has been key for establishing and testing a direct relationship between 'local' geography/climate and gcp-pc. not only have we tested the geography/climate hypothesis using many possible explanatory variables, importantly we have also predicted and reconstructed gcp-pc worldwide by retaining the most significant predictors. while this study confirms that latitude is the most important predictor for gcp-pc when taken in isolation, the accuracy of the gcp-pc prediction is greatly improved when other factors mainly related to variations in climatic variables, such as the variability in air pressure, rather than average climatic conditions as typically used, are considered. implications of these findings include an improved understanding of why economically better-off societies are geographically placed where they are", "categories": "econ.em physics.ao-ph", "created": "2018-06-17", "updated": "2019-01-03", "authors": ["alberto troccoli"], "url": "https://arxiv.org/abs/1806.06358"}, {"title": "the origin and the resolution of nonuniqueness in linear rational   expectations", "id": "1806.06657", "abstract": "the nonuniqueness of rational expectations is explained: in the stochastic, discrete-time, linear, constant-coefficients case, the associated free parameters are coefficients that determine the public's most immediate reactions to shocks. the requirement of model-consistency may leave these parameters completely free, yet when their values are appropriately specified, a unique solution is determined. in a broad class of models, the requirement of least-square forecast errors determines the parameter values, and therefore defines a unique solution. this approach is independent of dynamical stability, and generally does not suppress model dynamics.   application to a standard new keynesian example shows that the traditional solution suppresses precisely those dynamics that arise from rational expectations. the uncovering of those dynamics reveals their incompatibility with the new i-s equation and the expectational phillips curve.", "categories": "q-fin.ec econ.em econ.th", "created": "2018-06-18", "updated": "2019-04-29", "authors": ["john g. thistle"], "url": "https://arxiv.org/abs/1806.06657"}, {"title": "on the relation between sion's minimax theorem and existence of nash   equilibrium in asymmetric multi-players zero-sum game with only one alien", "id": "1806.07253", "abstract": "we consider the relation between sion's minimax theorem for a continuous function and a nash equilibrium in an asymmetric multi-players zero-sum game in which only one player is different from other players, and the game is symmetric for the other players. then,   1. the existence of a nash equilibrium, which is symmetric for players other than one player, implies sion's minimax theorem for pairs of this player and one of other players with symmetry for the other players.   2. sion's minimax theorem for pairs of one player and one of other players with symmetry for the other players implies the existence of a nash equilibrium which is symmetric for the other players.   thus, they are equivalent.", "categories": "econ.em", "created": "2018-06-16", "updated": "", "authors": ["atsuhiro satoh", "yasuhito tanaka"], "url": "https://arxiv.org/abs/1806.07253"}, {"title": "cluster-robust standard errors for linear regression models with many   controls", "id": "1806.07314", "abstract": "it is common practice in empirical work to employ cluster-robust standard errors when using the linear regression model to estimate some structural/causal effect of interest. researchers also often include a large set of regressors in their model specification in order to control for observed and unobserved confounders. in this paper we develop inference methods for linear regression models with many controls and clustering. we show that inference based on the usual cluster-robust standard errors by liang and zeger (1986) is invalid in general when the number of controls is a non-vanishing fraction of the sample size. we then propose a new clustered standard errors formula that is robust to the inclusion of many controls and allows to carry out valid inference in a variety of high-dimensional linear regression models, including fixed effects panel data models and the semiparametric partially linear model. monte carlo evidence supports our theoretical results and shows that our proposed variance estimator performs well in finite samples. the proposed method is also illustrated with an empirical application that re-visits donohue iii and levitt's (2001) study of the impact of abortion on crime.", "categories": "econ.em", "created": "2018-06-19", "updated": "2019-04-08", "authors": ["riccardo d'adamo"], "url": "https://arxiv.org/abs/1806.07314"}, {"title": "quantum nash equilibrium in the thermodynamic limit", "id": "1806.07343", "abstract": "the quantum nash equilibrium in the thermodynamic limit is studied for games like quantum prisoner's dilemma and the quantum game of chicken. a phase transition is seen in both games as a function of the entanglement in the game. we observe that for maximal entanglement irrespective of the classical payoffs, a majority of players choose quantum strategy over defect in the thermodynamic limit.", "categories": "quant-ph cond-mat.stat-mech cs.gt econ.em physics.soc-ph", "created": "2018-06-19", "updated": "2019-03-07", "authors": ["shubhayan sarkar", "colin benjamin"], "url": "https://arxiv.org/abs/1806.07343"}, {"title": "two different methods for modelling the likely upper economic limit of   the future united kingdom wind fleet", "id": "1806.07436", "abstract": "methods for predicting the likely upper economic limit for the wind fleet in the united kingdom should be simple to use whilst being able to cope with evolving technologies, costs and grid management strategies. this paper present two such models, both of which use data on historical wind patterns but apply different approaches to estimating the extent of wind shedding as a function of the size of the wind fleet. it is clear from the models that as the wind fleet increases in size, wind shedding will progressively increase, and as a result the overall economic efficiency of the wind fleet will be reduced. the models provide almost identical predictions of the efficiency loss and suggest that the future upper economic limit of the wind fleet will be mainly determined by the wind fleet headroom, a concept described in some detail in the paper. the results, which should have general applicability, are presented in graphical form, and should obviate the need for further modelling using the primary data. the paper also discusses the effectiveness of the wind fleet in decarbonising the grid, and the growing competition between wind and solar fleets as sources of electrical energy for the united kingdom.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2018-06-19", "updated": "", "authors": ["anthony d stephens", "david r walwyn"], "url": "https://arxiv.org/abs/1806.07436"}, {"title": "adaptive bayesian estimation of mixed discrete-continuous distributions   under smoothness and sparsity", "id": "1806.07484", "abstract": "we consider nonparametric estimation of a mixed discrete-continuous distribution under anisotropic smoothness conditions and possibly increasing number of support points for the discrete part of the distribution. for these settings, we derive lower bounds on the estimation rates in the total variation distance. next, we consider a nonparametric mixture of normals model that uses continuous latent variables for the discrete part of the observations. we show that the posterior in this model contracts at rates that are equal to the derived lower bounds up to a log factor. thus, bayesian mixture of normals models can be used for optimal adaptive estimation of mixed discrete-continuous distributions.", "categories": "math.st econ.em stat.th", "created": "2018-06-19", "updated": "", "authors": ["andriy norets", "justinas pelenis"], "url": "https://arxiv.org/abs/1806.07484"}, {"title": "is vix still the investor fear gauge? evidence for the us and bric   markets", "id": "1806.07556", "abstract": "we investigate the relationships of the vix with us and bric markets. in detail, we pick up the analysis from the point left off by (sarwar, 2012), and we focus on the period: jan 2007 - feb 2018, thus capturing the relations before, during and after the 2008 financial crisis. results pinpoint frequent structural breaks in the vix and suggest an enhancement around 2008 of the fear transmission in response to negative market moves; largely depending on overlaps in trading hours, this has become even stronger post-crisis for the us, while for bric countries has gone back towards pre-crisis levels.", "categories": "q-fin.gn econ.em", "created": "2018-06-20", "updated": "2018-07-23", "authors": ["marco neffelli", "marina resta"], "url": "https://arxiv.org/abs/1806.07556"}, {"title": "national debts and government deficits within european monetary union:   statistical evidence of economic issues", "id": "1806.07830", "abstract": "this study analyzes public debts and deficits between european countries. the statistical evidence here seems in general to reveal that sovereign debts and government deficits of countries within european monetary unification-in average- are getting worse than countries outside european monetary unification, in particular after the introduction of euro currency. this socioeconomic issue might be due to maastricht treaty, the stability and growth pact, the new fiscal compact, strict balanced-budget rules, etc. in fact, this economic policy of european union, in phases of economic recession, may generate delay and rigidity in the application of prompt counter-cycle (or acyclical) interventions to stimulate the economy when it is in a downturn within countries. some implications of economic policy are discussed.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2018-06-05", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1806.07830"}, {"title": "shift-share designs: theory and inference", "id": "1806.07928", "abstract": "we study inference in shift-share regression designs, such as when a regional outcome is regressed on a weighted average of sectoral shocks, using regional sector shares as weights. we conduct a placebo exercise in which we estimate the effect of a shift-share regressor constructed with randomly generated sectoral shocks on actual labor market outcomes across u.s. commuting zones. tests based on commonly used standard errors with 5\\% nominal significance level reject the null of no effect in up to 55\\% of the placebo samples. we use a stylized economic model to show that this overrejection problem arises because regression residuals are correlated across regions with similar sectoral shares, independently of their geographic location. we derive novel inference methods that are valid under arbitrary cross-regional correlation in the regression residuals. we show using popular applications of shift-share designs that our methods may lead to substantially wider confidence intervals in practice.", "categories": "econ.em", "created": "2018-06-20", "updated": "2019-08-09", "authors": ["rodrigo ad\u00e3o", "michal koles\u00e1r", "eduardo morales"], "url": "https://arxiv.org/abs/1806.07928"}, {"title": "the transmission of uncertainty shocks on income inequality: state-level   evidence from the united states", "id": "1806.08278", "abstract": "in this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the united states. using a novel large-scale macroeconometric model, we shed light on regional disparities of inequality responses to a national uncertainty shock. the results suggest that income inequality decreases in most states, with a pronounced degree of heterogeneity in terms of shapes and magnitudes of the dynamic responses. by contrast, some few states, mostly located in the west and south census region, display increasing levels of income inequality over time. we find that this directional pattern in responses is mainly driven by the income composition and labor market fundamentals. in addition, forecast error variance decompositions allow for a quantitative assessment of the importance of uncertainty shocks in explaining income inequality. the findings highlight that volatility shocks account for a considerable fraction of forecast error variance for most states considered. finally, a regression-based analysis sheds light on the driving forces behind differences in state-specific inequality responses.", "categories": "econ.em", "created": "2018-06-21", "updated": "", "authors": ["manfred m. fischer", "florian huber", "michael pfarrhofer"], "url": "https://arxiv.org/abs/1806.08278"}, {"title": "semiparametrically point-optimal hybrid rank tests for unit roots", "id": "1806.09304", "abstract": "we propose a new class of unit root tests that exploits invariance properties in the locally asymptotically brownian functional limit experiment associated to the unit root model. the invariance structures naturally suggest tests that are based on the ranks of the increments of the observations, their average, and an assumed reference density for the innovations. the tests are semiparametric in the sense that they are valid, i.e., have the correct (asymptotic) size, irrespective of the true innovation density. for a correctly specified reference density, our test is point-optimal and nearly efficient. for arbitrary reference densities, we establish a chernoff-savage type result, i.e., our test performs as well as commonly used tests under gaussian innovations but has improved power under other, e.g., fat-tailed or skewed, innovation distributions. to avoid nonparametric estimation, we propose a simplified version of our test that exhibits the same asymptotic properties, except for the chernoff-savage result that we are only able to demonstrate by means of simulations.", "categories": "econ.em", "created": "2018-06-25", "updated": "", "authors": ["bo zhou", "ramon van den akker", "bas j. m. werker"], "url": "https://arxiv.org/abs/1806.09304"}, {"title": "testability of instrument validity under continuous endogenous variables", "id": "1806.09517", "abstract": "instrumental variables have informed the research on causal inference in economics over the last century. despite their ubiquity and decided usefulness, the current consensus in the literature is that the validity of an instrument cannot be tested, particularly when the treatment is continuous. this note addresses this issue in two ways. as a first contribution, it presents a proof confirming the conjecture in pearl (1995), showing that the validity of an instrument indeed cannot be tested in the most general case when the endogenous variable is continuous. however, as a second contribution, it shows that already weak restrictions on the instrumental variable model reestablish theoretical testability. continuity is already enough. monotonicity introduces further testable implications.", "categories": "econ.em math.st stat.th", "created": "2018-06-25", "updated": "2019-10-21", "authors": ["florian gunsilius"], "url": "https://arxiv.org/abs/1806.09517"}, {"title": "point-identification in multivariate nonseparable triangular models", "id": "1806.09680", "abstract": "in this article we introduce a general nonparametric point-identification result for nonseparable triangular models with a multivariate first- and second stage. based on this we prove point-identification of hedonic models with multivariate heterogeneity and endogenous observable characteristics, extending and complementing identification results from the literature which all require exogeneity. as an additional application of our theoretical result, we show that the blp model (berry et al. 1995) can also be identified without index restrictions.", "categories": "econ.em", "created": "2018-06-25", "updated": "", "authors": ["florian gunsilius"], "url": "https://arxiv.org/abs/1806.09680"}, {"title": "implementing convex optimization in r: two econometric examples", "id": "1806.10423", "abstract": "economists specify high-dimensional models to address heterogeneity in empirical studies with complex big data. estimation of these models calls for optimization techniques to handle a large number of parameters. convex problems can be effectively executed in modern statistical programming languages. we complement koenker and mizera (2014)'s work on numerical implementation of convex optimization, with focus on high-dimensional econometric estimators. combining r and the convex solver mosek achieves faster speed and equivalent accuracy, demonstrated by examples from su, shi, and phillips (2016) and shi (2016). robust performance of convex optimization is witnessed cross platforms. the convenience and reliability of convex optimization in r make it easy to turn new ideas into prototypes.", "categories": "stat.co econ.em", "created": "2018-06-27", "updated": "2019-08-03", "authors": ["zhan gao", "zhentao shi"], "url": "https://arxiv.org/abs/1806.10423"}, {"title": "quantitative analysis on the disparity of regional economic development   in china and its evolution from 1952 to 2000", "id": "1806.10794", "abstract": "domestic and foreign scholars have already done much research on regional disparity and its evolution in china, but there is a big difference in conclusions. what is the reason for this? we think it is mainly due to different analytic approaches, perspectives, spatial units, statistical indicators and different periods for studies. on the basis of previous analyses and findings, we have done some further quantitative computation and empirical study, and revealed the inter-provincial disparity and regional disparity of economic development and their evolution trends from 1952-2000. the results shows that (a) regional disparity in economic development in china, including the inter-provincial disparity, inter-regional disparity and intra-regional disparity, has existed for years; (b) gini coefficient and theil coefficient have revealed a similar dynamic trend for comparative disparity in economic development between provinces in china. from 1952 to 1978, except for the \"great leap forward\" period, comparative disparity basically assumes a upward trend and it assumed a slowly downward trend from 1979 to1990. afterwards from 1991 to 2000 the disparity assumed a slowly upward trend again; (c) a comparison between shanghai and guizhou shows that absolute inter-provincial disparity has been quite big for years; and (d) the hurst exponent (h=0.5) in the period of 1966-1978 indicates that the comparative inter-provincial disparity of economic development showed a random characteristic, and in the hurst exponent (h>0.5) in period of 1979-2000 indicates that in this period the evolution of the comparative inter-provincial disparity of economic development in china has a long-enduring characteristic.", "categories": "stat.ap econ.em nlin.cd", "created": "2018-06-28", "updated": "", "authors": ["jianhua xu", "nanshan ai", "yan lu", "yong chen", "yiying ling", "wenze yue"], "url": "https://arxiv.org/abs/1806.10794"}, {"title": "subvector inference in partially identified models with many moment   inequalities", "id": "1806.11466", "abstract": "this paper considers inference for a function of a parameter vector in a partially identified model with many moment inequalities. this framework allows the number of moment conditions to grow with the sample size, possibly at exponential rates. our main motivating application is subvector inference, i.e., inference on a single component of the partially identified parameter vector associated with a treatment effect or a policy variable of interest.   our inference method compares a minmax test statistic (minimum over parameters satisfying $h_0$ and maximum over moment inequalities) against critical values that are based on bootstrap approximations or analytical bounds. we show that this method controls asymptotic size uniformly over a large class of data generating processes despite the partially identified many moment inequality setting. the finite sample analysis allows us to obtain explicit rates of convergence on the size control. our results are based on combining non-asymptotic approximations and new high-dimensional central limit theorems for the minmax of the components of random matrices. unlike the previous literature on functional inference in partially identified models, our results do not rely on weak convergence results based on donsker's class assumptions and, in fact, our test statistic may not even converge in distribution. our bootstrap approximation requires the choice of a tuning parameter sequence that can avoid the excessive concentration of our test statistic. to this end, we propose an asymptotically valid data-driven method to select this tuning parameter sequence. this method generalizes the selection of tuning parameter sequences to problems outside the donsker's class assumptions and may also be of independent interest. our procedures based on self-normalized moderate deviation bounds are relatively more conservative but easier to implement.", "categories": "math.st econ.em stat.th", "created": "2018-06-29", "updated": "", "authors": ["alexandre belloni", "federico bugni", "victor chernozhukov"], "url": "https://arxiv.org/abs/1806.11466"}, {"title": "the bretton woods experience and erm", "id": "1807.00418", "abstract": "historical examination of the bretton woods system allows comparisons to be made with the current evolution of the ems.", "categories": "econ.em", "created": "2018-07-01", "updated": "", "authors": ["chris kirrane"], "url": "https://arxiv.org/abs/1807.00418"}, {"title": "maastricht and monetary cooperation", "id": "1807.00419", "abstract": "this paper describes the opportunities and also the difficulties of emu with regard to international monetary cooperation. even though the institutional and intellectual assistance to the coordination of monetary policy in the eu will probably be strengthened with the emu, among the shortcomings of the maastricht treaty concerns the relationship between the founder members and those countries who wish to remain outside monetary union.", "categories": "econ.em", "created": "2018-07-01", "updated": "", "authors": ["chris kirrane"], "url": "https://arxiv.org/abs/1807.00419"}, {"title": "stochastic model specification in markov switching vector error   correction models", "id": "1807.00529", "abstract": "this paper proposes a hierarchical modeling approach to perform stochastic model specification in markov switching vector error correction models. we assume that a common distribution gives rise to the regime-specific regression coefficients. the mean as well as the variances of this distribution are treated as fully stochastic and suitable shrinkage priors are used. these shrinkage priors enable to assess which coefficients differ across regimes in a flexible manner. in the case of similar coefficients, our model pushes the respective regions of the parameter space towards the common distribution. this allows for selecting a parsimonious model while still maintaining sufficient flexibility to control for sudden shifts in the parameters, if necessary. we apply our modeling approach to real-time euro area data and assume transition probabilities between expansionary and recessionary regimes to be driven by the cointegration errors. the results suggest that the regime allocation is governed by a subset of short-run adjustment coefficients and regime-specific variance-covariance matrices. these findings are complemented by an out-of-sample forecast exercise, illustrating the advantages of the model for predicting euro area inflation in real time.", "categories": "econ.em", "created": "2018-07-02", "updated": "2019-09-05", "authors": ["niko hauzenberger", "florian huber", "michael pfarrhofer", "thomas o. z\u00f6rner"], "url": "https://arxiv.org/abs/1807.00529"}, {"title": "indirect inference through prediction", "id": "1807.01579", "abstract": "by recasting indirect inference estimation as a prediction rather than a minimization and by using regularized regressions, we can bypass the three major problems of estimation: selecting the summary statistics, defining the distance function and minimizing it numerically. by substituting regression with classification we can extend this approach to model selection as well. we present three examples: a statistical fit, the parametrization of a simple real business cycle model and heuristics selection in a fishery agent-based model. the outcome is a method that automatically chooses summary statistics, weighs them and use them to parametrize models without running any direct minimization.", "categories": "econ.em", "created": "2018-07-04", "updated": "", "authors": ["ernesto carrella", "richard m. bailey", "jens koed madsen"], "url": "https://arxiv.org/abs/1807.01579"}, {"title": "on the identifying content of instrument monotonicity", "id": "1807.01661", "abstract": "this paper studies the identifying content of the instrument monotonicity assumption of imbens and angrist (1994) on the distribution of potential outcomes in a model with a binary outcome, a binary treatment and an exogenous binary instrument. specifically, i derive necessary and sufficient conditions on the distribution of the data under which the identified set for the distribution of potential outcomes when the instrument monotonicity assumption is imposed can be a strict subset of that when it is not imposed.", "categories": "econ.em", "created": "2018-07-04", "updated": "2019-10-16", "authors": ["vishal kamat"], "url": "https://arxiv.org/abs/1807.01661"}, {"title": "bring a friend! privately or publicly?", "id": "1807.01994", "abstract": "we study the optimal referral strategy of a seller and its relationship with the type of communication channels among consumers. the seller faces a partially uninformed population of consumers, interconnected through a directed social network. in the network, the seller offers rewards to informed consumers (influencers) conditional on inducing purchases by uninformed consumers (influenced). rewards are needed to bear a communication cost and to induce word-of-mouth (wom) either privately (cost-per-contact) or publicly (fixed cost to inform all friends). from the seller's viewpoint, eliciting private wom is more costly than eliciting public wom. we investigate (i) the incentives for the seller to move to a denser network, inducing either private or public wom and (ii) the optimal mix between the two types of communication. a denser network is found to be always better, not only for information diffusion but also for seller's profits, as long as private wom is concerned. differently, under public wom, the seller may prefer an environment with less competition between informed consumers and the presence of highly connected influencers (hubs) is the main driver to make network density beneficial to profits. when the seller is able to discriminate between private and public wom, the optimal strategy is to cheaply incentivize the more connected people to pass on the information publicly and then offer a high bonus for private wom.", "categories": "physics.soc-ph econ.em", "created": "2018-07-04", "updated": "2018-12-26", "authors": ["elias carroni", "paolo pin", "simone righi"], "url": "https://arxiv.org/abs/1807.01994"}, {"title": "the role of the propensity score in fixed effect models", "id": "1807.02099", "abstract": "we develop a new approach for estimating average treatment effects in the observational studies with unobserved group-level heterogeneity. a common approach in such settings is to use linear fixed effect specifications estimated by least squares regression. such methods severely limit the extent of the heterogeneity between groups by making the restrictive assumption that linearly adjusting for differences between groups in average covariate values addresses all concerns with cross-group comparisons. we start by making two observations. first we note that the fixed effect method in effect adjusts only for differences between groups by adjusting for the average of covariate values and average treatment. second, we note that weighting by the inverse of the propensity score would remove biases for comparisons between treated and control units under the fixed effect set up. we then develop three generalizations of the fixed effect approach based on these two observations. first, we suggest more general, nonlinear, adjustments for the average covariate values. second, we suggest robustifying the estimators by using propensity score weighting. third, we motivate and develop implementations for adjustments that also adjust for group characteristics beyond the average covariate values.", "categories": "econ.em stat.me", "created": "2018-07-05", "updated": "2019-04-13", "authors": ["dmitry arkhangelsky", "guido imbens"], "url": "https://arxiv.org/abs/1807.02099"}, {"title": "minimizing sensitivity to model misspecification", "id": "1807.02161", "abstract": "we propose a framework for estimation and inference when the model may be misspecified. we rely on a local asymptotic approach where the degree of misspecification is indexed by the sample size. we construct estimators whose mean squared error is minimax in a neighborhood of the reference model, based on simple one-step adjustments. in addition, we provide confidence intervals that contain the true parameter under local misspecification. to interpret the degree of misspecification, we map it to the local power of a specification test of the reference model. our approach allows for systematic sensitivity analysis when the parameter of interest may be partially or irregularly identified. as illustrations, we study two binary choice models: a cross-sectional model where the error distribution is misspecified, and a dynamic panel data model where the number of time periods is small and the distribution of individual effects is misspecified.", "categories": "econ.em stat.me", "created": "2018-07-05", "updated": "2020-07-08", "authors": ["st\u00e9phane bonhomme", "martin weidner"], "url": "https://arxiv.org/abs/1807.02161"}, {"title": "state-varying factor models of large dimensions", "id": "1807.02248", "abstract": "this paper develops an inferential theory for state-varying factor models of large dimensions. unlike constant factor models, loadings are general functions of some recurrent state process. we develop an estimator for the latent factors and state-varying loadings under a large cross-section and time dimension. our estimator combines nonparametric methods with principal component analysis. we derive the rate of convergence and limiting normal distribution for the factors, loadings and common components. in addition, we develop a statistical test for a change in the factor structure in different states. we apply the estimator to u.s. treasury yields and s&p500 stock returns. the systematic factor structure in treasury yields differs in times of booms and recessions as well as in periods of high market volatility. state-varying factors based on the vix capture significantly more variation and pricing information in individual stocks than constant factor models.", "categories": "econ.em", "created": "2018-07-06", "updated": "2020-04-21", "authors": ["markus pelger", "ruoxuan xiong"], "url": "https://arxiv.org/abs/1807.02248"}, {"title": "autoregressive wild bootstrap inference for nonparametric trends", "id": "1807.02357", "abstract": "in this paper we propose an autoregressive wild bootstrap method to construct confidence bands around a smooth deterministic trend. the bootstrap method is easy to implement and does not require any adjustments in the presence of missing data, which makes it particularly suitable for climatological applications. we establish the asymptotic validity of the bootstrap method for both pointwise and simultaneous confidence bands under general conditions, allowing for general patterns of missing data, serial dependence and heteroskedasticity. the finite sample properties of the method are studied in a simulation study. we use the method to study the evolution of trends in daily measurements of atmospheric ethane obtained from a weather station in the swiss alps, where the method can easily deal with the many missing observations due to adverse weather conditions.", "categories": "stat.me econ.em", "created": "2018-07-06", "updated": "2019-11-25", "authors": ["marina friedrich", "stephan smeekes", "jean-pierre urbain"], "url": "https://arxiv.org/abs/1807.02357"}, {"title": "maximizing welfare in social networks under a utility driven influence   diffusion model", "id": "1807.02502", "abstract": "motivated by applications such as viral marketing, the problem of influence maximization (im) has been extensively studied in the literature. the goal is to select a small number of users to adopt an item such that it results in a large cascade of adoptions by others. existing works have three key limitations. (1) they do not account for economic considerations of a user in buying/adopting items. (2) most studies on multiple items focus on competition, with complementary items receiving limited attention. (3) for the network owner, maximizing social welfare is important to ensure customer loyalty, which is not addressed in prior work in the im literature. in this paper, we address all three limitations and propose a novel model called uic that combines utility-driven item adoption with influence propagation over networks. focusing on the mutually complementary setting, we formulate the problem of social welfare maximization in this novel setting. we show that while the objective function is neither submodular nor supermodular, surprisingly a simple greedy allocation algorithm achieves a factor of $(1-1/e-\\epsilon)$ of the optimum expected social welfare. we develop \\textsf{bundlegrd}, a scalable version of this approximation algorithm, and demonstrate, with comprehensive experiments on real and synthetic datasets, that it significantly outperforms all baselines.", "categories": "cs.si econ.em", "created": "2018-07-06", "updated": "2019-05-30", "authors": ["prithu banerjee", "wei chen", "laks v. s. lakshmanan"], "url": "https://arxiv.org/abs/1807.02502"}, {"title": "measurement errors as bad leverage points", "id": "1807.02814", "abstract": "errors-in-variables is a long-standing, difficult issue in linear regression; and progress depends in part on new identifying assumptions. i characterize measurement error as bad-leverage points and assume that fewer than half the sample observations are heavily contaminated, in which case a high-breakdown robust estimator may be able to isolate and down weight or discard the problematic data. in simulations of simple and multiple regression where eiv affects 25% of the data and r-squared is mediocre, certain high-breakdown estimators have small bias and reliable confidence intervals.", "categories": "econ.em stat.ap stat.me", "created": "2018-07-08", "updated": "2020-03-16", "authors": ["eric blankmeyer"], "url": "https://arxiv.org/abs/1807.02814"}, {"title": "transaction costs and institutional change of trade litigations in   bulgaria", "id": "1807.03034", "abstract": "the methods of new institutional economics for identifying the transaction costs of trade litigations in bulgaria are used in the current paper. for the needs of the research, an indicative model, measuring this type of costs on microeconomic level, is applied in the study. the main purpose of the model is to forecast the rational behavior of trade litigation parties in accordance with the transaction costs in the process of enforcing the execution of the signed commercial contract. the application of the model is related to the more accurate measurement of the transaction costs on microeconomic level, which fact could lead to better prediction and management of these costs in order market efficiency and economic growth to be achieved. in addition, it is made an attempt to be analysed the efficiency of the institutional change of the commercial justice system and the impact of the reform of the judicial system over the economic turnover. the augmentation or lack of reduction of the transaction costs in trade litigations would mean inefficiency of the reform of the judicial system. jel codes: o43, p48, d23, k12", "categories": "econ.em", "created": "2018-07-09", "updated": "", "authors": ["shteryo nozharov", "petya koralova-nozharova"], "url": "https://arxiv.org/abs/1807.03034"}, {"title": "cancer risk messages: a light bulb model", "id": "1807.03040", "abstract": "the meaning of public messages such as \"one in x people gets cancer\" or \"one in y people gets cancer by age z\" can be improved. one assumption commonly invoked is that there is no other cause of death, a confusing assumption. we develop a light bulb model to clarify cumulative risk and we use markov chain modeling, incorporating the assumption widely in place, to evaluate transition probabilities. age-progression in the cancer risk is then reported on australian data. future modelling can elicit realistic assumptions.", "categories": "econ.em", "created": "2018-07-09", "updated": "2018-07-10", "authors": ["ka c. chan", "ruth f. g. williams", "christopher t. lenard", "terence m. mills"], "url": "https://arxiv.org/abs/1807.03040"}, {"title": "cancer risk messages: public health and economic welfare", "id": "1807.03045", "abstract": "statements for public health purposes such as \"1 in 2 will get cancer by age 85\" have appeared in public spaces. the meaning drawn from such statements affects economic welfare, not just public health. both markets and government use risk information on all kinds of risks, useful information can, in turn, improve economic welfare, however inaccuracy can lower it. we adapt the contingency table approach so that a quoted risk is cross-classified with the states of nature. we show that bureaucratic objective functions regarding the accuracy of a reported cancer risk can then be stated.", "categories": "econ.em", "created": "2018-07-09", "updated": "2018-07-10", "authors": ["ruth f. g. williams", "ka c. chan", "christopher t. lenard", "terence m. mills"], "url": "https://arxiv.org/abs/1807.03045"}, {"title": "simulation modelling of inequality in cancer service access", "id": "1807.03048", "abstract": "this paper applies economic concepts from measuring income inequality to an exercise in assessing spatial inequality in cancer service access in regional areas. we propose a mathematical model for accessing chemotherapy among local government areas (lgas). our model incorporates a distance factor. with a simulation we report results for a single inequality measure: the lorenz curve is depicted for our illustrative data. we develop this approach in order to move incrementally towards its application to actual data and real-world health service regions. we seek to develop the exercises that can lead policy makers to relevant policy information on the most useful data collections to be collected and modeling for cancer service access in regional areas.", "categories": "econ.em", "created": "2018-07-09", "updated": "", "authors": ["ka c. chan", "ruth f. g. williams", "christopher t. lenard", "terence m. mills"], "url": "https://arxiv.org/abs/1807.03048"}, {"title": "stochastic switching games", "id": "1807.03893", "abstract": "we study nonzero-sum stochastic switching games. two players compete for market dominance through controlling (via timing options) the discrete-state market regime $m$. switching decisions are driven by a continuous stochastic factor $x$ that modulates instantaneous revenue rates and switching costs. this generates a competitive feedback between the short-term fluctuations due to $x$ and the medium-term advantages based on $m$. we construct threshold-type feedback nash equilibria which characterize stationary strategies describing long-run dynamic equilibrium market organization. two sequential approximation schemes link the switching equilibrium to (i) constrained optimal switching, (ii) multi-stage timing games. we provide illustrations using an ornstein-uhlenbeck $x$ that leads to a recurrent equilibrium $m^\\ast$ and a geometric brownian motion $x$ that makes $m^\\ast$ eventually \"absorbed\" as one player eventually gains permanent advantage. explicit computations and comparative statics regarding the emergent macroscopic market equilibrium are also provided.", "categories": "econ.gn q-fin.ec", "created": "2018-07-10", "updated": "", "authors": ["liangchen li", "michael ludkovski"], "url": "https://arxiv.org/abs/1807.03893"}, {"title": "clustering macroeconomic time series", "id": "1807.04004", "abstract": "the data mining technique of time series clustering is well established in many fields. however, as an unsupervised learning method, it requires making choices that are nontrivially influenced by the nature of the data involved. the aim of this paper is to verify usefulness of the time series clustering method for macroeconomics research, and to develop the most suitable methodology.   by extensively testing various possibilities, we arrive at a choice of a dissimilarity measure (compression-based dissimilarity measure, or cdm) which is particularly suitable for clustering macroeconomic variables. we check that the results are stable in time and reflect large-scale phenomena such as crises. we also successfully apply our findings to analysis of national economies, specifically to identifying their structural relations.", "categories": "econ.em", "created": "2018-07-11", "updated": "2018-07-18", "authors": ["iwo augusty\u0144ski", "pawe\u0142 lasko\u015b-grabowski"], "url": "https://arxiv.org/abs/1807.04004"}, {"title": "factor models with many assets: strong factors, weak factors, and the   two-pass procedure", "id": "1807.04094", "abstract": "this paper re-examines the problem of estimating risk premia in linear factor pricing models. typically, the data used in the empirical literature are characterized by weakness of some pricing factors, strong cross-sectional dependence in the errors, and (moderately) high cross-sectional dimensionality. using an asymptotic framework where the number of assets/portfolios grows with the time span of the data while the risk exposures of weak factors are local-to-zero, we show that the conventional two-pass estimation procedure delivers inconsistent estimates of the risk premia. we propose a new estimation procedure based on sample-splitting instrumental variables regression. the proposed estimator of risk premia is robust to weak included factors and to the presence of strong unaccounted cross-sectional error dependence. we derive the many-asset weak factor asymptotic distribution of the proposed estimator, show how to construct its standard errors, verify its performance in simulations, and revisit some empirical studies.", "categories": "econ.em", "created": "2018-07-11", "updated": "2019-04-08", "authors": ["stanislav anatolyev", "anna mikusheva"], "url": "https://arxiv.org/abs/1807.04094"}, {"title": "heterogeneous effects of unconventional monetary policy on loan demand   and supply. insights from the bank lending survey", "id": "1807.04161", "abstract": "this paper analyzes the bank lending channel and the heterogeneous effects on the euro area, providing evidence that the channel is indeed working. the analysis of the transmission mechanism is based on structural impulse responses to an unconventional monetary policy shock on bank loans. the bank lending survey (bls) is exploited in order to get insights on developments of loan demand and supply. the contribution of this paper is to use country-specific data to analyze the consequences of unconventional monetary policy, instead of taking an aggregate stance by using euro area data. this approach provides a deeper understanding of the bank lending channel and its effects. that is, an expansionary monetary policy shock leads to an increase in loan demand, supply and output growth. a small north-south disparity between the countries can be observed.", "categories": "econ.em", "created": "2018-07-11", "updated": "", "authors": ["martin guth"], "url": "https://arxiv.org/abs/1807.04161"}, {"title": "analysis of a dynamic voluntary contribution mechanism public good game", "id": "1807.04621", "abstract": "i present a dynamic, voluntary contribution mechanism, public good game and derive its potential outcomes. in each period, players endogenously determine contribution productivity by engaging in costly investment. the level of contribution productivity carries from period to period, creating a dynamic link between periods. the investment mimics investing in the stock of technology for producing public goods such as national defense or a clean environment. after investing, players decide how much of their remaining money to contribute to provision of the public good, as in traditional public good games. i analyze three kinds of outcomes of the game: the lowest payoff outcome, the nash equilibria, and socially optimal behavior. in the lowest payoff outcome, all players receive payoffs of zero. nash equilibrium occurs when players invest any amount and contribute all or nothing depending on the contribution productivity. therefore, there are infinitely many nash equilibria strategies. finally, the socially optimal result occurs when players invest everything in early periods, then at some point switch to contributing everything. my goal is to discover and explain this point. i use mathematical analysis and computer simulation to derive the results.", "categories": "econ.em", "created": "2018-07-12", "updated": "", "authors": ["dmytro bogatov"], "url": "https://arxiv.org/abs/1807.04621"}, {"title": "markets beyond nash welfare for leontief utilities", "id": "1807.05293", "abstract": "we study the allocation of divisible goods to competing agents via a market mechanism, focusing on agents with leontief utilities. the majority of the economics and mechanism design literature has focused on \\emph{linear} prices, meaning that the cost of a good is proportional to the quantity purchased. equilibria for linear prices are known to be exactly the maximum nash welfare allocations.   \\emph{price curves} allow the cost of a good to be any (increasing) function of the quantity purchased. we show that price curve equilibria are not limited to maximum nash welfare allocations with two main results. first, we show that an allocation can be supported by strictly increasing price curves if and only if it is \\emph{group-domination-free}. a similarly characterization holds for weakly increasing price curves. we use this to show that given any allocation, we can compute strictly (or weakly) increasing price curves that support it (or show that none exist) in polynomial time. these results involve a connection to the \\emph{agent-order matrix} of an allocation, which may have other applications. second, we use duality to show that in the bandwidth allocation setting, any allocation maximizing a ces welfare function can be supported by price curves.", "categories": "cs.gt econ.th", "created": "2018-07-13", "updated": "2019-12-23", "authors": ["ashish goel", "reyna hulett", "benjamin plaut"], "url": "https://arxiv.org/abs/1807.05293"}, {"title": "nearly optimal pricing algorithms for production constrained and laminar   bayesian selection", "id": "1807.05477", "abstract": "we study online pricing algorithms for the bayesian selection problem with production constraints and its generalization to the laminar matroid bayesian online selection problem. consider a firm producing (or receiving) multiple copies of different product types over time. the firm can offer the products to arriving buyers, where each buyer is interested in one product type and has a private valuation drawn independently from a possibly different but known distribution.   our goal is to find an adaptive pricing for serving the buyers that maximizes the expected social-welfare (or revenue) subject to two constraints. first, at any time the total number of sold items of each type is no more than the number of produced items. second, the total number of sold items does not exceed the total shipping capacity. this problem is a special case of the well-known matroid bayesian online selection problem studied in [kleinberg and weinberg, 2012], when the underlying matroid is laminar.   we give the first polynomial-time approximation scheme (ptas) for the above problem as well as its generalization to the laminar matroid bayesian online selection problem when the depth of the laminar family is bounded by a constant. our approach is based on rounding the solution of a hierarchy of linear programming relaxations that systematically strengthen the commonly used ex-ante linear programming formulation of these problems and approximate the optimum online solution with any degree of accuracy. our rounding algorithm respects the relaxed constraints of higher-levels of the laminar tree only in expectation, and exploits the negative dependency of the selection rule of lower-levels to achieve the required concentration that guarantees the feasibility with high probability.", "categories": "cs.gt cs.ds econ.th", "created": "2018-07-14", "updated": "", "authors": ["nima anari", "rad niazadeh", "amin saberi", "ali shameli"], "url": "https://arxiv.org/abs/1807.05477"}, {"title": "a simple and efficient estimation of the average treatment effect in the   presence of unmeasured confounders", "id": "1807.05678", "abstract": "wang and tchetgen tchetgen (2017) studied identification and estimation of the average treatment effect when some confounders are unmeasured. under their identification condition, they showed that the semiparametric efficient influence function depends on five unknown functionals. they proposed to parameterize all functionals and estimate the average treatment effect from the efficient influence function by replacing the unknown functionals with estimated functionals. they established that their estimator is consistent when certain functionals are correctly specified and attains the semiparametric efficiency bound when all functionals are correctly specified. in applications, it is likely that those functionals could all be misspecified. consequently their estimator could be inconsistent or consistent but not efficient. this paper presents an alternative estimator that does not require parameterization of any of the functionals. we establish that the proposed estimator is always consistent and always attains the semiparametric efficiency bound. a simple and intuitive estimator of the asymptotic variance is presented, and a small scale simulation study reveals that the proposed estimation outperforms the existing alternatives in finite samples.", "categories": "econ.em", "created": "2018-07-16", "updated": "", "authors": ["chunrong ai", "lukang huang", "zheng zhang"], "url": "https://arxiv.org/abs/1807.05678"}, {"title": "consumption smoothing in the working-class households of interwar japan", "id": "1807.05737", "abstract": "i analyze osaka factory worker households in the early 1920s, whether idiosyncratic income shocks were shared efficiently, and which consumption categories were robust to shocks. while the null hypothesis of full risk-sharing of total expenditures was rejected, factory workers maintained their households, in that they paid for essential expenditures (rent, utilities, and commutation) during economic hardship. additionally, children's education expenditures were possibly robust to idiosyncratic income shocks. the results suggest that temporary income is statistically significantly increased if disposable income drops due to idiosyncratic shocks. historical documents suggest microfinancial lending and saving institutions helped mitigate risk-based vulnerabilities.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2018-07-16", "updated": "2020-06-27", "authors": ["kota ogasawara"], "url": "https://arxiv.org/abs/1807.05737"}, {"title": "a mathematical model for optimal decisions in a representative democracy", "id": "1807.06157", "abstract": "direct democracy is a special case of an ensemble of classifiers, where every person (classifier) votes on every issue. this fails when the average voter competence (classifier accuracy) falls below 50%, which can happen in noisy settings where voters have only limited information, or when there are multiple topics and the average voter competence may not be high enough for some topics. representative democracy, where voters choose representatives to vote, can be an elixir in both these situations. representative democracy is a specific way to improve the ensemble of classifiers. we introduce a mathematical model for studying representative democracy, in particular understanding the parameters of a representative democracy that gives maximum decision making capability. our main result states that under general and natural conditions,   1. representative democracy can make the correct decisions simultaneously for multiple noisy issues.   2. when the cost of voting is fixed, the optimal representative democracy requires that representatives are elected from constant sized groups: the number of representatives should be linear in the number of voters.   3. when the cost and benefit of voting are both polynomial, the optimal group size is close to linear in the number of voters. this work sets the mathematical foundation for studying the quality-quantity tradeoff in a representative democracy-type ensemble (fewer highly qualified representatives versus more less qualified representatives).", "categories": "cs.gt cs.dm econ.th", "created": "2018-07-16", "updated": "", "authors": ["malik magdon-ismail", "lirong xia"], "url": "https://arxiv.org/abs/1807.06157"}, {"title": "limit theorems for factor models", "id": "1807.06338", "abstract": "the paper establishes the central limit theorems and proposes how to perform valid inference in factor models. we consider a setting where many counties/regions/assets are observed for many time periods, and when estimation of a global parameter includes aggregation of a cross-section of heterogeneous micro-parameters estimated separately for each entity. the central limit theorem applies for quantities involving both cross-sectional and time series aggregation, as well as for quadratic forms in time-aggregated errors. the paper studies the conditions when one can consistently estimate the asymptotic variance, and proposes a bootstrap scheme for cases when one cannot. a small simulation study illustrates performance of the asymptotic and bootstrap procedures. the results are useful for making inferences in two-step estimation procedures related to factor models, as well as in other related contexts. our treatment avoids structural modeling of cross-sectional dependence but imposes time-series independence.", "categories": "econ.em math.st stat.th", "created": "2018-07-17", "updated": "2020-09-23", "authors": ["stanislav anatolyev", "anna mikusheva"], "url": "https://arxiv.org/abs/1807.06338"}, {"title": "pink work: same-sex marriage, employment and discrimination", "id": "1807.06698", "abstract": "this paper analyzes how the legalization of same-sex marriage in the u.s. affected gay and lesbian couples in the labor market. results from a difference-in-difference model show that both partners in same-sex couples were more likely to be employed, to have a full-time contract, and to work longer hours in states that legalized same-sex marriage. in line with a theoretical search model of discrimination, suggestive empirical evidence supports the hypothesis that marriage equality led to an improvement in employment outcomes among gays and lesbians and lower occupational segregation thanks to a decrease in discrimination towards sexual minorities.", "categories": "econ.em", "created": "2018-07-17", "updated": "", "authors": ["dario sansone"], "url": "https://arxiv.org/abs/1807.06698"}, {"title": "customer sharing in economic networks with costs", "id": "1807.06822", "abstract": "in an economic market, sellers, infomediaries and customers constitute an economic network. each seller has her own customer group and the seller's private customers are unobservable to other sellers. therefore, a seller can only sell commodities among her own customers unless other sellers or infomediaries share her sale information to their customer groups. however, a seller is not incentivized to share others' sale information by default, which leads to inefficient resource allocation and limited revenue for the sale. to tackle this problem, we develop a novel mechanism called customer sharing mechanism (csm) which incentivizes all sellers to share each other's sale information to their private customer groups. furthermore, csm also incentivizes all customers to truthfully participate in the sale. in the end, csm not only allocates the commodities efficiently but also optimizes the seller's revenue.", "categories": "cs.gt cs.ai cs.ma cs.si econ.th", "created": "2018-07-18", "updated": "", "authors": ["bin li", "dong hao", "dengji zhao", "tao zhou"], "url": "https://arxiv.org/abs/1807.06822"}, {"title": "stochastic dominance under independent noise", "id": "1807.06927", "abstract": "stochastic dominance is a crucial tool for the analysis of choice under risk. it is typically analyzed as a property of two gambles that are taken in isolation. we study how additional independent sources of risk (e.g. uninsurable labor risk, house price risk, etc.) can affect the ordering of gambles. we show that, perhaps surprisingly, background risk can be strong enough to render lotteries that are ranked by their expectation ranked in terms of first-order stochastic dominance. we extend our results to second order stochastic dominance, and show how they lead to a novel, and elementary, axiomatization of mean-variance preferences.", "categories": "math.pr econ.th", "created": "2018-07-18", "updated": "2019-05-20", "authors": ["luciano pomatto", "philipp strack", "omer tamuz"], "url": "https://arxiv.org/abs/1807.06927"}, {"title": "quantile-regression inference with adaptive control of size", "id": "1807.06977", "abstract": "regression quantiles have asymptotic variances that depend on the conditional densities of the response variable given regressors. this paper develops a new estimate of the asymptotic variance of regression quantiles that leads any resulting wald-type test or confidence region to behave as well in large samples as its infeasible counterpart in which the true conditional response densities are embedded. we give explicit guidance on implementing the new variance estimator to control adaptively the size of any resulting wald-type test. monte carlo evidence indicates the potential of our approach to deliver powerful tests of heterogeneity of quantile treatment effects in covariates with good size performance over different quantile levels, data-generating processes and sample sizes. we also include an empirical example. supplementary material is available online.", "categories": "econ.em", "created": "2018-07-18", "updated": "2019-09-26", "authors": ["juan carlos escanciano", "chuan goh"], "url": "https://arxiv.org/abs/1807.06977"}, {"title": "cross validation based model selection via generalized method of moments", "id": "1807.06993", "abstract": "structural estimation is an important methodology in empirical economics, and a large class of structural models are estimated through the generalized method of moments (gmm). traditionally, selection of structural models has been performed based on model fit upon estimation, which take the entire observed samples. in this paper, we propose a model selection procedure based on cross-validation (cv), which utilizes sample-splitting technique to avoid issues such as over-fitting. while cv is widely used in machine learning communities, we are the first to prove its consistency in model selection in gmm framework. its empirical property is compared to existing methods by simulations of iv regressions and oligopoly market model. in addition, we propose the way to apply our method to mathematical programming of equilibrium constraint (mpec) approach. finally, we perform our method to online-retail sales data to compare dynamic market model to static model.", "categories": "econ.em stat.me", "created": "2018-07-18", "updated": "", "authors": ["junpei komiyama", "hajime shimao"], "url": "https://arxiv.org/abs/1807.06993"}, {"title": "a new index of human capital to predict economic growth", "id": "1807.07051", "abstract": "the accumulation of knowledge required to produce economic value is a process that often relates to nations economic growth. such a relationship, however, is misleading when the proxy of such accumulation is the average years of education. in this paper, we show that the predictive power of this proxy started to dwindle in 1990 when nations schooling began to homogenized. we propose a metric of human capital that is less sensitive than average years of education and remains as a significant predictor of economic growth when tested with both cross-section data and panel data. we argue that future research on economic growth will discard educational variables based on quantity as predictor given the thresholds that these variables are reaching.", "categories": "econ.em", "created": "2018-07-18", "updated": "", "authors": ["henry laverde", "juan c. correa", "klaus jaffe"], "url": "https://arxiv.org/abs/1807.07051"}, {"title": "take a look around: using street view and satellite images to estimate   house prices", "id": "1807.07155", "abstract": "when an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. some amenities, such as air quality, are measurable while others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. two issues have led to this neglect. not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective.   we show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. we propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in london, uk. we make use of traditional housing features such as age, size, and accessibility as well as visual features from google street view images and bing aerial images in estimating the house price model. we find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen london boroughs.   we explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract proxy variables for visual desirability of neighborhoods that are both of interest in their own right, and could be used as inputs to other econometric methods. this is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the visual appeal of london streets.", "categories": "econ.em cs.cv", "created": "2018-07-18", "updated": "2019-10-21", "authors": ["stephen law", "brooks paige", "chris russell"], "url": "https://arxiv.org/abs/1807.07155"}, {"title": "machine learning classifiers do not improve the prediction of academic   risk: evidence from australia", "id": "1807.07215", "abstract": "machine learning methods tend to outperform traditional statistical models at prediction. in the prediction of academic achievement, ml models have not shown substantial improvement over logistic regression. so far, these results have almost entirely focused on college achievement, due to the availability of administrative datasets, and have contained relatively small sample sizes by ml standards. in this article we apply popular machine learning models to a large dataset ($n=1.2$ million) containing primary and middle school performance on a standardized test given annually to australian students. we show that machine learning models do not outperform logistic regression for detecting students who will perform in the `below standard' band of achievement upon sitting their next test, even in a large-$n$ setting.", "categories": "stat.ml cs.lg econ.em", "created": "2018-07-18", "updated": "2020-01-28", "authors": ["sarah cornell-farrow", "robert garrard"], "url": "https://arxiv.org/abs/1807.07215"}, {"title": "stability in emu", "id": "1807.07730", "abstract": "the public debt and deficit ceilings of the maastricht treaty are the subject of recurring controversy. first, there is debate about the role and impact of these criteria in the initial phase of the introduction of the single currency. secondly, it must be specified how these will then be applied, in a permanent regime, when the single currency is well established.", "categories": "econ.em", "created": "2018-07-20", "updated": "", "authors": ["theo peeters"], "url": "https://arxiv.org/abs/1807.07730"}, {"title": "self-regulation promotes cooperation in social networks", "id": "1807.07848", "abstract": "cooperative behavior in real social dilemmas is often perceived as a phenomenon emerging from norms and punishment. to overcome this paradigm, we highlight the interplay between the influence of social networks on individuals, and the activation of spontaneous self-regulating mechanisms, which may lead them to behave cooperatively, while interacting with others and taking conflicting decisions over time. by extending evolutionary game theory over networks, we prove that cooperation partially or fully emerges whether self-regulating mechanisms are sufficiently stronger than social pressure. interestingly, even few cooperative individuals act as catalyzing agents for the cooperation of others, thus activating a recruiting mechanism, eventually driving the whole population to cooperate.", "categories": "physics.soc-ph cs.gt cs.si econ.th", "created": "2018-07-20", "updated": "", "authors": ["dario madeo", "chiara mocenni"], "url": "https://arxiv.org/abs/1807.07848"}, {"title": "asymptotic results under multiway clustering", "id": "1807.07925", "abstract": "if multiway cluster-robust standard errors are used routinely in applied economics, surprisingly few theoretical results justify this practice. this paper aims to fill this gap. we first prove, under nearly the same conditions as with i.i.d. data, the weak convergence of empirical processes under multiway clustering. this result implies central limit theorems for sample averages but is also key for showing the asymptotic normality of nonlinear estimators such as gmm estimators. we then establish consistency of various asymptotic variance estimators, including that of cameron et al. (2011) but also a new estimator that is positive by construction. next, we show the general consistency, for linear and nonlinear estimators, of the pigeonhole bootstrap, a resampling scheme adapted to multiway clustering. monte carlo simulations suggest that inference based on our two preferred methods may be accurate even with very few clusters, and significantly improve upon inference based on cameron et al. (2011).", "categories": "econ.em", "created": "2018-07-20", "updated": "2018-08-03", "authors": ["laurent davezies", "xavier d'haultfoeuille", "yannick guyonvarch"], "url": "https://arxiv.org/abs/1807.07925"}, {"title": "emu and ecb conflicts", "id": "1807.08097", "abstract": "in dynamical framework the conflict between government and the central bank according to the exchange rate of payment of fixed rates and fixed rates of fixed income (emu) convergence criteria such that the public debt / gdp ratio the method consists of calculating private public debt management in a public debt management system purpose there is no mechanism to allow naturally for this adjustment.", "categories": "econ.em", "created": "2018-07-21", "updated": "", "authors": ["william mackenzie"], "url": "https://arxiv.org/abs/1807.08097"}, {"title": "score permutation based finite sample inference for generalized   autoregressive conditional heteroskedasticity (garch) models", "id": "1807.08390", "abstract": "a standard model of (conditional) heteroscedasticity, i.e., the phenomenon that the variance of a process changes over time, is the generalized autoregressive conditional heteroskedasticity (garch) model, which is especially important for economics and finance. garch models are typically estimated by the quasi-maximum likelihood (qml) method, which works under mild statistical assumptions. here, we suggest a finite sample approach, called scope, to construct distribution-free confidence regions around the qml estimate, which have exact coverage probabilities, despite no additional assumptions about moments are made. scope is inspired by the recently developed sign-perturbed sums (sps) method, which however cannot be applied in the garch case. scope works by perturbing the score function using randomly permuted residuals. this produces alternative samples which lead to exact confidence regions. experiments on simulated and stock market data are also presented, and scope is compared with the asymptotic theory and bootstrap approaches.", "categories": "stat.me cs.lg econ.em math.ds q-fin.st", "created": "2018-07-22", "updated": "", "authors": ["bal\u00e1zs csan\u00e1d cs\u00e1ji"], "url": "https://arxiv.org/abs/1807.08390"}, {"title": "an impossibility theorem for wealth in heterogeneous-agent models with   limited heterogeneity", "id": "1807.08404", "abstract": "it has been conjectured that canonical bewley--huggett--aiyagari heterogeneous-agent models cannot explain the joint distribution of income and wealth. the results stated below verify this conjecture and clarify its implications under very general conditions. we show in particular that if (i) agents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have constant discount factors, then the wealth distribution inherits the tail behavior of income shocks (e.g., light-tailedness or the pareto exponent). our restrictions on utility require only that relative risk aversion is bounded, and a large variety of income processes are admitted. our results show conclusively that it is necessary to go beyond standard models to explain the empirical fact that wealth is heavier-tailed than income. we demonstrate through examples that relaxing any of the above three conditions can generate pareto tails.", "categories": "econ.gn q-fin.ec", "created": "2018-07-22", "updated": "2019-01-25", "authors": ["john stachurski", "alexis akira toda"], "url": "https://arxiv.org/abs/1807.08404"}, {"title": "artificial increasing returns to scale and the problem of sampling from   lognormals", "id": "1807.09424", "abstract": "we show how increasing returns to scale in urban scaling can artificially emerge, systematically and predictably, without any sorting or positive externalities. we employ a model where individual productivities are independent and identically distributed lognormal random variables across all cities. we use extreme value theory to demonstrate analytically the paradoxical emergence of increasing returns to scale when the variance of log-productivity is larger than twice the log-size of the population size of the smallest city in a cross-sectional regression. our contributions are to derive an analytical prediction for the artificial scaling exponent arising from this mechanism and to develop a simple statistical test to try to tell whether a given estimate is real or an artifact. our analytical results are validated analyzing simulations and real microdata of wages across municipalities in colombia. we show how an artificial scaling exponent emerges in the colombian data when the sizes of random samples of workers per municipality are $1\\%$ or less of their total size.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2018-07-24", "updated": "2020-07-15", "authors": ["andres gomez-lievano", "vladislav vysotsky", "jose lobo"], "url": "https://arxiv.org/abs/1807.09424"}, {"title": "cap and monetary policy", "id": "1807.09475", "abstract": "despite the importance of cap-related agricultural market regulation mechanisms within europe, the agricultural sectors in european countries retain a degree of sensitivity to macroeconomic activity and policies. this reality now raises the question of the effects to be expected from the implementation of the single monetary policy on these agricultural sectors within the monetary union.", "categories": "econ.gn q-fin.ec", "created": "2018-07-25", "updated": "", "authors": ["carl duisberg"], "url": "https://arxiv.org/abs/1807.09475"}, {"title": "apologia pro vita sua: the vanishing of the white whale in the mists", "id": "1807.09577", "abstract": "there are many analogies among fortune hunting in business, politics, and science. the prime task of the gold digger was to go to the klondikes, find the right mine and mine the richest veins. this task requires motivation, sense of purpose and ability. techniques and equipment must be developed. fortune hunting in new england was provided at one time by hunting for whales. one went to a great whalers' station such as new bedford and joined the whale hunters. the hunt in academic research is similar. a single-minded passion is called for. these notes here are the wrap-up comments containing some terminal observations of mine on a hunt for a theory money and financial institutions.", "categories": "econ.gn q-fin.ec", "created": "2018-07-25", "updated": "", "authors": ["martin shubik"], "url": "https://arxiv.org/abs/1807.09577"}, {"title": "towards equation of state for a market: a thermodynamical paradigm of   economics", "id": "1807.09595", "abstract": "foundations of equilibrium thermodynamics are the equation of state (eos) and four postulated laws of thermodynamics. we use equilibrium thermodynamics paradigms in constructing the eos for microeconomics system that is a market. this speculation is hoped to be first step towards whole pictures of thermodynamical paradigm of economics.", "categories": "econ.gn physics.class-ph q-fin.ec", "created": "2018-07-22", "updated": "2018-07-26", "authors": ["burin gumjudpai"], "url": "https://arxiv.org/abs/1807.09595"}, {"title": "hospitality students' perceptions towards working in hotels: a case   study of the faculty of tourism and hotels in alexandria university", "id": "1807.09660", "abstract": "the tourism and hospitality industry worldwide has been confronted with the problem of attracting and retaining quality employees. if today's students are to become the effective practitioners of tomorrow, it is fundamental to understand their perceptions of tourism employment. therefore, this research aims at investigating the perceptions of hospitality students at the faculty of tourism in alexandria university towards the industry as a career choice. a self-administrated questionnaire was developed to rate the importance of 20 factors in influencing career choice, and the extent to which hospitality as a career offers these factors. from the results, it is clear that students generally do not believe that the hospitality career will offer them the factors they found important. however, most of respondents (70.6%) indicated that they would work in the industry after graduation. finally, a set of specific remedial actions that hospitality stakeholders could initiate to improve the perceptions of hospitality career are discussed.", "categories": "econ.gn q-fin.ec", "created": "2018-07-22", "updated": "", "authors": ["sayed el-houshy"], "url": "https://arxiv.org/abs/1807.09660"}, {"title": "two-step estimation and inference with possibly many included covariates", "id": "1807.10100", "abstract": "we study the implications of including many covariates in a first-step estimate entering a two-step estimation procedure. we find that a first order bias emerges when the number of \\textit{included} covariates is \"large\" relative to the square-root of sample size, rendering standard inference procedures invalid. we show that the jackknife is able to estimate this \"many covariates\" bias consistently, thereby delivering a new automatic bias-corrected two-step point estimator. the jackknife also consistently estimates the standard error of the original two-step point estimator. for inference, we develop a valid post-bias-correction bootstrap approximation that accounts for the additional variability introduced by the jackknife bias-correction. we find that the jackknife bias-corrected point estimator and the bootstrap post-bias-correction inference perform excellent in simulations, offering important improvements over conventional two-step point estimators and inference procedures, which are not robust to including many covariates. we apply our results to an array of distinct treatment effect, policy evaluation, and other applied microeconomics settings. in particular, we discuss production function and marginal treatment effect estimation in detail.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-07-26", "updated": "", "authors": ["matias d. cattaneo", "michael jansson", "xinwei ma"], "url": "https://arxiv.org/abs/1807.10100"}, {"title": "a new and stable estimation method of country economic fitness and   product complexity", "id": "1807.10276", "abstract": "we present a new metric estimating fitness of countries and complexity of products by exploiting a non-linear non-homogeneous map applied to the publicly available information on the goods exported by a country. the non homogeneous terms guarantee both convergence and stability. after a suitable rescaling of the relevant quantities, the non homogeneous terms are eventually set to zero so that this new metric is parameter free. this new map almost reproduces the results of the original homogeneous metrics already defined in literature and allows for an approximate analytic solution in case of actual binarized matrices based on the revealed comparative advantage (rca) indicator. this solution is connected with a new quantity describing the neighborhood of nodes in bipartite graphs, representing in this work the relations between countries and exported products. moreover, we define the new indicator of country net-efficiency quantifying how a country efficiently invests in capabilities able to generate innovative complex high quality products. eventually, we demonstrate analytically the local convergence of the algorithm involved.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2018-07-23", "updated": "2018-10-12", "authors": ["vito d. p. servedio", "paolo butt\u00e0", "dario mazzilli", "andrea tacchella", "luciano pietronero"], "url": "https://arxiv.org/abs/1807.10276"}, {"title": "a maximum entropy network reconstruction of macroeconomic models", "id": "1807.10464", "abstract": "in this article the problem of reconstructing the pattern of connection between agents from partial empirical data in a macro-economic model is addressed, given a set of behavioral equations. this systemic point of view puts the focus on distributional and network effects, rather than time-dependence. using the theory of complex networks we compare several models to reconstruct both the topology and the flows of money of the different types of monetary transactions, while imposing a series of constraints related to national accounts, and to empirical network sparsity. some properties of reconstructed networks are compared with their empirical counterpart.", "categories": "econ.gn cs.cc physics.data-an physics.soc-ph q-fin.ec", "created": "2018-07-27", "updated": "2018-12-07", "authors": ["aur\u00e9lien hazan"], "url": "https://arxiv.org/abs/1807.10464"}, {"title": "investigating wheat price with a multi-agent model", "id": "1807.10537", "abstract": "in this paper, we build a computational model for the analysis of international wheat spot price formation, its dynamics and the dynamics of internationally exchanged quantities. the model has been calibrated using faostat data to evaluate its in-sample predictive power. the model is able to generate wheat prices in twelve international markets and wheat used quantities in twenty-four world regions. the time span considered goes from 1992 to 2013. in our study, a particular attention was paid to the impact of russian federation's 2010 grain export ban on wheat price and internationally traded quantities. among other results, we find that wheat average weighted world price in 2013 would have been 3.55\\% lower than the observed one if the russian federation would not have imposed the export ban in 2010.", "categories": "econ.th", "created": "2018-07-27", "updated": "", "authors": ["gianfranco giulioni", "edmondo di giuseppe", "massimiliano pasqui", "piero toscano", "francesco miglietta"], "url": "https://arxiv.org/abs/1807.10537"}, {"title": "exceeding expectations: stochastic dominance as a general decision   theory", "id": "1807.10895", "abstract": "the principle that rational agents should maximize expected utility or choiceworthiness is intuitively plausible in many ordinary cases of decision-making under uncertainty. but it is less plausible in cases of extreme, low-probability risk (like pascal's mugging), and intolerably paradoxical in cases like the st. petersburg and pasadena games. in this paper i show that, under certain conditions, stochastic dominance reasoning can capture most of the plausible implications of expectational reasoning while avoiding most of its pitfalls. specifically, given sufficient background uncertainty about the choiceworthiness of one's options, many expectation-maximizing gambles that do not stochastically dominate their alternatives \"in a vacuum\" become stochastically dominant in virtue of that background uncertainty. but, even under these conditions, stochastic dominance will not require agents to accept options whose expectational superiority depends on sufficiently small probabilities of extreme payoffs. the sort of background uncertainty on which these results depend looks unavoidable for any agent who measures the choiceworthiness of her options in part by the total amount of value in the resulting world. at least for such agents, then, stochastic dominance offers a plausible general principle of choice under uncertainty that can explain more of the apparent rational constraints on such choices than has previously been recognized.", "categories": "econ.th", "created": "2018-07-28", "updated": "2020-08-08", "authors": ["christian tarsney"], "url": "https://arxiv.org/abs/1807.10895"}, {"title": "banking stability system: does it matter if the rate of return is fixed   or stochastic?", "id": "1807.11102", "abstract": "the purpose is to compare the perfect stochastic return (sr) model like islamic banks to the fixed return (fr) model as in conventional banks by measuring up their impacts at the macroeconomic level. we prove that if the optimal choice of investor share in sr model {\\alpha}* realizes the indifference of the financial institution toward sr and fr models, there exists {\\alpha} less than {\\alpha}* such that the banks strictly prefers the sr model. also, there exists {\\alpha}, {\\gamma} and {\\lambda} verifying the conditions of {\\alpha}-sharing such that each party in economy can be better under the sr model and the economic welfare could be improved in a pareto-efficient way.", "categories": "econ.th", "created": "2018-07-29", "updated": "", "authors": ["hassan ghassan"], "url": "https://arxiv.org/abs/1807.11102"}, {"title": "local linear forests", "id": "1807.11408", "abstract": "random forests are a powerful method for non-parametric regression, but are limited in their ability to fit smooth signals, and can show poor predictive performance in the presence of strong, smooth effects. taking the perspective of random forests as an adaptive kernel method, we pair the forest kernel with a local linear regression adjustment to better capture smoothness. the resulting procedure, local linear forests, enables us to improve on asymptotic rates of convergence for random forests with smooth signals, and provides substantial gains in accuracy on both real and simulated data. we prove a central limit theorem valid under regularity conditions on the forest and smoothness constraints, and propose a computationally efficient construction for confidence intervals. moving to a causal inference application, we discuss the merits of local regression adjustments for heterogeneous treatment effect estimation, and give an example on a dataset exploring the effect word choice has on attitudes to the social safety net. last, we include simulation results on real and generated data.", "categories": "stat.ml cs.lg econ.em math.st stat.th", "created": "2018-07-30", "updated": "2020-09-04", "authors": ["rina friedberg", "julie tibshirani", "susan athey", "stefan wager"], "url": "https://arxiv.org/abs/1807.11408"}, {"title": "polarization under rising inequality and economic decline", "id": "1807.11477", "abstract": "social and political polarization is a significant source of conflict and poor governance in many societies. thus, understanding its causes has become a priority of scholars across many disciplines. here we demonstrate that shifts in socialization strategies analogous to political polarization and identity politics can arise as a locally-beneficial response to both rising wealth inequality and economic decline. adopting a perspective of cultural evolution, we develop a framework to study the emergence of polarization under shifting economic environments. in many contexts, interacting with diverse out-groups confers benefits from innovation and exploration greater than those that arise from interacting exclusively with a homogeneous in-group. however, when the economic environment favors risk-aversion, a strategy of seeking low-risk interactions can be important to maintaining individual solvency. to capture this dynamic, we assume that in-group interactions have a lower expected outcome, but a more certain one. thus in-group interactions are less risky than out-group interactions. our model shows that under conditions of economic decline or increasing wealth inequality, some members of the population benefit from adopting a risk-averse, in-group favoring strategy. moreover, we show that such in-group polarization can spread rapidly to the whole population and persist even when the conditions that produced it have reversed. finally we offer empirical support for the role of income inequality as a driver of affective polarization in the united states, mirroring findings on a panel of developed democracies. our work provides a framework for studying how disparate forces interplay, via cultural evolution, to shape patterns of identity, and unifies what are often seen as conflicting explanations for political polarization: identity threat versus economic anxiety.", "categories": "econ.gn physics.soc-ph q-bio.pe q-fin.ec", "created": "2018-07-28", "updated": "2020-04-24", "authors": ["alexander j. stewart", "nolan mccarty", "joanna j. bryson"], "url": "https://arxiv.org/abs/1807.11477"}, {"title": "preference identification", "id": "1807.11585", "abstract": "an experimenter seeks to learn a subject's preference relation. the experimenter produces pairs of alternatives. for each pair, the subject is asked to choose. we argue that, in general, large but finite data do not give close approximations of the subject's preference, even when the limiting (countably infinite) data are enough to infer the preference perfectly. we provide sufficient conditions on the set of alternatives, preferences, and sequences of pairs so that the observation of finitely many choices allows the experimenter to learn the subject's preference with arbitrary precision. while preferences can be identified under our sufficient conditions, we show that it is harder to identify utility functions. we illustrate our results with several examples, including consumer choice, expected utility, and preferences in the anscombe-aumann model.", "categories": "econ.th", "created": "2018-07-30", "updated": "", "authors": ["christopher p. chambers", "federico echenique", "nicolas s. lambert"], "url": "https://arxiv.org/abs/1807.11585"}, {"title": "a critique of the econometrics of happiness: are we underestimating the   returns to education and income?", "id": "1807.11835", "abstract": "a large \"happiness\", or life satisfaction, literature in economics makes use of likert-like scales in assessing survey respondents' cognitive evaluations of their lives. these measures are being used to estimate economic benefits in every empirical field of economics. typically, analysis of these data have shown remarkably low direct returns of education for improving subjective well-being. in addition, arguably, the inferred impact of material wealth and income using this method is also unexpectedly low as compared with other, social factors, and as compared with economists' prior expectations which underlie, in some sense, support for using gdp as a proxy for more general quality of life goals. discrete response scales used ubiquitously for the reporting of life satisfaction pose cognitive challenges to survey respondents, so differing cognitive abilities result in different uses of the scale, and thus potential bias in statistical inference. this problem has so far gone unnoticed. an overlooked feature of the distribution of responses to life satisfaction questions is that they exhibit certain enhancements at focal values, in particular at 0, 5, and 10 on the eleven-point scale. in this paper, i investigate the reasons for, and implications of, these response patterns. i use a model to account for the focal-value behavior using a latent variable approach to capture the \"internal\" cognitive evaluation before it is translated to the discrete scale of a survey question. this approach, supported by other more heuristic ones, finds a significant upward correction for the effects of both education and income on life satisfaction.", "categories": "econ.em econ.gn q-fin.ec", "created": "2018-07-31", "updated": "", "authors": ["christopher p barrington-leigh"], "url": "https://arxiv.org/abs/1807.11835"}, {"title": "on the unbiased asymptotic normality of quantile regression with fixed   effects", "id": "1807.11863", "abstract": "nonlinear panel data models with fixed individual effects provide an important set of tools for describing microeconometric data. in a large class of such models (including probit, proportional hazard and quantile regression to name just a few) it is impossible to difference out individual effects, and inference is usually justified in a `large n large t' asymptotic framework. however, there is a considerable gap in the type of assumptions that are currently imposed in models with smooth score functions (such as probit, and proportional hazard) and quantile regression. in the present paper we show that this gap can be bridged and establish asymptotic unbiased normality for quantile regression panels under conditions on n,t that are very close to what is typically assumed in standard nonlinear panels. our results considerably improve upon existing theory and show that quantile regression is applicable to the same type of panel data (in terms of n,t) as other commonly used nonlinear panel data models. thorough numerical experiments confirm our theoretical findings.", "categories": "econ.em math.st stat.th", "created": "2018-07-31", "updated": "2020-02-06", "authors": ["antonio f. galvao", "jiaying gu", "stanislav volgushev"], "url": "https://arxiv.org/abs/1807.11863"}, {"title": "strictly strategy-proof auctions", "id": "1807.11864", "abstract": "a strictly strategy-proof mechanism is one that asks agents to use strictly dominant strategies. in the canonical one-dimensional mechanism design setting with private values, we show that strict strategy-proofness is equivalent to strict monotonicity plus the envelope formula, echoing a well-known characterisation of (weak) strategy-proofness. a consequence is that strategy-proofness can be made strict by an arbitrarily small modification, so that strictness is 'essentially for free'.", "categories": "econ.th", "created": "2018-07-31", "updated": "2020-07-25", "authors": ["matteo escud\u00e9", "ludvig sinander"], "url": "https://arxiv.org/abs/1807.11864"}, {"title": "a theory of dichotomous valuation with applications to variable   selection", "id": "1808.00131", "abstract": "an econometric or statistical model may undergo a marginal gain if we admit a new variable to the model, and a marginal loss if we remove an existing variable from the model. assuming equality of opportunity among all candidate variables, we derive a valuation framework by the expected marginal gain and marginal loss in all potential modeling scenarios. however, marginal gain and loss are not symmetric; thus, we introduce three unbiased solutions. when used in variable selection, our new approaches significantly outperform several popular methods used in practice. the results also explore some novel traits of the shapley value.", "categories": "stat.ml cs.lg econ.em math.oc math.st stat.th", "created": "2018-07-31", "updated": "2020-03-13", "authors": ["xingwei hu"], "url": "https://arxiv.org/abs/1808.00131"}, {"title": "mapping the privacy-utility tradeoff in mobile phone data for   development", "id": "1808.00160", "abstract": "today's age of data holds high potential to enhance the way we pursue and monitor progress in the fields of development and humanitarian action. we study the relation between data utility and privacy risk in large-scale behavioral data, focusing on mobile phone metadata as paradigmatic domain. to measure utility, we survey experts about the value of mobile phone metadata at various spatial and temporal granularity levels. to measure privacy, we propose a formal and intuitive measure of reidentification risk$\\unicode{x2014}$the information ratio$\\unicode{x2014}$and compute it at each granularity level. our results confirm the existence of a stark tradeoff between data utility and reidentifiability, where the most valuable datasets are also most prone to reidentification. when data is specified at zip-code and hourly levels, outside knowledge of only 7% of a person's data suffices for reidentification and retrieval of the remaining 93%. in contrast, in the least valuable dataset, specified at municipality and daily levels, reidentification requires on average outside knowledge of 51%, or 31 data points, of a person's data to retrieve the remaining 49%. overall, our findings show that coarsening data directly erodes its value, and highlight the need for using data-coarsening, not as stand-alone mechanism, but in combination with data-sharing models that provide adjustable degrees of accountability and security.", "categories": "cs.cy cs.cr econ.gn q-fin.ec", "created": "2018-08-01", "updated": "", "authors": ["alejandro noriega-campero", "alex rutherford", "oren lederman", "yves a. de montjoye", "alex pentland"], "url": "https://arxiv.org/abs/1808.00160"}, {"title": "dynamic random subjective expected utility", "id": "1808.00296", "abstract": "dynamic random subjective expected utility (dr-seu) allows to model choice data observed from an agent or a population of agents whose beliefs about objective payoff-relevant states and tastes can both evolve stochastically. our observable, the augmented stochastic choice function (ascf) allows, in contrast to previous work in decision theory, for a direct test of whether the agent's beliefs reflect the true data-generating process conditional on their private information as well as identification of the possibly incorrect beliefs. we give an axiomatic characterization of when an agent satisfies the model, both in a static as well as in a dynamic setting. we look at the case when the agent has correct beliefs about the evolution of objective states as well as at the case when her beliefs are incorrect but unforeseen contingencies are impossible.   we also distinguish two subvariants of the dynamic model which coincide in the static setting: evolving seu, where a sophisticated agent's utility evolves according to a bellman equation and gradual learning, where the agent is learning about her taste. we prove easy and natural comparative statics results on the degree of belief incorrectness as well as on the speed of learning about taste.   auxiliary results contained in the online appendix extend previous decision theory work in the menu choice and stochastic choice literature from a technical as well as a conceptual perspective.", "categories": "econ.th", "created": "2018-08-01", "updated": "", "authors": ["jetlir duraj"], "url": "https://arxiv.org/abs/1808.00296"}, {"title": "can network theory-based targeting increase technology adoption?", "id": "1808.01205", "abstract": "in order to induce farmers to adopt a productive new agricultural technology, we apply simple and complex contagion diffusion models on rich social network data from 200 villages in malawi to identify seed farmers to target and train on the new technology. a randomized controlled trial compares these theory-driven network targeting approaches to simpler strategies that either rely on a government extension worker or an easily measurable proxy for the social network (geographic distance between households) to identify seed farmers. our results indicate that technology diffusion is characterized by a complex contagion learning environment in which most farmers need to learn from multiple people before they adopt themselves. network theory based targeting can out-perform traditional approaches to extension, and we identify methods to realize these gains at low cost to policymakers.   keywords: social learning, agricultural technology adoption, complex contagion, malawi   jel classification codes: o16, o13", "categories": "econ.gn q-fin.ec", "created": "2018-08-03", "updated": "", "authors": ["lori beaman", "ariel benyishay", "jeremy magruder", "ahmed mushfiq mobarak"], "url": "https://arxiv.org/abs/1808.01205"}, {"title": "a characterization of \"phelpsian\" statistical discrimination", "id": "1808.01351", "abstract": "we establish that statistical discrimination is possible if and only if it is impossible to uniquely identify the signal structure observed by an employer from a realized empirical distribution of skills. the impossibility of statistical discrimination is shown to be equivalent to the existence of a fair, skill-dependent, remuneration for workers. finally, we connect the statistical discrimination literature to bayesian persuasion, establishing that if discrimination is absent, then the optimal signaling problem results in a linear payoff function (as well as a kind of converse).", "categories": "econ.th", "created": "2018-08-03", "updated": "", "authors": ["christopher p. chambers", "federico echenique"], "url": "https://arxiv.org/abs/1808.01351"}, {"title": "coverage error optimal confidence intervals for local polynomial   regression", "id": "1808.01398", "abstract": "we characterize the minimax bound on coverage error of wald-type confidence intervals for nonparametric local polynomial regression. this bound depends on the smoothness of the population regression function, the smoothness exploited by the inference procedure, and on whether the evaluation point of interest is in the interior or on the boundary of the support of the regression function. our results also cover inference on derivatives of the regression function, in which case we find that the minimax coverage error bound does not depend on the order of the derivative being estimated. we show that robust bias corrected confidence intervals are able to attain the minimax rate when coupled with the principled, inference-optimal tuning parameter selections we propose. in addition, we show how the large-sample interval length can be further optimized through choice of the kernel function and other tuning parameters. our main theoretical results rely on novel edgeworth expansions that are proven to hold uniformly over relevant classes of data generating processes. these higher-order expansions allow for the uniform kernel and any derivative order, improving on previous technical results available in the literature.", "categories": "econ.em math.st stat.th", "created": "2018-08-03", "updated": "2020-05-28", "authors": ["sebastian calonico", "matias d. cattaneo", "max h. farrell"], "url": "https://arxiv.org/abs/1808.01398"}, {"title": "robust pricing with refunds", "id": "1808.02233", "abstract": "before purchase, a buyer of an experience good learns about the product's fit using various information sources, including some of which the seller may be unaware of. the buyer, however, can conclusively learn the fit only after purchasing and trying out the product. we show that the seller can use a simple mechanism to best take advantage of the buyer's post-purchase learning to maximize his guaranteed-profit. we show that this mechanism combines a generous refund, which performs well when the buyer is relatively informed, with non-refundable random discounts, which work well when the buyer is relatively uninformed.", "categories": "cs.gt econ.th", "created": "2018-08-07", "updated": "2020-05-20", "authors": ["toomas hinnosaar", "keiichi kawai"], "url": "https://arxiv.org/abs/1808.02233"}, {"title": "machine learning for dynamic discrete choice", "id": "1808.02569", "abstract": "dynamic discrete choice models often discretize the state vector and restrict its dimension in order to achieve valid inference. i propose a novel two-stage estimator for the set-identified structural parameter that incorporates a high-dimensional state space into the dynamic model of imperfect competition. in the first stage, i estimate the state variable's law of motion and the equilibrium policy function using machine learning tools. in the second stage, i plug the first-stage estimates into a moment inequality and solve for the structural parameter. the moment function is presented as the sum of two components, where the first one expresses the equilibrium assumption and the second one is a bias correction term that makes the sum insensitive (i.e., orthogonal) to first-stage bias. the proposed estimator uniformly converges at the root-n rate and i use it to construct confidence regions. the results developed here can be used to incorporate high-dimensional state space into classic dynamic discrete choice models, for example, those considered in rust (1987), bajari et al. (2007), and scott (2013).", "categories": "econ.em", "created": "2018-08-07", "updated": "2018-11-05", "authors": ["vira semenova"], "url": "https://arxiv.org/abs/1808.02569"}, {"title": "lattice studies of gerrymandering strategies", "id": "1808.02826", "abstract": "we propose three novel gerrymandering algorithms which incorporate the spatial distribution of voters with the aim of constructing gerrymandered, equal-population, connected districts. moreover, we develop lattice models of voter distributions, based on analogies to electrostatic potentials, in order to compare different gerrymandering strategies. due to the probabilistic population fluctuations inherent to our voter models, monte carlo methods can be applied to the districts constructed via our gerrymandering algorithms. through monte carlo studies we quantify the effectiveness of each of our gerrymandering algorithms and we also argue that gerrymandering strategies which do not include spatial data lead to (legally prohibited) highly disconnected districts. of the three algorithms we propose, two are based on different strategies for packing opposition voters, and the third is a new approach to algorithmic gerrymandering based on genetic algorithms, which automatically guarantees that all districts are connected. furthermore, we use our lattice voter model to examine the effectiveness of isoperimetric quotient tests and our results provide further quantitative support for implementing compactness tests in real-world political redistricting.", "categories": "physics.soc-ph cs.cy econ.gn q-fin.ec", "created": "2018-08-08", "updated": "", "authors": ["kyle gatesman", "james unwin"], "url": "https://arxiv.org/abs/1808.02826"}, {"title": "information content of dsge forecasts", "id": "1808.02910", "abstract": "this paper examines the question whether information is contained in forecasts from dsge models beyond that contained in lagged values, which are extensively used in the models. four sets of forecasts are examined. the results are encouraging for dsge forecasts of real gdp. the results suggest that there is information in the dsge forecasts not contained in forecasts based only on lagged values and that there is no information in the lagged-value forecasts not contained in the dsge forecasts. the opposite is true for forecasts of the gdp deflator.   keywords: dsge forecasts, lagged values   jel classification codes: e10, e17, c53", "categories": "econ.gn q-fin.ec", "created": "2018-08-08", "updated": "", "authors": ["ray fair"], "url": "https://arxiv.org/abs/1808.02910"}, {"title": "network-based referral mechanism in a crowdfunding-based marketing   pattern", "id": "1808.03070", "abstract": "crowdfunding is gradually becoming a modern marketing pattern. by noting that the success of crowdfunding depends on network externalities, our research aims to utilize them to provide an applicable referral mechanism in a crowdfunding-based marketing pattern. in the context of network externalities, measuring the value of leading customers is chosen as the key to coping with the research problem by considering that leading customers take a critical stance in forming a referral network. accordingly, two sequential-move game models (i.e., basic model and extended model) were established to measure the value of leading customers, and a skill of matrix transformation was adopted to solve the model by transforming a complicated multi-sequence game into a simple simultaneous-move game. based on the defined value of leading customers, a network-based referral mechanism was proposed by exploring exactly how many awards are allocated along the customer sequence to encourage the leading customers' actions of successful recommendation and by demonstrating two general rules of awarding the referrals in our model setting. moreover, the proposed solution approach helps deepen an understanding of the effect of the leading position, which is meaningful for designing more numerous referral approaches.", "categories": "econ.th econ.gn math.oc q-fin.ec", "created": "2018-08-09", "updated": "", "authors": ["yongli li", "zhi-ping fan", "wei zhang"], "url": "https://arxiv.org/abs/1808.03070"}, {"title": "change point estimation in panel data with time-varying individual   effects", "id": "1808.03109", "abstract": "this paper proposes a method for estimating multiple change points in panel data models with unobserved individual effects via ordinary least-squares (ols). typically, in this setting, the ols slope estimators are inconsistent due to the unobserved individual effects bias. as a consequence, existing methods remove the individual effects before change point estimation through data transformations such as first-differencing. we prove that under reasonable assumptions, the unobserved individual effects bias has no impact on the consistent estimation of change points. our simulations show that since our method does not remove any variation in the dataset before change point estimation, it performs better in small samples compared to first-differencing methods. we focus on short panels because they are commonly used in practice, and allow for the unobserved individual effects to vary over time. our method is illustrated via two applications: the environmental kuznets curve and the u.s. house price expectations after the financial crisis.", "categories": "econ.em stat.ap", "created": "2018-08-09", "updated": "", "authors": ["otilia boldea", "bettina drepper", "zhuojiong gan"], "url": "https://arxiv.org/abs/1808.03109"}, {"title": "existence of equilibrium prices: a pedagogical proof", "id": "1808.03129", "abstract": "under the same assumptions made by mas-colell et al. (1995), i develop a short, simple, and complete proof of existence of equilibrium prices based on excess demand functions. the result is obtained by applying the brouwer fixed point theorem to a trimmed simplex which does not contain prices equal to zero. the mathematical techniques are based on some results obtained in neuefeind (1980) and geanakoplos (2003).", "categories": "econ.th", "created": "2018-08-09", "updated": "2018-09-23", "authors": ["simone tonin"], "url": "https://arxiv.org/abs/1808.03129"}, {"title": "a panel quantile approach to attrition bias in big data: evidence from a   randomized experiment", "id": "1808.03364", "abstract": "this paper introduces a quantile regression estimator for panel data models with individual heterogeneity and attrition. the method is motivated by the fact that attrition bias is often encountered in big data applications. for example, many users sign-up for the latest program but few remain active users several months later, making the evaluation of such interventions inherently very challenging. building on earlier work by hausman and wise (1979), we provide a simple identification strategy that leads to a two-step estimation procedure. in the first step, the coefficients of interest in the selection equation are consistently estimated using parametric or nonparametric methods. in the second step, standard panel quantile methods are employed on a subset of weighted observations. the estimator is computationally easy to implement in big data applications with a large number of subjects. we investigate the conditions under which the parameter estimator is asymptotically gaussian and we carry out a series of monte carlo simulations to investigate the finite sample properties of the estimator. lastly, using a simulation exercise, we apply the method to the evaluation of a recent time-of-day electricity pricing experiment inspired by the work of aigner and hausman (1980).", "categories": "econ.em cs.lg stat.ap stat.me", "created": "2018-08-09", "updated": "", "authors": ["matthew harding", "carlos lamarche"], "url": "https://arxiv.org/abs/1808.03364"}, {"title": "hysteresis of economic networks in an xy model", "id": "1808.03404", "abstract": "many-body systems can have multiple equilibria. though the energy of equilibria might be the same, still systems may resist to switch from an unfavored equilibrium to a favored one. in this paper we investigate occurrence of such phenomenon in economic networks. in times of crisis when governments intend to stimulate economy, a relevant question is on the proper size of stimulus bill. to address the answer, we emphasize the role of hysteresis in economic networks. in times of crises, firms and corporations cut their productions; now since their level of activity is correlated, metastable features in the network become prominent. this means that economic networks resist against the recovery actions. to measure the size of resistance in the network against recovery, we deploy the xy model. though theoretically the xy model has no hysteresis, when it comes to the kinetic behavior in the deterministic regimes, we observe a dynamic hysteresis. we find that to overcome the hysteresis of the network, a minimum size of stimulation is needed for success. our simulations show that as long as the networks are watts-strogatz, such minimum is independent of the characteristics of the networks.", "categories": "physics.soc-ph cond-mat.stat-mech econ.gn q-fin.ec", "created": "2018-08-09", "updated": "", "authors": ["ali hosseiny", "mohammadreza absalan", "mohammad sherafati", "mauro gallegati"], "url": "https://arxiv.org/abs/1808.03404"}, {"title": "exeum: a decentralized financial platform for price-stable   cryptocurrencies", "id": "1808.03482", "abstract": "price stability has often been cited as a key reason that cryptocurrencies have not gained widespread adoption as a medium of exchange and continue to prove incapable of powering the economy of decentralized applications (dapps) efficiently. exeum proposes a novel method to provide price stable digital tokens whose values are pegged to real world assets, serving as a bridge between the real world and the decentralized economy.   pegged tokens issued by exeum - for example, usde refers to a stable token issued by the system whose value is pegged to usd - are backed by virtual assets in a virtual asset exchange where users can deposit the base token of the system and take long or short positions. guaranteeing the stability of the pegged tokens boils down to the problem of maintaining the peg of the virtual assets to real world assets, and the main mechanism used by exeum is controlling the swap rate of assets. if the swap rate is fully controlled by the system, arbitrageurs can be incentivized enough to restore a broken peg; exeum distributes statistical arbitrage trading software to decentralize this type of market making activity. the last major component of the system is a central bank equivalent that determines the long term interest rate of the base token, pays interest on the deposit by inflating the supply if necessary, and removes the need for stability fees on pegged tokens, improving their usability.   to the best of our knowledge, exeum is the first to propose a truly decentralized method for developing a stablecoin that enables 1:1 value conversion between the base token and pegged assets, completely removing the mismatch between supply and demand. in this paper, we will also discuss its applications, such as improving staking based dapp token models, price stable gas fees, pegging to an index of dapp tokens, and performing cross-chain asset transfer of legacy crypto assets.", "categories": "cs.cr econ.gn q-fin.ec", "created": "2018-08-10", "updated": "", "authors": ["jaehyung lee", "minhyung cho"], "url": "https://arxiv.org/abs/1808.03482"}, {"title": "the impact of age on nationality bias: evidence from ski jumping", "id": "1808.03804", "abstract": "this empirical research explores the impact of age on nationality bias. world cup competition data suggest that judges of professional ski jumping competitions prefer jumpers of their own nationality and exhibit this preference by rewarding them with better marks. furthermore, the current study reveals that this nationality bias is diminished among younger judges, in accordance with the reported lower levels of national discrimination among younger generations. globalisation and its effect in reducing class-based thinking may explain this reduced bias in judgment of others.", "categories": "econ.gn q-fin.ec", "created": "2018-08-11", "updated": "", "authors": ["sandra schneemann", "hendrik scholten", "christian deutscher"], "url": "https://arxiv.org/abs/1808.03804"}, {"title": "engineering and economic analysis for electric vehicle charging   infrastructure --- placement, pricing, and market design", "id": "1808.03897", "abstract": "this dissertation is to study the interplay between large-scale electric vehicle (ev) charging and the power system. we address three important issues pertaining to ev charging and integration into the power system: (1) charging station placement, (2) pricing policy and energy management strategy, and (3) electricity trading market and distribution network design to facilitate integrating ev and renewable energy source (res) into the power system.   for charging station placement problem, we propose a multi-stage consumer behavior based placement strategy with incremental ev penetration rates and model the ev charging industry as an oligopoly where the entire market is dominated by a few charging service providers (oligopolists). the optimal placement policy for each service provider is obtained by solving a bayesian game.   for pricing and energy management of ev charging stations, we provide guidelines for charging service providers to determine charging price and manage electricity reserve to balance the competing objectives of improving profitability, enhancing customer satisfaction, and reducing impact on the power system. two algorithms --- stochastic dynamic programming (sdp) algorithm and greedy algorithm (benchmark algorithm) are applied to derive the pricing and electricity procurement strategy.   we design a novel electricity trading market and distribution network, which supports seamless res integration, grid to vehicle (g2v), vehicle to grid (v2g), vehicle to vehicle (v2v), and distributed generation (dg) and storage. we apply a sharing economy model to the electricity sector to stimulate different entities to exchange and monetize their underutilized electricity. a fitness-score (fs)-based supply-demand matching algorithm is developed by considering consumer surplus, electricity network congestion, and economic dispatch.", "categories": "econ.em eess.sp", "created": "2018-08-12", "updated": "", "authors": ["chao luo"], "url": "https://arxiv.org/abs/1808.03897"}, {"title": "mechanism design with news utility", "id": "1808.04020", "abstract": "news utility is the idea that the utility of an agent depends on changes in her beliefs over consumption and money. we introduce news utility into otherwise classical static bayesian mechanism design models. we show that a key role is played by the timeline of the mechanism, i.e. whether there are delays between the announcement stage, the participation stage, the play stage and the realization stage of a mechanism. depending on the timing, agents with news utility can experience two additional news utility effects: a surprise effect derived from comparing to pre-mechanism beliefs, as well as a realization effect derived from comparing post-play beliefs with the actual outcome of the mechanism.   we look at two distinct mechanism design settings reflecting the two main strands of the classical literature. in the first model, a monopolist screens an agent according to the magnitude of her loss aversion. in the second model, we consider a general multi-agent bayesian mechanism design setting where the uncertainty of each player stems from not knowing the intrinsic types of the other agents. we give applications to auctions and public good provision which illustrate how news utility changes classical results.   for both models we characterize the optimal design of the timeline. a timeline featuring no delay between participation and play but a delay in realization is never optimal in either model. in the screening model the optimal timeline is one without delays. in auction settings, under fairly natural assumptions the optimal timeline has delays between all three stages of the mechanism.", "categories": "econ.th", "created": "2018-08-12", "updated": "", "authors": ["jetlir duraj"], "url": "https://arxiv.org/abs/1808.04020"}, {"title": "a predictive model for oil market under uncertainty: data-driven system   dynamics approach", "id": "1808.04150", "abstract": "in recent years, there have been a lot of sharp changes in the oil price. these rapid changes cause the traditional models to fail in predicting the price behavior. the main reason for the failure of the traditional models is that they consider the actual value of parameters instead of their expectational ones. in this paper, we propose a system dynamics model that incorporates expectational variables in determining the oil price. in our model, the oil price is determined by the expected demand and supply vs. their actual values. our core model is based upon regression analysis on several historic time series and adjusted by adding many casual loops in the oil market. the proposed model in simulated in different scenarios that have happened in the past and our results comply with the trends of the oil price in each of the scenarios.", "categories": "econ.gn q-fin.ec", "created": "2018-08-13", "updated": "", "authors": ["sina aghaei", "amirreza safari langroudi", "masoud fekri"], "url": "https://arxiv.org/abs/1808.04150"}, {"title": "extrapolating treatment effects in multi-cutoff regression discontinuity   designs", "id": "1808.04416", "abstract": "in non-experimental settings, the regression discontinuity (rd) design is one of the most credible identification strategies for program evaluation and causal inference. however, rd treatment effect estimands are necessarily local, making statistical methods for the extrapolation of these effects a key area for development. we introduce a new method for extrapolation of rd effects that relies on the presence of multiple cutoffs, and is therefore design-based. our approach employs an easy-to-interpret identifying assumption that mimics the idea of \"common trends\" in difference-in-differences designs. we illustrate our methods with data on a subsidized loan program on post-education attendance in colombia, and offer new evidence on program effects for students with test scores away from the cutoff that determined program eligibility.", "categories": "econ.em stat.ap stat.me", "created": "2018-08-13", "updated": "2020-04-01", "authors": ["matias d. cattaneo", "luke keele", "rocio titiunik", "gonzalo vazquez-bare"], "url": "https://arxiv.org/abs/1808.04416"}, {"title": "latent agents in networks: estimation and pricing", "id": "1808.04878", "abstract": "we focus on a setting where agents in a social network consume a product that exhibits positive local network externalities. a seller has access to data on past consumption decisions/prices for a subset of observable agents, and can target these agents with appropriate discounts to exploit network effects and increase her revenues. a novel feature of the model is that the observable agents potentially interact with additional latent agents. these latent agents can purchase the same product from a different channel, and are not observed by the seller. observable agents influence each other both directly and indirectly through the influence they exert on the latent agents. the seller knows the connection structure of neither the observable nor the latent part of the network.   due to the presence of network externalities, an agent's consumption decision depends not only on the price offered to her, but also on the consumption decisions of (and in turn the prices offered to) her neighbors in the underlying network. we investigate how the seller can use the available data to estimate the matrix that captures the dependence of observable agents' consumption decisions on the prices offered to them. we provide an algorithm for estimating this matrix under an approximate sparsity condition, and obtain convergence rates for the proposed estimator despite the high dimensionality that allows more agents than observations. importantly, we then show that this approximate sparsity condition holds under standard conditions present in the literature and hence our algorithms are applicable to a large class of networks. we establish that by using the estimated matrix the seller can construct prices that lead to a small revenue loss relative to revenue-maximizing prices under complete information, and the optimality gap vanishes relative to the size of the network.", "categories": "cs.si cs.cv econ.th math.st stat.th", "created": "2018-08-14", "updated": "2018-12-12", "authors": ["baris ata", "alexandre belloni", "ozan candogan"], "url": "https://arxiv.org/abs/1808.04878"}, {"title": "a unified framework for efficient estimation of general treatment models", "id": "1808.04936", "abstract": "this paper presents a weighted optimization framework that unifies the binary,multi-valued, continuous, as well as mixture of discrete and continuous treatment, under the unconfounded treatment assignment. with a general loss function, the framework includes the average, quantile and asymmetric least squares causal effect of treatment as special cases. for this general framework, we first derive the semiparametric efficiency bound for the causal effect of treatment, extending the existing bound results to a wider class of models. we then propose a generalized optimization estimation for the causal effect with weights estimated by solving an expanding set of equations. under some sufficient conditions, we establish consistency and asymptotic normality of the proposed estimator of the causal effect and show that the estimator attains our semiparametric efficiency bound, thereby extending the existing literature on efficient estimation of causal effect to a wider class of applications. finally, we discuss etimation of some causal effect functionals such as the treatment effect curve and the average outcome. to evaluate the finite sample performance of the proposed procedure, we conduct a small scale simulation study and find that the proposed estimation has practical value. to illustrate the applicability of the procedure, we revisit the literature on campaign advertise and campaign contributions. unlike the existing procedures which produce mixed results, we find no evidence of campaign advertise on campaign contribution.", "categories": "econ.em", "created": "2018-08-14", "updated": "2018-08-16", "authors": ["chunrong ai", "oliver linton", "kaiji motegi", "zheng zhang"], "url": "https://arxiv.org/abs/1808.04936"}, {"title": "can gdp measurement be further improved? data revision and   reconciliation", "id": "1808.04970", "abstract": "recent years have seen many attempts to combine expenditure-side estimates of u.s. real output (gde) growth with income-side estimates (gdi) to improve estimates of real gdp growth. we show how to incorporate information from multiple releases of noisy data to provide more precise estimates while avoiding some of the identifying assumptions required in earlier work. this relies on a new insight: using multiple data releases allows us to distinguish news and noise measurement errors in situations where a single vintage does not.   our new measure, gdp++, fits the data better than gdp+, the gdp growth measure of aruoba et al. (2016) published by the federal reserve bank of philadephia. historical decompositions show that gde releases are more informative than gdi, while the use of multiple data releases is particularly important in the quarters leading up to the great recession.", "categories": "econ.em", "created": "2018-08-15", "updated": "", "authors": ["jan p. a. m. jacobs", "samad sarferaz", "jan-egbert sturm", "simon van norden"], "url": "https://arxiv.org/abs/1808.04970"}, {"title": "brexit: the belated threat", "id": "1808.05142", "abstract": "debates on an eu-leaving referendum arose in several member states after brexit. we want to highlight how the exit of an additional country affects the power distribution in the council of the european union. we inspect the power indices of the member states both with and without the country which might leave the union. our results show a pattern connected to a change in the threshold of the number of member states required for a decision. an exit that modifies this threshold benefits the countries with high population, while an exit that does not cause such a change benefits the small member states. according to our calculations, the threat of brexit would have worked differently before the entry of croatia.", "categories": "econ.gn q-fin.ec", "created": "2018-08-14", "updated": "", "authors": ["d\u00f3ra gr\u00e9ta petr\u00f3czy", "mark francis rogers", "l\u00e1szl\u00f3 \u00e1. k\u00f3czy"], "url": "https://arxiv.org/abs/1808.05142"}, {"title": "design-based analysis in difference-in-differences settings with   staggered adoption", "id": "1808.05293", "abstract": "in this paper we study estimation of and inference for average treatment effects in a setting with panel data. we focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. we take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. we show that under random assignment of the adoption date the standard difference-in-differences estimator is is an unbiased estimator of a particular weighted average causal effect. we characterize the proeperties of this estimand, and show that the standard variance estimator is conservative.", "categories": "econ.em cs.lg math.st stat.th", "created": "2018-08-15", "updated": "2018-09-01", "authors": ["susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1808.05293"}, {"title": "when do households invest in solar photovoltaics? an application of   prospect theory", "id": "1808.05572", "abstract": "while investments in renewable energy sources (res) are incentivized around the world, the policy tools that do so are still poorly understood, leading to costly misadjustments in many cases. as a case study, the deployment dynamics of residential solar photovoltaics (pv) invoked by the german feed-in tariff legislation are investigated. here we report a model showing that the question of when people invest in residential pv systems is found to be not only determined by profitability, but also by profitability's change compared to the status quo. this finding is interpreted in the light of loss aversion, a concept developed in kahneman and tversky's prospect theory. the model is able to reproduce most of the dynamics of the uptake with only a few financial and behavioral assumptions", "categories": "econ.em", "created": "2018-08-16", "updated": "", "authors": ["martin klein", "marc deissenroth"], "url": "https://arxiv.org/abs/1808.05572"}, {"title": "estimation in a generalization of bivariate probit models with dummy   endogenous regressors", "id": "1808.05792", "abstract": "the purpose of this paper is to provide guidelines for empirical researchers who use a class of bivariate threshold crossing models with dummy endogenous variables. a common practice employed by the researchers is the specification of the joint distribution of the unobservables as a bivariate normal distribution, which results in a bivariate probit model. to address the problem of misspecification in this practice, we propose an easy-to-implement semiparametric estimation framework with parametric copula and nonparametric marginal distributions. we establish asymptotic theory, including root-n normality, for the sieve maximum likelihood estimators that can be used to conduct inference on the individual structural parameters and the average treatment effect (ate). in order to show the practical relevance of the proposed framework, we conduct a sensitivity analysis via extensive monte carlo simulation exercises. the results suggest that the estimates of the parameters, especially the ate, are sensitive to parametric specification, while semiparametric estimation exhibits robustness to underlying data generating processes. we then provide an empirical illustration where we estimate the effect of health insurance on doctor visits. in this paper, we also show that the absence of excluded instruments may result in identification failure, in contrast to what some practitioners believe.", "categories": "econ.em", "created": "2018-08-17", "updated": "2019-03-04", "authors": ["sukjin han", "sungwon lee"], "url": "https://arxiv.org/abs/1808.05792"}, {"title": "quantifying the computational advantage of forward orthogonal deviations", "id": "1808.05995", "abstract": "under suitable conditions, one-step generalized method of moments (gmm) based on the first-difference (fd) transformation is numerically equal to one-step gmm based on the forward orthogonal deviations (fod) transformation. however, when the number of time periods ($t$) is not small, the fod transformation requires less computational work. this paper shows that the computational complexity of the fd and fod transformations increases with the number of individuals ($n$) linearly, but the computational complexity of the fod transformation increases with $t$ at the rate $t^{4}$ increases, while the computational complexity of the fd transformation increases at the rate $t^{6}$ increases. simulations illustrate that calculations exploiting the fod transformation are performed orders of magnitude faster than those using the fd transformation. the results in the paper indicate that, when one-step gmm based on the fd and fod transformations are the same, monte carlo experiments can be conducted much faster if the fod version of the estimator is used.", "categories": "econ.em", "created": "2018-08-17", "updated": "", "authors": ["robert f. phillips"], "url": "https://arxiv.org/abs/1808.05995"}, {"title": "$k$th price auctions and catalan numbers", "id": "1808.05996", "abstract": "this paper establishes an interesting link between $k$th price auctions and catalan numbers by showing that for distributions that have linear density, the bid function at any symmetric, increasing equilibrium of a $k$th price auction with $k\\geq 3$ can be represented as a finite series of $k-2$ terms whose $\\ell$th term involves the $\\ell$th catalan number. using an integral representation of catalan numbers, together with some classical combinatorial identities, we derive the closed form of the unique symmetric, increasing equilibrium of a $k$th price auction for a non-uniform distribution.", "categories": "econ.th", "created": "2018-08-17", "updated": "", "authors": ["abdel-hameed nawar", "debapriya sen"], "url": "https://arxiv.org/abs/1808.05996"}, {"title": "deep learning, deep change? mapping the development of the artificial   intelligence general purpose technology", "id": "1808.06355", "abstract": "general purpose technologies (gpts) that can be applied in many industries are an important driver of economic growth and national and regional competitiveness. in spite of this, the geography of their development and diffusion has not received significant attention in the literature. we address this with an analysis of deep learning (dl), a core technique in artificial intelligence (ai) increasingly being recognized as the latest gpt. we identify dl papers in a novel dataset from arxiv, a popular preprints website, and use crunchbase, a technology business directory to measure industrial capabilities related to it. after showing that dl conforms with the definition of a gpt, having experienced rapid growth and diffusion into new fields where it has generated an impact, we describe changes in its geography. our analysis shows china's rise in ai rankings and relative decline in several european countries. we also find that initial volatility in the geography of dl has been followed by consolidation, suggesting that the window of opportunity for new entrants might be closing down as new dl research hubs become dominant. finally, we study the regional drivers of dl clustering. we find that competitive dl clusters tend to be based in regions combining research and industrial activities related to it. this could be because gpt developers and adopters located close to each other can collaborate and share knowledge more easily, thus overcoming coordination failures in gpt deployment. our analysis also reveals a chinese comparative advantage in dl after we control for other explanatory factors, perhaps underscoring the importance of access to data and supportive policies for the successful development of this complex, `omni-use' technology.", "categories": "cs.cy econ.em", "created": "2018-08-20", "updated": "", "authors": ["j. klinger", "j. mateos-garcia", "k. stathoulopoulos"], "url": "https://arxiv.org/abs/1808.06355"}, {"title": "the story of conflict and cooperation", "id": "1808.06750", "abstract": "the story of conflict and cooperation has started millions of years ago, and now it is everywhere: in biology, computer science, economics, political science, and psychology. examples include wars, airline alliances, trade, oligopolistic cartels, the evolution of species and genes, and team sports. however, neither cooperative games nor noncooperative games---in which \"each player participant independently without collaboration with any of the others\" (nash, 1951)---fully capture the competition between and across individuals and groups, and the strategic partnerships that give rise to such groups. thus, one needs to extend the noncooperative framework to study strategic games like scientific publication, which is a rather competitive game, yet (strategic) collaboration is widespread. in this paper, i propose a novel framework and a solution concept: i introduce the recursive backward induction solution in coalitional extensive-form games in which players are free to cooperate to coordinate their actions or act independently.", "categories": "econ.th cs.ma q-bio.pe", "created": "2018-08-20", "updated": "2020-05-27", "authors": ["mehmet s. ismail"], "url": "https://arxiv.org/abs/1808.06750"}, {"title": "catch-up: a rule that makes service sports more competitive", "id": "1808.06922", "abstract": "service sports include two-player contests such as volleyball, badminton, and squash. we analyze four rules, including the standard rule (sr), in which a player continues to serve until he or she loses. the catch-up rule (cr) gives the serve to the player who has lost the previous point - as opposed to the player who won the previous point, as under sr. we also consider two trailing rules that make the server the player who trails in total score. surprisingly, compared with sr, only cr gives the players the same probability of winning a game while increasing its expected length, thereby making it more competitive and exciting to watch. unlike one of the trailing rules, cr is strategy-proof. by contrast, the rules of tennis fix who serves and when; its tiebreaker, however, keeps play competitive by being fair - not favoring either the player who serves first or who serves second.", "categories": "math.ho econ.th", "created": "2018-08-17", "updated": "", "authors": ["steven j. brams", "mehmet s. ismail", "d. marc kilgour", "walter stromquist"], "url": "https://arxiv.org/abs/1808.06922"}, {"title": "sensitivity analysis using approximate moment condition models", "id": "1808.07387", "abstract": "we consider inference in models defined by approximate moment conditions. we show that near-optimal confidence intervals (cis) can be formed by taking a generalized method of moments (gmm) estimator, and adding and subtracting the standard error times a critical value that takes into account the potential bias from misspecification of the moment conditions. in order to optimize performance under potential misspecification, the weighting matrix for this gmm estimator takes into account this potential bias, and therefore differs from the one that is optimal under correct specification. to formally show the near-optimality of these cis, we develop asymptotic efficiency bounds for inference in the locally misspecified gmm setting. these bounds may be of independent interest, due to their implications for the possibility of using moment selection procedures when conducting inference in moment condition models. we apply our methods in an empirical application to automobile demand, and show that adjusting the weighting matrix can shrink the cis by a factor of 3 or more.", "categories": "econ.em stat.me", "created": "2018-08-22", "updated": "2020-07-29", "authors": ["timothy b. armstrong", "michal koles\u00e1r"], "url": "https://arxiv.org/abs/1808.07387"}, {"title": "optimizing the tie-breaker regression discontinuity design", "id": "1808.07563", "abstract": "motivated by customer loyalty plans and scholarship programs, we study tie-breaker designs which are hybrids of randomized controlled trials (rcts) and regression discontinuity designs (rdds). we quantify the statistical efficiency of a tie-breaker design in which a proportion $\\delta$ of observed subjects are in the rct. in a two line regression, statistical efficiency increases monotonically with $\\delta$, so efficiency is maximized by an rct. we point to additional advantages of tie-breakers versus rdd: for a nonparametric regression the boundary bias is much less severe and for quadratic regression, the variance is greatly reduced. for a two line model we can quantify the short term value of the treatment allocation and this comparison favors smaller $\\delta$ with the rdd being best. we solve for the optimal tradeoff between these exploration and exploitation goals. the usual tie-breaker design applies an rct on the middle $\\delta$ subjects as ranked by the assignment variable. we quantify the efficiency of other designs such as experimenting only in the second decile from the top. we also show that in some general parametric models a monte carlo evaluation can be replaced by matrix algebra.", "categories": "stat.me econ.em", "created": "2018-08-22", "updated": "2020-07-30", "authors": ["art b. owen", "hal varian"], "url": "https://arxiv.org/abs/1808.07563"}, {"title": "voting power of political parties in the senate of chile during the   whole binomial system period: 1990-2017", "id": "1808.07854", "abstract": "the binomial system is an electoral system unique in the world. it was used to elect the senators and deputies of chile during 27 years, from the return of democracy in 1990 until 2017. in this paper we study the real voting power of the different political parties in the senate of chile during the whole binomial period. we not only consider the different legislative periods, but also any party changes between one period and the next. the real voting power is measured by considering power indices from cooperative game theory, which are based on the capability of the political parties to form winning coalitions. with this approach, we can do an analysis that goes beyond the simple count of parliamentary seats.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2018-08-14", "updated": "2019-09-14", "authors": ["fabi\u00e1n riquelme", "pablo gonz\u00e1lez-cantergiani", "gabriel godoy"], "url": "https://arxiv.org/abs/1808.07854"}, {"title": "on the normality of negative interest rates", "id": "1808.07909", "abstract": "we argue that a negative interest rate policy (nirp) can be an effect tool for macroeconomic stabilization. we first discuss how implementing negative rates on reserves held at a central bank does not pose any theoretical difficulty, with a reduction in rates operating in exactly the same way when rates are positive or negative, and show that this is compatible with an endogenous money point of view. we then propose a simplified stock-flow consistent macroeconomic model where rates are allowed to become arbitrarily negative and present simulation evidence for their stabilizing effects. in practice, the existence of physical cash imposes a lower bound for interest rates, which in our view is the main reason for the lack of effectiveness of negative interest rates in the countries that adopted them as part of their monetary policy. we conclude by discussing alternative ways to overcome this lower bound , in particular the use of central bank digital currencies.", "categories": "econ.gn q-fin.ec q-fin.mf", "created": "2018-08-23", "updated": "", "authors": ["matheus r. grasselli", "alexander lipton"], "url": "https://arxiv.org/abs/1808.07909"}, {"title": "the structure of equilibria in trading networks with frictions", "id": "1808.07924", "abstract": "several structural results for the set of competitive equilibria in trading networks with frictions are established: the lattice theorem, the rural hospitals theorem, the existence of side-optimal equilibria, and a group-incentive-compatibility result hold with imperfectly transferable utility and in the presence of frictions. while our results are developed in a trading network model, they also imply analogous (and new) results for exchange economies with combinatorial demand and for two-sided matching markets with transfers.", "categories": "econ.th", "created": "2018-08-23", "updated": "2020-06-30", "authors": ["jan christoph schlegel"], "url": "https://arxiv.org/abs/1808.07924"}, {"title": "solving quadratic multi-leader-follower games by smoothing the   follower's best response", "id": "1808.07941", "abstract": "we derive nash equilibria for a class of quadratic multi-leader-follower games using the nonsmooth best response function. to overcome the challenge of nonsmoothness, we pursue a smoothing approach resulting in a reformulation as a smooth nash equilibrium problem. the existence and uniqueness of solutions are proven for all smoothing parameters. accumulation points of nash equilibria exist for a decreasing sequence of these smoothing parameters and we show that these candidates fulfill the conditions of s-stationarity and are nash equilibria to the multi-leader-follower game. finally, we propose an update on the leader variables for efficient computation and numerically compare nonsmooth newton and subgradient methods.", "categories": "math.oc econ.th", "created": "2018-08-23", "updated": "2020-04-29", "authors": ["michael herty", "sonja steffensen", "anna th\u00fcnen"], "url": "https://arxiv.org/abs/1808.07941"}, {"title": "supporting crowd-powered science in economics: fracti, a conceptual   framework for large-scale collaboration and transparent investigation in   financial markets", "id": "1808.07959", "abstract": "modern investigation in economics and in other sciences requires the ability to store, share, and replicate results and methods of experiments that are often multidisciplinary and yield a massive amount of data. given the increasing complexity and growing interaction across diverse bodies of knowledge it is becoming imperative to define a platform to properly support collaborative research and track origin, accuracy and use of data. this paper starts by defining a set of methods leveraging scientific principles and advocating the importance of those methods in multidisciplinary, computer intensive fields like computational finance. the next part of this paper defines a class of systems called scientific support systems, vis-a-vis usages in other research fields such as bioinformatics, physics and engineering. we outline a basic set of fundamental concepts, and list our goals and motivation for leveraging such systems to enable large-scale investigation, \"crowd powered science\", in economics. the core of this paper provides an outline of fracti in five steps. first we present definitions related to scientific support systems intrinsic to finance and describe common characteristics of financial use cases. the second step concentrates on what can be exchanged through the definition of shareable entities called contributions. the third step is the description of a classification system for building blocks of the conceptual framework, called facets. the fourth step introduces the meta-model that will enable provenance tracking and representation of data fragments and simulation. finally we describe intended cases of use to highlight main strengths of fracti: application of the scientific method for investigation in computational finance, large-scale collaboration and simulation.", "categories": "q-fin.cp econ.em econ.gn q-fin.ec q-fin.tr", "created": "2018-08-23", "updated": "", "authors": ["jorge faleiro", "edward tsang"], "url": "https://arxiv.org/abs/1808.07959"}, {"title": "complexity of products: the effect of data regularisation", "id": "1808.08249", "abstract": "among several developments, the field of economic complexity (ec) has notably seen the introduction of two new techniques. one is the bootstrapped selective predictability scheme (spsb), which can provide quantitative forecasts of the gross domestic product of countries. the other, hidden markov model (hmm) regularisation, denoises the datasets typically employed in the literature. we contribute to ec along three different directions. first, we prove the convergence of the spsb algorithm to a well-known statistical learning technique known as nadaraya-watson kernel regression. the latter has significantly lower time complexity, produces deterministic results, and it is interchangeable with spsb for the purpose of making predictions. second, we study the effects of hmm regularization on the product complexity and logprody metrics, for which a model of time evolution has been recently proposed. we find confirmation for the original interpretation of the logprody model as describing the change in the global market structure of products with new insights allowing a new interpretation of the complexity measure, for which we propose a modification. third, we explore new effects of regularisation on the data. we find that it reduces noise, and observe for the first time that it increases nestedness in the export network adjacency matrix.", "categories": "econ.gn q-fin.ec", "created": "2018-08-24", "updated": "2018-10-15", "authors": ["orazio angelini", "tiziana di matteo"], "url": "https://arxiv.org/abs/1808.08249"}, {"title": "evolutionary dynamics of cryptocurrency transaction networks: an   empirical study", "id": "1808.08585", "abstract": "cryptocurrency is a well-developed blockchain technology application that is currently a heated topic throughout the world. the public availability of transaction histories offers an opportunity to analyze and compare different cryptocurrencies. in this paper, we present a dynamic network analysis of three representative blockchain-based cryptocurrencies: bitcoin, ethereum, and namecoin. by analyzing the accumulated network growth, we find that, unlike most other networks, these cryptocurrency networks do not always densify over time, and they are changing all the time with relatively low node and edge repetition ratios. therefore, we then construct separate networks on a monthly basis, trace the changes of typical network characteristics (including degree distribution, degree assortativity, clustering coefficient, and the largest connected component) over time, and compare the three. we find that the degree distribution of these monthly transaction networks cannot be well fitted by the famous power-law distribution, at the same time, different currency still has different network properties, e.g., both bitcoin and ethereum networks are heavy-tailed with disassortative mixing, however, only the former can be treated as a small world. these network properties reflect the evolutionary characteristics and competitive power of these three cryptocurrencies and provide a foundation for future research.", "categories": "q-fin.st cs.cr cs.dc econ.gn q-fin.ec", "created": "2018-08-26", "updated": "", "authors": ["jiaqi liang", "linjing li", "daniel zeng"], "url": "https://arxiv.org/abs/1808.08585"}, {"title": "economics of carbon-dioxide abatement under an exogenous constraint on   cumulative emissions", "id": "1808.08717", "abstract": "the fossil-fuel induced contribution to further warming over the 21st century will be determined largely by integrated co2 emissions over time rather than the precise timing of the emissions, with a relation of near-proportionality between global warming and cumulative co2 emissions. this paper examines optimal abatement pathways under an exogenous constraint on cumulative emissions. least cost abatement pathways have carbon tax rising at the risk-free interest rate, but if endogenous learning or climate damage costs are included in the analysis, the carbon tax grows more slowly. the inclusion of damage costs in the optimization leads to a higher initial carbon tax, whereas the effect of learning depends on whether it appears as an additive or multiplicative contribution to the marginal cost curve. multiplicative models are common in the literature and lead to delayed abatement and a smaller initial tax. the required initial carbon tax increases with the cumulative abatement goal and is higher for lower interest rates. delaying the start of abatement is costly owing to the increasing marginal abatement cost. lower interest rates lead to higher relative costs of delaying abatement because these induce higher abatement rates early on. the fraction of business-as-usual emissions (bau) avoided in optimal pathways increases for low interest rates and rapid growth of the abatement cost curve, which allows a lower threshold global warming goal to become attainable without overshoot in temperature. each year of delay in starting abatement raises this threshold by an increasing amount, because the abatement rate increases exponentially with time.", "categories": "econ.gn q-fin.ec", "created": "2018-08-27", "updated": "2020-06-07", "authors": ["ashwin k seshadri"], "url": "https://arxiv.org/abs/1808.08717"}, {"title": "tests for price indices in a dynamic item universe", "id": "1808.08995", "abstract": "there is generally a need to deal with quality change and new goods in the consumer price index due to the underlying dynamic item universe. traditionally axiomatic tests are defined for a fixed universe. we propose five tests explicitly formulated for a dynamic item universe, and motivate them both from the perspectives of a cost-of-goods index and a cost-of-living index. none of the indices satisfies all the tests at the same time, which are currently available for making use of scanner data that comprises the whole item universe. the set of tests provides a rigorous diagnostic for whether an index is completely appropriate in a dynamic item universe, as well as pointing towards the directions of possible remedies. we thus outline a large index family that potentially can satisfy all the tests.", "categories": "econ.em", "created": "2018-08-27", "updated": "2018-10-05", "authors": ["li-chun zhang", "ingvild johansen", "ragnhild nygaard"], "url": "https://arxiv.org/abs/1808.08995"}, {"title": "downstream effects of affirmative action", "id": "1808.09004", "abstract": "we study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. students are drawn from one of two populations, which might have different type distributions. we assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. we then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. for example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. we show that both goals can be achieved when the college does not report grades. on the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.", "categories": "cs.gt cs.lg econ.th", "created": "2018-08-27", "updated": "", "authors": ["sampath kannan", "aaron roth", "juba ziani"], "url": "https://arxiv.org/abs/1808.09004"}, {"title": "a residual bootstrap for conditional value-at-risk", "id": "1808.09125", "abstract": "this paper proposes a fixed-design residual bootstrap method for the two-step estimator of francq and zako\\\"ian (2015) associated with the conditional value-at-risk. the bootstrap's consistency is proven for a general class of volatility models and intervals are constructed for the conditional value-at-risk. a simulation study reveals that the equal-tailed percentile bootstrap interval tends to fall short of its nominal value. in contrast, the reversed-tails bootstrap interval yields accurate coverage. we also compare the theoretically analyzed fixed-design bootstrap with the recursive-design bootstrap. it turns out that the fixed-design bootstrap performs equally well in terms of average coverage, yet leads on average to shorter intervals in smaller samples. an empirical application illustrates the interval estimation.", "categories": "econ.em", "created": "2018-08-28", "updated": "2020-07-10", "authors": ["eric beutner", "alexander heinemann", "stephan smeekes"], "url": "https://arxiv.org/abs/1808.09125"}, {"title": "inference based on kotlarski's identity", "id": "1808.09375", "abstract": "kotlarski's identity has been widely used in applied economic research. however, how to conduct inference based on this popular identification approach has been an open question for two decades. this paper addresses this open problem by constructing a novel confidence band for the density function of a latent variable in repeated measurement error model. the confidence band builds on our finding that we can rewrite kotlarski's identity as a system of linear moment restrictions. the confidence band controls the asymptotic size uniformly over a class of data generating processes, and it is consistent against all fixed alternatives. simulation studies support our theoretical results.", "categories": "econ.em", "created": "2018-08-28", "updated": "2019-09-08", "authors": ["kengo kato", "yuya sasaki", "takuya ura"], "url": "https://arxiv.org/abs/1808.09375"}, {"title": "almost envy-free allocations with connected bundles", "id": "1808.09406", "abstract": "we study the existence of allocations of indivisible goods that are envy-free up to one good (ef1), under the additional constraint that each bundle needs to be connected in an underlying item graph g. when the items are arranged in a path, we show that ef1 allocations are guaranteed to exist for arbitrary monotonic utility functions over bundles, provided that either there are at most four agents, or there are any number of agents but they all have identical utility functions. our existence proofs are based on classical arguments from the divisible cake-cutting setting, and involve discrete analogues of cut-and-choose, of stromquist's moving-knife protocol, and of the su-simmons argument based on sperner's lemma. sperner's lemma can also be used to show that on a path, an ef2 allocation exists for any number of agents. except for the results using sperner's lemma, all of our procedures can be implemented by efficient algorithms. our positive results for paths imply the existence of connected ef1 or ef2 allocations whenever g is traceable, i.e., contains a hamiltonian path. for the case of two agents, we completely characterize the class of graphs $g$ that guarantee the existence of ef1 allocations as the class of graphs whose biconnected components are arranged in a path. this class is strictly larger than the class of traceable graphs; one can be check in linear time whether a graph belongs to this class, and if so return an ef1 allocation.", "categories": "cs.gt econ.th", "created": "2018-08-28", "updated": "", "authors": ["vittorio bil\u00f2", "ioannis caragiannis", "michele flammini", "ayumi igarashi", "gianpiero monaco", "dominik peters", "cosimo vinci", "william s. zwicker"], "url": "https://arxiv.org/abs/1808.09406"}, {"title": "switching cost models as hypothesis tests", "id": "1808.09686", "abstract": "we relate models based on costs of switching beliefs (e.g. due to inattention) to hypothesis tests. specifically, for an inference problem with a penalty for mistakes and for switching the inferred value, a band of inaction is optimal. we show this band is equivalent to a confidence interval, and therefore to a two-sided hypothesis test.", "categories": "econ.gn math.oc q-fin.ec", "created": "2018-08-29", "updated": "", "authors": ["samuel n. cohen", "timo henckel", "gordon d. menzies", "johannes muhle-karbe", "daniel j. zizzo"], "url": "https://arxiv.org/abs/1808.09686"}, {"title": "enforcing regulation under illicit adaptation", "id": "1808.09887", "abstract": "attempts to curb illegal activity by enforcing regulations gets complicated when agents react to the new regulatory regime in unanticipated ways to circumvent enforcement. we present a research strategy that uncovers such reactions, and permits program evaluation net of such adaptive behaviors. our interventions were designed to reduce over-fishing of the critically endangered pacific hake by either (a) monitoring and penalizing vendors that sell illegal fish or (b) discouraging consumers from purchasing using an information campaign. vendors attempt to circumvent the ban through hidden sales and other means, which we track using mystery shoppers. instituting random monitoring visits are much more effective in reducing true hake availability by limiting such cheating, compared to visits that occur on a predictable schedule. monitoring at higher frequency (designed to limit temporal displacement of illegal sales) backfires, because targeted agents learn faster, and cheat more effectively. sophisticated policy design is therefore crucial for determining the sustained, longer-term effects of enforcement. data collected from fishermen, vendors, and consumers allow us to document the upstream, downstream, spillover, and equilibrium effects of enforcement on the entire supply chain. the consumer information campaign generates two-thirds of the gains compared to random monitoring, but is simpler for the government to implement and almost as cost-effective.", "categories": "econ.gn q-fin.ec", "created": "2018-08-29", "updated": "", "authors": ["andres gonzalez lira", "ahmed mushfiq mobarak"], "url": "https://arxiv.org/abs/1808.09887"}, {"title": "the role of complex analysis in modeling economic growth", "id": "1808.10428", "abstract": "development and growth are complex and tumultuous processes. modern economic growth theories identify some key determinants of economic growth. however, the relative importance of the determinants remains unknown, and additional variables may help clarify the directions and dimensions of the interactions. the novel stream of literature on economic complexity goes beyond aggregate measures of productive inputs, and considers instead a more granular and structural view of the productive possibilities of countries, i.e. their capabilities. different endowments of capabilities are crucial ingredients in explaining differences in economic performances. in this paper we employ economic fitness, a measure of productive capabilities obtained through complex network techniques. focusing on the combined roles of fitness and some more traditional drivers of growth, we build a bridge between economic growth theories and the economic complexity literature. our findings, in agreement with other recent empirical studies, show that fitness plays a crucial role in fostering economic growth and, when it is included in the analysis, can be either complementary to traditional drivers of growth or can completely overshadow them.", "categories": "econ.gn q-fin.ec", "created": "2018-08-30", "updated": "", "authors": ["angelica sbardella", "emanuele pugliese", "andrea zaccaria", "pasquale scaramozzino"], "url": "https://arxiv.org/abs/1808.10428"}, {"title": "uniform inference in high-dimensional gaussian graphical models", "id": "1808.10532", "abstract": "graphical models have become a very popular tool for representing dependencies within a large set of variables and are key for representing causal structures. we provide results for uniform inference on high-dimensional graphical models with the number of target parameters $d$ being possible much larger than sample size. this is in particular important when certain features or structures of a causal model should be recovered. our results highlight how in high-dimensional settings graphical models can be estimated and recovered with modern machine learning methods in complex data sets. to construct simultaneous confidence regions on many target parameters, sufficiently fast estimation rates of the nuisance functions are crucial. in this context, we establish uniform estimation rates and sparsity guarantees of the square-root estimator in a random design under approximate sparsity conditions that might be of independent interest for related problems in high-dimensions. we also demonstrate in a comprehensive simulation study that our procedure has good small sample properties.", "categories": "stat.me cs.lg econ.em stat.ml", "created": "2018-08-30", "updated": "2018-12-03", "authors": ["sven klaassen", "jannis k\u00fcck", "martin spindler", "victor chernozhukov"], "url": "https://arxiv.org/abs/1808.10532"}, {"title": "a self-attention network for hierarchical data structures with an   application to claims management", "id": "1808.10543", "abstract": "insurance companies must manage millions of claims per year. while most of these claims are non-fraudulent, fraud detection is core for insurance companies. the ultimate goal is a predictive model to single out the fraudulent claims and pay out the non-fraudulent ones immediately. modern machine learning methods are well suited for this kind of problem. health care claims often have a data structure that is hierarchical and of variable length. we propose one model based on piecewise feed forward neural networks (deep learning) and another model based on self-attention neural networks for the task of claim management. we show that the proposed methods outperform bag-of-words based models, hand designed features, and models based on convolutional neural networks, on a data set of two million health care claims. the proposed self-attention method performs the best.", "categories": "cs.lg econ.em stat.ml", "created": "2018-08-30", "updated": "", "authors": ["leander l\u00f6w", "martin spindler", "eike brechmann"], "url": "https://arxiv.org/abs/1808.10543"}, {"title": "identifying the discount factor in dynamic discrete choice models", "id": "1808.10651", "abstract": "empirical research often cites observed choice responses to variation that shifts expected discounted future utilities, but not current utilities, as an intuitive source of information on time preferences. we study the identification of dynamic discrete choice models under such economically motivated exclusion restrictions on primitive utilities. we show that each exclusion restriction leads to an easily interpretable moment condition with the discount factor as the only unknown parameter. the identified set of discount factors that solves this condition is finite, but not necessarily a singleton. consequently, in contrast to common intuition, an exclusion restriction does not in general give point identification. finally, we show that exclusion restrictions have nontrivial empirical content: the implied moment conditions impose restrictions on choices that are absent from the unconstrained model.", "categories": "econ.em econ.gn q-fin.ec", "created": "2018-08-31", "updated": "2019-09-16", "authors": ["jaap h. abbring", "\u00f8ystein daljord"], "url": "https://arxiv.org/abs/1808.10651"}, {"title": "repeated coordination with private learning", "id": "1809.00051", "abstract": "we study a repeated game with payoff externalities and observable actions where two players receive information over time about an underlying payoff-relevant state, and strategically coordinate their actions. players learn about the true state from private signals, as well as the actions of others. they commonly learn the true state (cripps et al., 2008), but do not coordinate in every equilibrium. we show that there exist stable equilibria in which players can overcome unfavorable signal realizations and eventually coordinate on the correct action, for any discount factor. for high discount factors, we show that in addition players can also achieve efficient payoffs.", "categories": "econ.th", "created": "2018-08-31", "updated": "", "authors": ["pathikrit basu", "kalyan chatterjee", "tetsuya hoshino", "omer tamuz"], "url": "https://arxiv.org/abs/1809.00051"}, {"title": "finding a promising venture capital project with todim under   probabilistic hesitant fuzzy circumstance", "id": "1809.00128", "abstract": "considering the risk aversion for gains and the risk seeking for losses of venture capitalists, the todim has been chosen as the decision-making method. moreover, group decision is an available way to avoid the limited ability and knowledge etc. of venture capitalists.simultaneously, venture capitalists may be hesitant among several assessed values with different probabilities to express their real perceptionbecause of the uncertain decision-making environment. however, the probabilistic hesitant fuzzy information can solve such problems effectively. therefore, the todim has been extended to probabilistic hesitant fuzzy circumstance for the sake of settling the decision-making problem of venture capitalists in this paper. moreover, due to the uncertain investment environment, the criteria weights are considered as probabilistic hesitant fuzzyinformation as well. then, a case study has been used to verify the feasibility and validity of the proposed todim.also, the todim with hesitant fuzzy information has been carried out to analysis the same case.from the comparative analysis, the superiority of the proposed todim in this paper has already appeared.", "categories": "econ.gn q-fin.ec", "created": "2018-09-01", "updated": "", "authors": ["weike zhang", "jiang du", "xiaoli tian"], "url": "https://arxiv.org/abs/1809.00128"}, {"title": "optimal bandwidth choice for robust bias corrected inference in   regression discontinuity designs", "id": "1809.00236", "abstract": "modern empirical work in regression discontinuity (rd) designs often employs local polynomial estimation and inference with a mean square error (mse) optimal bandwidth choice. this bandwidth yields an mse-optimal rd treatment effect estimator, but is by construction invalid for inference. robust bias corrected (rbc) inference methods are valid when using the mse-optimal bandwidth, but we show they yield suboptimal confidence intervals in terms of coverage error. we establish valid coverage error expansions for rbc confidence interval estimators and use these results to propose new inference-optimal bandwidth choices for forming these intervals. we find that the standard mse-optimal bandwidth for the rd point estimator is too large when the goal is to construct rbc confidence intervals with the smallest coverage error. we further optimize the constant terms behind the coverage error to derive new optimal choices for the auxiliary bandwidth required for rbc inference. our expansions also establish that rbc inference yields higher-order refinements (relative to traditional undersmoothing) in the context of rd designs. our main results cover sharp and sharp kink rd designs under conditional heteroskedasticity, and we discuss extensions to fuzzy and other rd designs, clustered sampling, and pre-intervention covariates adjustments. the theoretical findings are illustrated with a monte carlo experiment and an empirical application, and the main methodological results are available in \\texttt{r} and \\texttt{stata} packages.", "categories": "econ.em stat.me", "created": "2018-09-01", "updated": "2020-01-02", "authors": ["sebastian calonico", "matias d. cattaneo", "max h. farrell"], "url": "https://arxiv.org/abs/1809.00236"}, {"title": "the indirect cost of information", "id": "1809.00697", "abstract": "we study the indirect cost of information from sequential information cost minimization. a key sub-additivity condition, together with monotonicity equivalently characterizes the class of indirect cost functions generated from any direct information cost. adding an extra (uniform) posterior separability condition equivalently characterizes the indirect cost generated from any direct cost favoring incremental evidences. we also provide the necessary and sufficient condition when prior independent direct cost generates posterior separable indirect cost.", "categories": "econ.th", "created": "2018-09-03", "updated": "2020-04-29", "authors": ["weijie zhong"], "url": "https://arxiv.org/abs/1809.00697"}, {"title": "\"read my lips\": using automatic text analysis to classify politicians by   party and ideology", "id": "1809.00741", "abstract": "the increasing digitization of political speech has opened the door to studying a new dimension of political behavior using text analysis. this work investigates the value of word-level statistical data from the us congressional record--which contains the full text of all speeches made in the us congress--for studying the ideological positions and behavior of senators. applying machine learning techniques, we use this data to automatically classify senators according to party, obtaining accuracy in the 70-95% range depending on the specific method used. we also show that using text to predict dw-nominate scores, a common proxy for ideology, does not improve upon these already-successful results. this classification deteriorates when applied to text from sessions of congress that are four or more years removed from the training set, pointing to a need on the part of voters to dynamically update the heuristics they use to evaluate party based on political speech. text-based predictions are less accurate than those based on voting behavior, supporting the theory that roll-call votes represent greater commitment on the part of politicians and are thus a more accurate reflection of their ideological preferences. however, the overall success of the machine learning approaches studied here demonstrates that political speeches are highly predictive of partisan affiliation. in addition to these findings, this work also introduces the computational tools and methods relevant to the use of political speech data.", "categories": "econ.gn cs.cl q-fin.ec", "created": "2018-09-03", "updated": "", "authors": ["eitan sapiro-gheiler"], "url": "https://arxiv.org/abs/1809.00741"}, {"title": "how to model fake news", "id": "1809.00964", "abstract": "over the past three years it has become evident that fake news is a danger to democracy. however, until now there has been no clear understanding of how to define fake news, much less how to model it. this paper addresses both these issues. a definition of fake news is given, and two approaches for the modelling of fake news and its impact in elections and referendums are introduced. the first approach, based on the idea of a representative voter, is shown to be suitable to obtain a qualitative understanding of phenomena associated with fake news at a macroscopic level. the second approach, based on the idea of an election microstructure, describes the collective behaviour of the electorate by modelling the preferences of individual voters. it is shown through a simulation study that the mere knowledge that pieces of fake news may be in circulation goes a long way towards mitigating the impact of fake news.", "categories": "stat.ap cs.it cs.si econ.gn math.it math.pr q-fin.ec", "created": "2018-09-04", "updated": "2018-10-26", "authors": ["dorje c. brody", "david m. meier"], "url": "https://arxiv.org/abs/1809.00964"}, {"title": "shape-enforcing operators for point and interval estimators", "id": "1809.01038", "abstract": "a common problem in econometrics, statistics, and machine learning is to estimate and make inference on functions that satisfy shape restrictions. for example, distribution functions are nondecreasing and range between zero and one, height growth charts are nondecreasing in age, and production functions are nondecreasing and quasi-concave in input quantities. we propose a method to enforce these restrictions ex post on point and interval estimates of the target function by applying functional operators. if an operator satisfies certain properties that we make precise, the shape-enforced point estimates are closer to the target function than the original point estimates and the shape-enforced interval estimates have greater coverage and shorter length than the original interval estimates. we show that these properties hold for six different operators that cover commonly used shape restrictions in practice: range, convexity, monotonicity, monotone convexity, quasi-convexity, and monotone quasi-convexity. we illustrate the results with two empirical applications to the estimation of a height growth chart for infants in india and a production function for chemical firms in china.", "categories": "econ.em stat.me", "created": "2018-09-04", "updated": "2020-05-20", "authors": ["xi chen", "victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "scott kostyshak", "ye luo"], "url": "https://arxiv.org/abs/1809.01038"}, {"title": "multi-agent economics and the emergence of critical markets", "id": "1809.01332", "abstract": "the dual crises of the sub-prime mortgage crisis and the global financial crisis has prompted a call for explanations of non-equilibrium market dynamics. recently a promising approach has been the use of agent based models (abms) to simulate aggregate market dynamics. a key aspect of these models is the endogenous emergence of critical transitions between equilibria, i.e. market collapses, caused by multiple equilibria and changing market parameters. several research themes have developed microeconomic based models that include multiple equilibria: social decision theory (brock and durlauf), quantal response models (mckelvey and palfrey), and strategic complementarities (goldstein). a gap that needs to be filled in the literature is a unified analysis of the relationship between these models and how aggregate criticality emerges from the individual agent level. this article reviews the agent-based foundations of markets starting with the individual agent perspective of mcfadden and the aggregate perspective of catastrophe theory emphasising connections between the different approaches. it is shown that changes in the uncertainty agents have in the value of their interactions with one another, even if these changes are one-sided, plays a central role in systemic market risks such as market instability and the twin crises effect. these interactions can endogenously cause crises that are an emergent phenomena of markets.", "categories": "econ.gn nlin.ao q-fin.cp q-fin.ec", "created": "2018-09-05", "updated": "", "authors": ["michael s. harr\u00e9"], "url": "https://arxiv.org/abs/1809.01332"}, {"title": "the core of an economy with an endogenous social division of labour", "id": "1809.01470", "abstract": "this paper considers the core of a competitive market economy with an endogenous social division of labour. the theory is founded on the notion of a \"consumer-producer\", who consumes as well as produces commodities. first, we show that the core of such an economy with an endogenous social division of labour can be founded on deviations of coalitions of arbitrary size, extending the seminal insights of vind and schmeidler for pure exchange economies. furthermore, we establish the equivalence between the core and the set of competitive equilibria for continuum economies with an endogenous social division of labour. our analysis also concludes that self-organisation in a social division of labour can be incorporated into the edgeworthian barter process directly. this is formulated as a core equivalence result stated for a structured core concept based on renegotiations among fully specialised economic agents, i.e., coalitions that use only fully developed internal divisions of labour. our approach bridges the gap between standard economies with social production and coalition production economies. therefore, a more straightforward and natural interpretation of coalitional improvement and the core can be developed than for coalition production economies.", "categories": "econ.th", "created": "2018-09-05", "updated": "", "authors": ["robert p. gilles"], "url": "https://arxiv.org/abs/1809.01470"}, {"title": "efficient difference-in-differences estimation with high-dimensional   common trend confounding", "id": "1809.01643", "abstract": "this study considers various semiparametric difference-in-differences models under different assumptions on the relation between the treatment group identifier, time and covariates for cross-sectional and panel data. the variance lower bound is shown to be sensitive to the model assumptions imposed implying a robustness-efficiency trade-off. the obtained efficient influence functions lead to estimators that are rate double robust and have desirable asymptotic properties under weak first stage convergence conditions. this enables to use sophisticated machine-learning algorithms that can cope with settings where common trend confounding is high-dimensional. the usefulness of the proposed estimators is assessed in an empirical example. it is shown that the efficiency-robustness trade-offs and the choice of first stage predictors can lead to divergent empirical results in practice.", "categories": "econ.em", "created": "2018-09-05", "updated": "2020-08-14", "authors": ["michael zimmert"], "url": "https://arxiv.org/abs/1809.01643"}, {"title": "change-point testing and estimation for risk measures in time series", "id": "1809.02303", "abstract": "we investigate methods of change-point testing and confidence interval construction for nonparametric estimators of expected shortfall and related risk measures in weakly dependent time series. a key aspect of our work is the ability to detect general multiple structural changes in the tails of time series marginal distributions. unlike extant approaches for detecting tail structural changes using quantities such as tail index, our approach does not require parametric modeling of the tail and detects more general changes in the tail. additionally, our methods are based on the recently introduced self-normalization technique for time series, allowing for statistical analysis without the issues of consistent standard error estimation. the theoretical foundation for our methods are functional central limit theorems, which we develop under weak assumptions. an empirical study of s&p 500 returns and us 30-year treasury bonds illustrates the practical use of our methods in detecting and quantifying market instability via the tails of financial time series during times of financial crisis.", "categories": "econ.em stat.me", "created": "2018-09-07", "updated": "", "authors": ["lin fan", "peter w. glynn", "markus pelger"], "url": "https://arxiv.org/abs/1809.02303"}, {"title": "nash equilibrium of partially asymmetric three-players zero-sum game   with two strategic variables", "id": "1809.02465", "abstract": "we consider a partially asymmetric three-players zero-sum game with two strategic variables. two players (a and b) have the same payoff functions, and player c does not. two strategic variables are $t_i$'s and $s_i$'s for $i=a, b, c$. mainly we will show the following results.   1. the equilibrium when all players choose $t_i$'s is equivalent to the equilibrium when players a and b choose $t_i$'s and player c chooses $s_c$ as their strategic variables. 2. the equilibrium when all players choose $s_i$'s is equivalent to the equilibrium when players a and b choose $s_i$'s and player c chooses $t_c$ as their strategic variables.   the equilibrium when all players choose $t_i$'s and the equilibrium when all players choose $s_i$'s are not equivalent although they are equivalent in a symmetric game in which all players have the same payoff functions.", "categories": "econ.gn q-fin.ec", "created": "2018-09-04", "updated": "", "authors": ["atsuhiro satoh", "yasuhito tanaka"], "url": "https://arxiv.org/abs/1809.02465"}, {"title": "sion's mini-max theorem and nash equilibrium in a five-players game with   two groups which is zero-sum and symmetric in each group", "id": "1809.02466", "abstract": "we consider the relation between sion's minimax theorem for a continuous function and a nash equilibrium in a five-players game with two groups which is zero-sum and symmetric in each group. we will show the following results.   1. the existence of nash equilibrium which is symmetric in each group implies sion's minimax theorem for a pair of playes in each group. 2. sion's minimax theorem for a pair of playes in each group imply the existence of a nash equilibrium which is symmetric in each group.   thus, they are equivalent. an example of such a game is a relative profit maximization game in each group under oligopoly with two groups such that firms in each group have the same cost functions and maximize their relative profits in each group, and the demand functions are symmetric for the firms in each group.", "categories": "econ.gn q-fin.ec", "created": "2018-09-04", "updated": "", "authors": ["atsuhiro satoh", "yasuhito tanaka"], "url": "https://arxiv.org/abs/1809.02466"}, {"title": "worldcoin: a hypothetical cryptocurrency for the people and its   government", "id": "1809.02769", "abstract": "the world of cryptocurrency is not transparent enough though it was established for innate transparent tracking of capital flows. the most contributing factor is the violation of securities laws and scam in initial coin offering (ico) which is used to raise capital through crowdfunding. there is a lack of proper regularization and appreciation from governments around the world which is a serious problem for the integrity of cryptocurrency market. we present a hypothetical case study of a new cryptocurrency to establish the transparency and equal right for every citizen to be part of a global system through the collaboration between people and government. the possible outcome is a model of a regulated and trusted cryptocurrency infrastructure that can be further tailored to different sectors with a different scheme.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2018-09-08", "updated": "", "authors": ["sheikh rabiul islam"], "url": "https://arxiv.org/abs/1809.02769"}, {"title": "a moral framework for understanding of fair ml through economic models   of equality of opportunity", "id": "1809.03400", "abstract": "we map the recently proposed notions of algorithmic fairness to economic models of equality of opportunity (eop)---an extensively studied ideal of fairness in political philosophy. we formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of eop. in this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. last but not least and inspired by luck egalitarian models of eop, we propose a new family of measures for algorithmic fairness. we illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.", "categories": "cs.lg econ.th stat.ml", "created": "2018-09-10", "updated": "2018-11-27", "authors": ["hoda heidari", "michele loi", "krishna p. gummadi", "andreas krause"], "url": "https://arxiv.org/abs/1809.03400"}, {"title": "the ladder theory of behavioral decision making", "id": "1809.03442", "abstract": "we study individual decision-making behavioral on generic view. using a formal mathematical model, we investigate the action mechanism of decision behavioral under subjective perception changing of task attributes. our model is built on work in two kinds classical behavioral decision making theory: \"prospect theory (pt)\" and \"image theory (it)\". we consider subjective attributes preference of decision maker under the whole decision process. strategies collection and selection mechanism are induced according the description of multi-attributes decision making. a novel behavioral decision-making framework named \"ladder theory (lt)\" is proposed. by real four cases comparing, the results shows that the lt have better explanation and prediction ability then pt and it under some decision situations. furthermore, we use our model to shed light on that the lt theory can cover pt and it ideally. it is the enrichment and development for classical behavioral decision theory and, it has positive theoretical value and instructive significance for explaining plenty of real decision-making phenomena. it may facilitate our understanding of how individual decision-making performed actually.", "categories": "econ.gn q-fin.ec", "created": "2018-09-07", "updated": "2018-09-13", "authors": ["xingguang chen"], "url": "https://arxiv.org/abs/1809.03442"}, {"title": "characteristic-sorted portfolios: estimation and inference", "id": "1809.03584", "abstract": "portfolio sorting is ubiquitous in the empirical finance literature, where it has been widely used to identify pricing anomalies. despite its popularity, little attention has been paid to the statistical properties of the procedure. we develop a general framework for portfolio sorting by casting it as a nonparametric estimator. we present valid asymptotic inference methods and a valid mean square error expansion of the estimator leading to an optimal choice for the number of portfolios. in practical settings, the optimal choice may be much larger than the standard choices of 5 or 10. to illustrate the relevance of our results, we revisit the size and momentum anomalies.", "categories": "econ.em econ.gn q-fin.ec stat.me", "created": "2018-09-10", "updated": "2019-10-05", "authors": ["matias d. cattaneo", "richard k. crump", "max h. farrell", "ernst schaumburg"], "url": "https://arxiv.org/abs/1809.03584"}, {"title": "non-asymptotic inference in instrumental variables estimation", "id": "1809.03600", "abstract": "this paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear iv models under weak assumptions. the method is non-asymptotic in the sense that it provides a finite sample bound on the difference between the true and nominal probabilities of rejecting a correct null hypothesis. the method is a non-studentized version of the anderson-rubin test but is motivated and analyzed differently. in contrast to the conventional anderson-rubin test, the method proposed here does not require restrictive distributional assumptions, linearity of the estimated model, or simultaneous equations. nor does it require knowledge of whether the instruments are strong or weak. it does not require testing or estimating the strength of the instruments. the method can be applied to quantile iv models that may be nonlinear and can be used to test a parametric iv model against a nonparametric alternative. the results presented here hold in finite samples, regardless of the strength of the instruments.", "categories": "econ.em math.st stat.th", "created": "2018-09-10", "updated": "", "authors": ["joel l. horowitz"], "url": "https://arxiv.org/abs/1809.03600"}, {"title": "house price modeling with digital census", "id": "1809.03834", "abstract": "urban house prices are strongly associated with local socioeconomic factors. in literature, house price modeling is based on socioeconomic variables from traditional census, which is not real-time, dynamic and comprehensive. inspired by the emerging concept of \"digital census\" - using large-scale digital records of human activities to measure urban population dynamics and socioeconomic conditions, we introduce three typical datasets, namely 311 complaints, crime complaints and taxi trips, into house price modeling. based on the individual housing sales data in new york city, we provide comprehensive evidence that these digital census datasets can substantially improve the modeling performances on both house price levels and changes, regardless whether traditional census is included or not. hence, digital census can serve as both effective alternatives and complements to traditional census for house price modeling.", "categories": "econ.em physics.soc-ph", "created": "2018-08-29", "updated": "", "authors": ["enwei zhu", "stanislav sobolevsky"], "url": "https://arxiv.org/abs/1809.03834"}, {"title": "nash equilibria in the response strategy of correlated games", "id": "1809.03860", "abstract": "in nature and society problems arise when different interests are difficult to reconcile, which are modeled in game theory. while most applications assume uncorrelated games, a more detailed modeling is necessary to consider the correlations that influence the decisions of the players. the current theory for correlated games, however, enforces the players to obey the instructions from a third party or \"correlation device\" to reach equilibrium, but this cannot be achieved for all initial correlations. we extend here the existing framework of correlated games and find that there are other interesting and previously unknown nash equilibria that make use of correlations to obtain the best payoff. this is achieved by allowing the players the freedom to follow or not to follow the suggestions of the correlation device. by assigning independent probabilities to follow every possible suggestion, the players engage in a response game that turns out to have a rich structure of nash equilibria that goes beyond the correlated equilibrium and mixed-strategy solutions. we determine the nash equilibria for all possible correlated snowdrift games, which we find to be describable by ising models in thermal equilibrium. we believe that our approach paves the way to a study of correlations in games that uncovers the existence of interesting underlying interaction mechanisms, without compromising the independence of the players.", "categories": "physics.soc-ph cs.gt econ.th q-bio.pe", "created": "2018-09-07", "updated": "", "authors": ["a. d. correia", "h. t. c. stoof"], "url": "https://arxiv.org/abs/1809.03860"}, {"title": "regression discontinuity designs using covariates", "id": "1809.03904", "abstract": "we study regression discontinuity designs when covariates are included in the estimation. we examine local polynomial estimators that include discrete or continuous covariates in an additive separable way, but without imposing any parametric restrictions on the underlying population regression functions. we recommend a covariate-adjustment approach that retains consistency under intuitive conditions, and characterize the potential for estimation and inference improvements. we also present new covariate-adjusted mean squared error expansions and robust bias-corrected inference procedures, with heteroskedasticity-consistent and cluster-robust standard errors. an empirical illustration and an extensive simulation study is presented. all methods are implemented in \\texttt{r} and \\texttt{stata} software packages.", "categories": "econ.em stat.me", "created": "2018-09-11", "updated": "", "authors": ["sebastian calonico", "matias d. cattaneo", "max h. farrell", "rocio titiunik"], "url": "https://arxiv.org/abs/1809.03904"}, {"title": "an alternative quality of life ranking on the basis of remittances", "id": "1809.03977", "abstract": "remittances mean an important connection between people working abroad and their home countries. this paper considers these transfers as a measure of preferences revealed by the workers, underlying a ranking of countries around the world. we use the world bank bilateral remittances data of international salaries and interpersonal transfers between 2010 and 2015 to compare european countries. the suggested least squares method implies that the ranking is invariant to country sizes and satisfies the axiom of bridge country independence. our ranking reveals a crucial aspect of quality of life and may become an alternative to various composite indices.", "categories": "econ.gn q-fin.ec", "created": "2018-09-11", "updated": "2019-11-12", "authors": ["d\u00f3ra gr\u00e9ta petr\u00f3czy"], "url": "https://arxiv.org/abs/1809.03977"}, {"title": "bootstrap methods in econometrics", "id": "1809.04016", "abstract": "the bootstrap is a method for estimating the distribution of an estimator or test statistic by re-sampling the data or a model estimated from the data. under conditions that hold in a wide variety of econometric applications, the bootstrap provides approximations to distributions of statistics, coverage probabilities of confidence intervals, and rejection probabilities of hypothesis tests that are more accurate than the approximations of first-order asymptotic distribution theory. the reductions in the differences between true and nominal coverage or rejection probabilities can be very large. in addition, the bootstrap provides a way to carry out inference in certain settings where obtaining analytic distributional approximations is difficult or impossible. this article explains the usefulness and limitations of the bootstrap in contexts of interest in econometrics. the presentation is informal and expository. it provides an intuitive understanding of how the bootstrap works. mathematical details are available in references that are cited.", "categories": "econ.em", "created": "2018-09-11", "updated": "", "authors": ["joel l. horowitz"], "url": "https://arxiv.org/abs/1809.04016"}, {"title": "a note on contests with a constrained choice set of effort", "id": "1809.04436", "abstract": "we consider a symmetric two-player contest, in which the choice set of effort is constrained. we apply a fundamental property of the payoff function to show that, under standard assumptions, there exists a unique nash equilibrium in pure strategies. it is shown that all equilibria are near the unconstrained equilibrium. perhaps surprisingly, this is not the case when players have different prize evaluations.", "categories": "econ.th", "created": "2018-09-12", "updated": "2020-09-13", "authors": ["doron klunover", "john morgan"], "url": "https://arxiv.org/abs/1809.04436"}, {"title": "bayesian shrinkage in mixture of experts models: identifying robust   determinants of class membership", "id": "1809.04853", "abstract": "a method for implicit variable selection in mixture of experts frameworks is proposed. we introduce a prior structure where information is taken from a set of independent covariates. robust class membership predictors are identified using a normal gamma prior. the resulting model setup is used in a finite mixture of bernoulli distributions to find homogenous clusters of women in mozambique based on their information sources on hiv. fully bayesian inference is carried out via the implementation of a gibbs sampler.", "categories": "econ.em", "created": "2018-09-13", "updated": "2019-01-12", "authors": ["gregor zens"], "url": "https://arxiv.org/abs/1809.04853"}, {"title": "valid simultaneous inference in high-dimensional settings (with the hdm   package for r)", "id": "1809.04951", "abstract": "due to the increasing availability of high-dimensional empirical applications in many research disciplines, valid simultaneous inference becomes more and more important. for instance, high-dimensional settings might arise in economic studies due to very rich data sets with many potential covariates or in the analysis of treatment heterogeneities. also the evaluation of potentially more complicated (non-linear) functional forms of the regression relationship leads to many potential variables for which simultaneous inferential statements might be of interest. here we provide a review of classical and modern methods for simultaneous inference in (high-dimensional) settings and illustrate their use by a case study using the r package hdm. the r package hdm implements valid joint powerful and efficient hypothesis tests for a potentially large number of coeffcients as well as the construction of simultaneous confidence intervals and, therefore, provides useful methods to perform valid post-selection inference based on the lasso.", "categories": "econ.em stat.ml", "created": "2018-09-13", "updated": "", "authors": ["philipp bach", "victor chernozhukov", "martin spindler"], "url": "https://arxiv.org/abs/1809.04951"}, {"title": "time preference and information acquisition", "id": "1809.05120", "abstract": "i consider the sequential implementation of a target information structure. i characterize the set of decision time distributions induced by all signal processes that satisfy a per-period learning capacity constraint. i find that all decision time distributions have the same expectation, and the maximal and minimal elements by mean-preserving spread order are deterministic distribution and exponential distribution. the result implies that when time preference is risk loving (e.g. standard or hyperbolic discounting), poisson signal is optimal since it induces the most risky exponential decision time distribution. when time preference is risk neutral (e.g. constant delay cost), all signal processes are equally optimal.", "categories": "econ.th", "created": "2018-09-13", "updated": "2018-10-20", "authors": ["weijie zhong"], "url": "https://arxiv.org/abs/1809.05120"}, {"title": "automatic debiased machine learning of causal and structural effects", "id": "1809.05224", "abstract": "many causal and structural effects depend on regressions. examples include average treatment effects, policy effects, average derivatives, regression decompositions, economic average equivalent variation, and parameters of economic structural models. the regressions may be high dimensional. plugging machine learners into identifying equations can lead to poor inference due to bias and/or model selection. this paper gives automatic debiasing for estimating equations and valid asymptotic inference for the estimators of effects of interest. the debiasing is automatic in that its construction uses the identifying equations without the full form of the bias correction and is performed by machine learning. novel results include convergence rates for lasso and dantzig learners of the bias correction, primitive conditions for asymptotic inference for important examples, and general conditions for gmm. a variety of regression learners and identifying equations are covered. automatic debiased machine learning (auto-dml) is applied to estimating the average treatment effect on the treated for the nsw job training data and to estimating demand elasticities from nielsen scanner data while allowing preferences to be correlated with prices and income.", "categories": "math.st econ.em stat.th", "created": "2018-09-13", "updated": "2020-07-11", "authors": ["victor chernozhukov", "whitney k newey", "rahul singh"], "url": "https://arxiv.org/abs/1809.05224"}, {"title": "on the choice of instruments in mixed frequency specification tests", "id": "1809.05503", "abstract": "time averaging has been the traditional approach to handle mixed sampling frequencies. however, it ignores information possibly embedded in high frequency. mixed data sampling (midas) regression models provide a concise way to utilize the additional information in high-frequency variables. in this paper, we propose a specification test to choose between time averaging and midas models, based on a durbin-wu-hausman test. in particular, a set of instrumental variables is proposed and theoretically validated when the frequency ratio is large. as a result, our method tends to be more powerful than existing methods, as reconfirmed through the simulations.", "categories": "econ.em", "created": "2018-09-14", "updated": "", "authors": ["yun liu", "yeonwoo rho"], "url": "https://arxiv.org/abs/1809.05503"}, {"title": "control variables, discrete instruments, and identification of   structural functions", "id": "1809.05706", "abstract": "control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. we allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. we consider many structural objects of interest, such as average or quantile treatment effects. we illustrate our results with an empirical application to engel curve estimation.", "categories": "econ.em stat.me", "created": "2018-09-15", "updated": "2019-12-05", "authors": ["whitney newey", "sami stouli"], "url": "https://arxiv.org/abs/1809.05706"}, {"title": "trends in the diffusion of misinformation on social media", "id": "1809.05901", "abstract": "we measure trends in the diffusion of misinformation on facebook and twitter between january 2015 and july 2018. we focus on stories from 570 sites that have been identified as producers of false stories. interactions with these sites on both facebook and twitter rose steadily through the end of 2016. interactions then fell sharply on facebook while they continued to rise on twitter, with the ratio of facebook engagements to twitter shares falling by approximately 60 percent. we see no similar pattern for other news, business, or culture sites, where interactions have been relatively stable over time and have followed similar trends on the two platforms both before and after the election.", "categories": "cs.si econ.gn q-fin.ec", "created": "2018-09-16", "updated": "", "authors": ["hunt allcott", "matthew gentzkow", "chuan yu"], "url": "https://arxiv.org/abs/1809.05901"}, {"title": "a flexible design for funding public goods", "id": "1809.06421", "abstract": "we propose a design for philanthropic or publicly-funded seeding to allow (near) optimal provision of a decentralized, self-organizing ecosystem of public goods. the concept extends ideas from quadratic voting to a funding mechanism for endogenous community formation. individuals make public goods contributions to projects of value to them. the amount received by the project is (proportional to) the square of the sum of the square roots of contributions received. under the \"standard model\" this yields first best public goods provision. variations can limit the cost, help protect against collusion and aid coordination. we discuss applications to campaign finance, open source software ecosystems, news media finance and urban public projects. more broadly, we offer a resolution to the classic liberal-communitarian debate in political philosophy by providing neutral and non-authoritarian rules that nonetheless support collective organization.", "categories": "econ.gn q-fin.ec", "created": "2018-09-17", "updated": "2020-08-16", "authors": ["vitalik buterin", "zoe hitzig", "e. glen weyl"], "url": "https://arxiv.org/abs/1809.06421"}, {"title": "estimating grouped data models with a binary dependent variable and   fixed effects: what are the issues", "id": "1809.06505", "abstract": "this article deals with asimple issue: if we have grouped data with a binary dependent variable and want to include fixed effects (group specific intercepts) in the specification, is ordinary least squares (ols) in any way superior to a (conditional) logit form? in particular, what are the consequences of using ols instead of a fixed effects logit model with respect to the latter dropping all units which show no variability in the dependent variable while the former allows for estimation using all units. first, we show that the discussion of fthe incidental parameters problem is based on an assumption about the kinds of data being studied; for what appears to be the common use of fixed effect models in political science the incidental parameters issue is illusory. turning to linear models, we see that ols yields a linear combination of the estimates for the units with and without variation in the dependent variable, and so the coefficient estimates must be carefully interpreted. the article then compares two methods of estimating logit models with fixed effects, and shows that the chamberlain conditional logit is as good as or better than a logit analysis which simply includes group specific intercepts (even though the conditional logit technique was designed to deal with the incidental parameters problem!). related to this, the article discusses the estimation of marginal effects using both ols and logit. while it appears that a form of logit with fixed effects can be used to estimate marginal effects, this method can be improved by starting with conditional logit and then using the those parameter estimates to constrain the logit with fixed effects model. this method produces estimates of sample average marginal effects that are at least as good as ols, and much better when group size is small or the number of groups is large. .", "categories": "econ.em", "created": "2018-09-17", "updated": "", "authors": ["nathaniel beck"], "url": "https://arxiv.org/abs/1809.06505"}, {"title": "the structure of the environment and the complexity of rational choice   procedures", "id": "1809.06766", "abstract": "beginning with herbert simon [10], the literature on bounded rationality has investigated in great detail how internal limitations affect an agent's choice process. the structure of the choice environment, deemed as important as internal limitations by simon [11], has been mostly ignored. we introduce a model of the environment and its interaction with an agent's choice process. focusing on online environments where an agent can use filter and sort functionality to support his decision-making, we show, a choice process relying on the environment can be rationalized. moreover, for sufficiently many alternatives, filtering and sorting are quick ways to choose rationally.", "categories": "econ.th", "created": "2018-09-18", "updated": "2018-09-25", "authors": ["paulo oliva", "philipp zahn"], "url": "https://arxiv.org/abs/1809.06766"}, {"title": "selling information", "id": "1809.06770", "abstract": "i consider the monopolistic pricing of informational good. a buyer's willingness to pay for information is from inferring the unknown payoffs of actions in decision making. a monopolistic seller and the buyer each observes a private signal about the payoffs. the seller's signal is binary and she can commit to sell any statistical experiment of her signal to the buyer. assuming that buyer's decision problem involves rich actions, i characterize the profit maximizing menu. it contains a continuum of experiments, each containing different amount of information. i also find a complementarity between buyer's private information and information provision: when buyer's private signal is more informative, the optimal menu contains more informative experiments.", "categories": "econ.th", "created": "2018-09-18", "updated": "2018-10-16", "authors": ["weijie zhong"], "url": "https://arxiv.org/abs/1809.06770"}, {"title": "matching in dynamic imbalanced markets", "id": "1809.06824", "abstract": "we study dynamic matching in exchange markets with easy- and hard-to-match agents. a greedy policy, which attempts to match agents upon arrival, ignores the positive externality that waiting agents generate by facilitating future matchings. we prove that this trade-off between a ``thicker'' market and faster matching vanishes in large markets; a greedy policy leads to shorter waiting times, and more agents matched than any other policy. we empirically confirm these findings in data from the national kidney registry. greedy matching achieves as many transplants as commonly-used policies (1.6\\% more than monthly-batching), and shorter patient waiting times.", "categories": "econ.th", "created": "2018-09-18", "updated": "2019-06-13", "authors": ["itai ashlagi", "afshin nikzad", "philipp strack"], "url": "https://arxiv.org/abs/1809.06824"}, {"title": "focused econometric estimation for noisy and small datasets: a bayesian   minimum expected loss estimator approach", "id": "1809.06996", "abstract": "central to many inferential situations is the estimation of rational functions of parameters. the mainstream in statistics and econometrics estimates these quantities based on the plug-in approach without consideration of the main objective of the inferential situation. we propose the bayesian minimum expected loss (melo) approach focusing explicitly on the function of interest, and calculating its frequentist variability. asymptotic properties of the melo estimator are similar to the plug-in approach. nevertheless, simulation exercises show that our proposal is better in situations characterized by small sample sizes and noisy models. in addition, we observe in the applications that our approach gives lower standard errors than frequently used alternatives when datasets are not very informative.", "categories": "econ.em stat.me", "created": "2018-09-18", "updated": "", "authors": ["andres ramirez-hassan", "manuel correa-giraldo"], "url": "https://arxiv.org/abs/1809.06996"}, {"title": "transmission of macroeconomic shocks to risk parameters: their uses in   stress testing", "id": "1809.07401", "abstract": "in this paper, we are interested in evaluating the resilience of financial portfolios under extreme economic conditions. therefore, we use empirical measures to characterize the transmission process of macroeconomic shocks to risk parameters. we propose the use of an extensive family of models, called general transfer function models, which condense well the characteristics of the transmission described by the impact measures. the procedure for estimating the parameters of these models is described employing the bayesian approach and using the prior information provided by the impact measures. in addition, we illustrate the use of the estimated models from the credit risk data of a portfolio.", "categories": "stat.ap econ.em stat.ot", "created": "2018-09-19", "updated": "2019-05-17", "authors": ["helder rojas", "david dias"], "url": "https://arxiv.org/abs/1809.07401"}, {"title": "shapley-like values without symmetry", "id": "1809.07747", "abstract": "following the work of lloyd shapley on the shapley value, and tangentially the work of guillermo owen, we offer an alternative non-probabilistic formulation of part of the work of robert j. weber in his 1978 paper \"probabilistic values for games.\" specifically, we focus upon efficient but not symmetric allocations of value for cooperative games. we retain standard efficiency and linearity, and offer an alternative condition, \"reasonableness,\" to replace the other usual axioms. in the pursuit of the result, we discover properties of the linear maps that describe the allocations. this culminates in a special class of games for which any other map that is \"reasonable, efficient\" can be written as a convex combination of members of this special class of allocations, via an application of the krein-milman theorem.", "categories": "econ.th math.co", "created": "2018-09-20", "updated": "2019-05-10", "authors": ["jacob north clark", "stephen montgomery-smith"], "url": "https://arxiv.org/abs/1809.07747"}, {"title": "the \"power\" dimension in a process of exchange", "id": "1809.08293", "abstract": "the field of study of this paper is the analysis of the exchange between two subjects. circumscribed to the micro dimension, it is however expanded with respect to standard economic theory by introducing both the dimension of power and the motivation to exchange. the basic reference is made by the reflections of those economists, preeminently john kenneth galbraith, who criticize the removal from the neoclassical economy of the \"power\" dimension. we have also referred to the criticism that galbraith, among others, makes to the assumption of neoclassical economists that the \"motivation\" in exchanges is solely linked to the reward, to the money obtained in the exchange. we have got around the problem of having a large number of types of power and also a large number of forms of motivation by directly taking into account the effects on the welfare of each subject, regardless of the means with which they are achieved: that is, referring to everything that happens in the negotiation process to the potential or real variations of the welfare function induced in each subject due to the exercise of the specific form of power, on a case by case basis, and of the intensity of the motivation to perform the exchange. in the construction of a mathematical model we paid great attention to its usability in field testing.", "categories": "econ.gn q-fin.ec", "created": "2018-09-21", "updated": "2019-11-12", "authors": ["alberto banterle"], "url": "https://arxiv.org/abs/1809.08293"}, {"title": "eliciting the endowment effect under assigned ownership", "id": "1809.08500", "abstract": "in this study we present evidence that endowment effect can be elicited merely by assigned ownership. using google customer survey, we administered a survey were participants (n=495) were randomly split into 4 groups. each group was assigned ownership of either legroom or their ability to recline on an airline. using this experiment setup we were able to generate endowment effect, a 15-20x (at p<0.05) increase between participant's willingness to pay (wtp) and their willingness to accept (wta).", "categories": "econ.gn q-fin.ec", "created": "2018-09-22", "updated": "2018-10-04", "authors": ["patrick barranger", "rohit nair", "rob mulla", "shane conner"], "url": "https://arxiv.org/abs/1809.08500"}, {"title": "financial accumulation implies ever-increasing wealth inequality", "id": "1809.08681", "abstract": "wealth inequality is an important matter for economic theory and policy. ongoing debates have been discussing recent rise in wealth inequality in connection with recent development of active financial markets around the world. existing literature on wealth distribution connects the origins of wealth inequality with a variety of drivers. our approach develops a minimalist modelling strategy that combines three featuring mechanisms: active financial markets; individual wealth accumulation; and compound interest structure. we provide mathematical proof that accumulated financial investment returns involve ever-increasing wealth concentration and inequality across individual investors through time. this cumulative effect through space and time depends on the financial accumulation process and holds also under efficient financial markets, which generate some fair investment game that individual investors do repeatedly play through time.", "categories": "q-fin.gn econ.gn physics.soc-ph q-fin.ec q-fin.st", "created": "2018-09-23", "updated": "2019-11-26", "authors": ["yuri biondi", "stefano olla"], "url": "https://arxiv.org/abs/1809.08681"}, {"title": "central bank communication and the yield curve: a semi-automatic   approach using non-negative matrix factorization", "id": "1809.08718", "abstract": "communication is now a standard tool in the central bank's monetary policy toolkit. theoretically, communication provides the central bank an opportunity to guide public expectations, and it has been shown empirically that central bank communication can lead to financial market fluctuations. however, there has been little research into which dimensions or topics of information are most important in causing these fluctuations. we develop a semi-automatic methodology that summarizes the fomc statements into its main themes, automatically selects the best model based on coherency, and assesses whether there is a significant impact of these themes on the shape of the u.s treasury yield curve using topic modeling methods from the machine learning literature. our findings suggest that the fomc statements can be decomposed into three topics: (i) information related to the economic conditions and the mandates, (ii) information related to monetary policy tools and intermediate targets, and (iii) information related to financial markets and the financial crisis. we find that statements are most influential during the financial crisis and the effects are mostly present in the curvature of the yield curve through information related to the financial theme.", "categories": "econ.gn cs.cl q-fin.ec", "created": "2018-09-23", "updated": "", "authors": ["ancil crayton"], "url": "https://arxiv.org/abs/1809.08718"}, {"title": "an automated approach towards sparse single-equation cointegration   modelling", "id": "1809.08889", "abstract": "in this paper we propose the single-equation penalized error correction selector (specs) as an automated estimation procedure for dynamic single-equation models with a large number of potentially (co)integrated variables. by extending the classical single-equation error correction model, specs enables the researcher to model large cointegrated datasets without necessitating any form of pre-testing for the order of integration or cointegrating rank. under an asymptotic regime in which both the number of parameters and time series observations jointly diverge to infinity, we show that specs is able to consistently estimate an appropriate linear combination of the cointegrating vectors that may occur in the underlying dgp. in addition, specs is shown to enable the correct recovery of sparsity patterns in the parameter space and to posses the same limiting distribution as the ols oracle procedure. a simulation study shows strong selective capabilities, as well as superior predictive performance in the context of nowcasting compared to high-dimensional models that ignore cointegration. an empirical application to nowcasting dutch unemployment rates using google trends confirms the strong practical performance of our procedure.", "categories": "econ.em stat.me", "created": "2018-09-24", "updated": "2020-07-22", "authors": ["stephan smeekes", "etienne wijler"], "url": "https://arxiv.org/abs/1809.08889"}, {"title": "on a gap between rational annuitization price for producer and price for   customer", "id": "1809.08960", "abstract": "the paper studies pricing of insurance products focusing on the pricing of annuities under uncertainty. this pricing problem is crucial for financial decision making and was studied intensively, however, many open questions still remain. in particular, there is a so-called \"annuity puzzle\" related to certain inconsistency of existing financial theory with the empirical observations for the annuities market. the paper suggests a pricing method based on the risk minimization such that both producer and customer seek to minimize the mean square hedging error accepted as a measure of risk. this leads to two different versions of the pricing problem: the selection of the annuity price given the rate of regular payments, and the selection of the rate of payments given the annuity price. it appears that solutions of these two problems are different. this can contribute to explanation for the \"annuity puzzle\".", "categories": "econ.gn q-fin.ec q-fin.mf", "created": "2018-09-24", "updated": "", "authors": ["nikolai dokuchaev"], "url": "https://arxiv.org/abs/1809.08960"}, {"title": "mostly harmless simulations? using monte carlo studies for estimator   selection", "id": "1809.09527", "abstract": "we consider two recent suggestions for how to perform an empirically motivated monte carlo study to help select a treatment effect estimator under unconfoundedness. we show theoretically that neither is likely to be informative except under restrictive conditions that are unlikely to be satisfied in many contexts. to test empirical relevance, we also apply the approaches to a real-world setting where estimator performance is known. both approaches are worse than random at selecting estimators which minimise absolute bias. they are better when selecting estimators that minimise mean squared error. however, using a simple bootstrap is at least as good and often better. for now researchers would be best advised to use a range of estimators and compare estimates for robustness.", "categories": "econ.em stat.me stat.ot", "created": "2018-09-25", "updated": "2019-04-17", "authors": ["arun advani", "toru kitagawa", "tymon s\u0142oczy\u0144ski"], "url": "https://arxiv.org/abs/1809.09527"}, {"title": "a big data based method for pass rates optimization in mathematics   university lower division courses", "id": "1809.09724", "abstract": "in this paper an algorithm designed for large databases is introduced for the enhancement of pass rates in mathematical university lower division courses with several sections. using integer programming techniques, the algorithm finds the optimal pairing of students and lecturers in order to maximize the success chances of the students' body. the students-lecturer success probability is computed according to their corresponding profiles stored in the data bases.", "categories": "econ.gn cs.dm math.oc q-fin.ec", "created": "2018-09-18", "updated": "2020-10-07", "authors": ["fernando a morales", "cristian c chica", "carlos a osorio", "daniel cabarcas j"], "url": "https://arxiv.org/abs/1809.09724"}, {"title": "extended opportunity cost model to find near equilibrium electricity   prices under non-convexities", "id": "1809.09734", "abstract": "this paper finds near equilibrium prices for electricity markets with nonconvexities due to binary variables, in order to reduce the market participants' opportunity costs, such as generators' unrecovered costs. the opportunity cost is defined as the difference between the profit when the instructions of the market operator are followed and when the market participants can freely make their own decisions based on the market prices. we use the minimum complementarity approximation to the minimum total opportunity cost (mtoc) model, from previous research, with tests on a much more realistic unit commitment (uc) model than in previous research, including features such as reserve requirements, ramping constraints, and minimum up and down times. the developed model incorporates flexible price responsive demand, as in previous research, but since not all demand is price responsive, we consider the more realistic case that total demand is a mixture of fixed and flexible. another improvement over previous mtoc research is computational: whereas the previous research had nonconvex terms among the objective function's continuous variables, we convert the objective to an equivalent form that contains only linear and convex quadratic terms in the continuous variables. we compare the unit commitment model with the standard social welfare optimization version of uc, in a series of sensitivity analyses, varying flexible demand to represent varying degrees of future penetration of electric vehicles and smart appliances, different ratios of generation availability, and different values of transmission line capacities to consider possible congestion. the minimum total opportunity cost and social welfare solutions are mostly very close in different scenarios, except in some extreme cases.", "categories": "econ.gn q-fin.ec", "created": "2018-09-25", "updated": "", "authors": ["hassan shavandi", "mehrdad pirnia", "j. david fuller"], "url": "https://arxiv.org/abs/1809.09734"}, {"title": "multivariate stochastic volatility model with realized volatilities and   pairwise realized correlations", "id": "1809.09928", "abstract": "although stochastic volatility and garch (generalized autoregressive conditional heteroscedasticity) models have successfully described the volatility dynamics of univariate asset returns, extending them to the multivariate models with dynamic correlations has been difficult due to several major problems. first, there are too many parameters to estimate if available data are only daily returns, which results in unstable estimates. one solution to this problem is to incorporate additional observations based on intraday asset returns, such as realized covariances. second, since multivariate asset returns are not synchronously traded, we have to use the largest time intervals such that all asset returns are observed in order to compute the realized covariance matrices. however, in this study, we fail to make full use of the available intraday informations when there are less frequently traded assets. third, it is not straightforward to guarantee that the estimated (and the realized) covariance matrices are positive definite. our contributions are the following: (1) we obtain the stable parameter estimates for the dynamic correlation models using the realized measures, (2) we make full use of intraday informations by using pairwise realized correlations, (3) the covariance matrices are guaranteed to be positive definite, (4) we avoid the arbitrariness of the ordering of asset returns, (5) we propose the flexible correlation structure model (e.g., such as setting some correlations to be zero if necessary), and (6) the parsimonious specification for the leverage effect is proposed. our proposed models are applied to the daily returns of nine u.s. stocks with their realized volatilities and pairwise realized correlations and are shown to outperform the existing models with respect to portfolio performances.", "categories": "econ.em stat.co", "created": "2018-09-26", "updated": "2019-03-13", "authors": ["yuta yamauchi", "yasuhiro omori"], "url": "https://arxiv.org/abs/1809.09928"}, {"title": "deep neural networks for estimation and inference", "id": "1809.09953", "abstract": "we study deep neural networks and their use in semiparametric inference. we establish novel rates of convergence for deep feedforward neural nets. our new rates are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear unit activation function and a depth explicitly diverging with the sample size. we discuss other architectures as well, including fixed-width, very deep networks. we establish nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. we then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment effects, expected welfare, and decomposition effects. inference in many other semiparametric contexts can be readily obtained. we demonstrate the effectiveness of deep learning with a monte carlo analysis and an empirical application to direct mail marketing.", "categories": "econ.em cs.lg math.st stat.ml stat.th", "created": "2018-09-26", "updated": "2019-09-18", "authors": ["max h. farrell", "tengyuan liang", "sanjog misra"], "url": "https://arxiv.org/abs/1809.09953"}, {"title": "some nontrivial properties of a formula for compound interest", "id": "1809.10566", "abstract": "we analyze the classical model of compound interest with a constant per-period payment and interest rate. we examine the outstanding balance function as well as the periodic payment function and show that the outstanding balance function is not generally concave in the interest rate, but instead may be initially convex on its domain and then concave.", "categories": "econ.gn q-fin.ec q-fin.gn q-fin.mf q-fin.pm", "created": "2018-09-27", "updated": "", "authors": ["isaac m. sonin", "mark whitmeyer"], "url": "https://arxiv.org/abs/1809.10566"}, {"title": "influence of introducing high speed railways on intercity travel   behavior in vietnam", "id": "1810.00155", "abstract": "it is one of hottest topics in vietnam whether to construct a high speed rail (hsr) system or not in near future. to analyze the impacts of introducing the hsr on the intercity travel behavior, this research develops an integrated intercity demand forecasting model to represent trip generation and frequency, destination choice and travel mode choice behavior. for this purpose, a comprehensive questionnaire survey with both revealed preference (rp) information (an inter-city trip diary) and stated preference (sp) information was conducted in hanoi in 2011. in the sp part, not only hsr, but also low cost carrier is included in the choice set, together with other existing inter-city travel modes. to make full use of the advantages of each type of data and to overcome their disadvantages, rp and sp data are combined to describe the destination choice and mode choice behavior, while trip generation and frequency are represented by using the rp data. the model estimation results show the inter-relationship between trip generation and frequency, destination choice and travel mode choice, and confirm that those components should not dealt with separately.", "categories": "econ.gn q-fin.ec", "created": "2018-09-29", "updated": "", "authors": ["tho v. le", "junyi zhang", "makoto chikaraishi", "akimasa fujiwara"], "url": "https://arxiv.org/abs/1810.00155"}, {"title": "a new form of banking -- concept and mathematical model of venture   banking", "id": "1810.00516", "abstract": "this model contains concept, equations, and graphical results for venture banking. a system of 27 equations describes the behavior of the venture-bank and underwriter system allowing phase-space type graphs that show where profits and losses occur. these results confirm and expand those obtained from the original spreadsheet based model. an example investment in a castle at a loss is provided to clarify concept. this model requires that all investments are in enterprises that create new utility value. the assessed utility value created is the new money out of which the venture bank and underwriter are paid. the model presented chooses parameters that ensure that the venture-bank experiences losses before the underwriter does. parameters are: din premium, 0.05; clawback lien fraction, 0.77; clawback bonds and equity futures discount, 1.5 x (usa 12 month libor); range of clawback bonds sold, 0 to 100%; range of equity futures sold 0 to 70%.", "categories": "econ.gn q-fin.ec", "created": "2018-09-30", "updated": "2020-01-28", "authors": ["brian p hanley"], "url": "https://arxiv.org/abs/1810.00516"}, {"title": "selectivity correction in discrete-continuous models for the willingness   to work as crowd-shippers and travel time tolerance", "id": "1810.00985", "abstract": "the objective of this study is to understand the different behavioral considerations that govern the choice of people to engage in a crowd-shipping market. using novel data collected by the researchers in the us, we develop discrete-continuous models. a binary logit model has been used to estimate crowd-shippers' willingness to work, and an ordinary least-square regression model has been employed to calculate crowd-shippers' maximum tolerance for shipping and delivery times. a selectivity-bias term has been included in the model to correct for the conditional relationships of the crowd-shipper's willingness to work and their maximum travel time tolerance. the results show socio-demographic characteristics (e.g. age, gender, race, income, and education level), transporting freight experience, and number of social media usages significant influence the decision to participate in the crowd-shipping market. in addition, crowd-shippers pay expectations were found to be reasonable and concurrent with the literature on value-of-time. findings from this research are helpful for crowd-shipping companies to identify and attract potential shippers. in addition, an understanding of crowd-shippers - their behaviors, perceptions, demographics, pay expectations, and in which contexts they are willing to divert from their route - are valuable to the development of business strategies such as matching criteria and compensation schemes for driver-partners.", "categories": "econ.gn q-fin.ec", "created": "2018-10-01", "updated": "", "authors": ["tho v. le", "satish v. ukkusuri"], "url": "https://arxiv.org/abs/1810.00985"}, {"title": "covariate distribution balance via propensity scores", "id": "1810.01370", "abstract": "this paper proposes new estimators for the propensity score that aim to maximize the covariate distribution balance among different treatment groups. heuristically, our proposed procedure attempts to estimate a propensity score model by making the underlying covariate distribution of different treatment groups as close to each other as possible. our estimators are data-driven, do not rely on tuning parameters such as bandwidths, admit an asymptotic linear representation, and can be used to estimate different treatment effect parameters under different identifying assumptions, including unconfoundedness and local treatment effects. we derive the asymptotic properties of inverse probability weighted estimators for the average, distributional, and quantile treatment effects based on the proposed propensity score estimator and illustrate their finite sample performance via monte carlo simulations and two empirical applications.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-10-02", "updated": "2020-04-03", "authors": ["pedro h. c. sant'anna", "xiaojun song", "qi xu"], "url": "https://arxiv.org/abs/1810.01370"}, {"title": "interpreting ols estimands when treatment effects are heterogeneous:   smaller groups get larger weights", "id": "1810.01576", "abstract": "applied work often studies the effect of a binary variable (\"treatment\") using linear models with additive effects. i study the interpretation of the ols estimands in such models when treatment effects are heterogeneous. i show that the treatment coefficient is a convex combination of two parameters, which under certain conditions can be interpreted as the average treatment effects on the treated and untreated. the weights on these parameters are inversely related to the proportion of observations in each group. reliance on these implicit weights can have serious consequences for applied work, as i illustrate with two well-known applications. i develop simple diagnostic tools that empirical researchers can use to avoid potential biases. software for implementing these methods is available in r and stata. in an important special case, my diagnostics only require the knowledge of the proportion of treated units.", "categories": "econ.em stat.ap stat.me", "created": "2018-10-03", "updated": "2020-05-19", "authors": ["tymon s\u0142oczy\u0144ski"], "url": "https://arxiv.org/abs/1810.01576"}, {"title": "granger causality on horizontal sum of boolean algebras", "id": "1810.01654", "abstract": "the intention of this paper is to discuss the mathematical model of causality introduced by c.w.j. granger in 1969. the granger's model of causality has become well-known and often used in various econometric models describing causal systems, e.g., between commodity prices and exchange rates.   our paper presents a new mathematical model of causality between two measured objects. we have slightly modified the well-known kolmogorovian probability model. in particular, we use the horizontal sum of set $\\sigma$-algebras instead of their direct product.", "categories": "econ.em", "created": "2018-10-03", "updated": "", "authors": ["m. bohdalov\u00e1", "m. kalina", "o. n\u00e1n\u00e1siov\u00e1"], "url": "https://arxiv.org/abs/1810.01654"}, {"title": "disability for hiv and disincentives for health: the impact of south   africa's disability grant on hiv/aids recovery", "id": "1810.01971", "abstract": "south africa's disability grants program is tied to its hiv/aids recovery program, such that individuals who are ill enough may qualify. qualification is historically tied to a cd4 count of 200 cells/mm3, which improve when a person adheres to antiretroviral therapy. this creates a potential unintended consequence where poor individuals, faced with potential loss of their income, may choose to limit their recovery through non-adherence. to test for manipulation caused by grant rules, we identify differences in disability grant recipients and non-recipients' rate of cd4 recovery around the qualification threshold, implemented as a fixed-effects difference-in-difference around the threshold. we use data from the africa health research institute demographic and health surveillance system (ahri dss) in rural kwazulu-natal, south africa, utilizing dg status and laboratory cd4 count records for 8,497 individuals to test whether there are any systematic differences in cd4 recover rates among eligible patients. we find that disability grant threshold rules caused recipients to have a relatively slower cd4 recovery rate of about 20-30 cells/mm3/year, or a 20% reduction in the speed of recovery around the threshold.", "categories": "econ.gn q-fin.ec", "created": "2018-10-03", "updated": "", "authors": ["noah haber", "till b\u00e4rnighausen", "jacob bor", "jessica cohen", "frank tanser", "deenan pillay", "g\u00fcnther fink"], "url": "https://arxiv.org/abs/1810.01971"}, {"title": "topological connectedness and behavioral assumptions on preferences: a   two-way relationship", "id": "1810.02004", "abstract": "this paper offers a comprehensive treatment of the question as to whether a binary relation can be consistent (transitive) without being decisive (complete), or decisive without being consistent, or simultaneously inconsistent or indecisive, in the presence of a continuity hypothesis that is, in principle, non-testable. it identifies topological connectedness of the (choice) set over which the continuous binary relation is defined as being crucial to this question. referring to the two-way relationship as the eilenberg-sonnenschein (es) research program, it presents four synthetic, and complete, characterizations of connectedness, and its natural extensions; and two consequences that only stem from it. the six theorems are novel to both the economic and the mathematical literature: they generalize pioneering results of eilenberg (1941), sonnenschein (1965), schmeidler (1971) and sen (1969), and are relevant to several applied contexts, as well as to ongoing theoretical work.", "categories": "econ.th math.gn", "created": "2018-10-03", "updated": "2018-10-25", "authors": ["m. ali khan", "metin uyan\u0131k"], "url": "https://arxiv.org/abs/1810.02004"}, {"title": "district heating systems under high co2 emission prices: the role of the   pass-through from emission cost to electricity prices", "id": "1810.02109", "abstract": "low co2 prices have prompted discussion about political measures aimed at increasing the cost of carbon dioxide emissions. these costs affect, inter alia, integrated district heating system operators (dhso), often owned by municipalities with some political influence, that use a variety of (co2 emis- sion intense) heat generation technologies. we examine whether dhsos have an incentive to support measures that increase co2 emission prices in the short term. therefore, we (i) develop a simplified analytical framework to analyse optimal decisions of a district heating operator, and (ii) investigate the market-wide effects of increasing emission prices, in particular the pass- through from emission costs to electricity prices. using a numerical model of the common austrian and german power system, we estimate a pass-through from co2 emission prices to power prices between 0.69 and 0.53 as of 2017, depending on the absolute emission price level. we find the co2 emission cost pass-through to be sufficiently high so that low-emission district heating systems operating at least moderately efficient generation units benefit from rising co2 emission prices in the short term.", "categories": "econ.gn q-fin.ec", "created": "2018-10-04", "updated": "", "authors": ["sebastian wehrle", "johannes schmidt"], "url": "https://arxiv.org/abs/1810.02109"}, {"title": "super-replication of the best pairs trade in hindsight", "id": "1810.02444", "abstract": "this paper derives a robust on-line equity trading algorithm that achieves the greatest possible percentage of the final wealth of the best pairs rebalancing rule in hindsight. a pairs rebalancing rule chooses some pair of stocks in the market and then perpetually executes rebalancing trades so as to maintain a target fraction of wealth in each of the two. after each discrete market fluctuation, a pairs rebalancing rule will sell a precise amount of the outperforming stock and put the proceeds into the underperforming stock. under typical conditions, in hindsight one can find pairs rebalancing rules that would have spectacularly beaten the market. our trading strategy, which extends ordentlich and cover's (1998) \"max-min universal portfolio,\" guarantees to achieve an acceptable percentage of the hindsight-optimized wealth, a percentage which tends to zero at a slow (polynomial) rate. this means that on a long enough investment horizon, the trader can enforce a compound-annual growth rate that is arbitrarily close to that of the best pairs rebalancing rule in hindsight. the strategy will \"beat the market asymptotically\" if there turns out to exist a pairs rebalancing rule that grows capital at a higher asymptotic rate than the market index. the advantages of our algorithm over the ordentlich and cover (1998) strategy are twofold. first, their strategy is impossible to compute in practice. second, in considering the more modest benchmark (instead of the best all-stock rebalancing rule in hindsight), we reduce the \"cost of universality\" and achieve a higher learning rate.", "categories": "q-fin.pm econ.gn econ.th q-fin.ec q-fin.gn q-fin.pr", "created": "2018-10-04", "updated": "2019-03-14", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1810.02444"}, {"title": "multilinear superhedging of lookback options", "id": "1810.02447", "abstract": "in a pathbreaking paper, cover and ordentlich (1998) solved a max-min portfolio game between a trader (who picks an entire trading algorithm, $\\theta(\\cdot)$) and \"nature,\" who picks the matrix $x$ of gross-returns of all stocks in all periods. their (zero-sum) game has the payoff kernel $w_\\theta(x)/d(x)$, where $w_\\theta(x)$ is the trader's final wealth and $d(x)$ is the final wealth that would have accrued to a $\\$1$ deposit into the best constant-rebalanced portfolio (or fixed-fraction betting scheme) determined in hindsight. the resulting \"universal portfolio\" compounds its money at the same asymptotic rate as the best rebalancing rule in hindsight, thereby beating the market asymptotically under extremely general conditions. smitten with this (1998) result, the present paper solves the most general tractable version of cover and ordentlich's (1998) max-min game. this obtains for performance benchmarks (read: derivatives) that are separately convex and homogeneous in each period's gross-return vector. for completely arbitrary (even non-measurable) performance benchmarks, we show how the axiom of choice can be used to \"find\" an exact maximin strategy for the trader.", "categories": "q-fin.pr econ.th q-fin.cp q-fin.gn q-fin.pm", "created": "2018-10-04", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1810.02447"}, {"title": "completeness and transitivity of preferences on mixture sets", "id": "1810.02454", "abstract": "in this paper, we show that the presence of the archimedean and the mixture-continuity properties of a binary relation, both empirically non-falsifiable in principle, foreclose the possibility of consistency (transitivity) without decisiveness (completeness), or decisiveness without consistency, or in the presence of a weak consistency condition, neither. the basic result can be sharpened when specialized from the context of a generalized mixture set to that of a mixture set in the sense of herstein-milnor (1953). we relate the results to the antecedent literature, and view them as part of an investigation into the interplay of the structure of the choice space and the behavioral assumptions on the binary relation defined on it; the es research program due to eilenberg (1941) and sonnenschein (1965), and one to which schmeidler (1971) is an especially influential contribution.", "categories": "econ.th", "created": "2018-10-04", "updated": "", "authors": ["tsogbadral galaabaatar", "m. ali khan", "metin uyan\u0131k"], "url": "https://arxiv.org/abs/1810.02454"}, {"title": "exact replication of the best rebalancing rule in hindsight", "id": "1810.02485", "abstract": "this paper prices and replicates the financial derivative whose payoff at $t$ is the wealth that would have accrued to a $\\$1$ deposit into the best continuously-rebalanced portfolio (or fixed-fraction betting scheme) determined in hindsight. for the single-stock black-scholes market, ordentlich and cover (1998) only priced this derivative at time-0, giving $c_0=1+\\sigma\\sqrt{t/(2\\pi)}$. of course, the general time-$t$ price is not equal to $1+\\sigma\\sqrt{(t-t)/(2\\pi)}$. i complete the ordentlich-cover (1998) analysis by deriving the price at any time $t$. by contrast, i also study the more natural case of the best levered rebalancing rule in hindsight. this yields $c(s,t)=\\sqrt{t/t}\\cdot\\,\\exp\\{rt+\\sigma^2b(s,t)^2\\cdot t/2\\}$, where $b(s,t)$ is the best rebalancing rule in hindsight over the observed history $[0,t]$. i show that the replicating strategy amounts to betting the fraction $b(s,t)$ of wealth on the stock over the interval $[t,t+dt].$ this fact holds for the general market with $n$ correlated stocks in geometric brownian motion: we get $c(s,t)=(t/t)^{n/2}\\exp(rt+b'\\sigma b\\cdot t/2)$, where $\\sigma$ is the covariance of instantaneous returns per unit time. this result matches the $\\mathcal{o}(t^{n/2})$ \"cost of universality\" derived by cover in his \"universal portfolio theory\" (1986, 1991, 1996, 1998), which super-replicates the same derivative in discrete-time. the replicating strategy compounds its money at the same asymptotic rate as the best levered rebalancing rule in hindsight, thereby beating the market asymptotically. naturally enough, we find that the american-style version of cover's derivative is never exercised early in equilibrium.", "categories": "q-fin.pr econ.gn econ.th q-fin.ec q-fin.mf q-fin.pm", "created": "2018-10-04", "updated": "2019-03-14", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1810.02485"}, {"title": "exploring the nuances in the relationship \"culture-strategy\" for the   business world", "id": "1810.02613", "abstract": "the current article explores interesting, significant and recently identified nuances in the relationship \"culture-strategy\". the shared views of leading scholars at the university of national and world economy in relation with the essence, direction, structure, role and hierarchy of \"culture-strategy\" relation are defined as a starting point of the analysis. the research emphasis is directed on recent developments in interpreting the observed realizations of the aforementioned link among the community of international scholars and consultants, publishing in selected electronic scientific databases. in this way a contemporary notion of the nature of \"culture-strategy\" relationship for the entities from the world of business is outlined.", "categories": "econ.gn q-fin.ec", "created": "2018-10-05", "updated": "", "authors": ["kiril dimitrov"], "url": "https://arxiv.org/abs/1810.02613"}, {"title": "talent management - an etymological study", "id": "1810.02615", "abstract": "the current article unveils and analyzes important shades of meaning for the widely discussed term talent management. it not only grounds the outlined perspectives in incremental formulation and elaboration of this construct, but also is oriented to exploring the underlying reasons for the social actors, proposing new nuances. thus, a mind map and a fish-bone diagram are constructed to depict effectively and efficiently the current state of development for talent management and make easier the realizations of future research endeavours in this field.", "categories": "econ.gn q-fin.ec", "created": "2018-10-05", "updated": "", "authors": ["kiril dimitrov"], "url": "https://arxiv.org/abs/1810.02615"}, {"title": "dominating attributes of professed firm culture of holding companies -   members of the bulgarian industrial capital association", "id": "1810.02617", "abstract": "this article aims to outline the diversity of cultural phenomena that occur at organizational level, emphasizing the place and role of the key attributes of professed firm culture for the survival and successful development of big business organizations. the holding companies, members of the bulgarian industrial capital association, are chosen as a survey object as the mightiest driving engines of the local economy. that is why their emergence and development in the transition period is monitored and analyzed. based on an empirical study of relevant website content, important implications about dominating attributes of professed firm culture on them are found and several useful recommendations to their senior management are made.", "categories": "econ.gn q-fin.ec", "created": "2018-10-05", "updated": "", "authors": ["kiril dimitrov", "marin geshkov"], "url": "https://arxiv.org/abs/1810.02617"}, {"title": "geert hofstede et al's set of national cultural dimensions - popularity   and criticisms", "id": "1810.02621", "abstract": "this article outlines different stages in development of the national culture model, created by geert hofstede and his affiliates. this paper reveals and synthesizes the contemporary review of the application spheres of this framework. numerous applications of the dimensions set are used as a source of identifying significant critiques, concerning different aspects in model's operation. these critiques are classified and their underlying reasons are also outlined by means of a fishbone diagram.", "categories": "econ.gn q-fin.ec", "created": "2018-10-05", "updated": "", "authors": ["kiril dimitrov"], "url": "https://arxiv.org/abs/1810.02621"}, {"title": "contemporary facets of business successes among leading companies,   operating in bulgaria", "id": "1810.02622", "abstract": "the current article unveils and analyzes some important factors, influencing diversity in strategic decision-making approaches in local companies. researcher's attention is oriented to survey important characteristics of the strategic moves, undertaken by leading companies in bulgaria.", "categories": "econ.gn q-fin.ec", "created": "2018-10-05", "updated": "", "authors": ["kiril dimitrov"], "url": "https://arxiv.org/abs/1810.02622"}, {"title": "a general sensitivity analysis approach for demand response   optimizations", "id": "1810.02815", "abstract": "it is well-known that demand response can improve the system efficiency as well as lower consumers' (prosumers') electricity bills. however, it is not clear how we can either qualitatively identify the prosumer with the most impact potential or quantitatively estimate each prosumer's contribution to the total social welfare improvement when additional resource capacity/flexibility is introduced to the system with demand response, such as allowing net-selling behavior. in this work, we build upon existing literature on the electricity market, which consists of price-taking prosumers each with various appliances, an electric utility company and a social welfare optimizing distribution system operator, to design a general sensitivity analysis approach (gsaa) that can estimate the potential of each consumer's contribution to the social welfare when given more resource capacity. gsaa is based on existence of an efficient competitive equilibrium, which we establish in the paper. when prosumers' utility functions are quadratic, gsaa can give closed forms characterization on social welfare improvement based on duality analysis. furthermore, we extend gsaa to a general convex settings, i.e., utility functions with strong convexity and lipschitz continuous gradient. even without knowing the specific forms the utility functions, we can derive upper and lower bounds of the social welfare improvement potential of each prosumer, when extra resource is introduced. for both settings, several applications and numerical examples are provided: including extending ac comfort zone, ability of ev to discharge and net selling. the estimation results show that gsaa can be used to decide how to allocate potentially limited market resources in the most impactful way.", "categories": "cs.ce econ.gn math.oc q-fin.ec", "created": "2018-10-07", "updated": "", "authors": ["ding xiang", "ermin wei"], "url": "https://arxiv.org/abs/1810.02815"}, {"title": "the model selection curse", "id": "1810.02888", "abstract": "a \"statistician\" takes an action on behalf of an agent, based on the agent's self-reported personal data and a sample involving other people. the action that he takes is an estimated function of the agent's report. the estimation procedure involves model selection. we ask the following question: is truth-telling optimal for the agent given the statistician's procedure? we analyze this question in the context of a simple example that highlights the role of model selection. we suggest that our simple exercise may have implications for the broader issue of human interaction with \"machine learning\" algorithms.", "categories": "econ.th", "created": "2018-10-05", "updated": "", "authors": ["kfir eliaz", "ran spiegler"], "url": "https://arxiv.org/abs/1810.02888"}, {"title": "social capital at venture capital firms and their financial performance:   evidence from china", "id": "1810.02952", "abstract": "this paper studies the extent to which social capital drives performance in the chinese venture capital market and explores the trend toward vc syndication in china. first, we propose a hybrid model based on syndicated social networks and the latent-variable model, which describes the social capital at venture capital firms and builds relationships between social capital and performance at vc firms. then, we build three hypotheses about the relationships and test the hypotheses using our proposed model. some numerical simulations are given to support the test results. finally, we show that the correlations between social capital and financial performance at venture capital firms are weak in china and find that china's venture capital firms lack mature social capital links.", "categories": "econ.gn q-fin.ec", "created": "2018-10-06", "updated": "", "authors": ["qi-lin cao", "hua-yun xiang", "you-jia mao", "ben-zhang yang"], "url": "https://arxiv.org/abs/1810.02952"}, {"title": "on lasso for predictive regression", "id": "1810.03140", "abstract": "a typical predictive regression employs a multitude of potential regressors with various degrees of persistence while their signal strength in explaining the dependent variable is often low. variable selection in such context is of great importance. in this paper, we explore the pitfalls and possibilities of lasso methods in this predictive regression framework with mixed degrees of persistence. in the presence of stationary, unit root and cointegrated predictors, we show that the adaptive lasso asymptotically breaks cointegrated groups although it cannot wipe out all inactive cointegrating variables. this new finding motivates a simple but novel post-selection adaptive lasso, which we call the twin adaptive lasso (talasso), to fix variable selection inconsistency. talasso's penalty scheme accommodates the system of heterogeneous regressors, and it recovers the well-known oracle property that implies variable selection consistency and optimal rate of convergence for all three types of regressors. on the contrary, conventional lasso fails to attain coefficient estimation consistency and variable screening in all components simultaneously, since its penalty is imposed according to the marginal behavior of each individual regressor only. we demonstrate the theoretical properties via extensive monte carlo simulations. these lasso-type methods are applied to evaluate short- and long-horizon predictability of s&p 500 excess return.", "categories": "econ.em", "created": "2018-10-07", "updated": "2020-01-02", "authors": ["ji hyung lee", "zhentao shi", "zhan gao"], "url": "https://arxiv.org/abs/1810.03140"}, {"title": "simple inference on functionals of set-identified parameters defined by   linear moments", "id": "1810.03180", "abstract": "this paper considers uniformly valid (over a class of data generating processes) inference for linear functionals of partially identified parameters in cases where the identified set is defined by linear (in the parameter) moment inequalities. we propose a bootstrap procedure for constructing uniformly valid confidence sets for a linear functional of a partially identified parameter. the proposed method amounts to bootstrapping the value functions of a linear optimization problem, and subsumes subvector inference as a special case. in other words, this paper shows the conditions under which ``naively'' bootstrapping a linear program can be used to construct a confidence set with uniform correct coverage for a partially identified linear functional. unlike other proposed subvector inference procedures, our procedure does not require the researcher to repeatedly invert a hypothesis test, and is extremely computationally efficient. in addition to the new procedure, the paper also discusses connections between the literature on optimization and the literature on subvector inference in partially identified models.", "categories": "econ.em", "created": "2018-10-07", "updated": "2019-09-08", "authors": ["joonhwan cho", "thomas m. russell"], "url": "https://arxiv.org/abs/1810.03180"}, {"title": "evaluating regulatory reform of network industries: a survey of   empirical models based on categorical proxies", "id": "1810.03348", "abstract": "proxies for regulatory reforms based on categorical variables are increasingly used in empirical evaluation models. we surveyed 63 studies that rely on such indices to analyze the effects of entry liberalization, privatization, unbundling, and independent regulation of the electricity, natural gas, and telecommunications sectors. we highlight methodological issues related to the use of these proxies. next, taking stock of the literature, we provide practical advice for the design of the empirical strategy and discuss the selection of control and instrumental variables to attenuate endogeneity problems undermining identification of the effects of regulatory reforms.", "categories": "econ.gn econ.em q-fin.ec", "created": "2018-10-08", "updated": "", "authors": ["andrea bastianin", "paolo castelnovo", "massimo florio"], "url": "https://arxiv.org/abs/1810.03348"}, {"title": "k-price auctions and combination auctions", "id": "1810.03494", "abstract": "we provide an exact analytical solution of the nash equilibrium for $k$- price auctions. we also introduce a new type of auction and demonstrate that it has fair solutions other than the second price auctions, therefore paving the way for replacing second price auctions.", "categories": "q-fin.mf cs.gt econ.gn q-fin.ec", "created": "2018-09-26", "updated": "2019-03-05", "authors": ["martin mihelich", "yan shu"], "url": "https://arxiv.org/abs/1810.03494"}, {"title": "critical review of models, containing cultural levels beyond the   organizational one", "id": "1810.03605", "abstract": "the current article traces back the scientific interest to cultural levels across the organization at the university of national and world economy, and especially in the series of economic alternatives - an official scientific magazine, issued by this institution. further, a wider and critical review of international achievements in this field is performed, revealing diverse analysis perspectives with respect to cultural levels. also, a useful model of exploring and teaching the cultural levels beyond the organization is proposed.   keywords: globalization, national culture, organization culture, cultural levels, cultural economics. jel: m14, z10.", "categories": "econ.gn q-fin.ec", "created": "2018-10-08", "updated": "", "authors": ["kiril dimitrov"], "url": "https://arxiv.org/abs/1810.03605"}, {"title": "the incidental parameters problem in testing for remaining cross-section   correlation", "id": "1810.03715", "abstract": "in this paper we consider the properties of the pesaran (2004, 2015a) cd test for cross-section correlation when applied to residuals obtained from panel data models with many estimated parameters. we show that the presence of period-specific parameters leads the cd test statistic to diverge as length of the time dimension of the sample grows. this result holds even if cross-section dependence is correctly accounted for and hence constitutes an example of the incidental parameters problem. the relevance of this problem is investigated both for the classical time fixed effects estimator as well as the common correlated effects estimator of pesaran (2006). we suggest a weighted cd test statistic which re-establishes standard normal inference under the null hypothesis. given the widespread use of the cd test statistic to test for remaining cross-section correlation, our results have far reaching implications for empirical researchers.", "categories": "econ.em", "created": "2018-10-08", "updated": "2019-10-28", "authors": ["arturas juodis", "simon reese"], "url": "https://arxiv.org/abs/1810.03715"}, {"title": "university rankings from the revealed preferences of the applicants", "id": "1810.04087", "abstract": "a methodology is presented to rank universities on the basis of the lists of programmes the students applied for. we exploit a crucial feature of the centralised assignment system to higher education in hungary: a student is admitted to the first programme where the score limit is achieved. this makes it possible to derive a partial preference order of each applicant. our approach integrates the information from all students participating in the system, is free of multicollinearity among the indicators, and contains few ad hoc parameters. the procedure is implemented to rank faculties in the hungarian higher education between 2001 and 2016. we demonstrate that the ranking given by the least squares method has favourable theoretical properties, is robust with respect to the aggregation of preferences, and performs well in practice. the suggested ranking is worth considering as a reasonable alternative to the standard composite indices.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2018-10-09", "updated": "2020-02-26", "authors": ["l\u00e1szl\u00f3 csat\u00f3", "csaba t\u00f3th"], "url": "https://arxiv.org/abs/1810.04087"}, {"title": "prices, profits, proxies, and production", "id": "1810.04697", "abstract": "this paper studies nonparametric identification and counterfactual bounds for heterogeneous firms that can be ranked in terms of productivity. our approach works when quantities and prices are latent rendering standard approaches inapplicable. instead, we require observation of profits or other optimizing-values such as costs or revenues, and either prices or price proxies of flexibly chosen variables. we extend classical duality results for price-taking firms to a setup with discrete heterogeneity, endogeneity, and limited variation in possibly latent prices. finally, we show that convergence results for nonparametric estimators may be directly converted to convergence results for production sets.", "categories": "econ.em", "created": "2018-10-10", "updated": "2020-03-15", "authors": ["victor h. aguiar", "roy allen", "nail kashaev"], "url": "https://arxiv.org/abs/1810.04697"}, {"title": "integrating electricity markets: impacts of increasing trade on prices   and emissions in the western united states", "id": "1810.04759", "abstract": "this paper presents empirically-estimated average hourly relationships between regional electricity trade in the united states and prices, emissions, and generation from 2015 through 2018. consistent with economic theory, the analysis finds a negative relationship between electricity prices in california and regional trade, conditional on local demand. each 1 gigawatt-hour increase in california electricity imports is associated with an average $0.15 per megawatt-hour decrease in the california independent system operator's wholesale electricity price. there is a net-negative short term relationship between carbon dioxide emissions in california and electricity imports that is partially offset by positive emissions from exporting neighbors. specifically, each 1 gwh increase in regional trade is associated with a net 70-ton average decrease in co2 emissions across the western u.s., conditional on demand levels. the results provide evidence that electricity imports mostly displace natural gas generation on the margin in the california electricity market. a small positive relationship is observed between short-run so2 and nox emissions in neighboring regions and california electricity imports. the magnitude of the so2 and nox results suggest an average increase of 0.1 mwh from neighboring coal plants is associated with a 1 mwh increase in imports to california.", "categories": "econ.gn q-fin.ec", "created": "2018-10-10", "updated": "2019-10-16", "authors": ["steven dahlke"], "url": "https://arxiv.org/abs/1810.04759"}, {"title": "offline multi-action policy learning: generalization and optimization", "id": "1810.04778", "abstract": "in many settings, a decision-maker wishes to learn a rule, or policy, that maps from observable characteristics of an individual to an action. examples include selecting offers, prices, advertisements, or emails to send to consumers, as well as the problem of determining which medication to prescribe to a patient. while there is a growing body of literature devoted to this problem, most existing results are focused on the case where data comes from a randomized experiment, and further, there are only two possible actions, such as giving a drug to a patient or not. in this paper, we study the offline multi-action policy learning problem with observational data and where the policy may need to respect budget constraints or belong to a restricted policy class such as decision trees. we build on the theory of efficient semi-parametric inference in order to propose and implement a policy learning algorithm that achieves asymptotically minimax-optimal regret. to the best of our knowledge, this is the first result of this type in the multi-action setup, and it provides a substantial performance improvement over the existing learning algorithms. we then consider additional computational challenges that arise in implementing our method for the case where the policy is restricted to take the form of a decision tree. we propose two different approaches, one using a mixed integer program formulation and the other using a tree-search based algorithm.", "categories": "stat.ml cs.lg econ.em", "created": "2018-10-10", "updated": "2018-11-19", "authors": ["zhengyuan zhou", "susan athey", "stefan wager"], "url": "https://arxiv.org/abs/1810.04778"}, {"title": "deriving the factor endowment--commodity output relationship for   thailand (1920-1927) using a three-factor two-good general equilibrium trade   model", "id": "1810.04819", "abstract": "feeny (1982, pp. 26-28) referred to a three-factor two-good general equilibrium trade model, when he explained the relative importance of trade and factor endowments in thailand 1880-1940. for example, feeny (1982) stated that the growth in labor stock would be responsible for a substantial increase in rice output relative to textile output. is feeny's statement plausible? the purpose of this paper is to derive the rybczynski sign patterns, which express the factor endowment--commodity output relationship, for thailand during the period 1920 to 1927 using the ews (economy-wide substitution)-ratio vector. a 'strong rybczynski result' necessarily holds. i derived three rybczynski sign patterns. however, a more detailed estimate allowed a reduction from three candidates to two. i restrict the analysis to the period 1920-1927 because of data availability. the results imply that feeny's statement might not necessarily hold. hence, labor stock might not affect the share of exportable sector in national income positively. moreover, the percentage of chinese immigration in the total population growth was not as large as expected. this study will be useful when simulating real wage in thailand.", "categories": "econ.gn q-fin.ec", "created": "2018-10-10", "updated": "", "authors": ["yoshiaki nakada"], "url": "https://arxiv.org/abs/1810.04819"}, {"title": "stochastic revealed preferences with measurement error", "id": "1810.05287", "abstract": "a long-standing question about consumer behavior is whether individuals' observed purchase decisions satisfy the revealed preference (rp) axioms of the utility maximization theory (umt). researchers using survey or experimental panel data sets on prices and consumption to answer this question face the well-known problem of measurement error. we show that ignoring measurement error in the rp approach may lead to overrejection of the umt. to solve this problem, we propose a new statistical rp framework for consumption panel data sets that allows for testing the umt in the presence of measurement error. our test is applicable to all consumer models that can be characterized by their first-order conditions. our approach is nonparametric, allows for unrestricted heterogeneity in preferences, and requires only a centering condition on measurement error. we develop two applications that provide new evidence about the umt. first, we find support in a survey data set for the dynamic and time-consistent umt in single-individual households, in the presence of \\emph{nonclassical} measurement error in consumption. in the second application, we cannot reject the static umt in a widely used experimental data set in which measurement error in prices is assumed to be the result of price misperception due to the experimental design. the first finding stands in contrast to the conclusions drawn from the deterministic rp test of browning (1989). the second finding reverses the conclusions drawn from the deterministic rp test of afriat (1967) and varian (1982).", "categories": "econ.em", "created": "2018-10-11", "updated": "2020-09-18", "authors": ["victor h. aguiar", "nail kashaev"], "url": "https://arxiv.org/abs/1810.05287"}, {"title": "the broad consequences of narrow banking", "id": "1810.05689", "abstract": "we investigate the macroeconomic consequences of narrow banking in the context of stock-flow consistent models. we begin with an extension of the goodwin-keen model incorporating time deposits, government bills, cash, and central bank reserves to the base model with loans and demand deposits and use it to describe a fractional reserve banking system. we then characterize narrow banking by a full reserve requirement on demand deposits and describe the resulting separation between the payment system and lending functions of the resulting banking sector. by way of numerical examples, we explore the properties of fractional and full reserve versions of the model and compare their asymptotic properties. we find that narrow banking does not lead to any loss in economic growth when the models converge to a finite equilibrium, while allowing for more direct monitoring and prevention of financial breakdowns in the case of explosive asymptotic behaviour.", "categories": "econ.gn q-fin.ec q-fin.mf", "created": "2018-10-12", "updated": "", "authors": ["matheus r grasselli", "alexander lipton"], "url": "https://arxiv.org/abs/1810.05689"}, {"title": "using generalized estimating equations to estimate nonlinear models with   spatial data", "id": "1810.05855", "abstract": "in this paper, we study estimation of nonlinear models with cross sectional data using two-step generalized estimating equations (gee) in the quasi-maximum likelihood estimation (qmle) framework. in the interest of improving efficiency, we propose a grouping estimator to account for the potential spatial correlation in the underlying innovations. we use a poisson model and a negative binomial ii model for count data and a probit model for binary response data to demonstrate the gee procedure. under mild weak dependency assumptions, results on estimation consistency and asymptotic normality are provided. monte carlo simulations show efficiency gain of our approach in comparison of different estimation methods for count data and binary response data. finally we apply the gee approach to study the determinants of the inflow foreign direct investment (fdi) to china.", "categories": "econ.em", "created": "2018-10-13", "updated": "", "authors": ["cuicui lu", "weining wang", "jeffrey m. wooldridge"], "url": "https://arxiv.org/abs/1810.05855"}, {"title": "aggressive economic incentives and physical activity: the role of choice   and technology decision aids", "id": "1810.06698", "abstract": "aggressive incentive schemes that allow individuals to impose economic punishment on themselves if they fail to meet health goals present a promising approach for encouraging healthier behavior. however, the element of choice inherent in these schemes introduces concerns that only non-representative sectors of the population will select aggressive incentives, leaving value on the table for those who don't opt in. in a field experiment conducted over a 29 week period on individuals wearing fitbit activity trackers, we find modest and short lived increases in physical activity for those provided the choice of aggressive incentives. in contrast, we find significant and persistent increases for those assigned (oftentimes against their stated preference) to the same aggressive incentives. the modest benefits for those provided a choice seems to emerge because those who benefited most from the aggressive incentives were the least likely to choose them, and it was those who did not need them who opted in. these results are confirmed in a follow up lab experiment. we also find that benefits to individuals assigned to aggressive incentives were pronounced if they also updated their step target in the fitbit mobile application to match the new activity goal we provided them. our findings have important implications for incentive based interventions to improve health behavior. for firms and policy makers, our results suggest that one effective strategy for encouraging sustained healthy behavior combines exposure to aggressive incentive schemes to jolt individuals out of their comfort zones with technology decision aids that help individuals sustain this behavior after incentives end.", "categories": "econ.gn q-fin.ec", "created": "2018-10-15", "updated": "2018-11-07", "authors": ["idris adjerid", "rachael purta", "aaron striegel", "george loewenstein"], "url": "https://arxiv.org/abs/1810.06698"}, {"title": "opinion dynamics via search engines (and other algorithmic gatekeepers)", "id": "1810.06973", "abstract": "ranking algorithms are the information gatekeepers of the internet era. we develop a stylized model to study the effects of ranking algorithms on opinion dynamics. we consider a search engine that uses an algorithm based on popularity and on personalization. we find that popularity-based rankings generate an advantage of the fewer effect: fewer websites reporting a given signal attract relatively more traffic overall. this highlights a novel, ranking-driven channel that explains the diffusion of misinformation, as websites reporting incorrect information may attract an amplified amount of traffic precisely because they are few. furthermore, when individuals provide sufficiently positive feedback to the ranking algorithm, popularity-based rankings tend to aggregate information while personalization acts in the opposite direction.", "categories": "cs.si econ.gn q-fin.ec", "created": "2018-10-16", "updated": "2018-10-17", "authors": ["fabrizio germano", "francesco sobbrio"], "url": "https://arxiv.org/abs/1810.06973"}, {"title": "constructing energy accounts for wiod 2016 release", "id": "1810.07112", "abstract": "most of today's products and services are made in global supply chains. as a result, a consumption of goods and services in one country is associated with various environmental pressures all over the world due to international trade. advances in global multi-region input-output models have allowed researchers to draw detailed, international supply-chain connections between production and consumptions activities and associated environmental impacts. due to a limited data availability there is little evidence about the more recent trends in global energy footprint. in order to expand the analytical potential of the existing wiod 2016 dataset to a wider range of research themes, this paper develops energy accounts and presents the global energy footprint trends for the period 2000-2014.", "categories": "econ.gn q-fin.ec", "created": "2018-10-16", "updated": "", "authors": ["viktoras kulionis"], "url": "https://arxiv.org/abs/1810.07112"}, {"title": "a path integral approach to business cycle models with large number of   agents", "id": "1810.07178", "abstract": "this paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agents' complexity. this formalism does not seek to aggregate agents. it rather replaces the standard optimization approach by a probabilistic description of both the entire system and agents'behaviors. this is done in two distinct steps. a first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. in such a setting, individual optimization problems need not be resolved. each agent is described by a time-dependent probability distribution centered around his utility optimum. the entire system of agents is thus defined by a composite probability depending on time, agents' interactions and forward-looking behaviors. this dynamic system is described by a path integral formalism in an abstract space-the space of the agents' actions-and is very similar to a statistical physics or quantum mechanics system. we show that this description, applied to the space of agents'actions, reduces to the usual optimization results in simple cases. compared to a standard optimization, such a description markedly eases the treatment of systems with small number of agents. it becomes however useless for a large number of agents. in a second step therefore, we show that for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. this yields an analytical though approximate treatment of the system. this field theory does not model the aggregation of a microeconomic system in the usual sense. it rather describes an environment of a large number of interacting agents. from this description, various phases or equilibria may be retrieved, along with individual agents' behaviors and their interactions with the environment. for illustrative purposes, this paper studies a business cycle model with a large number of agents.", "categories": "econ.gn cond-mat.stat-mech q-fin.ec q-fin.gn", "created": "2018-10-16", "updated": "", "authors": ["a\u00efleen lotz", "pierre gosselin", "marc wambst"], "url": "https://arxiv.org/abs/1810.07178"}, {"title": "accounting for unobservable heterogeneity in cross section using spatial   first differences", "id": "1810.07216", "abstract": "we develop a cross-sectional research design to identify causal effects in the presence of unobservable heterogeneity without instruments. when units are dense in physical space, it may be sufficient to regress the \"spatial first differences\" (sfd) of the outcome on the treatment and omit all covariates. the identifying assumptions of sfd are similar in mathematical structure and plausibility to other quasi-experimental designs. we use sfd to obtain new estimates for the effects of time-invariant geographic factors, soil and climate, on long-run agricultural productivities --- relationships crucial for economic decisions, such as land management and climate policy, but notoriously confounded by unobservables.", "categories": "econ.em stat.ap stat.ml", "created": "2018-10-16", "updated": "2019-08-21", "authors": ["hannah druckenmiller", "solomon hsiang"], "url": "https://arxiv.org/abs/1810.07216"}, {"title": "optimal policy design for the sugar tax", "id": "1810.07243", "abstract": "healthy nutrition promotions and regulations have long been regarded as a tool for increasing social welfare. one of the avenues taken in the past decade is sugar consumption regulation by introducing a sugar tax. such a tax increases the price of extensive sugar containment in products such as soft drinks. in this article we consider a typical problem of optimal regulatory policy design, where the task is to determine the sugar tax rate maximizing the social welfare. we model the problem as a sequential game represented by the three-level mathematical program. on the upper level, the government decides upon the tax rate. on the middle level, producers decide on the product pricing. on the lower level, consumers decide upon their preferences towards the products. while the general problem is computationally intractable, the problem with a few product types is polynomially solvable, even for an arbitrary number of heterogeneous consumers. this paper presents a simple, intuitive and easily implementable framework for computing optimal sugar tax in a market with a few products. this resembles the reality as the soft drinks, for instance, are typically categorized in either regular or no-sugar drinks, e.g. coca-cola and coca-cola zero. we illustrate the algorithm using an example based on the real data and draw conclusions for a specific local market.", "categories": "econ.th", "created": "2018-10-16", "updated": "", "authors": ["kelly geyskens", "alexander grigoriev", "niels holtrop", "anastasia nedelko"], "url": "https://arxiv.org/abs/1810.07243"}, {"title": "a consistent heteroskedasticity robust lm type specification test for   semiparametric models", "id": "1810.07620", "abstract": "this paper develops a consistent heteroskedasticity robust lagrange multiplier (lm) type specification test for semiparametric conditional mean models. consistency is achieved by turning a conditional moment restriction into a growing number of unconditional moment restrictions using series methods. the proposed test statistic is straightforward to compute and is asymptotically standard normal under the null. compared with the earlier literature on series-based specification tests in parametric models, i rely on the projection property of series estimators and derive a different normalization of the test statistic. compared with the recent test in gupta (2018), i use a different way of accounting for heteroskedasticity. i demonstrate using monte carlo studies that my test has superior finite sample performance compared with the existing tests. i apply the test to one of the semiparametric gasoline demand specifications from yatchew and no (2001) and find no evidence against it.", "categories": "econ.em", "created": "2018-10-17", "updated": "2019-11-08", "authors": ["ivan korolev"], "url": "https://arxiv.org/abs/1810.07620"}, {"title": "dynkin games with incomplete and asymmetric information", "id": "1810.07674", "abstract": "we study the value and the optimal strategies for a two-player zero-sum optimal stopping game with incomplete and asymmetric information. in our bayesian set-up, the drift of the underlying diffusion process is unknown to one player (incomplete information feature), but known to the other one (asymmetric information feature). we formulate the problem and reduce it to a fully markovian setup where the uninformed player optimises over stopping times and the informed one uses randomised stopping times in order to hide their informational advantage. then we provide a general verification result which allows us to find the value of the game and players' optimal strategies by solving suitable quasi-variational inequalities with some non-standard constraints. finally, we study an example with linear payoffs, in which an explicit solution of the corresponding quasi-variational inequalities can be obtained.", "categories": "math.pr econ.gn math.oc q-fin.ec", "created": "2018-10-17", "updated": "2020-07-14", "authors": ["tiziano de angelis", "erik ekstr\u00f6m", "kristoffer glover"], "url": "https://arxiv.org/abs/1810.07674"}, {"title": "fair cake-cutting in practice", "id": "1810.08243", "abstract": "using a lab experiment, we investigate the real-life performance of envy-free and proportional cake-cutting procedures with respect to fairness and preference manipulation. we find that envy-free procedures, in particular selfridge-conway, are fairer and also are perceived as fairer than their proportional counterparts, despite the fact that agents very often manipulate them. our results support the practical use of the celebrated selfridge-conway procedure, and more generally, of envy-free cake-cutting mechanisms.   we also find that subjects learn their opponents' preferences after repeated interaction and use this knowledge to improve their allocated share of the cake. learning reduces truth-telling behavior, but also reduces envy.", "categories": "cs.gt econ.th", "created": "2018-10-18", "updated": "2018-11-26", "authors": ["maria kyropoulou", "josu\u00e9 ortega", "erel segal-halevi"], "url": "https://arxiv.org/abs/1810.08243"}, {"title": "quantile regression under memory constraint", "id": "1810.08264", "abstract": "this paper studies the inference problem in quantile regression (qr) for a large sample size $n$ but under a limited memory constraint, where the memory can only store a small batch of data of size $m$. a natural method is the na\\\"ive divide-and-conquer approach, which splits data into batches of size $m$, computes the local qr estimator for each batch, and then aggregates the estimators via averaging. however, this method only works when $n=o(m^2)$ and is computationally expensive. this paper proposes a computationally efficient method, which only requires an initial qr estimator on a small batch of data and then successively refines the estimator via multiple rounds of aggregations. theoretically, as long as $n$ grows polynomially in $m$, we establish the asymptotic normality for the obtained estimator and show that our estimator with only a few rounds of aggregations achieves the same efficiency as the qr estimator computed on all the data. moreover, our result allows the case that the dimensionality $p$ goes to infinity. the proposed method can also be applied to address the qr problem under distributed computing environment (e.g., in a large-scale sensor network) or for real-time streaming data.", "categories": "stat.me econ.em stat.ml", "created": "2018-10-18", "updated": "", "authors": ["xi chen", "weidong liu", "yichen zhang"], "url": "https://arxiv.org/abs/1810.08264"}, {"title": "treatment effect models with strategic interaction in treatment   decisions", "id": "1810.08350", "abstract": "this study considers treatment effect models in which others' treatment decisions can affect one's own treatment and outcome. focusing on the case of two-player interactions, we formulate treatment decision behavior as a complete information game with multiple equilibria. using a latent index framework and assuming a stochastic equilibrium selection, we prove that the marginal treatment effect from one's own treatment and that from the partner can be identified separately. based on our constructive identification results, we propose a two-step semiparametric procedure for estimating the marginal treatment effects using series approximation. we show that the proposed estimator is uniformly consistent and asymptotically normally distributed. as an empirical illustration, we investigate the impacts of risky behaviors on adolescents' academic performance.", "categories": "econ.em", "created": "2018-10-18", "updated": "2020-08-19", "authors": ["tadao hoshino", "takahide yanagi"], "url": "https://arxiv.org/abs/1810.08350"}, {"title": "does the price of strategic commodities respond to u.s. partisan   conflict?", "id": "1810.08396", "abstract": "a noteworthy feature of u.s. politics in recent years is serious partisan conflict, which has led to intensifying polarization and exacerbating high policy uncertainty. the us is a significant player in oil and gold markets. oil and gold also form the basis of important strategic reserves in the us. we investigate whether u.s. partisan conflict affects the returns and price volatility of oil and gold using a parametric test of granger causality in quantiles. the empirical results suggest that u.s. partisan conflict has an effect on the returns of oil and gold, and the effects are concentrated at the tail of the conditional distribution of returns. more specifically, the partisan conflict mainly affects oil returns when the crude oil market is in a bearish state (lower quantiles). by contrast, partisan conflict matters for gold returns only when the gold market is in a bullish scenario (higher quantiles). in addition, for the volatility of oil and gold, the predictability of partisan conflict index virtually covers the entire distribution of volatility.", "categories": "econ.gn q-fin.ec", "created": "2018-10-19", "updated": "2020-02-27", "authors": ["yong jiang", "yi-shuai ren", "chao-qun ma", "jiang-long liu", "basil sharp"], "url": "https://arxiv.org/abs/1810.08396"}, {"title": "probabilistic forecasting in day-ahead electricity markets: simulating   peak and off-peak prices", "id": "1810.08418", "abstract": "in this paper we include dependency structures for electricity price forecasting and forecasting evaluation. we work with off-peak and peak time series from the german-austrian day-ahead price, hence we analyze bivariate data. we first estimate the mean of the two time series, and then in a second step we estimate the residuals. the mean equation is estimated by ols and elastic net and the residuals are estimated by maximum likelihood. our contribution is to include a bivariate jump component on a mean reverting jump diffusion model in the residuals. the models' forecasts are evaluated using four different criteria, including the energy score to measure whether the correlation structure between the time series is properly included or not. in the results it is observed that the models with bivariate jumps provide better results with the energy score, which means that it is important to consider this structure in order to properly forecast correlated time series.", "categories": "econ.em", "created": "2018-10-19", "updated": "2019-12-02", "authors": ["peru muniain", "florian ziel"], "url": "https://arxiv.org/abs/1810.08418"}, {"title": "forecasting time series with varma recursions on graphs", "id": "1810.08581", "abstract": "graph-based techniques emerged as a choice to deal with the dimensionality issues in modeling multivariate time series. however, there is yet no complete understanding of how the underlying structure could be exploited to ease this task. this work provides contributions in this direction by considering the forecasting of a process evolving over a graph. we make use of the (approximate) time-vertex stationarity assumption, i.e., timevarying graph signals whose first and second order statistical moments are invariant over time and correlated to a known graph topology. the latter is combined with var and varma models to tackle the dimensionality issues present in predicting the temporal evolution of multivariate time series. we find out that by projecting the data to the graph spectral domain: (i) the multivariate model estimation reduces to that of fitting a number of uncorrelated univariate arma models and (ii) an optimal low-rank data representation can be exploited so as to further reduce the estimation costs. in the case that the multivariate process can be observed at a subset of nodes, the proposed models extend naturally to kalman filtering on graphs allowing for optimal tracking. numerical experiments with both synthetic and real data validate the proposed approach and highlight its benefits over state-of-the-art alternatives.", "categories": "eess.sp cs.sy econ.em", "created": "2018-10-19", "updated": "2019-07-10", "authors": ["elvin isufi", "andreas loukas", "nathanael perraudin", "geert leus"], "url": "https://arxiv.org/abs/1810.08581"}, {"title": "optimal electricity demand response contracting with responsiveness   incentives", "id": "1810.09063", "abstract": "despite the success of demand response programs in retail electricity markets in reducing average consumption, the random responsiveness of consumers to price event makes their efficiency questionable to achieve the flexibility needed for electric systems with a large share of renewable energy. the variance of consumers' responses depreciates the value of these mechanisms and makes them weakly reliable. this paper aims at designing demand response contracts which allow to act on both the average consumption and its variance. the interaction between a risk--averse producer and a risk--averse consumer is modelled through a principal--agent problem, thus accounting for the moral hazard underlying demand response contracts. we provide closed--form solution for the optimal contract in the case of constant marginal costs of energy and volatility for the producer and constant marginal value of energy for the consumer. we show that the optimal contract has a rebate form where the initial condition of the consumption serves as a baseline. further, the consumer cannot manipulate the baseline at his own advantage. the second--best price for energy and volatility are non--constant and non--increasing in time. the price for energy is lower (resp. higher) than the marginal cost of energy during peak--load (resp. off--peak) periods. we illustrate the potential benefit issued from the implementation of an incentive mechanism on the responsiveness of the consumer by calibrating our model with publicly available data. we predict a significant increase of responsiveness under our optimal contract and a significant increase of the producer satisfaction.", "categories": "math.oc econ.gn math.pr q-fin.ec", "created": "2018-10-21", "updated": "2019-05-25", "authors": ["ren\u00e9 a\u00efd", "dylan possama\u00ef", "nizar touzi"], "url": "https://arxiv.org/abs/1810.09063"}, {"title": "causal tree estimation of heterogeneous household response to   time-of-use electricity pricing schemes", "id": "1810.09179", "abstract": "we examine the household-specific effects of the introduction of time-of-use (tou) electricity pricing schemes. using a causal forest (athey and imbens, 2016; wager and athey, 2018; athey et al., 2019), we consider the association between past consumption and survey variables, and the effect of tou pricing on household electricity demand. we describe the heterogeneity in household variables across quartiles of estimated demand response and utilise variable importance measures.   household-specific estimates produced by a causal forest exhibit reasonable associations with covariates. for example, households that are younger, more educated, and that consume more electricity, are predicted to respond more to a new pricing scheme. in addition, variable importance measures suggest that some aspects of past consumption information may be more useful than survey information in producing these estimates.", "categories": "econ.gn q-fin.ec", "created": "2018-10-22", "updated": "2019-10-16", "authors": ["eoghan o'neill", "melvyn weeks"], "url": "https://arxiv.org/abs/1810.09179"}, {"title": "model selection techniques -- an overview", "id": "1810.09583", "abstract": "in the era of big data, analysts usually explore various statistical models or machine learning methods for observed data in order to facilitate scientific discoveries or gain predictive power. whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus central to scientific studies in fields such as ecology, economics, engineering, finance, political science, biology, and epidemiology. there has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. a considerable number of methods have been proposed, following different philosophies and exhibiting varying performances. the purpose of this article is to bring a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. we provide integrated and practically relevant discussions on theoretical properties of state-of- the-art model selection approaches. we also share our thoughts on some controversial views on the practice of model selection.", "categories": "stat.ml cs.it cs.lg econ.em math.it physics.app-ph", "created": "2018-10-22", "updated": "", "authors": ["jie ding", "vahid tarokh", "yuhong yang"], "url": "https://arxiv.org/abs/1810.09583"}, {"title": "a community microgrid architecture with an internal local market", "id": "1810.09803", "abstract": "this work fits in the context of community microgrids, where members of a community can exchange energy and services among themselves, without going through the usual channels of the public electricity grid. we introduce and analyze a framework to operate a community microgrid, and to share the resulting revenues and costs among its members. a market-oriented pricing of energy exchanges within the community is obtained by implementing an internal local market based on the marginal pricing scheme. the market aims at maximizing the social welfare of the community, thanks to the more efficient allocation of resources, the reduction of the peak power to be paid, and the increased amount of reserve, achieved at an aggregate level. a community microgrid operator, acting as a benevolent planner, redistributes revenues and costs among the members, in such a way that the solution achieved by each member within the community is not worse than the solution it would achieve by acting individually. in this way, each member is incentivized to participate in the community on a voluntary basis. the overall framework is formulated in the form of a bilevel model, where the lower level problem clears the market, while the upper level problem plays the role of the community microgrid operator. numerical results obtained on a real test case implemented in belgium show around 54% cost savings on a yearly scale for the community, as compared to the case when its members act individually.", "categories": "cs.sy econ.gn q-fin.ec", "created": "2018-10-23", "updated": "2019-02-20", "authors": ["bertrand corn\u00e9lusse", "iacopo savelli", "simone paoletti", "antonio giannitrapani", "antonio vicino"], "url": "https://arxiv.org/abs/1810.09803"}, {"title": "religion and terrorism: evidence from ramadan fasting", "id": "1810.09869", "abstract": "we study the effect of religion and intense religious experiences on terrorism by focusing on one of the five pillars of islam: ramadan fasting. for identification, we exploit two facts: first, daily fasting from dawn to sunset during ramadan is considered mandatory for most muslims. second, the islamic calendar is not synchronized with the solar cycle. we find a robust negative effect of more intense ramadan fasting on terrorist events within districts and country-years in predominantly muslim countries. this effect seems to operate partly through decreases in public support for terrorism and the operational capabilities of terrorist groups.", "categories": "econ.gn q-fin.ec", "created": "2018-10-23", "updated": "2020-03-05", "authors": ["roland hodler", "paul raschky", "anthony strittmatter"], "url": "https://arxiv.org/abs/1810.09869"}, {"title": "expropriations, property confiscations and new offshore entities:   evidence from the panama papers", "id": "1810.09876", "abstract": "using the panama papers, we show that the beginning of media reporting on expropriations and property confiscations in a country increases the probability that offshore entities are incorporated by agents from the same country in the same month. this result is robust to the use of country-year fixed effects and the exclusion of tax havens. further analysis shows that the effect is driven by countries with non-corrupt and effective governments, which supports the notion that offshore entities are incorporated when reasonably well-intended and well-functioning governments become more serious about fighting organized crime by confiscating proceeds of crime.", "categories": "econ.gn q-fin.ec", "created": "2018-10-23", "updated": "", "authors": ["ralph-christopher bayer", "roland hodler", "paul raschky", "anthony strittmatter"], "url": "https://arxiv.org/abs/1810.09876"}, {"title": "the losses from integration in matching markets can be large", "id": "1810.10287", "abstract": "although the integration of two-sided matching markets using stable mechanisms generates expected gains from integration, i show that there are worst-case scenarios in which these are negative. the losses from integration can be large enough that the average rank of an agent's spouse decreases by 37.5% of the length of their preference list in any stable matching mechanism.", "categories": "econ.th", "created": "2018-10-24", "updated": "", "authors": ["josu\u00e9 ortega"], "url": "https://arxiv.org/abs/1810.10287"}, {"title": "deep neural networks for choice analysis: a statistical learning theory   perspective", "id": "1810.10465", "abstract": "while researchers increasingly use deep neural networks (dnn) to analyze individual choices, overfitting and interpretability issues remain as obstacles in theory and practice. by using statistical learning theory, this study presents a framework to examine the tradeoff between estimation and approximation errors, and between prediction and interpretation losses. it operationalizes the dnn interpretability in the choice analysis by formulating the metrics of interpretation loss as the difference between true and estimated choice probability functions. this study also uses the statistical learning theory to upper bound the estimation error of both prediction and interpretation losses in dnn, shedding light on why dnn does not have the overfitting issue. three scenarios are then simulated to compare dnn to binary logit model (bnl). we found that dnn outperforms bnl in terms of both prediction and interpretation for most of the scenarios, and larger sample size unleashes the predictive power of dnn but not bnl. dnn is also used to analyze the choice of trip purposes and travel modes based on the national household travel survey 2017 (nhts2017) dataset. these experiments indicate that dnn can be used for choice analysis beyond the current practice of demand forecasting because it has the inherent utility interpretation, the flexibility of accommodating various information formats, and the power of automatically learning utility specification. dnn is both more predictive and interpretable than bnl unless the modelers have complete knowledge about the choice task, and the sample size is small. overall, statistical learning theory can be a foundation for future studies in the non-asymptotic data regime or using high-dimensional statistical models in choice analysis, and the experiments show the feasibility and effectiveness of dnn for its wide applications to policy and behavioral analysis.", "categories": "econ.gn q-fin.ec", "created": "2018-10-24", "updated": "2019-09-16", "authors": ["shenhao wang", "qingyi wang", "nate bailey", "jinhua zhao"], "url": "https://arxiv.org/abs/1810.10465"}, {"title": "revealed stochastic preference: a one-paragraph proof and generalization", "id": "1810.10604", "abstract": "mcfadden and richter (1991) and later mcfadden (2005) show that the axiom of revealed stochastic preference characterizes rationalizability of choice probabilities through random utility models on finite universal choice spaces. this note proves the result in one short, elementary paragraph and extends it to set valued choice. the latter requires a different axiom than is reported in mcfadden (2005).", "categories": "econ.th", "created": "2018-10-24", "updated": "2019-02-20", "authors": ["j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1810.10604"}, {"title": "the case for formation of isp-content providers consortiums by nash   bargaining for internet content delivery", "id": "1810.10660", "abstract": "the formation of consortiums of a broadband access internet service provider (isp) and multiple content providers (cp) is considered for large-scale content caching. the consortium members share costs from operations and investments in the supporting infrastructure. correspondingly, the model's cost function includes marginal and fixed costs; the latter has been important in determining industry structure. also, if net neutrality regulations permit, additional network capacity on the isp's last mile may be contracted by the cps. the number of subscribers is determined by a combination of users' price elasticity of demand and quality of experience. the profit generated by a coalition after pricing and design optimization determines the game's characteristic function. coalition formation is by a bargaining procedure due to okada (1996) based on random proposers in a non-cooperative, multi-player game-theoretic framework. a necessary and sufficient condition is obtained for the grand coalition to form, which bounds subsidies from large to small contributors. caching is generally supported even under net neutrality regulations. the grand coalition's profit matches upper bounds. numerical results illustrate the analytic results.", "categories": "econ.gn q-fin.ec", "created": "2018-10-24", "updated": "", "authors": ["debasis mitra", "abhinav sridhar"], "url": "https://arxiv.org/abs/1810.10660"}, {"title": "spanning tests for markowitz stochastic dominance", "id": "1810.10800", "abstract": "we derive properties of the cdf of random variables defined as saddle-type points of real valued continuous stochastic processes. this facilitates the derivation of the first-order asymptotic properties of tests for stochastic spanning given some stochastic dominance relation. we define the concept of markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. we construct a non-parametric test for spanning based on subsampling, and derive its asymptotic exactness and consistency. the spanning methodology determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by markowitz stochastic dominance. in an application to standard data sets of historical stock market returns, we reject market portfolio markowitz efficiency as well as two-fund separation. hence, we find evidence that equity management through base assets can outperform the market, for investors with markowitz type preferences.", "categories": "q-fin.st econ.em q-fin.pm", "created": "2018-10-25", "updated": "", "authors": ["stelios arvanitis", "olivier scaillet", "nikolas topaloglou"], "url": "https://arxiv.org/abs/1810.10800"}, {"title": "nuclear norm regularized estimation of panel regression models", "id": "1810.10987", "abstract": "in this paper we investigate panel regression models with interactive fixed effects. we propose two new estimation methods that are based on minimizing convex objective functions. the first method minimizes the sum of squared residuals with a nuclear (trace) norm regularization. the second method minimizes the nuclear norm of the residuals. we establish the consistency of the two resulting estimators. those estimators have a very important computational advantage compared to the existing least squares (ls) estimator, in that they are defined as minimizers of a convex objective function. in addition, the nuclear norm penalization helps to resolve a potential identification problem for interactive fixed effect models, in particular when the regressors are low-rank and the number of the factors is unknown. we also show how to construct estimators that are asymptotically equivalent to the least squares (ls) estimator in bai (2009) and moon and weidner (2017) by using our nuclear norm regularized or minimized estimators as initial values for a finite number of ls minimizing iteration steps. this iteration avoids any non-convex minimization, while the original ls estimation problem is generally non-convex, and can have multiple local minima.", "categories": "econ.em stat.ml", "created": "2018-10-25", "updated": "2019-03-28", "authors": ["hyungsik roger moon", "martin weidner"], "url": "https://arxiv.org/abs/1810.10987"}, {"title": "factor-driven two-regime regression", "id": "1810.11109", "abstract": "we propose a novel two-regime regression model where regime switching is driven by a vector of possibly unobservable factors. when the factors are latent, we estimate them by the principal component analysis of a panel data set. we show that the optimization problem can be reformulated as mixed integer optimization, and we present two alternative computational algorithms. we derive the asymptotic distribution of the resulting estimator under the scheme that the threshold effect shrinks to zero. in particular, we establish a phase transition that describes the effect of first-stage factor estimation as the cross-sectional dimension of panel data increases relative to the time-series dimension. moreover, we develop bootstrap inference and illustrate our methods via numerical studies.", "categories": "econ.em", "created": "2018-10-25", "updated": "2020-09-10", "authors": ["sokbae lee", "yuan liao", "myung hwan seo", "youngki shin"], "url": "https://arxiv.org/abs/1810.11109"}, {"title": "robust inference using inverse probability weighting", "id": "1810.11397", "abstract": "inverse probability weighting (ipw) is widely used in empirical work in economics and other disciplines. as gaussian approximations perform poorly in the presence of \"small denominators,\" trimming is routinely employed as a regularization strategy. however, ad hoc trimming of the observations renders usual inference procedures invalid for the target estimand, even in large samples. in this paper, we first show that the ipw estimator can have different (gaussian or non-gaussian) asymptotic distributions, depending on how \"close to zero\" the probability weights are and on how large the trimming threshold is. as a remedy, we propose an inference procedure that is robust not only to small probability weights entering the ipw estimator but also to a wide range of trimming threshold choices, by adapting to these different asymptotic distributions. this robustness is achieved by employing resampling techniques and by correcting a non-negligible trimming bias. we also propose an easy-to-implement method for choosing the trimming threshold by minimizing an empirical analogue of the asymptotic mean squared error. in addition, we show that our inference procedure remains valid with the use of a data-driven trimming threshold. we illustrate our method by revisiting a dataset from the national supported work program.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-10-26", "updated": "2019-05-24", "authors": ["xinwei ma", "jingshen wang"], "url": "https://arxiv.org/abs/1810.11397"}, {"title": "the politics of attention", "id": "1810.11449", "abstract": "we develop an equilibrium theory of attention and politics. in a spatial model of electoral competition where candidates have varying policy preferences, we examine what kinds of political behaviors capture voters' limited attention and how this concern affects the overall political outcomes. following the seminal works of downs (1957) and sims (1998), we assume that voters are rationally inattentive and can process information about the policies at a cost proportional to entropy reduction. the main finding is an equilibrium phenomenon called attention- and media-driven extremism, namely as we increase the attention cost or garble the news technology, a truncated set of the equilibria captures voters' attention through enlarging the policy differentials between the varying types of the candidates. we supplement our analysis with historical accounts, and discuss its relevance in the new era featured with greater media choices and distractions, as well as the rise of partisan media and fake news.", "categories": "econ.gn econ.th q-fin.ec", "created": "2018-10-26", "updated": "2019-01-20", "authors": ["li hu", "anqi li"], "url": "https://arxiv.org/abs/1810.11449"}, {"title": "optimal incentive contract with endogenous monitoring technology", "id": "1810.11471", "abstract": "recent technology advances have enabled firms to flexibly process and analyze sophisticated employee performance data at a reduced and yet significant cost. we develop a theory of optimal incentive contracting where the monitoring technology that governs the above procedure is part of the designer's strategic planning. in otherwise standard principal-agent models with moral hazard, we allow the principal to partition agents' performance data into any finite categories and to pay for the amount of information the output signal carries. through analysis of the trade-off between giving incentives to agents and saving the monitoring cost, we obtain characterizations of optimal monitoring technologies such as information aggregation, strict mlrp, likelihood ratio-convex performance classification, group evaluation in response to rising monitoring costs, and assessing multiple task performances according to agents' endogenous tendencies to shirk. we examine the implications of these results for workforce management and firms' internal organizations.", "categories": "econ.th econ.gn q-fin.ec", "created": "2018-10-26", "updated": "2019-11-20", "authors": ["anqi li", "ming yang"], "url": "https://arxiv.org/abs/1810.11471"}, {"title": "intermediated implementation", "id": "1810.11475", "abstract": "we examine problems of ``intermediated implementation,'' in which a single principal can only regulate limited aspects of the consumption bundles traded between intermediaries and agents with hidden characteristics. an example is sales, in which retailers offer menus of consumption bundles to customers with hidden tastes, whereas a manufacturer with a potentially different goal from retailers' is limited to regulating sold consumption goods but not retail prices by legal barriers. we study how the principal can implement through intermediaries any social choice rule that is incentive compatible and individually rational for agents. we demonstrate the effectiveness of per-unit fee schedules and distribution regulations, which hinges on whether intermediaries have private or interdependent values. we give further applications to healthcare regulation and income redistribution.", "categories": "econ.th econ.gn q-fin.ec", "created": "2018-10-26", "updated": "2020-01-20", "authors": ["anqi li", "yiqing xing"], "url": "https://arxiv.org/abs/1810.11475"}, {"title": "semiparametrically efficient estimation of the average linear regression   function", "id": "1810.12511", "abstract": "let y be an outcome of interest, x a vector of treatment measures, and w a vector of pre-treatment control variables. here x may include (combinations of) continuous, discrete, and/or non-mutually exclusive \"treatments\". consider the linear regression of y onto x in a subpopulation homogenous in w = w (formally a conditional linear predictor). let b0(w) be the coefficient vector on x in this regression. we introduce a semiparametrically efficient estimate of the average beta0 = e[b0(w)]. when x is binary-valued (multi-valued) our procedure recovers the (a vector of) average treatment effect(s). when x is continuously-valued, or consists of multiple non-exclusive treatments, our estimand coincides with the average partial effect (ape) of x on y when the underlying potential response function is linear in x, but otherwise heterogenous across agents. when the potential response function takes a general nonlinear/heterogenous form, and x is continuously-valued, our procedure recovers a weighted average of the gradient of this response across individuals and values of x. we provide a simple, and semiparametrically efficient, method of covariate adjustment for settings with complicated treatment regimes. our method generalizes familiar methods of covariate adjustment used for program evaluation as well as methods of semiparametric regression (e.g., the partially linear regression model).", "categories": "econ.em", "created": "2018-10-29", "updated": "", "authors": ["bryan s. graham", "cristine campos de xavier pinto"], "url": "https://arxiv.org/abs/1810.12511"}, {"title": "nighttime light, superlinear growth, and economic inequalities at the   country level", "id": "1810.12996", "abstract": "research has highlighted relationships between size and scaled growth across a large variety of biological and social organisms, ranging from bacteria, through animals and plants, to cities an companies. yet, heretofore, identifying a similar relationship at the country level has proven challenging. one reason is that, unlike the former, countries have predefined borders, which limit their ability to grow \"organically.\" this paper addresses this issue by identifying and validating an effective measure of organic growth at the country level: nighttime light emissions, which serve as a proxy of energy allocations where more productive activity takes place. this indicator is compared to population size to illustrate that while nighttime light emissions are associated with superlinear growth, population size at the country level is associated with sublinear growth. these relationships and their implications for economic inequalities are then explored using high-resolution geospatial datasets spanning the last three decades.", "categories": "econ.gn q-fin.ec", "created": "2018-10-30", "updated": "", "authors": ["ore koren", "laura mann"], "url": "https://arxiv.org/abs/1810.12996"}, {"title": "dynamic assortment optimization with changing contextual information", "id": "1810.13069", "abstract": "in this paper, we study the dynamic assortment optimization problem under a finite selling season of length $t$. at each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (mnl) model. in many practical applications, feature/contexutal information of products is readily available. in this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. in addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. to solve the dynamic assortment optimization under this changing contextual mnl model, we need to simultaneously learn the underlying unknown coefficient and makes the decision on the assortment. to this end, we develop an upper confidence bound (ucb) based policy and establish the regret bound on the order of $\\widetilde o(d\\sqrt{t})$, where $d$ is the dimension of the feature and $\\widetilde o$ suppresses logarithmic dependence. we further established the lower bound $\\omega(d\\sqrt{t}/k)$ where $k$ is the cardinality constraint of an offered assortment, which is usually small. when $k$ is a constant, our policy is optimal up to logarithmic factors. in the exploitation phase of the ucb algorithm, we need to solve a combinatorial optimization for assortment optimization based on the learned information. we further develop an approximation algorithm and an efficient greedy heuristic. the effectiveness of the proposed policy is further demonstrated by our numerical studies.", "categories": "econ.em cs.lg stat.ml", "created": "2018-10-30", "updated": "2019-01-17", "authors": ["xi chen", "yining wang", "yuan zhou"], "url": "https://arxiv.org/abs/1810.13069"}, {"title": "partial mean processes with generated regressors: continuous treatment   effects and nonseparable models", "id": "1811.00157", "abstract": "partial mean with generated regressors arises in several econometric problems, such as the distribution of potential outcomes with continuous treatments and the quantile structural function in a nonseparable triangular model. this paper proposes a nonparametric estimator for the partial mean process, where the second step consists of a kernel regression on regressors that are estimated in the first step. the main contribution is a uniform expansion that characterizes in detail how the estimation error associated with the generated regressor affects the limiting distribution of the marginal integration estimator. the general results are illustrated with two examples: the generalized propensity score for a continuous treatment (hirano and imbens, 2004) and control variables in triangular models (newey, powell, and vella, 1999; imbens and newey, 2009). an empirical application to the job corps program evaluation demonstrates the usefulness of the method.", "categories": "econ.em", "created": "2018-10-31", "updated": "", "authors": ["ying-ying lee"], "url": "https://arxiv.org/abs/1811.00157"}, {"title": "corrigendum to \"managerial incentive problems: a dynamic perspective\"", "id": "1811.00455", "abstract": "this paper corrects some mathematical errors in holmstr\\\"om (1999) and clarifies the assumptions that are sufficient for the results of holmstr\\\"om (1999). the results remain qualitatively the same.", "categories": "econ.th", "created": "2018-10-30", "updated": "2018-11-01", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1811.00455"}, {"title": "identification and estimation of partial effects with proxy variables", "id": "1811.00667", "abstract": "i develop a new identification approach for partial effects in nonseparable models with endogeneity. i use a proxy variable for the unobserved heterogeneity correlated with the endogenous variable to construct a valid control function, where the definition of a proxy variable is the same as in the measurement error literature. the identifying assumptions are distinct from existing methods, in particular instrumental variables and selection on observables approaches, and i provide an alternative identification strategy in settings where existing approaches are not applicable. building on the identification result, i consider three estimation approaches, ranging from nonparametric to flexible parametric methods, and characterize asymptotic properties of the proposed estimators.", "categories": "econ.em", "created": "2018-11-01", "updated": "2020-10-02", "authors": ["kenichi nagasawa"], "url": "https://arxiv.org/abs/1811.00667"}, {"title": "quantum structures in human decision-making: towards quantum expected   utility", "id": "1811.00875", "abstract": "{\\it ellsberg thought experiments} and empirical confirmation of ellsberg preferences pose serious challenges to {\\it subjective expected utility theory} (seut). we have recently elaborated a quantum-theoretic framework for human decisions under uncertainty which satisfactorily copes with the ellsberg paradox and other puzzles of seut. we apply here the quantum-theoretic framework to the {\\it ellsberg two-urn example}, showing that the paradox can be explained by assuming a state change of the conceptual entity that is the object of the decision ({\\it decision-making}, or {\\it dm}, {\\it entity}) and representing subjective probabilities by quantum probabilities. we also model the empirical data we collected in a dm test on human participants within the theoretic framework above. the obtained results are relevant, as they provide a line to model real life, e.g., financial and medical, decisions that show the same empirical patterns as the two-urn experiment.", "categories": "cs.ai econ.gn q-bio.nc q-fin.ec quant-ph", "created": "2018-10-29", "updated": "", "authors": ["sandro sozzo"], "url": "https://arxiv.org/abs/1811.00875"}, {"title": "decision-making in livestock biosecurity practices amidst environmental   and social uncertainty: evidence from an experimental game", "id": "1811.01081", "abstract": "livestock industries are vulnerable to disease threats, which can cost billions of dollars and have substantial negative social ramifications. losses are mitigated through increased use of disease-related biosecurity practices, making increased biosecurity an industry goal. currently, there is no industry-wide standard for sharing information about disease incidence or on-site biosecurity strategies, resulting in uncertainty regarding disease prevalence and biosecurity strategies employed by industry stakeholders. using an experimental simulation game, we examined human participant's willingness to invest in biosecurity when confronted with disease outbreak scenarios. we varied the scenarios by changing the information provided about 1) disease incidence and 2) biosecurity strategy or response by production facilities to the threat of disease. here we show that willingness to invest in biosecurity increases with increased information about disease incidence, but decreases with increased information about biosecurity practices used by nearby facilities. thus, the type or context of the uncertainty confronting the decision maker may be a major factor influencing behavior. our findings suggest that policies and practices that encourage greater sharing of disease incidence information should have the greatest benefit for protecting herd health.", "categories": "econ.gn q-fin.ec", "created": "2018-11-02", "updated": "2019-02-01", "authors": ["scott c. merrill", "christopher j. koliba", "susan m. moegenburg", "asim zia", "jason parker", "timothy sellnow", "serge wiltshire", "gabriela bucini", "caitlin danehy", "julia m. smith"], "url": "https://arxiv.org/abs/1811.01081"}, {"title": "equitable voting rules", "id": "1811.01227", "abstract": "may's theorem (1952), a celebrated result in social choice, provides the foundation for majority rule. may's crucial assumption of symmetry, often thought of as a procedural equity requirement, is violated by many choice procedures that grant voters identical roles. we show that a weakening of may's symmetry assumption allows for a far richer set of rules that still treat voters equally. we show that such rules can have minimal winning coalitions comprising a vanishing fraction of the population, but not less than the square root of the population size. methodologically, we introduce techniques from group theory and illustrate their usefulness for the analysis of social choice questions.", "categories": "econ.th math.gr", "created": "2018-11-03", "updated": "2020-08-29", "authors": ["laurent bartholdi", "wade hann-caruthers", "maya josyula", "omer tamuz", "leeat yariv"], "url": "https://arxiv.org/abs/1811.01227"}, {"title": "uncertainty and robustness of surplus extraction", "id": "1811.01320", "abstract": "this paper studies a robust version of the classic surplus extraction problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are suitable for all possible beliefs in that set. we derive necessary and sufficient conditions for full extraction in this setting, and show that these are natural set-valued analogues of the classic convex independence condition identified by cremer and mclean (1985, 1988). we show that full extraction is neither generically possible nor generically impossible, in contrast to the standard setting in which full extraction is generic. when full extraction fails, we show that natural additional conditions can restrict both the nature of the contracts a designer can offer and the surplus the designer can obtain.", "categories": "econ.th", "created": "2018-11-04", "updated": "", "authors": ["giuseppe lopomo", "luca rigotti", "chris shannon"], "url": "https://arxiv.org/abs/1811.01320"}, {"title": "characterizing permissibility, proper rationalizability, and iterated   admissibility by incomplete information", "id": "1811.01933", "abstract": "we characterize three interrelated concepts in epistemic game theory: permissibility, proper rationalizability, and iterated admissibility. we define the lexicographic epistemic model for a game with incomplete information. based on it, we give two groups of characterizations. the first group characterizes permissibility and proper rationalizability. the second group characterizes permissibility in an alternative way and iterated admissibility. in each group, the conditions for the latter are stronger than those for the former, which corresponds to the fact that proper rationalizability and iterated admissibility are two (compatible) refinements of permissibility within the complete information framework. the intrinsic difference between the two groups are the role of rationality: the first group does not need it, while the second group does.", "categories": "econ.th", "created": "2018-11-04", "updated": "", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1811.01933"}, {"title": "randomization tests for equality in dependence structure", "id": "1811.02105", "abstract": "we develop a new statistical procedure to test whether the dependence structure is identical between two groups. rather than relying on a single index such as pearson's correlation coefficient or kendall's tau, we consider the entire dependence structure by investigating the dependence functions (copulas). the critical values are obtained by a modified randomization procedure designed to exploit asymptotic group invariance conditions. implementation of the test is intuitive and simple, and does not require any specification of a tuning parameter or weight function. at the same time, the test exhibits excellent finite sample performance, with the null rejection rates almost equal to the nominal level even when the sample size is extremely small. two empirical applications concerning the dependence between income and consumption, and the brexit effect on european financial market integration are provided.", "categories": "econ.em", "created": "2018-11-05", "updated": "", "authors": ["juwon seo"], "url": "https://arxiv.org/abs/1811.02105"}, {"title": "the impact of air transport availability on research collaboration: a   case study of four universities", "id": "1811.02106", "abstract": "this paper analyzes the impact of air transport connectivity and accessibility on scientific collaboration.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2018-11-05", "updated": "", "authors": ["adam ploszaj", "xiaoran yan", "katy borner"], "url": "https://arxiv.org/abs/1811.02106"}, {"title": "time will tell - recovering preferences when choices are noisy", "id": "1811.02497", "abstract": "the ability to uncover preferences from choices is fundamental for both positive economics and welfare analysis. overwhelming evidence shows that choice is stochastic, which has given rise to random utility models as the dominant paradigm in applied microeconomics. however, as is well known, it is not possible to infer the structure of preferences in the absence of assumptions on the structure of noise. this makes it impossible to empirically test the structure of noise independently from the structure of preferences. here, we show that the difficulty can be bypassed if data sets are enlarged to include response times. a simple condition on response time distributions (a weaker version of first order stochastic dominance) ensures that choices reveal preferences without assumptions on the structure of utility noise. sharper results are obtained if the analysis is restricted to specific classes of models. under symmetric noise, response times allow to uncover preferences for choice pairs outside the data set, and if noise is fechnerian, even choice probabilities can be forecast out of sample. we conclude by showing that standard random utility models from economics and standard drift-diffusion models from psychology necessarily generate data sets fulfilling our sufficient condition on response time distributions.", "categories": "econ.gn q-fin.ec", "created": "2018-11-06", "updated": "", "authors": ["carlos alos-ferrer", "ernst fehr", "nick netzer"], "url": "https://arxiv.org/abs/1811.02497"}, {"title": "nonparametric analysis of finite mixtures", "id": "1811.02727", "abstract": "finite mixture models are useful in applied econometrics. they can be used to model unobserved heterogeneity, which plays major roles in labor economics, industrial organization and other fields. mixtures are also convenient in dealing with contaminated sampling models and models with multiple equilibria. this paper shows that finite mixture models are nonparametrically identified under weak assumptions that are plausible in economic applications. the key is to utilize the identification power implied by information in covariates variation. first, three identification approaches are presented, under distinct and non-nested sets of sufficient conditions. observable features of data inform us which of the three approaches is valid. these results apply to general nonparametric switching regressions, as well as to structural econometric models, such as auction models with unobserved heterogeneity. second, some extensions of the identification results are developed. in particular, a mixture regression where the mixing weights depend on the value of the regressors in a fully unrestricted manner is shown to be nonparametrically identifiable. this means a finite mixture model with function-valued unobserved heterogeneity can be identified in a cross-section setting, without restricting the dependence pattern between the regressor and the unobserved heterogeneity. in this aspect it is akin to fixed effects panel data models which permit unrestricted correlation between unobserved heterogeneity and covariates. third, the paper shows that fully nonparametric estimation of the entire mixture model is possible, by forming a sample analogue of one of the new identification strategies. the estimator is shown to possess a desirable polynomial rate of convergence as in a standard nonparametric estimation problem, despite nonregular features of the model.", "categories": "econ.em", "created": "2018-11-06", "updated": "", "authors": ["yuichi kitamura", "louise laage"], "url": "https://arxiv.org/abs/1811.02727"}, {"title": "nonparametric maximum likelihood methods for binary response models with   random coefficients", "id": "1811.03329", "abstract": "single index linear models for binary response with random coefficients have been extensively employed in many econometric settings under various parametric specifications of the distribution of the random coefficients. nonparametric maximum likelihood estimation (npmle) as proposed by cosslett (1983) and ichimura and thompson (1998), in contrast, has received less attention in applied work due primarily to computational difficulties. we propose a new approach to computation of npmles for binary response models that significantly increase their computational tractability thereby facilitating greater flexibility in applications. our approach, which relies on recent developments involving the geometry of hyperplane arrangements, is contrasted with the recently proposed deconvolution method of gautier and kitamura (2013). an application to modal choice for the journey to work in the washington dc area illustrates the methods.", "categories": "econ.em", "created": "2018-11-08", "updated": "2020-01-14", "authors": ["jiaying gu", "roger koenker"], "url": "https://arxiv.org/abs/1811.03329"}, {"title": "mechanism design with limited commitment", "id": "1811.03579", "abstract": "we develop a tool akin to the revelation principle for mechanism design with limited commitment. we identify a canonical class of mechanisms rich enough to replicate the payoffs of any equilibrium in a mechanism-selection game between an uninformed designer and a privately informed agent. a cornerstone of our methodology is the idea that a mechanism should encode not only the rules that determine the allocation, but also the information the designer obtains from the interaction with the agent. therefore, how much the designer learns, which is the key tension in design with limited commitment, becomes an explicit part of the design. we show how this insight can be used to transform the designer's problem into a constrained optimization one: to the usual truthtelling and participation constraints, one must add the designer's sequential rationality constraint.", "categories": "econ.th", "created": "2018-11-08", "updated": "2020-08-25", "authors": ["laura doval", "vasiliki skreta"], "url": "https://arxiv.org/abs/1811.03579"}, {"title": "constrained information design: toolkit", "id": "1811.03588", "abstract": "these notes show the tools in le treust and tomala(2017) extend to the case of multiple inequality and equality constraints. this showcases the power of the results in that paper to analyze problems of information design subject to constraints. in fact, we show in doval and skreta (2018) that they can be used to provide an upper bound on the number of posteriors a designer with limited commitment uses in his optimal mechanism.", "categories": "econ.th", "created": "2018-11-08", "updated": "", "authors": ["laura doval", "vasiliki skreta"], "url": "https://arxiv.org/abs/1811.03588"}, {"title": "incentivising participation in liquid democracy with breadth-first   delegation", "id": "1811.03710", "abstract": "liquid democracy allows members of an electorate to either directly vote over alternatives, or delegate their voting rights to someone they trust. most of the liquid democracy literature and implementations allow each voter to nominate only one delegate per election. however, if that delegate abstains, the voting rights assigned to her are left unused. to minimise the number of unused delegations, it has been suggested that each voter should declare a personal ranking over voters she trusts. in this paper, we show that even if personal rankings over voters are declared, the standard delegation method of liquid democracy remains problematic. more specifically, we show that when personal rankings over voters are declared, it could be undesirable to receive delegated voting rights, which is contrary to what liquid democracy fundamentally relies on. to solve this issue, we propose a new method to delegate voting rights in an election, called breadth-first delegation. additionally, the proposed method prioritises assigning voting rights to individuals closely connected to the voters who delegate.", "categories": "econ.th cs.gt", "created": "2018-11-08", "updated": "2019-02-25", "authors": ["grammateia kotsialou", "luke riley"], "url": "https://arxiv.org/abs/1811.03710"}, {"title": "estimation of a structural break point in linear regression models", "id": "1811.03720", "abstract": "this study proposes a point estimator of the break location for a one-time structural break in linear regression models. if the break magnitude is small, the least-squares estimator of the break date has two modes at the ends of the finite sample period, regardless of the true break location. to solve this problem, i suggest an alternative estimator based on a modification of the least-squares objective function. the modified objective function incorporates estimation uncertainty that varies across potential break dates. the new break point estimator is consistent and has a unimodal finite sample distribution under small break magnitudes. a limit distribution is provided under an in-fill asymptotic framework. monte carlo simulation results suggest that the new estimator outperforms the least-squares estimator. i apply the method to estimate the break date in u.s. real gdp growth and u.s. and uk stock return prediction models.", "categories": "econ.em", "created": "2018-11-08", "updated": "2020-06-03", "authors": ["yaein baek"], "url": "https://arxiv.org/abs/1811.03720"}, {"title": "how does stock market volatility react to oil shocks?", "id": "1811.03820", "abstract": "we study the impact of oil price shocks on the u.s. stock market volatility. we jointly analyze three different structural oil market shocks (i.e., aggregate demand, oil supply, and oil-specific demand shocks) and stock market volatility using a structural vector autoregressive model. identification is achieved by assuming that the price of crude oil reacts to stock market volatility only with delay. this implies that innovations to the price of crude oil are not strictly exogenous, but predetermined with respect to the stock market. we show that volatility responds significantly to oil price shocks caused by unexpected changes in aggregate and oil-specific demand, whereas the impact of supply-side shocks is negligible.", "categories": "econ.em econ.gn q-fin.ec", "created": "2018-11-09", "updated": "", "authors": ["andrea bastianin", "matteo manera"], "url": "https://arxiv.org/abs/1811.03820"}, {"title": "bootstrapping structural change tests", "id": "1811.04125", "abstract": "this paper analyses the use of bootstrap methods to test for parameter change in linear models estimated via two stage least squares (2sls). two types of test are considered: one where the null hypothesis is of no change and the alternative hypothesis involves discrete change at k unknown break-points in the sample; and a second test where the null hypothesis is that there is discrete parameter change at l break-points in the sample against an alternative in which the parameters change at l + 1 break-points. in both cases, we consider inferences based on a sup-wald-type statistic using either the wild recursive bootstrap or the wild fixed bootstrap. we establish the asymptotic validity of these bootstrap tests under a set of general conditions that allow the errors to exhibit conditional and/or unconditional heteroskedasticity, and report results from a simulation study that indicate the tests yield reliable inferences in the sample sizes often encountered in macroeconomics. the analysis covers the cases where the first-stage estimation of 2sls involves a model whose parameters are either constant or themselves subject to discrete parameter change. if the errors exhibit unconditional heteroskedasticity and/or the reduced form is unstable then the bootstrap methods are particularly attractive because the limiting distributions of the test statistics are not pivotal.", "categories": "econ.em", "created": "2018-11-09", "updated": "", "authors": ["otilia boldea", "adriana cornea-madeira", "alastair r. hall"], "url": "https://arxiv.org/abs/1811.04125"}, {"title": "the augmented synthetic control method", "id": "1811.04170", "abstract": "the synthetic control method (scm) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. the \"synthetic control\" is a weighted average of control units that balances the treated unit's pre-treatment outcomes as closely as possible. a critical feature of the original proposal is to use scm only when the fit on pre-treatment outcomes is excellent. we propose augmented scm as an extension of scm to settings where such pre-treatment fit is infeasible. analogous to bias correction for inexact matching, augmented scm uses an outcome model to estimate the bias due to imperfect pre-treatment fit and then de-biases the original scm estimate. our main proposal, which uses ridge regression as the outcome model, directly controls pre-treatment fit while minimizing extrapolation from the convex hull. this estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. we bound the estimation error of this approach under different data generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. we demonstrate gains from augmented scm with extensive simulation studies and apply this framework to estimate the impact of the 2012 kansas tax cuts on economic growth. we implement the proposed method in the new augsynth r package.", "categories": "stat.me econ.em", "created": "2018-11-09", "updated": "2020-07-23", "authors": ["eli ben-michael", "avi feller", "jesse rothstein"], "url": "https://arxiv.org/abs/1811.04170"}, {"title": "multilateral index number systems for international price comparisons:   properties, existence and uniqueness", "id": "1811.04197", "abstract": "over the past five decades a number of multilateral index number systems have been proposed for spatial and cross-country price comparisons. these multilateral indexes are usually expressed as solutions to systems of linear or nonlinear equations. in this paper, we provide general theorems that can be used to establish necessary and sufficient conditions for the existence and uniqueness of the geary-khamis, idb, neary and rao indexes as well as potential new systems including two generalized systems of index numbers. one of our main results is that the necessary and sufficient conditions for existence and uniqueness of solutions can often be stated in terms of graph-theoretic concepts and a verifiable condition based on observed quantities of commodities.", "categories": "econ.th econ.gn q-fin.ec", "created": "2018-11-10", "updated": "2018-12-13", "authors": ["gholamreza hajargasht", "prasada rao"], "url": "https://arxiv.org/abs/1811.04197"}, {"title": "a model of competing narratives", "id": "1811.04232", "abstract": "we formalize the argument that political disagreements can be traced to a \"clash of narratives\". drawing on the \"bayesian networks\" literature, we model a narrative as a causal model that maps actions into consequences, weaving a selection of other random variables into the story. an equilibrium is defined as a probability distribution over narrative-policy pairs that maximizes a representative agent's anticipatory utility, capturing the idea that public opinion favors hopeful narratives. our equilibrium analysis sheds light on the structure of prevailing narratives, the variables they involve, the policies they sustain and their contribution to political polarization.", "categories": "econ.th", "created": "2018-11-10", "updated": "", "authors": ["kfir eliaz", "ran spiegler"], "url": "https://arxiv.org/abs/1811.04232"}, {"title": "capital structure and speed of adjustment in u.s. firms. a comparative   study in microeconomic and macroeconomic conditions - a quantille regression   approach", "id": "1811.04473", "abstract": "the major perspective of this paper is to provide more evidence regarding how \"quickly\", in different macroeconomic states, companies adjust their capital structure to their leverage targets. this study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic factors across all quantiles of distribution of leverage (book leverage and market leverage). therefore, depending on a partial adjustment model, we find that the adjustment speed fluctuated in different stages of book versus market leverage. furthermore, while macroeconomic states change, we detect clear differentiations of the contribution and the effects of the firm-specific and the macroeconomic variables between market leverage and book leverage debt ratios. consequently, we deduce that across different macroeconomic states the nature and maturity of borrowing influence the persistence and endurance of the relation between determinants and borrowing.", "categories": "econ.em econ.gn q-fin.ec", "created": "2018-11-11", "updated": "", "authors": ["andreas kaloudis", "dimitrios tsolis"], "url": "https://arxiv.org/abs/1811.04473"}, {"title": "a simple combinatorial model of world economic history", "id": "1811.04502", "abstract": "we use a simple combinatorial model of technological change to explain the industrial revolution. the industrial revolution was a sudden large improvement in technology, which resulted in significant increases in human wealth and life spans. in our model, technological change is combining or modifying earlier goods to produce new goods. the underlying process, which has been the same for at least 200,000 years, was sure to produce a very long period of relatively slow change followed with probability one by a combinatorial explosion and sudden takeoff. thus, in our model, after many millennia of relative quiescence in wealth and technology, a combinatorial explosion created the sudden takeoff of the industrial revolution.", "categories": "econ.gn q-fin.ec", "created": "2018-11-11", "updated": "", "authors": ["roger koppl", "abigail devereaux", "jim herriot", "stuart kauffman"], "url": "https://arxiv.org/abs/1811.04502"}, {"title": "how to increase global wealth inequality for fun and profit", "id": "1811.04994", "abstract": "we point out a simple equities trading strategy that allows a sufficiently large, market-neutral, quantitative hedge fund to achieve outsized returns while simultaneously contributing significantly to increasing global wealth inequality. overnight and intraday return distributions in major equity indices in the united states, canada, france, germany, and japan suggest a few such firms have been implementing this strategy successfully for more than twenty-five years.", "categories": "econ.gn q-fin.ec q-fin.tr", "created": "2018-11-12", "updated": "", "authors": ["bruce knuteson"], "url": "https://arxiv.org/abs/1811.04994"}, {"title": "m equilibrium: a dual theory of beliefs and choices in games", "id": "1811.05138", "abstract": "we introduce a set-valued generalization of nash equilibrium, called m equilibrium, which is based on ordinal monotonicity - players' choice probabilities are ranked the same as the expected payoffs based on their beliefs - and ordinal consistency - players' beliefs yield the same ranking of expected payoffs as their choices. using results from semi-algebraic geometry, we prove there exist a finite number of m equilibria, each consisting of a finite number of connected components. generically, m-equilibria can be \"color coded\" by their ranks in the sense that choices and beliefs belonging to the same m equilibrium have the same color. we show that colorable m equilibria are behaviorally stable, a concept that strengthens strategic stability. furthermore, set-valued and parameter-free m equilibrium envelopes various parametric models based on fixed-points, including qre as well as a new and computationally simpler class of models called {\\mu} equilibrium. we report the results of several experiments designed to contrast m equilibrium predictions with those of existing behavioral game-theory models. a first experiment considers five variations of an asymmetric-matching pennies game that leave the predictions of nash, various versions of qre, and level-k unaltered. however, observed choice frequencies differ substantially and significantly across games as do players' beliefs. moreover, beliefs and choices are heterogeneous and beliefs do not match choices in any of the games. these findings contradict existing behavioral game-theory models but accord well with the unique m equilibrium. follow up experiments employ 3 by 3 games with a unique pure-strategy nash equilibrium and multiple m equilibria. the belief and choice data exhibit coordination problems that could not be anticipated through the lens of existing behavioral game-theory models.", "categories": "econ.th cs.gt", "created": "2018-11-13", "updated": "", "authors": ["jacob k. goeree", "philippos louis"], "url": "https://arxiv.org/abs/1811.05138"}, {"title": "health care expenditures, financial stability, and participation in the   supplemental nutrition assistance program (snap)", "id": "1811.05421", "abstract": "this paper examines the association between household healthcare expenses and participation in the supplemental nutrition assistance program (snap) when moderated by factors associated with financial stability of households. using a large longitudinal panel encompassing eight years, this study finds that an inter-temporal increase in out-of-pocket medical expenses increased the likelihood of household snap participation in the current period. financially stable households with precautionary financial assets to cover at least 6 months worth of household expenses were significantly less likely to participate in snap. the low income households who recently experienced an increase in out of pocket medical expenses but had adequate precautionary savings were less likely than similar households who did not have precautionary savings to participate in snap. implications for economists, policy makers, and household finance professionals are discussed.", "categories": "econ.gn q-fin.ec", "created": "2018-11-13", "updated": "", "authors": ["yunhee chang", "jinhee kim", "swarn chatterjee"], "url": "https://arxiv.org/abs/1811.05421"}, {"title": "identification of semiparametric discrete outcome models with bounded   covariates", "id": "1811.05555", "abstract": "identification of discrete outcome models is often established by using special covariates that have full support. this paper shows how these identification results can be extended to a large class of commonly used semiparametric discrete outcome models when all covariates are bounded. i apply the proposed methodology to multinomial choice models, bundles models, and finite games of complete information.", "categories": "econ.em", "created": "2018-11-13", "updated": "", "authors": ["nail kashaev"], "url": "https://arxiv.org/abs/1811.05555"}, {"title": "estimation of high-dimensional seemingly unrelated regression models", "id": "1811.05567", "abstract": "in this paper, we investigate seemingly unrelated regression (sur) models that allow the number of equations (n) to be large, and to be comparable to the number of the observations in each equation (t). it is well known in the literature that the conventional sur estimator, for example, the generalized least squares (gls) estimator of zellner (1962) does not perform well. as the main contribution of the paper, we propose a new feasible gls estimator called the feasible graphical lasso (fglasso) estimator. for a feasible implementation of the gls estimator, we use the graphical lasso estimation of the precision matrix (the inverse of the covariance matrix of the equation system errors) assuming that the underlying unknown precision matrix is sparse. we derive asymptotic theories of the new estimator and investigate its finite sample properties via monte-carlo simulations.", "categories": "econ.em", "created": "2018-11-13", "updated": "", "authors": ["lidan tan", "khai x. chiong", "hyungsik roger moon"], "url": "https://arxiv.org/abs/1811.05567"}, {"title": "navigating the cryptocurrency landscape: an islamic perspective", "id": "1811.05935", "abstract": "almost a decade on from the launch of bitcoin, cryptocurrencies continue to generate headlines and intense debate. what started as an underground experiment by a rag tag group of programmers armed with a libertarian manifesto has now resulted in a thriving $230 billion ecosystem, with constant on-going innovation. scholars and researchers alike are realizing that cryptocurrencies are far more than mere technical innovation; they represent a distinct and revolutionary new economic paradigm tending towards decentralization. unfortunately, this bold new universe is little explored from the perspective of islamic economics and finance. our work aims to address these deficiencies. our paper makes the following distinct contributions we significantly expand the discussion on whether cryptocurrencies qualify as \"money\" from an islamic perspective and we argue that this debate necessitates rethinking certain fundamental definitions. we conclude that the cryptocurrency phenomenon, with its radical new capabilities, may hold considerable opportunity which merits deeper investigation.", "categories": "econ.gn q-fin.ec", "created": "2018-11-14", "updated": "", "authors": ["hina binte haq", "syed taha ali"], "url": "https://arxiv.org/abs/1811.05935"}, {"title": "operator-theoretical treatment of ergodic theorem and its application to   dynamic models in economics", "id": "1811.06107", "abstract": "the purpose of this paper is to study the time average behavior of markov chains with transition probabilities being kernels of completely continuous operators, and therefore to provide a sufficient condition for a class of markov chains that are frequently used in dynamic economic models to be ergodic. the paper reviews the time average convergence of the quasi-weakly complete continuity markov operators to a unique projection operator. also, it shows that a further assumption of quasi-strongly complete continuity reduces the dependence of the unique invariant measure on its corresponding initial distribution through ergodic decomposition, and therefore guarantees the markov chain to be ergodic up to multiplication of constant coefficients. moreover, a sufficient and practical condition is provided for the ergodicity in economic state markov chains that are induced by exogenous random shocks and a correspondence between the exogenous space and the state space.", "categories": "math.pr econ.th", "created": "2018-11-14", "updated": "", "authors": ["shizhou xu"], "url": "https://arxiv.org/abs/1811.06107"}, {"title": "measuring knowledge for recognition and knowledge entropy", "id": "1811.06135", "abstract": "people employ their knowledge to recognize things. this paper is concerned with how to measure people's knowledge for recognition and how it changes. the discussion is based on three assumptions. firstly, we construct two evolution process equations, of which one is for uncertainty and knowledge, and the other for uncertainty and ignorance. secondly, by solving the equations, formulas for measuring the levels of knowledge and the levels of ignorance are obtained in two particular cases. thirdly, a new concept of knowledge entropy is introduced. its similarity with boltzmann's entropy and its difference with shannon's entropy are examined. finally, it is pointed out that the obtained formulas of knowledge and knowledge entropy reflect two fundamental principles: (1) the knowledge level of a group is not necessarily a simple sum of the individuals' knowledge levels; and (2) an individual's knowledge entropy never increases if the individual's thirst for knowledge never decreases.", "categories": "econ.th", "created": "2018-11-14", "updated": "", "authors": ["fujun hou"], "url": "https://arxiv.org/abs/1811.06135"}, {"title": "the effects of non-tariff measures on agri-food trade: a review and   meta-analysis of empirical evidence", "id": "1811.06323", "abstract": "the increasing policy interests and the vivid academic debate on non-tariff measures (ntms) has stimulated a growing literature on how ntms affect agrifood trade. the empirical literature provides contrasting and heterogeneous evidence, with some studies supporting the standards as catalysts view, and others favouring the standards as barriers explanation. to the extent that ntms can influence trade, understanding the prevailing effect, and the motivations behind one effect or the other, is a pressing issue. we review a large body of empirical evidence on the effect of ntms on agri-food trade and conduct a meta-analysis to disentangle potential determinants of heterogeneity in estimates. our findings show the role played by the publication process and by study-specific assumptions. some characteristics of the studies are correlated with positive significant estimates, others covary with negative significant estimates. overall, we found that the effects of ntms vary across types of ntms, proxy for ntms, and levels of details of studies. not negligible is the influence of methodological issues and publication process.", "categories": "econ.gn q-fin.ec", "created": "2018-11-15", "updated": "", "authors": ["fabio gaetano santeramo", "emilia lamonaca"], "url": "https://arxiv.org/abs/1811.06323"}, {"title": "economics of human-ai ecosystem: value bias and lost utility in   multi-dimensional gaps", "id": "1811.06606", "abstract": "in recent years, artificial intelligence (ai) decision-making and autonomous systems became an integrated part of the economy, industry, and society. the evolving economy of the human-ai ecosystem raising concerns regarding the risks and values inherited in ai systems. this paper investigates the dynamics of creation and exchange of values and points out gaps in perception of cost-value, knowledge, space and time dimensions. it shows aspects of value bias in human perception of achievements and costs that encoded in ai systems. it also proposes rethinking hard goals definitions and cost-optimal problem-solving principles in the lens of effectiveness and efficiency in the development of trusted machines. the paper suggests a value-driven with cost awareness strategy and principles for problem-solving and planning of effective research progress to address real-world problems that involve diverse forms of achievements, investments, and survival scenarios.", "categories": "cs.ai econ.gn q-fin.ec", "created": "2018-11-15", "updated": "2018-11-18", "authors": ["daniel muller"], "url": "https://arxiv.org/abs/1811.06606"}, {"title": "fair and efficient division among families", "id": "1811.06684", "abstract": "fair division theory mostly involves individual consumption. but resources are often allocated to groups, such as families or countries, whose members consume the same bundle but have different preferences. do fair and efficient allocations exist in such an \"economy of families\"? we adapt three common notions of fairness: fair-share, no-envy and egalitarian-equivalence, to an economy of families. the stronger adaptation --- individual fairness --- requires that each individual in each family perceives the division as fair; the weaker one --- family fairness --- requires that the family as a whole, treated as a single agent with (typically) incomplete preferences, perceives the division as fair. individual-fair-share, family-no-envy and family-egalitarian-equivalence are compatible with efficiency under broad conditions. the same holds for individual-no-envy when there are only two families. in contrast, individual-no-envy with three or more families and individual-egalitarian-equivalence with two or more families are typically incompatible with efficiency, unlike the situation in an economy of individuals. the common market equilibrium approach to fairness is of limited use in economies with families. in contrast, the leximin approach is broadly applicable: it yields an efficient, individual-fair-share, and family-egalitarian-equivalent allocation.", "categories": "econ.th", "created": "2018-11-16", "updated": "2018-11-22", "authors": ["sophie bade", "erel segal-halevi"], "url": "https://arxiv.org/abs/1811.06684"}, {"title": "incentivizing the dynamic workforce: learning contracts in the   gig-economy", "id": "1811.06736", "abstract": "in principal-agent models, a principal offers a contract to an agent to perform a certain task. the agent exerts a level of effort that maximizes her utility. the principal is oblivious to the agent's chosen level of effort, and conditions her wage only on possible outcomes. in this work, we consider a model in which the principal is unaware of the agent's utility and action space. she sequentially offers contracts to identical agents, and observes the resulting outcomes. we present an algorithm for learning the optimal contract under mild assumptions. we bound the number of samples needed for the principal obtain a contract that is within $\\epsilon$ of her optimal net profit for every $\\epsilon>0$.", "categories": "cs.gt cs.lg econ.th stat.ml", "created": "2018-11-16", "updated": "", "authors": ["alon cohen", "moran koren", "argyrios deligkas"], "url": "https://arxiv.org/abs/1811.06736"}, {"title": "evolution and structure of technological systems - an innovation output   network", "id": "1811.06772", "abstract": "this study examines the network of supply and use of significant innovations across industries in sweden, 1970-2013. it is found that 30% of innovation patterns can be predicted by network stimulus from backward and forward linkages. the network is hierarchical, characterized by hubs that connect diverse industries in closely knitted communities. to explain the network structure, a preferential weight assignment process is proposed as an adaptation of the classical preferential attachment process to weighted directed networks. the network structure is strongly predicted by this process where historical technological linkages and proximities matter, while human capital flows and economic input-output flows have conflicting effects on link formation. the results are consistent with the idea that innovations emerge in closely connected communities, but suggest that the transformation of technological systems are shaped by technological requirements, imbalances and opportunities that are not straightforwardly related to other proximities.", "categories": "econ.gn q-fin.ec", "created": "2018-11-16", "updated": "", "authors": ["josef taalbi"], "url": "https://arxiv.org/abs/1811.06772"}, {"title": "optimal iterative threshold-kernel estimation of jump diffusion   processes", "id": "1811.07499", "abstract": "in this paper, we propose a new threshold-kernel jump-detection method for jump-diffusion processes, which iteratively applies thresholding and kernel methods in an approximately optimal way to achieve improved finite-sample performance. we use the expected number of jump misclassifications as the objective function to optimally select the threshold parameter of the jump detection scheme. we prove that the objective function is quasi-convex and obtain a new second-order infill approximation of the optimal threshold in closed form. the approximate optimal threshold depends not only on the spot volatility, but also the jump intensity and the value of the jump density at the origin. estimation methods for these quantities are then developed, where the spot volatility is estimated by a kernel estimator with thresholding and the value of the jump density at the origin is estimated by a density kernel estimator applied to those increments deemed to contain jumps by the chosen thresholding criterion. due to the interdependency between the model parameters and the approximate optimal estimators built to estimate them, a type of iterative fixed-point algorithm is developed to implement them. simulation studies for a prototypical stochastic volatility model show that it is not only feasible to implement the higher-order local optimal threshold scheme but also that this is superior to those based only on the first order approximation and/or on average values of the parameters over the estimation time period.", "categories": "math.st econ.em q-fin.st stat.th", "created": "2018-11-18", "updated": "2020-04-04", "authors": ["jos\u00e9 e. figueroa-l\u00f3pez", "cheng li", "jeffrey nisen"], "url": "https://arxiv.org/abs/1811.07499"}, {"title": "complete subset averaging with many instruments", "id": "1811.08083", "abstract": "we propose a two-stage least squares (2sls) estimator whose first stage is the equal-weighted average over a complete subset with $k$ instruments among $k$ available, which we call the complete subset averaging (csa) 2sls. the approximate mean squared error (mse) is derived as a function of the subset size $k$ by the nagar (1959) expansion. the subset size is chosen by minimizing the sample counterpart of the approximate mse. we show that this method achieves the asymptotic optimality among the class of estimators with different subset sizes. to deal with averaging over a growing set of irrelevant instruments, we generalize the approximate mse to find that the optimal $k$ is larger than otherwise. an extensive simulation experiment shows that the csa-2sls estimator outperforms the alternative estimators when instruments are correlated. as an empirical illustration, we estimate the logistic demand function in berry, levinsohn, and pakes (1995) and find the csa-2sls estimate is better supported by economic theory than the alternative estimates.", "categories": "econ.em stat.me", "created": "2018-11-20", "updated": "2020-08-26", "authors": ["seojeong lee", "youngki shin"], "url": "https://arxiv.org/abs/1811.08083"}, {"title": "bayesian inference for structural vector autoregressions identified by   markov-switching heteroskedasticity", "id": "1811.08167", "abstract": "in this study, bayesian inference is developed for structural vector autoregressive models in which the structural parameters are identified via markov-switching heteroskedasticity. in such a model, restrictions that are just-identifying in the homoskedastic case, become over-identifying and can be tested. a set of parametric restrictions is derived under which the structural matrix is globally or partially identified and a savage-dickey density ratio is used to assess the validity of the identification conditions. the latter is facilitated by analytical derivations that make the computations fast and numerical standard errors small. as an empirical example, monetary models are compared using heteroskedasticity as an additional device for identification. the empirical results support models with money in the interest rate reaction function.", "categories": "econ.em", "created": "2018-11-20", "updated": "", "authors": ["helmut l\u00fctkepohl", "tomasz wo\u017aniak"], "url": "https://arxiv.org/abs/1811.08167"}, {"title": "a possible alternative evaluation method for the non-use and nonmarket   values of ecosystem services", "id": "1811.08376", "abstract": "monetization of the non-use and nonmarket values of ecosystem services is important especially in the areas of environmental cost-benefit analysis, management and environmental impact assessment. however, the reliability of valuation estimations has been criticized due to the biases that associated with methods like the popular contingent valuation method (cvm). in order to provide alternative valuation results for comparison purpose, we proposed the possibility of using a method that incorporates fact-based costs and contingent preferences for evaluating non-use and nonmarket values, which we referred to as value allotment method (vam). in this paper, we discussed the economic principles of vam, introduced the performing procedure, analyzed assumptions and potential biases that associated with the method and compared vam with cvm through a case study in guangzhou, china. the case study showed that the vam gave more conservative estimates than the cvm, which could be a merit since cvm often generates overestimated values. we believe that this method can be used at least as a referential alternative to cvm and might be particularly useful in assessing the non-use and nonmarket values of ecosystem services from human-invested ecosystems, such as restored ecosystems, man-made parks and croplands.", "categories": "econ.gn q-fin.ec", "created": "2018-11-20", "updated": "", "authors": ["shuyao wu", "shuangcheng li"], "url": "https://arxiv.org/abs/1811.08376"}, {"title": "the value of forecasts: quantifying the economic gains of accurate   quarter-hourly electricity price forecasts", "id": "1811.08604", "abstract": "we propose a multivariate elastic net regression forecast model for german quarter-hourly electricity spot markets. while the literature is diverse on day-ahead prediction approaches, both the intraday continuous and intraday call-auction prices have not been studied intensively with a clear focus on predictive power. besides electricity price forecasting, we check for the impact of early day-ahead (da) exaa prices on intraday forecasts. another novelty of this paper is the complementary discussion of economic benefits. a precise estimation is worthless if it cannot be utilized. we elaborate possible trading decisions based upon our forecasting scheme and analyze their monetary effects. we find that even simple electricity trading strategies can lead to substantial economic impact if combined with a decent forecasting technique.", "categories": "q-fin.st econ.em q-fin.pm q-fin.rm stat.ap", "created": "2018-11-21", "updated": "", "authors": ["christopher kath", "florian ziel"], "url": "https://arxiv.org/abs/1811.08604"}, {"title": "model instability in predictive exchange rate regressions", "id": "1811.08818", "abstract": "in this paper we aim to improve existing empirical exchange rate models by accounting for uncertainty with respect to the underlying structural representation. within a flexible bayesian non-linear time series framework, our modeling approach assumes that different regimes are characterized by commonly used structural exchange rate models, with their evolution being driven by a markov process. we assume a time-varying transition probability matrix with transition probabilities depending on a measure of the monetary policy stance of the central bank at the home and foreign country. we apply this model to a set of eight exchange rates against the us dollar. in a forecasting exercise, we show that model evidence varies over time and a model approach that takes this empirical evidence seriously yields improvements in accuracy of density forecasts for most currency pairs considered.", "categories": "econ.em", "created": "2018-11-21", "updated": "2018-12-03", "authors": ["niko hauzenberger", "florian huber"], "url": "https://arxiv.org/abs/1811.08818"}, {"title": "the transmission of liquidity shocks via china's segmented money market:   evidence from recent market events", "id": "1811.08949", "abstract": "this is the first study to explore the transmission paths for liquidity shocks in china's segmented money market. we examine how money market transactions create such pathways between china's closely-guarded banking sector and the rest of its financial system, and empirically capture the transmission of liquidity shocks through these pathways during two recent market events. we find strong indications that money market transactions allow liquidity shocks to circumvent certain regulatory restrictions and financial market segmentation in china. our findings suggest that a widespread illiquidity contagion facilitated by money market transactions can happen in china and new policy measures are needed to prevent such contagion.", "categories": "econ.gn q-fin.ec", "created": "2018-11-21", "updated": "", "authors": ["ruoxi lu", "david a. bessler", "david j. leatham"], "url": "https://arxiv.org/abs/1811.08949"}, {"title": "long-run consequences of health insurance promotion when mandates are   not enforceable: evidence from a field experiment in ghana", "id": "1811.09004", "abstract": "we study long-run selection and treatment effects of a health insurance subsidy in ghana, where mandates are not enforceable. we randomly provide different levels of subsidy (1/3, 2/3, and full), with follow-up surveys seven months and three years after the initial intervention. we find that a one-time subsidy promotes and sustains insurance enrollment for all treatment groups, but long-run health care service utilization increases only for the partial subsidy groups. we find evidence that selection explains this pattern: those who were enrolled due to the subsidy, especially the partial subsidy, are more ill and have greater health care utilization.", "categories": "econ.gn q-fin.ec", "created": "2018-11-21", "updated": "2019-06-15", "authors": ["patrick asuming", "hyuncheol bryant kim", "armand sim"], "url": "https://arxiv.org/abs/1811.09004"}, {"title": "new dynamics of energy use and co2 emissions in china", "id": "1811.09475", "abstract": "global achievement of climate change mitigation will heavy reply on how much of co2 emission has and will be released by china. after rapid growth of emissions during last decades, china co2 emissions declined since 2014 that driven by decreased coal consumption, suggesting a possible peak of china coal consumption and co2 emissions. here, by combining a updated methodology and underlying data from different sources, we reported the soaring 5.5% (range: +2.5% to +8.5% for one sigma) increase of china co2 emissions in 2018 compared to 2017, suggesting china co2 is not yet to peak and leaving a big uncertain to whether china emission will continue to rise in the future. although our best estimate of total emission (9.9gt co2 in 2018) is lower than international agencies in the same year, the results show robust on a record-high energy consumption and total co2 emission in 2018. during 2014-2016, china energy intensity (energy consumption per unit of gdp) and total co2 emissions has decreased driven by energy and economic structure optimization. however, the decrease in emissions is now offset by stimulates of heavy industry production under economic downturn that driving coal consumption (+5% in 2018), as well as the surging of natural gas consumption (+18% in 2018) due to the government led coal-to-gas energy transition to reduce local air pollutions. timing policy and actions are urgent needed to address on these new drivers to turn down the total emission growth trend.", "categories": "econ.gn q-fin.ec", "created": "2018-11-23", "updated": "", "authors": ["zhu liu", "bo zheng", "qiang zhang"], "url": "https://arxiv.org/abs/1811.09475"}, {"title": "high dimensional classification through $\\ell_0$-penalized empirical   risk minimization", "id": "1811.09540", "abstract": "we consider a high dimensional binary classification problem and construct a classification procedure by minimizing the empirical misclassification risk with a penalty on the number of selected features. we derive non-asymptotic probability bounds on the estimated sparsity as well as on the excess misclassification risk. in particular, we show that our method yields a sparse solution whose l0-norm can be arbitrarily close to true sparsity with high probability and obtain the rates of convergence for the excess misclassification risk. the proposed procedure is implemented via the method of mixed integer linear programming. its numerical performance is illustrated in monte carlo experiments.", "categories": "stat.me econ.em stat.ml", "created": "2018-11-23", "updated": "", "authors": ["le-yu chen", "sokbae lee"], "url": "https://arxiv.org/abs/1811.09540"}, {"title": "lee-carter method for forecasting mortality for peruvian population", "id": "1811.09622", "abstract": "in this article, we have modeled mortality rates of peruvian female and male populations during the period of 1950-2017 using the lee-carter (lc) model. the stochastic mortality model was introduced by lee and carter (1992) and has been used by many authors for fitting and forecasting the human mortality rates. the singular value decomposition (svd) approach is used for estimation of the parameters of the lc model. utilizing the best fitted auto regressive integrated moving average (arima) model we forecast the values of the time dependent parameter of the lc model for the next thirty years. the forecasted values of life expectancy at different age group with $95\\%$ confidence intervals are also reported for the next thirty years. in this research we use the data, obtained from the peruvian national institute of statistics (inei).", "categories": "econ.gn q-fin.ec", "created": "2018-11-22", "updated": "", "authors": ["j. cerda-hern\u00e1ndez", "a. sikov"], "url": "https://arxiv.org/abs/1811.09622"}, {"title": "heterogenous coefficients, discrete instruments, and identification of   treatment effects", "id": "1811.09837", "abstract": "multidimensional heterogeneity and endogeneity are important features of a wide class of econometric models. we consider heterogenous coefficients models where the outcome is a linear combination of known functions of treatment and heterogenous coefficients. we use control variables to obtain identification results for average treatment effects. with discrete instruments in a triangular model we find that average treatment effects cannot be identified when the number of support points is less than or equal to the number of coefficients. a sufficient condition for identification is that the second moment matrix of the treatment functions given the control is nonsingular with probability one. we relate this condition to identification of average treatment effects with multiple treatments.", "categories": "econ.em stat.me", "created": "2018-11-24", "updated": "", "authors": ["whitney k. newey", "sami stouli"], "url": "https://arxiv.org/abs/1811.09837"}, {"title": "generalized dynamic factor models and volatilities: consistency, rates,   and prediction intervals", "id": "1811.10045", "abstract": "volatilities, in high-dimensional panels of economic time series with a dynamic factor structure on the levels or returns, typically also admit a dynamic factor decomposition. we consider a two-stage dynamic factor model method recovering the common and idiosyncratic components of both levels and log-volatilities. specifically, in a first estimation step, we extract the common and idiosyncratic shocks for the levels, from which a log-volatility proxy is computed. in a second step, we estimate a dynamic factor model, which is equivalent to a multiplicative factor structure for volatilities, for the log-volatility panel. by exploiting this two-stage factor approach, we build one-step-ahead conditional prediction intervals for large $n \\times t$ panels of returns. those intervals are based on empirical quantiles, not on conditional variances; they can be either equal- or unequal- tailed. we provide uniform consistency and consistency rates results for the proposed estimators as both $n$ and $t$ tend to infinity. we study the finite-sample properties of our estimators by means of monte carlo simulations. finally, we apply our methodology to a panel of asset returns belonging to the s&p100 index in order to compute one-step-ahead conditional prediction intervals for the period 2006-2013. a comparison with the componentwise garch benchmark (which does not take advantage of cross-sectional information) demonstrates the superiority of our approach, which is genuinely multivariate (and high-dimensional), nonparametric, and model-free.", "categories": "econ.em", "created": "2018-11-25", "updated": "2019-07-10", "authors": ["matteo barigozzi", "marc hallin"], "url": "https://arxiv.org/abs/1811.10045"}, {"title": "lm-bic model selection in semiparametric models", "id": "1811.10676", "abstract": "this paper studies model selection in semiparametric econometric models. it develops a consistent series-based model selection procedure based on a bayesian information criterion (bic) type criterion to select between several classes of models. the procedure selects a model by minimizing the semiparametric lagrange multiplier (lm) type test statistic from korolev (2018) but additionally rewards simpler models. the paper also develops consistent upward testing (ut) and downward testing (dt) procedures based on the semiparametric lm type specification test. the proposed semiparametric lm-bic and ut procedures demonstrate good performance in simulations. to illustrate the use of these semiparametric model selection procedures, i apply them to the parametric and semiparametric gasoline demand specifications from yatchew and no (2001). the lm-bic procedure selects the semiparametric specification that is nonparametric in age but parametric in all other variables, which is in line with the conclusions in yatchew and no (2001). the results of the ut and dt procedures heavily depend on the choice of tuning parameters and assumptions about the model errors.", "categories": "econ.em", "created": "2018-11-26", "updated": "", "authors": ["ivan korolev"], "url": "https://arxiv.org/abs/1811.10676"}, {"title": "estimation of a heterogeneous demand function with berkson errors", "id": "1811.10690", "abstract": "berkson errors are commonplace in empirical microeconomics. in consumer demand this form of measurement error occurs when the price an individual pays is measured by the (weighted) average price paid by individuals in a specified group (e.g., a county), rather than the true transaction price. we show the importance of such measurement errors for the estimation of demand in a setting with nonseparable unobserved heterogeneity. we develop a consistent estimator using external information on the true distribution of prices. examining the demand for gasoline in the u.s., we document substantial within-market price variability, and show that there are significant spatial differences in the magnitude of berkson errors across regions of the u.s. accounting for berkson errors is found to be quantitatively important for estimating price effects and for welfare calculations. imposing the slutsky shape constraint greatly reduces the sensitivity to berkson errors.", "categories": "econ.em stat.ap", "created": "2018-11-26", "updated": "2019-08-27", "authors": ["richard blundell", "joel horowitz", "matthias parey"], "url": "https://arxiv.org/abs/1811.10690"}, {"title": "modelling social evolutionary processes and peer effects in agricultural   trade networks: the rubber value chain in indonesia", "id": "1811.11476", "abstract": "understanding market participants' channel choices is important to policy makers because it yields information on which channels are effective in transmitting information. these channel choices are the result of a recursive process of social interactions and determine the observable trading networks. they are characterized by feedback mechanisms due to peer interaction and therefore need to be understood as complex adaptive systems (cas). when modelling cas, conventional approaches like regression analyses face severe drawbacks since endogeneity is omnipresent. as an alternative, process-based analyses allow researchers to capture these endogenous processes and multiple feedback loops. this paper applies an agent-based modelling approach (abm) to the empirical example of the indonesian rubber trade. the feedback mechanisms are modelled via an innovative approach of a social matrix, which allows decisions made in a specific period to feed back into the decision processes in subsequent periods, and allows agents to systematically assign different weights to the decision parameters based on their individual characteristics. in the validation against the observed network, uncertainty in the found estimates, as well as under determination of the model, are dealt with via an approach of evolutionary calibration: a genetic algorithm finds the combination of parameters that maximizes the similarity between the simulated and the observed network. results indicate that the sellers' channel choice decisions are mostly driven by physical distance and debt obligations, as well as peer-interaction. within the social matrix, the most influential individuals are sellers that live close by to other traders, are active in social groups and belong to the ethnic majority in their village.", "categories": "econ.gn q-fin.ec", "created": "2018-11-28", "updated": "", "authors": ["thomas kopp", "jan salecker"], "url": "https://arxiv.org/abs/1811.11476"}, {"title": "simple local polynomial density estimators", "id": "1811.11512", "abstract": "this paper introduces an intuitive and easy-to-implement nonparametric density estimator based on local polynomial techniques. the estimator is fully boundary adaptive and automatic, but does not require pre-binning or any other transformation of the data. we study the main asymptotic properties of the estimator, and use these results to provide principled estimation, inference, and bandwidth selection methods. as a substantive application of our results, we develop a novel discontinuity in density testing procedure, an important problem in regression discontinuity designs and other program evaluation settings. an illustrative empirical application is given. two companion stata and r software packages are provided.", "categories": "econ.em stat.me", "created": "2018-11-28", "updated": "2019-06-07", "authors": ["matias d. cattaneo", "michael jansson", "xinwei ma"], "url": "https://arxiv.org/abs/1811.11512"}, {"title": "a residual bootstrap for conditional expected shortfall", "id": "1811.11557", "abstract": "this paper studies a fixed-design residual bootstrap method for the two-step estimator of francq and zako\\\"ian (2015) associated with the conditional expected shortfall. for a general class of volatility models the bootstrap is shown to be asymptotically valid under the conditions imposed by beutner et al. (2018). a simulation study is conducted revealing that the average coverage rates are satisfactory for most settings considered. there is no clear evidence to have a preference for any of the three proposed bootstrap intervals. this contrasts results in beutner et al. (2018) for the var, for which the reversed-tails interval has a superior performance.", "categories": "econ.em", "created": "2018-11-26", "updated": "", "authors": ["alexander heinemann", "sean telg"], "url": "https://arxiv.org/abs/1811.11557"}, {"title": "distribution regression with sample selection, with an application to   wage decompositions in the uk", "id": "1811.11603", "abstract": "we develop a distribution regression model under endogenous sample selection. this model is a semiparametric generalization of the heckman selection model that accommodates much richer patterns of heterogeneity in the selection process and effect of the covariates. the model applies to continuous, discrete and mixed outcomes. we study the identification of the model, and develop a computationally attractive two-step method to estimate the model parameters, where the first step is a probit regression for the selection equation and the second step consists of multiple distribution regressions with selection corrections for the outcome equation. we construct estimators of functionals of interest such as actual and counterfactual distributions of latent and observed outcomes via plug-in rule. we derive functional central limit theorems for all the estimators and show the validity of multiplier bootstrap to carry out functional inference. we apply the methods to wage decompositions in the uk using new data. here we decompose the difference between the male and female wage distributions into four effects: composition, wage structure, selection structure and selection sorting. after controlling for endogenous employment selection, we still find substantial gender wage gap -- ranging from 21% to 40% throughout the (latent) offered wage distribution that is not explained by observable labor market characteristics. we also uncover positive sorting for single men and negative sorting for married women that accounts for a substantive fraction of the gender wage gap at the top of the distribution. these findings can be interpreted as evidence of assortative matching in the marriage market and glass-ceiling in the labor market.", "categories": "econ.em stat.me", "created": "2018-11-28", "updated": "2019-03-01", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "siyi luo"], "url": "https://arxiv.org/abs/1811.11603"}, {"title": "dynamic competitive persuasion", "id": "1811.11664", "abstract": "we examine a dynamic game of competitive persuasion played between two long-lived sellers over $t \\leq \\infty$ periods. each period, each seller provides information via a blackwell experiment to a single short-lived buyer, who buys from the seller whose product has the highest expected quality. we solve for the unique subgame perfect equilibrium of this game, and conduct comparative statics: in particular we find that long horizons lead to less information.", "categories": "math.pr cs.gt econ.gn econ.th q-fin.ec", "created": "2018-11-28", "updated": "2020-03-23", "authors": ["mark whitmeyer"], "url": "https://arxiv.org/abs/1811.11664"}, {"title": "survival investment strategies in a continuous-time market model with   competition", "id": "1811.12491", "abstract": "we consider a stochastic game-theoretic model of an investment market in continuous time with short-lived assets and study strategies, called survival, which guarantee that the relative wealth of an investor who uses such a strategy remains bounded away from zero. the main results consist in obtaining a sufficient condition for a strategy to be survival and showing that all survival strategies are asymptotically close to each other. it is also proved that a survival strategy allows an investor to accumulate wealth in a certain sense faster than competitors.", "categories": "q-fin.mf econ.gn q-fin.ec", "created": "2018-11-29", "updated": "2019-09-04", "authors": ["mikhail zhitlukhin"], "url": "https://arxiv.org/abs/1811.12491"}, {"title": "why are prices proportional to embodied energies?", "id": "1811.12502", "abstract": "the observed proportionality between nominal prices and average embodied energies cannot be interpreted with conventional economic theory. a model is presented that places energy transfers as the focal point of scarcity based on the idea that (1) goods are material rearrangements, and (2) humans can only rearrange matter with energy transfers. modified consumer and producer problems for an autarkic agent show that the opportunity cost of goods are given by their marginal energy transfers, which depend on subjective and objective factors (e.g. consumer preferences and direct energy transfers). allowing for exchange and under perfect competition, nominal prices arise as social manifestations of goods' marginal energy transfers. the proportionality between nominal prices and average embodied energy follows given the relation between the latter and marginal energy transfers.", "categories": "econ.th", "created": "2018-11-29", "updated": "", "authors": ["benjamin leiva"], "url": "https://arxiv.org/abs/1811.12502"}, {"title": "fair odds for noisy probabilities", "id": "1811.12516", "abstract": "we suggest that one individual holds multiple degrees of belief about an outcome, given the evidence. we then investigate the implications of such noisy probabilities for a buyer and a seller of binary options and find the odds agreed upon to ensure zero-expectation betting, differ from those consistent with the relative frequency of outcomes. more precisely, the buyer and the seller agree to odds that are higher (lower) than the reciprocal of their averaged unbiased probabilities when this average indicates the outcome is more (less) likely to occur than chance. the favorite-longshot bias thereby emerges to establish the foundation of an equitable market. as corollaries, our work suggests the old-established way of revealing someone's degree of belief through wagers may be more problematic than previously thought, and implies that betting markets cannot generally promise to support rational decisions.", "categories": "econ.th", "created": "2018-11-29", "updated": "", "authors": ["ulrik w. nash"], "url": "https://arxiv.org/abs/1811.12516"}, {"title": "ordeal mechanisms, information, and the cost-effectiveness of subsidies:   evidence from subsidized eyeglasses in rural china", "id": "1812.00383", "abstract": "the cost-effectiveness of policies providing subsidized goods is often compromised by limited use of the goods provided. through a randomized trial, we test two approaches to improve the cost-effectiveness of a program distributing free eyeglasses to myopic children in rural china. requiring recipients to undergo an ordeal better targeted eyeglasses to those who used them without reducing usage relative to free delivery. an information campaign increased use when eyeglasses were freely delivered but not under an ordeal. free delivery plus information was determined to be the most socially cost-effective approach and obtained the highest rate of eyeglass use.", "categories": "econ.gn q-fin.ec", "created": "2018-12-02", "updated": "", "authors": ["sean sylvia", "xiaochen ma", "yaojiang shi", "scott rozelle", "c. -y. cynthia lin lawell"], "url": "https://arxiv.org/abs/1812.00383"}, {"title": "optimal resource allocation over networks via lottery-based mechanisms", "id": "1812.00501", "abstract": "we show that, in a resource allocation problem, the ex ante aggregate utility of players with cumulative-prospect-theoretic preferences can be increased over deterministic allocations by implementing lotteries. we formulate an optimization problem, called the system problem, to find the optimal lottery allocation. the system problem exhibits a two-layer structure comprised of a permutation profile and optimal allocations given the permutation profile. for any fixed permutation profile, we provide a market-based mechanism to find the optimal allocations and prove the existence of equilibrium prices. we show that the system problem has a duality gap, in general, and that the primal problem is np-hard. we then consider a relaxation of the system problem and derive some qualitative features of the optimal lottery structure.", "categories": "econ.th cs.gt cs.ni math.oc q-fin.rm", "created": "2018-12-02", "updated": "", "authors": ["soham r. phade", "venkat anantharam"], "url": "https://arxiv.org/abs/1812.00501"}, {"title": "strategically simple mechanisms", "id": "1812.00849", "abstract": "we define and investigate a property of mechanisms that we call \"strategic simplicity,\" and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require limited strategic sophistication. we define a mechanism to be strategically simple if choices can be based on first-order beliefs about the other agents' preferences and first-order certainty about the other agents' rationality alone, and there is no need for agents to form higher-order beliefs, because such beliefs are irrelevant to the optimal strategies. all dominant strategy mechanisms are strategically simple. but many more mechanisms are strategically simple. in particular, strategically simple mechanisms may be more flexible than dominant strategy mechanisms in the bilateral trade problem and the voting problem.", "categories": "econ.th", "created": "2018-12-03", "updated": "", "authors": ["tilman borgers", "jiangtao li"], "url": "https://arxiv.org/abs/1812.00849"}, {"title": "the income fluctuation problem with capital income risk: optimality and   stability", "id": "1812.01320", "abstract": "this paper studies the income fluctuation problem with capital income risk (i.e., dispersion in the rate of return to wealth). wealth returns and labor earnings are allowed to be serially correlated and mutually dependent. rewards can be bounded or unbounded. under rather general conditions, we develop a set of new results on the existence and uniqueness of solutions, stochastic stability of the model economy, as well as efficient computation of the ergodic wealth distribution. a variety of applications are discussed. quantitative analysis shows that both stochastic volatility and mean persistence in wealth returns have nontrivial impact on wealth inequality.", "categories": "econ.th", "created": "2018-12-04", "updated": "", "authors": ["qingyin ma", "john stachurski", "alexis akira toda"], "url": "https://arxiv.org/abs/1812.01320"}, {"title": "column generation algorithms for nonparametric analysis of random   utility models", "id": "1812.01400", "abstract": "kitamura and stoye (2014) develop a nonparametric test for linear inequality constraints, when these are are represented as vertices of a polyhedron instead of its faces. they implement this test for an application to nonparametric tests of random utility models. as they note in their paper, testing such models is computationally challenging. in this paper, we develop and implement more efficient algorithms, based on column generation, to carry out the test. these improved algorithms allow us to tackle larger datasets.", "categories": "econ.em", "created": "2018-12-04", "updated": "", "authors": ["bart smeulders"], "url": "https://arxiv.org/abs/1812.01400"}, {"title": "necessary and probably sufficient test for finding valid instrumental   variables", "id": "1812.01412", "abstract": "can instrumental variables be found from data? while instrumental variable (iv) methods are widely used to identify causal effect, testing their validity from observed data remains a challenge. this is because validity of an iv depends on two assumptions, exclusion and as-if-random, that are largely believed to be untestable from data. in this paper, we show that under certain conditions, testing for instrumental variables is possible. we build upon prior work on necessary tests to derive a test that characterizes the odds of being a valid instrument, thus yielding the name \"necessary and probably sufficient\". the test works by defining the class of invalid-iv and valid-iv causal models as bayesian generative models and comparing their marginal likelihood based on observed data. when all variables are discrete, we also provide a method to efficiently compute these marginal likelihoods.   we evaluate the test on an extensive set of simulations for binary data, inspired by an open problem for iv testing proposed in past work. we find that the test is most powerful when an instrument follows monotonicity---effect on treatment is either non-decreasing or non-increasing---and has moderate-to-weak strength; incidentally, such instruments are commonly used in observational studies. among as-if-random and exclusion, it detects exclusion violations with higher power. applying the test to ivs from two seminal studies on instrumental variables and five recent studies from the american economic review shows that many of the instruments may be flawed, at least when all variables are discretized. the proposed test opens the possibility of data-driven validation and search for instrumental variables.", "categories": "stat.me econ.em", "created": "2018-12-04", "updated": "", "authors": ["amit sharma"], "url": "https://arxiv.org/abs/1812.01412"}, {"title": "doubly robust difference-in-differences estimators", "id": "1812.01723", "abstract": "this article proposes doubly robust estimators for the average treatment effect on the treated (att) in difference-in-differences (did) research designs. in contrast to alternative did estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. we also derive the semiparametric efficiency bound for the att in did designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. finally, by paying articular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust did estimators for the att that are also doubly robust for inference. simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. open-source software for implementing the proposed policy evaluation tools is available.", "categories": "econ.em", "created": "2018-11-29", "updated": "2020-05-05", "authors": ["pedro h. c. sant'anna", "jun b. zhao"], "url": "https://arxiv.org/abs/1812.01723"}, {"title": "identifying the effect of persuasion", "id": "1812.02276", "abstract": "we set up an econometric model of persuasion and study identification of key parameters under various scenarios of data availability. we find that a commonly used measure of persuasion does not estimate the persuasion rate of any population in general. we provide formal identification results, recommend several new parameters to estimate, and discuss their interpretation. further, we propose methods for carrying out inference. we revisit the empirical literature on persuasion to show that the persuasive effect is highly heterogeneous. we also show that the existence of a continuous instrument opens up the possibility of point identification for the policy-relevant population.", "categories": "econ.em", "created": "2018-12-05", "updated": "2019-12-06", "authors": ["sung jae jun", "sokbae lee"], "url": "https://arxiv.org/abs/1812.02276"}, {"title": "improved inference on the rank of a matrix", "id": "1812.02337", "abstract": "this paper develops a general framework for conducting inference on the rank of an unknown matrix $\\pi_0$. a defining feature of our setup is the null hypothesis of the form $\\mathrm h_0: \\mathrm{rank}(\\pi_0)\\le r$. the problem is of first order importance because the previous literature focuses on $\\mathrm h_0': \\mathrm{rank}(\\pi_0)= r$ by implicitly assuming away $\\mathrm{rank}(\\pi_0)<r$, which may lead to invalid rank tests due to over-rejections. in particular, we show that limiting distributions of test statistics under $\\mathrm h_0'$ may not stochastically dominate those under $\\mathrm{rank}(\\pi_0)<r$. a multiple test on the nulls $\\mathrm{rank}(\\pi_0)=0,\\ldots,r$, though valid, may be substantially conservative. we employ a testing statistic whose limiting distributions under $\\mathrm h_0$ are highly nonstandard due to the inherent irregular natures of the problem, and then construct bootstrap critical values that deliver size control and improved power. since our procedure relies on a tuning parameter, a two-step procedure is designed to mitigate concerns on this nuisance. we additionally argue that our setup is also important for estimation. we illustrate the empirical relevance of our results through testing identification in linear iv models that allows for clustered data and inference on sorting dimensions in a two-sided matching model with transferrable utility.", "categories": "econ.em math.st stat.me stat.th", "created": "2018-12-05", "updated": "2019-03-25", "authors": ["qihui chen", "zheng fang"], "url": "https://arxiv.org/abs/1812.02337"}, {"title": "quantification of market efficiency based on informational-entropy", "id": "1812.02371", "abstract": "since the 1960s, the question whether markets are efficient or not is controversially discussed. one reason for the difficulty to overcome the controversy is the lack of a universal, but also precise, quantitative definition of efficiency that is able to graduate between different states of efficiency. the main purpose of this article is to fill this gap by developing a measure for the efficiency of markets that fulfill all the stated requirements. it is shown that the new definition of efficiency, based on informational-entropy, is equivalent to the two most used definitions of efficiency from fama and jensen. the new measure therefore enables steps to settle the dispute over the state of efficiency in markets. moreover, it is shown that inefficiency in a market can either arise from the possibility to use information to predict an event with higher than chance level, or can emerge from wrong pricing/ quotes that do not reflect the right probabilities of possible events. finally, the calculation of efficiency is demonstrated on a simple game (of coin tossing), to show how one could exactly quantify the efficiency in any market-like system, if all probabilities are known.", "categories": "q-fin.gn econ.th", "created": "2018-12-06", "updated": "", "authors": ["roland rothenstein"], "url": "https://arxiv.org/abs/1812.02371"}, {"title": "simulation of stylized facts in agent-based computational economic   market models", "id": "1812.02726", "abstract": "we study the qualitative and quantitative appearance of stylized facts in several agent-based computational economic market (abcem) models. we perform our simulations with the sabcemm (simulator for agent-based computational economic market models) tool recently introduced by the authors (trimborn et al. 2019). furthermore, we present novel abcem models created by recombining existing models and study them with respect to stylized facts as well. this can be efficiently performed by the sabcemm tool thanks to its object-oriented software design. the code is available on github (trimborn et al. 2018), such that all results can be reproduced by the reader.", "categories": "econ.gn econ.em q-fin.ec q-fin.tr", "created": "2018-11-27", "updated": "2019-11-15", "authors": ["maximilian beikirch", "simon cramer", "martin frank", "philipp otte", "emma pabich", "torsten trimborn"], "url": "https://arxiv.org/abs/1812.02726"}, {"title": "optimal dynamic auctions are virtual welfare maximizers", "id": "1812.02993", "abstract": "we are interested in the setting where a seller sells sequentially arriving items, one per period, via a dynamic auction. at the beginning of each period, each buyer draws a private valuation for the item to be sold in that period and this valuation is independent across buyers and periods. the auction can be dynamic in the sense that the auction at period $t$ can be conditional on the bids in that period and all previous periods, subject to certain appropriately defined incentive compatible and individually rational conditions. perhaps not surprisingly, the revenue optimal dynamic auctions are computationally hard to find and existing literatures that aim to approximate the optimal auctions are all based on solving complex dynamic programs. it remains largely open on the structural interpretability of the optimal dynamic auctions.   in this paper, we show that any optimal dynamic auction is a virtual welfare maximizer subject to some monotone allocation constraints. in particular, the explicit definition of the virtual value function above arises naturally from the primal-dual analysis by relaxing the monotone constraints. we further develop an ironing technique that gets rid of the monotone allocation constraints. quite different from myerson's ironing approach, our technique is more technically involved due to the interdependence of the virtual value functions across buyers. we nevertheless show that ironing can be done approximately and efficiently, which in turn leads to a fully polynomial time approximation scheme of the optimal dynamic auction.", "categories": "cs.gt econ.th", "created": "2018-12-07", "updated": "", "authors": ["vahab mirrokni", "renato paes leme", "pingzhong tang", "song zuo"], "url": "https://arxiv.org/abs/1812.02993"}, {"title": "a supreme test for periodic explosive garch", "id": "1812.03475", "abstract": "we develop a uniform test for detecting and dating explosive behavior of a strictly stationary garch$(r,s)$ (generalized autoregressive conditional heteroskedasticity) process. namely, we test the null hypothesis of a globally stable garch process with constant parameters against an alternative where there is an 'abnormal' period with changed parameter values. during this period, the change may lead to an explosive behavior of the volatility process. it is assumed that both the magnitude and the timing of the breaks are unknown. we develop a double supreme test for the existence of a break, and then provide an algorithm to identify the period of change. our theoretical results hold under mild moment assumptions on the innovations of the garch process. technically, the existing properties for the qmle in the garch model need to be reinvestigated to hold uniformly over all possible periods of change. the key results involve a uniform weak bahadur representation for the estimated parameters, which leads to weak convergence of the test statistic to the supreme of a gaussian process. in simulations we show that the test has good size and power for reasonably large time series lengths. we apply the test to apple asset returns and bitcoin returns.", "categories": "econ.em", "created": "2018-12-09", "updated": "", "authors": ["stefan richter", "weining wang", "wei biao wu"], "url": "https://arxiv.org/abs/1812.03475"}, {"title": "machine-learned patterns suggest that diversification drives economic   development", "id": "1812.03534", "abstract": "we develop a machine-learning-based method, principal smooth-dynamics analysis (prisda), to identify patterns in economic development and to automate the development of new theory of economic dynamics. traditionally, economic growth is modeled with a few aggregate quantities derived from simplified theoretical models. here, prisda identifies important quantities. applied to 55 years of data on countries' exports, prisda finds that what most distinguishes countries' export baskets is their diversity, with extra weight assigned to more sophisticated products. the weights are consistent with previous measures of product complexity in the literature. the second dimension of variation is a proficiency in machinery relative to agriculture. prisda then couples these quantities with per-capita income and infers the dynamics of the system over time. according to prisda, the pattern of economic development of countries is dominated by a tendency toward increased diversification. moreover, economies appear to become richer after they diversify (i.e., diversity precedes growth). the model predicts that middle-income countries with diverse export baskets will grow the fastest in the coming decades, and that countries will converge onto intermediate levels of income and specialization. prisda is generalizable and may illuminate dynamics of elusive quantities such as diversity and complexity in other natural and social systems.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2018-12-09", "updated": "", "authors": ["charles d. brummitt", "andres gomez-lievano", "ricardo hausmann", "matthew h. bonds"], "url": "https://arxiv.org/abs/1812.03534"}, {"title": "mutual conversion between preference maps and cook-seiford vectors", "id": "1812.03566", "abstract": "in group decision making, the preference map and cook-seiford vector are two concepts as ways of describing ties-permitted ordinal rankings. this paper shows that they are equivalent for representing ties-permitted ordinal rankings. transformation formulas from one to the other are given and the inherent consistency of the mutual conversion is discussed. the proposed methods are illustrated by some examples. some possible future applications of the proposed formulas are also pointed out.", "categories": "econ.th", "created": "2018-12-09", "updated": "", "authors": ["fujun hou"], "url": "https://arxiv.org/abs/1812.03566"}, {"title": "shattering the glass ceiling? how the institutional context mitigates   the gender gap in entrepreneurship", "id": "1812.03771", "abstract": "we examine how the institutional context affects the relationship between gender and opportunity entrepreneurship. to do this, we develop a multi-level model that connects feminist theory at the micro-level to institutional theory at the macro-level. it is hypothesized that the gender gap in opportunity entrepreneurship is more pronounced in low-quality institutional contexts and less pronounced in high-quality institutional contexts. using data from the global entrepreneurship monitor (gem) and regulation data from the economic freedom of the world index (efw), we test our predictions and find evidence in support of our model. our findings suggest that, while there is a gender gap in entrepreneurship, these disparities are reduced as the quality of the institutional context improves.", "categories": "econ.gn q-fin.ec", "created": "2018-12-10", "updated": "", "authors": ["christopher j. boudreaux", "boris nikolaev"], "url": "https://arxiv.org/abs/1812.03771"}, {"title": "influence of high-speed railway system on inter-city travel behavior in   vietnam", "id": "1812.04184", "abstract": "to analyze the influence of introducing the high-speed railway (hsr) system on business and non-business travel behavior, this study develops an integrated inter-city travel demand model to represent trip generations, destination choice, and travel mode choice behavior. the accessibility calculated from the rp/sp (revealed preference/stated preference) combined nested logit model of destination and mode choices is used as an explanatory variable in the trip frequency models. one of the important findings is that additional travel would be induced by introducing hsr. our simulation analyses also reveal that hsr and conventional airlines will be the main modes for middle distances and long distances, respectively. the development of zones may highly influence the destination choices for business purposes, while prices of hsr and low-cost carriers affect choices for non-business purposes. finally, the research reveals that people on non-business trips are more sensitive to changes in travel time, travel cost and regional attributes than people on business trips.", "categories": "econ.gn q-fin.ec", "created": "2018-12-10", "updated": "", "authors": ["tho v. le", "junyi zhang", "makoto chikaraishi", "akimasa fujiwara"], "url": "https://arxiv.org/abs/1812.04184"}, {"title": "the cost of information", "id": "1812.04211", "abstract": "we develop an axiomatic theory of information acquisition that captures the idea of constant marginal costs in information production: the cost of generating two independent signals is the sum of their costs, and generating a signal with probability half costs half its original cost. together with a monotonicity and a continuity conditions, these axioms determine the cost of a signal up to a vector of parameters. these parameters have a clear economic interpretation and determine the difficulty of distinguishing states. we argue that this cost function is a versatile modeling tool that leads to more realistic predictions than mutual information.", "categories": "econ.th", "created": "2018-12-10", "updated": "2019-02-04", "authors": ["luciano pomatto", "philipp strack", "omer tamuz"], "url": "https://arxiv.org/abs/1812.04211"}, {"title": "closing the u.s. gender wage gap requires understanding its   heterogeneity", "id": "1812.04345", "abstract": "in 2016, the majority of full-time employed women in the u.s. earned significantly less than comparable men. the extent to which women were affected by gender inequality in earnings, however, depended greatly on socio-economic characteristics, such as marital status or educational attainment. in this paper, we analyzed data from the 2016 american community survey using a high-dimensional wage regression and applying double lasso to quantify heterogeneity in the gender wage gap. we found that the gap varied substantially across women and was driven primarily by marital status, having children at home, race, occupation, industry, and educational attainment. we recommend that policy makers use these insights to design policies that will reduce discrimination and unequal pay more effectively.", "categories": "econ.em stat.ap stat.ml", "created": "2018-12-11", "updated": "", "authors": ["philipp bach", "victor chernozhukov", "martin spindler"], "url": "https://arxiv.org/abs/1812.04345"}, {"title": "deep neural networks for choice analysis: extracting complete economic   information for interpretation", "id": "1812.04528", "abstract": "while deep neural networks (dnns) have been increasingly applied to choice analysis showing high predictive power, it is unclear to what extent researchers can interpret economic information from dnns. this paper demonstrates that dnns can provide economic information as complete as classical discrete choice models (dcms). the economic information includes choice predictions, choice probabilities, market shares, substitution patterns of alternatives, social welfare, probability derivatives, elasticities, marginal rates of substitution (mrs), and heterogeneous values of time (vot). unlike dcms, dnns can automatically learn the utility function and reveal behavioral patterns that are not prespecified by domain experts. however, the economic information obtained from dnns can be unreliable because of the three challenges associated with the automatic learning capacity: high sensitivity to hyperparameters, model non-identification, and local irregularity. to demonstrate the strength and challenges of dnns, we estimated the dnns using a stated preference survey, extracted the full list of economic information from the dnns, and compared them with those from the dcms. we found that the economic information either aggregated over trainings or population is more reliable than the disaggregate information of the individual observations or trainings, and that even simple hyperparameter searching can significantly improve the reliability of the economic information extracted from the dnns. future studies should investigate other regularizations and dnn architectures, better optimization algorithms, and robust dnn training methods to address dnns' three challenges, to provide more reliable economic information from dnn-based choice models.", "categories": "econ.gn cs.lg q-fin.ec", "created": "2018-12-11", "updated": "2019-09-16", "authors": ["shenhao wang", "qingyi wang", "jinhua zhao"], "url": "https://arxiv.org/abs/1812.04528"}, {"title": "game-theoretic optimal portfolios for jump diffusions", "id": "1812.04603", "abstract": "this paper studies a two-person trading game in continuous time that generalizes garivaltis (2018) to allow for stock prices that both jump and diffuse. analogous to bell and cover (1988) in discrete time, the players start by choosing fair randomizations of the initial dollar, by exchanging it for a random wealth whose mean is at most 1. each player then deposits the resulting capital into some continuously-rebalanced portfolio that must be adhered to over $[0,t]$. we solve the corresponding `investment $\\phi$-game,' namely the zero-sum game with payoff kernel $\\mathbb{e}[\\phi\\{\\textbf{w}_1v_t(b)/(\\textbf{w}_2v_t(c))\\}]$, where $\\textbf{w}_i$ is player $i$'s fair randomization, $v_t(b)$ is the final wealth that accrues to a one dollar deposit into the rebalancing rule $b$, and $\\phi(\\bullet)$ is any increasing function meant to measure relative performance. we show that the unique saddle point is for both players to use the (leveraged) kelly rule for jump diffusions, which is ordinarily defined by maximizing the asymptotic almost-sure continuously-compounded capital growth rate. thus, the kelly rule for jump diffusions is the correct behavior for practically anybody who wants to outperform other traders (on any time frame) with respect to practically any measure of relative performance.", "categories": "econ.gn econ.th q-fin.ec q-fin.gn q-fin.mf q-fin.pm", "created": "2018-12-11", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1812.04603"}, {"title": "a theoretical framework to consider energy transfers within growth   theory", "id": "1812.05091", "abstract": "growth theory has rarely considered energy despite its invisible hand in all physical systems. we develop a theoretical framework that places energy transfers at centerstage of growth theory based on two principles: (1) goods are material rearrangements and (2) such rearrangements are done by energy transferred by prime movers (e.g. workers, engines). we derive the implications of these principles for an autarkic agent that maximizes utility subject to an energy budget constraint and maximizes energy surplus to relax such constraint. the solution to these problems shows that growth is driven by positive marginal energy surplus of energy goods (e.g. rice, oil), yet materializes through prime mover accumulation. this perspective brings under one framework several results from previous attempts to insert energy within growth theory, reconciles economics with natural sciences, and provides a basis for a general reinterpretation of economics and growth as the interplay between human desires and thermodynamic processes.", "categories": "econ.th", "created": "2018-12-12", "updated": "", "authors": ["benjamin leiva", "octavio ramirez", "john r. schramski"], "url": "https://arxiv.org/abs/1812.05091"}, {"title": "apropiaci\\'on privada de renta de recursos naturales? el caso del cobre   en chile", "id": "1812.05093", "abstract": "unexpected increases of natural resource prices can generate rents, value that should be recovered by the state to minimize inefficiencies, avoid arbitrary discrimination between citizens and keep a sustainable trajectory. as a case study about private appropriation of natural resource rent, this work explores the case of copper in chile since 1990, empirically analyzing if the 12 main private mining companies have recovered in present value more than their investment during their life cycle. the results of this exercise, applicable to other natural resources, indicate that some actually have, capturing about us$ 40 billion up to 2012. elaborating an adequate institutional framework for future deposits remain important challenges for chile to plentifully take advantage of its mining potential, as well as for any country with an abundant resource base to better enjoy its natural wealth. for that purpose, a concession known as least present value revenue (lpvr) is proposed.", "categories": "econ.gn q-fin.ec", "created": "2018-12-12", "updated": "", "authors": ["benjam\u00edn leiva"], "url": "https://arxiv.org/abs/1812.05093"}, {"title": "dynamic programming with recursive preferences: optimality and   applications", "id": "1812.05748", "abstract": "this paper provides new conditions for dynamic optimality in discrete time and uses them to establish fundamental dynamic programming results for several commonly used recursive preference specifications. these include epstein-zin preferences, risk-sensitive preferences, narrow framing models and recursive preferences with sensitivity to ambiguity. the results obtained for these applications include (i) existence of optimal policies, (ii) uniqueness of solutions to the bellman equation, (iii) a complete characterization of optimal policies via bellman's principle of optimality, and (iv) a globally convergent method of computation via value function iteration.", "categories": "econ.gn q-fin.ec", "created": "2018-12-13", "updated": "2020-06-22", "authors": ["guanlong ren", "john stachurski"], "url": "https://arxiv.org/abs/1812.05748"}, {"title": "what is the value added by using causal machine learning methods in a   welfare experiment evaluation?", "id": "1812.06533", "abstract": "recent studies have proposed causal machine learning (cml) methods to estimate conditional average treatment effects (cates). in this study, i investigate whether cml methods add value compared to conventional cate estimators by re-evaluating connecticut's jobs first welfare experiment. this experiment entails a mix of positive and negative work incentives. previous studies show that it is hard to tackle the effect heterogeneity of jobs first by means of cates. i report evidence that cml methods can provide support for the theoretical labor supply predictions. furthermore, i document reasons why some conventional cate estimators fail and discuss the limitations of cml methods.", "categories": "econ.em", "created": "2018-12-16", "updated": "2019-03-16", "authors": ["anthony strittmatter"], "url": "https://arxiv.org/abs/1812.06533"}, {"title": "does obamacare care? a fuzzy difference-in-discontinuities approach", "id": "1812.06537", "abstract": "this paper explores the use of a fuzzy regression discontinuity design where multiple treatments are applied at the threshold. the identification results show that, under the very strong assumption that the change in the probability of treatment at the cutoff is equal across treatments, a difference-in-discontinuities estimator identifies the treatment effect of interest. the point estimates of the treatment effect using a simple fuzzy difference-in-discontinuities design are biased if the change in the probability of a treatment applying at the cutoff differs across treatments. modifications of the fuzzy difference-in-discontinuities approach that rely on milder assumptions are also proposed. our results suggest caution is needed when applying before-and-after methods in the presence of fuzzy discontinuities. using data from the national health interview survey, we apply this new identification strategy to evaluate the causal effect of the affordable care act (aca) on older americans' health care access and utilization. our results suggest that the aca has (1) led to a 5% increase in the hospitalization rate of elderly americans, (2) increased the probability of delaying care for cost reasons by 3.6%, and (3) exacerbated cost-related barriers to follow-up care and continuity of care: 7% more elderly individuals could not afford prescriptions, 7% more could not see a specialist, and 5.5% more could not afford a follow-up visit. our results can be explained by an increase in the demand for health services without a corresponding adjustment in supply following the implementation of the aca.", "categories": "econ.em", "created": "2018-12-16", "updated": "2020-01-04", "authors": ["hector galindo-silva", "nibene habib some", "guy tchuente"], "url": "https://arxiv.org/abs/1812.06537"}, {"title": "how the network properties of shareholders vary with investor type and   country", "id": "1812.06694", "abstract": "we construct two examples of shareholder networks in which shareholders are connected if they have shares in the same company. we do this for the shareholders in turkish companies and we compare this against the network formed from the shareholdings in dutch companies. we analyse the properties of these two networks in terms of the different types of shareholder. we create a suitable randomised version of these networks to enable us to find significant features in our networks. for that we find the roles played by different types of shareholder in these networks, and also show how these roles differ in the two countries we study.", "categories": "q-fin.gn econ.gn physics.soc-ph q-fin.ec", "created": "2018-12-17", "updated": "2019-09-26", "authors": ["qing yao", "tim evans", "kim christensen"], "url": "https://arxiv.org/abs/1812.06694"}, {"title": "optimal dynamic allocation of attention", "id": "1812.06967", "abstract": "we consider a decision maker (dm) who, before taking an action, seeks information by allocating her limited attention dynamically over different news sources that are biased toward alternative actions. endogenous choice of information generates rich dynamics: the chosen news source either reinforces or weakens the prior, shaping subsequent attention choices, belief updating, and the final action. the dm adopts a learning strategy biased toward the current belief when the belief is extreme and against that belief when it is moderate. applied to consumption of news media, observed behavior exhibits an `echo-chamber' effect for partisan voters and a novel `anti echo-chamber' effect for moderates.", "categories": "math.oc econ.th", "created": "2018-12-15", "updated": "", "authors": ["yeon-koo che", "konrad mierendorff"], "url": "https://arxiv.org/abs/1812.06967"}, {"title": "causality: a decision theoretic approach", "id": "1812.07414", "abstract": "we propose a decision-theoretic model akin to savage (1972) that is useful for defining causal effects. within this framework, we define what it means for a decision maker (dm) to act as if the relation between the two variables is causal. next, we provide axioms on preferences and show that these axioms are equivalent to the existence of a (unique) directed acyclic graph (dag) that represents the dm's preference. the notion of representation has two components: the graph factorizes the conditional independence properties of the dm's subjective beliefs, and arrows point from cause to effect. finally, we explore the connection between our representation and models used in the statistical causality literature (for example, pearl (1995)).", "categories": "econ.th", "created": "2018-12-18", "updated": "2020-04-05", "authors": ["pablo schenone"], "url": "https://arxiv.org/abs/1812.07414"}, {"title": "spreading of an infectious disease between different locations", "id": "1812.07827", "abstract": "the endogenous adaptation of agents, that may adjust their local contact network in response to the risk of being infected, can have the perverse effect of increasing the overall systemic infectiveness of a disease. we study a dynamical model over two geographically distinct but interacting locations, to better understand theoretically the mechanism at play. moreover, we provide empirical motivation from the italian national bovine database, for the period 2006-2013.", "categories": "econ.gn q-fin.ec", "created": "2018-12-19", "updated": "", "authors": ["alessio muscillo", "paolo pin", "tiziano razzolini"], "url": "https://arxiv.org/abs/1812.07827"}, {"title": "social security and labor absenteeism in a regional health service", "id": "1812.08091", "abstract": "background: absenteism can generate important economic costs. aim: to analyze the determinants of the time off work for sick leaves granted to workers of a regional health service. material and methods: information about 2033 individuals, working at a health service, that were granted at least one sick leave during 2012, was analyzed. personal identification was censored. special emphasis was given to the type of health insurance system of the workers (public or private). results: workers ascribed to the chilean public health insurance system (fonasa) had 11 days more off work than their counterparts ascribed to private health insurance systems. a higher amount of time off work was observed among older subjects and women. conclusions: age, gender and the type of health insurance system influence the number of day off work due to sick leaves.", "categories": "econ.gn q-fin.ec", "created": "2018-12-19", "updated": "", "authors": ["ariel soto caro", "roberto herrera cofre", "rodrigo fuentes solis"], "url": "https://arxiv.org/abs/1812.08091"}, {"title": "estimating biomass migration parameters by analyzing the spatial   behavior of the fishing fleet", "id": "1812.08099", "abstract": "in this study, a method will be developed and applied for estimating biological migration parameters of the biomass of a fishery resource by means of a decision analysis of the spatial behavior of the fleet. first, a model of discrete selection is estimated, together with patch capture function. this will allow estimating the biomass availability on each patch. in the second regression, values of biomass are used in order to estimate a model of biological migration between patches. this method is proven in the chilean jack mackerel fishery. this will allow estimating statistically significant migration parameters, identifying migration patterns.", "categories": "econ.gn q-fin.ec", "created": "2018-12-19", "updated": "", "authors": ["hugo salgado", "ariel soto-caro"], "url": "https://arxiv.org/abs/1812.08099"}, {"title": "multifractal cross-correlations between the world oil and other   financial markets in 2012-2017", "id": "1812.08548", "abstract": "statistical and multiscaling characteristics of wti crude oil prices expressed in us dollar in relation to the most traded currencies as well as to gold futures and to the e-mini s$\\&$p500 futures prices on 5 min intra-day recordings in the period january 2012 - december 2017 are studied. it is shown that in most of the cases the tails of return distributions of the considered financial instruments follow the inverse cubic power law. the only exception is the russian ruble for which the distribution tail is heavier and scales with the exponent close to 2. from the perspective of multiscaling the analysed time series reveal the multifractal organization with the left-sided asymmetry of the corresponding singularity spectra. even more, all the considered financial instruments appear to be multifractally cross-correlated with oil, especially on the level of medium-size fluctuations, as the multifractal cross-correlation analysis carried out by means of the multifractal cross-correlation analysis (mfcca) and detrended cross-correlation coefficient $\\rho_q$ show. the degree of such cross-correlations is however varying among the financial instruments. the strongest ties to the oil characterize currencies of the oil extracting countries. strength of this multifractal coupling appears to depend also on the oil market trend. in the analysed time period the level of cross-correlations systematically increases during the bear phase on the oil market and it saturates after the trend reversal in 1st half of 2016. the same methodology is also applied to identify possible causal relations between considered observables. searching for some related asymmetry in the information flow mediating cross-correlations indicates that it was the oil price that led the russian ruble over the time period here considered rather than vice versa.", "categories": "q-fin.st cs.ce econ.em physics.data-an", "created": "2018-12-20", "updated": "2019-06-21", "authors": ["marcin w\u0105torek", "stanis\u0142aw dro\u017cd\u017c", "pawe\u0142 o\u015bwi\u0229cimka", "marek stanuszek"], "url": "https://arxiv.org/abs/1812.08548"}, {"title": "internal migration and education: a cross-national comparison", "id": "1812.08913", "abstract": "migration the main process shaping patterns of human settlement within and between countries. it is widely acknowledged to be integral to the process of human development as it plays a significant role in enhancing educational outcomes. at regional and national levels, internal migration underpins the efficient functioning of the economy by bringing knowledge and skills to the locations where they are needed. it is the multi-dimensional nature of migration that underlines its significance in the process of human development. human mobility extends in the spatial domain from local travel to international migration, and in the temporal dimension from short-term stays to permanent relocations. classification and measurement of such phenomena is inevitably complex, which has severely hindered progress in comparative research, with very few large-scale cross-national comparisons of migration. the linkages between migration and education have been explored in a separate line of inquiry that has predominantly focused on country-specific analyses as to the ways in which migration affects educational outcomes and how educational attainment affects migration behaviour. a recurrent theme has been the educational selectivity of migrants, which in turn leads to an increase of human capital in some regions, primarily cities, at the expense of others. questions have long been raised as to the links between education and migration in response to educational expansion, but have not yet been fully answered because of the absence, until recently, of adequate data for comparative analysis of migration. in this paper, we bring these two separate strands of research together to systematically explore links between internal migration and education across a global sample of 57 countries at various stages of development, using data drawn from the ipums database.", "categories": "econ.gn q-fin.ec", "created": "2018-12-20", "updated": "", "authors": ["aude bernard", "martin bell"], "url": "https://arxiv.org/abs/1812.08913"}, {"title": "econometric modelling and forecasting of intraday electricity prices", "id": "1812.09081", "abstract": "in the following paper, we analyse the id$_3$-price in the german intraday continuous electricity market using an econometric time series model. a multivariate approach is conducted for hourly and quarter-hourly products separately. we estimate the model using lasso and elastic net techniques and perform an out-of-sample, very short-term forecasting study. the model's performance is compared with benchmark models and is discussed in detail. forecasting results provide new insights to the german intraday continuous electricity market regarding its efficiency and to the id$_3$-price behaviour.", "categories": "q-fin.st econ.em stat.ap", "created": "2018-12-21", "updated": "2019-09-23", "authors": ["micha\u0142 narajewski", "florian ziel"], "url": "https://arxiv.org/abs/1812.09081"}, {"title": "approximate state space modelling of unobserved fractional components", "id": "1812.09142", "abstract": "we propose convenient inferential methods for potentially nonstationary multivariate unobserved components models with fractional integration and cointegration. based on finite-order arma approximations in the state space representation, maximum likelihood estimation can make use of the em algorithm and related techniques. the approximation outperforms the frequently used autoregressive or moving average truncation, both in terms of computational costs and with respect to approximation quality. monte carlo simulations reveal good estimation properties of the proposed methods for processes of different complexity and dimension.", "categories": "econ.em", "created": "2018-12-21", "updated": "2020-05-16", "authors": ["tobias hartl", "roland weigand"], "url": "https://arxiv.org/abs/1812.09142"}, {"title": "multivariate fractional components analysis", "id": "1812.09149", "abstract": "we propose a setup for fractionally cointegrated time series which is formulated in terms of latent integrated and short-memory components. it accommodates nonstationary processes with different fractional orders and cointegration of different strengths and is applicable in high-dimensional settings. in an application to realized covariance matrices, we find that orthogonal short- and long-memory components provide a reasonable fit and competitive out-of-sample performance compared to several competing methods.", "categories": "econ.em", "created": "2018-12-21", "updated": "2019-01-29", "authors": ["tobias hartl", "roland weigand"], "url": "https://arxiv.org/abs/1812.09149"}, {"title": "a primal-dual learning algorithm for personalized dynamic pricing with   an inventory constraint", "id": "1812.09234", "abstract": "a firm is selling a product to different types (based on the features such as education backgrounds, ages, etc.) of customers over a finite season with non-replenishable initial inventory. the type label of an arriving customer can be observed but the demand function associated with each type is initially unknown. the firm sets personalized prices dynamically for each type and attempts to maximize the revenue over the season. we provide a learning algorithm that is near-optimal when the demand and capacity scale in proportion. the algorithm utilizes the primal-dual formulation of the problem and learns the dual optimal solution explicitly. it allows the algorithm to overcome the curse of dimensionality (the rate of regret is independent of the number of types) and sheds light on novel algorithmic designs for learning problems with resource constraints.", "categories": "cs.lg econ.em stat.ml", "created": "2018-12-20", "updated": "", "authors": ["ningyuan chen", "guillermo gallego"], "url": "https://arxiv.org/abs/1812.09234"}, {"title": "growth, industrial externality, prospect dynamics, and well-being on   markets", "id": "1812.09302", "abstract": "functions or 'functionings' enable to give a structure to any activity and their combinations constitute the capabilities which characterize economic assets such as work utility. the basic law of supply and demand naturally emerges from that structure while integrating this utility within frames of reference in which conditions of growth and associated inflation are identified in the exchange mechanisms. growth sustainability is built step by step taking into account functional and organizational requirements which are followed through a project up to a product delivery with different levels of externalities. entering the market through that structure leads to designing basic equations of its dynamics and to finding canonical solutions, or particular equilibria, after specifying the notion of maturity introduced in order to refine the basic model. this approach allows to tackle behavioral foundations of prospect theory through a generalization of its probability weighting function for rationality analyses which apply to western, educated, industrialized, rich, and democratic societies as well as to the poorest ones. the nature of reality and well-being appears then as closely related to the relative satisfaction reached on the market, as it can be conceived by an agent, according to business cycles; this reality being the result of the complementary systems that govern human mind as structured by rational psychologists. the final concepts of growth integrate and extend the maturity part of the behavioral model into virtuous or erroneous sustainability.", "categories": "econ.gn q-fin.ec", "created": "2018-11-10", "updated": "2020-07-28", "authors": ["emmanuel chauvet"], "url": "https://arxiv.org/abs/1812.09302"}, {"title": "poverty, income inequality and growth in bangladesh: revisited karl-marx", "id": "1812.09385", "abstract": "this study tries to find the relationship among poverty inequality and growth. the major finding of this study is poverty has reduced significantly from 2000 to 2016, which is more than 100 percent but in recent time poverty reduction has slowed down. slower and unequal household consumption growth makes sloth the rate of poverty reduction. average annual consumption fell from 1.8 percent to 1.4 percent from 2010 to 2016 and poorer households experienced slower consumption growth compared to richer households.", "categories": "econ.gn q-fin.ec", "created": "2018-12-21", "updated": "", "authors": ["md niaz murshed chowdhury", "md mobarak hossain"], "url": "https://arxiv.org/abs/1812.09385"}, {"title": "population growth and economic development in bangladesh: revisited   malthus", "id": "1812.09393", "abstract": "bangladesh is the 2nd largest growing country in the world in 2016 with 7.1% gdp growth. this study undertakes an econometric analysis to examine the relationship between population growth and economic development. this result indicates population growth adversely related to per capita gdp growth, which means rapid population growth is a real problem for the development of bangladesh.", "categories": "econ.gn q-fin.ec", "created": "2018-12-21", "updated": "2019-01-05", "authors": ["md niaz murshed chowdhury", "md. mobarak hossain"], "url": "https://arxiv.org/abs/1812.09393"}, {"title": "many average partial effects: with an application to text regression", "id": "1812.09397", "abstract": "we study estimation, pointwise and simultaneous inference, and confidence intervals for many average partial effects of lasso logit. focusing on high-dimensional, cluster-sampling environments, we propose a new average partial effect estimator and explore its asymptotic properties. practical penalty choices compatible with our asymptotic theory are also provided. the proposed estimator allow for valid inference without requiring oracle property. we provide easy-to-implement algorithms for cluster-robust high-dimensional hypothesis testing and construction of simultaneously valid confidence intervals using a multiplier cluster bootstrap. we apply the proposed algorithms to the text regression model of wu (2018) to examine the presence of gendered language on the internet.", "categories": "econ.em", "created": "2018-12-21", "updated": "2019-09-26", "authors": ["harold d. chiang"], "url": "https://arxiv.org/abs/1812.09397"}, {"title": "functional sequential treatment allocation", "id": "1812.09408", "abstract": "consider a setting in which a policy maker assigns subjects to treatments, observing each outcome before the next subject arrives. initially, it is unknown which treatment is best, but the sequential nature of the problem permits learning about the effectiveness of the treatments. while the multi-armed-bandit literature has shed much light on the situation when the policy maker compares the effectiveness of the treatments through their mean, much less is known about other targets. this is restrictive, because a cautious decision maker may prefer to target a robust location measure such as a quantile or a trimmed mean. furthermore, socio-economic decision making often requires targeting purpose specific characteristics of the outcome distribution, such as its inherent degree of inequality, welfare or poverty. in the present paper we introduce and study sequential learning algorithms when the distributional characteristic of interest is a general functional of the outcome distribution. minimax expected regret optimality results are obtained within the subclass of explore-then-commit policies, and for the unrestricted class of all policies.", "categories": "econ.em", "created": "2018-12-21", "updated": "2020-08-12", "authors": ["anders bredahl kock", "david preinerstorfer", "bezirgen veliyev"], "url": "https://arxiv.org/abs/1812.09408"}, {"title": "the price of bitcoin: garch evidence from high frequency data", "id": "1812.09452", "abstract": "this is the first paper that estimates the price determinants of bitcoin in a generalised autoregressive conditional heteroscedasticity framework using high frequency data. derived from a theoretical model, we estimate bitcoin transaction demand and speculative demand equations in a garch framework using hourly data for the period 2013-2018. in line with the theoretical model, our empirical results confirm that both the bitcoin transaction demand and speculative demand have a statistically significant impact on the bitcoin price formation. the bitcoin price responds negatively to the bitcoin velocity, whereas positive shocks to the bitcoin stock, interest rate and the size of the bitcoin economy exercise an upward pressure on the bitcoin price.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2018-12-22", "updated": "", "authors": ["pavel ciaian", "d'artis kancs", "miroslava rajcaniova"], "url": "https://arxiv.org/abs/1812.09452"}, {"title": "modified causal forests for estimating heterogeneous causal effects", "id": "1812.09487", "abstract": "uncovering the heterogeneity of causal effects of policies and business decisions at various levels of granularity provides substantial value to decision makers. this paper develops new estimation and inference procedures for multiple treatment models in a selection-on-observables framework by modifying the causal forest approach suggested by wager and athey (2018) in several dimensions. the new estimators have desirable theoretical, computational and practical properties for various aggregation levels of the causal effects. while an empirical monte carlo study suggests that they outperform previously suggested estimators, an application to the evaluation of an active labour market programme shows the value of the new methods for applied research.", "categories": "econ.em stat.ml", "created": "2018-12-22", "updated": "2019-07-05", "authors": ["michael lechner"], "url": "https://arxiv.org/abs/1812.09487"}, {"title": "robust tests for convergence clubs", "id": "1812.09518", "abstract": "in many applications common in testing for convergence the number of cross-sectional units is large and the number of time periods are few. in these situations asymptotic tests based on an omnibus null hypothesis are characterised by a number of problems. in this paper we propose a multiple pairwise comparisons method based on an a recursive bootstrap to test for convergence with no prior information on the composition of convergence clubs. monte carlo simulations suggest that our bootstrap-based test performs well to correctly identify convergence clubs when compared with other similar tests that rely on asymptotic arguments. across a potentially large number of regions, using both cross-country and regional data for the european union, we find that the size distortion which afflicts standard tests and results in a bias towards finding less convergence, is ameliorated when we utilise our bootstrap test.", "categories": "econ.em", "created": "2018-12-22", "updated": "", "authors": ["luisa corrado", "melvyn weeks", "thanasis stengos", "m. ege yazgan"], "url": "https://arxiv.org/abs/1812.09518"}, {"title": "does random consideration explain behavior when choice is hard? evidence   from a large-scale experiment", "id": "1812.09619", "abstract": "we study population behavior when choice is hard because considering alternatives is costly. to simplify their choice problem, individuals may pay attention to only a subset of available alternatives. we design and implement a novel online experiment that exogenously varies choice sets and consideration costs for a large sample of individuals. we provide a theoretical and statistical framework that allows us to test random consideration at the population level. within this framework, we compare competing models of random consideration. we find that the standard random utility model fails to explain the population behavior. however, our results suggest that a model of random consideration with logit attention and heterogeneous preferences provides a good explanation for the population behavior. finally, we find that the random consideration rule that subjects use is different for different consideration costs while preferences are not. we observe that the higher the consideration cost the further behavior is from the full-consideration benchmark, which supports the hypothesis that hard choices have a substantial negative impact on welfare via limited consideration.", "categories": "econ.gn q-fin.ec", "created": "2018-12-22", "updated": "2019-06-17", "authors": ["victor h. aguiar", "maria jose boccardi", "nail kashaev", "jeongbin kim"], "url": "https://arxiv.org/abs/1812.09619"}, {"title": "duesenberry's theory of consumption: habit, learning, and ratcheting", "id": "1812.10038", "abstract": "this paper investigates the consumption and risk taking decision of an economic agent with partial irreversibility of consumption decision by formalizing the theory proposed by duesenberry (1949). the optimal policies exhibit a type of the (s, s) policy: there are two wealth thresholds within which consumption stays constant. consumption increases or decreases at the thresholds and after the adjustment new thresholds are set. the share of risky investment in the agent's total investment is inversely u-shaped within the (s, s) band, which generates time-varying risk aversion that can fluctuate widely over time. this property can explain puzzles and questions on asset pricing and households' portfolio choices, e.g., why aggregate consumption is so smooth whereas the high equity premium is high and the equity return has high volatility, why the risky share is so low whereas the estimated risk aversion by the micro-level data is small, and whether and when an increase in wealth has an impact on the risky share. also, the partial irreversibility model can explain both the excess sensitivity and the excess smoothness of consumption.", "categories": "econ.th", "created": "2018-12-25", "updated": "", "authors": ["kyoung jin choi", "junkee jeon", "hyeng keun koo"], "url": "https://arxiv.org/abs/1812.10038"}, {"title": "revisiting transformation and directional technology distance functions", "id": "1812.10108", "abstract": "in the first part of the paper, we prove the equivalence of the unsymmetric transformation function and an efficient joint production function (jpf) under strong monotonicity conditions imposed on input and output correspondences. monotonicity, continuity, and convexity properties sufficient for a symmetric transformation function to be an efficient jpf are also stated. in the second part, we show that the most frequently used functional form for the directional technology distance function (dtdf), the quadratic, does not satisfy homogeneity of degree $-1$ in the direction vector. this implies that the quadratic function is not the directional technology distance function. we provide derivation of the dtdf from a symmetric transformation function and show how this approach can be used to obtain functional forms that satisfy both translation property and homogeneity of degree $-1$ in the direction vector if the optimal solution of an underlying optimization problem can be expressed in closed form.", "categories": "econ.gn q-fin.ec", "created": "2018-12-25", "updated": "", "authors": ["yaryna kolomiytseva"], "url": "https://arxiv.org/abs/1812.10108"}, {"title": "cartel stability under quality differentiation", "id": "1812.10293", "abstract": "this note considers cartel stability when the cartelized products are vertically differentiated. if market shares are maintained at pre-collusive levels, then the firm with the lowest competitive price-cost margin has the strongest incentive to deviate from the collusive agreement. the lowest-quality supplier has the tightest incentive constraint when the difference in unit production costs is sufficiently small.", "categories": "econ.th", "created": "2018-12-26", "updated": "", "authors": ["iwan bos", "marco marini"], "url": "https://arxiv.org/abs/1812.10293"}, {"title": "equivalent choice functions and stable mechanisms", "id": "1812.10326", "abstract": "we study conditions for the existence of stable and group-strategy-proof mechanisms in a many-to-one matching model with contracts if students' preferences are monotone in contract terms. we show that \"equivalence\", properly defined, to a choice profile under which contracts are substitutes and the law of aggregate holds is a necessary and sufficient condition for the existence of a stable and group-strategy-proof mechanism.   our result can be interpreted as a (weak) embedding result for choice functions under which contracts are observable substitutes and the observable law of aggregate demand holds.", "categories": "econ.th", "created": "2018-12-26", "updated": "2019-01-22", "authors": ["jan christoph schlegel"], "url": "https://arxiv.org/abs/1812.10326"}, {"title": "how to avoid the zero-power trap in testing for correlation", "id": "1812.10752", "abstract": "in testing for correlation of the errors in regression models the power of tests can be very low for strongly correlated errors. this counterintuitive phenomenon has become known as the \"zero-power trap\". despite a considerable amount of literature devoted to this problem, mainly focusing on its detection, a convincing solution has not yet been found. in this article we first discuss theoretical results concerning the occurrence of the zero-power trap phenomenon. then, we suggest and compare three ways to avoid it. given an initial test that suffers from the zero-power trap, the method we recommend for practice leads to a modified test whose power converges to one as the correlation gets very strong. furthermore, the modified test has approximately the same power function as the initial test, and thus approximately preserves all of its optimality properties. we also provide some numerical illustrations in the context of testing for network generated correlation.", "categories": "math.st econ.em stat.me stat.th", "created": "2018-12-27", "updated": "", "authors": ["david preinerstorfer"], "url": "https://arxiv.org/abs/1812.10752"}, {"title": "practical and robust $t$-test based inference for synthetic control and   related methods", "id": "1812.10820", "abstract": "this paper proposes a practical and robust method for making inference on average treatment effects estimated by synthetic control and related methods. we develop a $k$-fold cross-fitting procedure for bias-correction. to avoid the difficult estimation of the long-run variance, inference is based on a self-normalized $t$-statistic, which has an asymptotically pivotal $t$-distribution. our procedure only requires consistent (in $\\ell_2$-norm) estimation of the parameters, which can be verified for synthetic control and many other popular estimators. the proposed method is easy to implement, provably robust against misspecification, more efficient than difference-in-differences, valid with non-stationary data, and demonstrates an excellent small sample performance.", "categories": "econ.em", "created": "2018-12-27", "updated": "2020-03-08", "authors": ["victor chernozhukov", "kaspar wuthrich", "yinchu zhu"], "url": "https://arxiv.org/abs/1812.10820"}, {"title": "semiparametric difference-in-differences with potentially many control   variables", "id": "1812.10846", "abstract": "this paper discusses difference-in-differences (did) estimation when there exist many control variables, potentially more than the sample size. in this case, traditional estimation methods, which require a limited number of variables, do not work. one may consider using statistical or machine learning (ml) methods. however, by the well-known theory of inference of ml methods proposed in chernozhukov et al. (2018), directly applying ml methods to the conventional semiparametric did estimators will cause significant bias and make these did estimators fail to be sqrt{n}-consistent. this article proposes three new did estimators for three different data structures, which are able to shrink the bias and achieve sqrt{n}-consistency and asymptotic normality with mean zero when applying ml methods. this leads to straightforward inferential procedures. in addition, i show that these new estimators have the small bias property (sbp), meaning that their bias will converge to zero faster than the pointwise bias of the nonparametric estimator on which it is based.", "categories": "econ.gn econ.em q-fin.ec", "created": "2018-12-27", "updated": "2019-01-08", "authors": ["neng-chieh chang"], "url": "https://arxiv.org/abs/1812.10846"}, {"title": "decentralization estimators for instrumental variable quantile   regression models", "id": "1812.10925", "abstract": "the instrumental variable quantile regression (ivqr) model (chernozhukov and hansen, 2005) is a popular tool for estimating causal quantile effects with endogenous covariates. however, estimation is complicated by the non-smoothness and non-convexity of the ivqr gmm objective function. this paper shows that the ivqr estimation problem can be decomposed into a set of conventional quantile regression sub-problems which are convex and can be solved efficiently. this reformulation leads to new identification results and to fast, easy to implement, and tuning-free estimators that do not require the availability of high-level \"black box\" optimization routines.", "categories": "econ.em", "created": "2018-12-28", "updated": "2020-09-16", "authors": ["hiroaki kaido", "kaspar wuthrich"], "url": "https://arxiv.org/abs/1812.10925"}, {"title": "predicting \"design gaps\" in the market: deep consumer choice models   under probabilistic design constraints", "id": "1812.11067", "abstract": "predicting future successful designs and corresponding market opportunity is a fundamental goal of product design firms. there is accordingly a long history of quantitative approaches that aim to capture diverse consumer preferences, and then translate those preferences to corresponding \"design gaps\" in the market. we extend this work by developing a deep learning approach to predict design gaps in the market. these design gaps represent clusters of designs that do not yet exist, but are predicted to be both (1) highly preferred by consumers, and (2) feasible to build under engineering and manufacturing constraints. this approach is tested on the entire u.s. automotive market using of millions of real purchase data. we retroactively predict design gaps in the market, and compare predicted design gaps with actual known successful designs. our preliminary results give evidence it may be possible to predict design gaps, suggesting this approach has promise for early identification of market opportunity.", "categories": "econ.em", "created": "2018-12-28", "updated": "", "authors": ["alex burnap", "john hauser"], "url": "https://arxiv.org/abs/1812.11067"}, {"title": "dynamic models with robust decision makers: identification and   estimation", "id": "1812.11246", "abstract": "this paper studies identification and estimation of a class of dynamic models in which the decision maker (dm) is uncertain about the data-generating process. the dm surrounds a benchmark model that he or she fears is misspecified by a set of models. decisions are evaluated under a worst-case model delivering the lowest utility among all models in this set. the dm's benchmark model and preference parameters are jointly underidentified. with the benchmark model held fixed, primitive conditions are established for identification of the dm's worst-case model and preference parameters. the key step in the identification analysis is to establish existence and uniqueness of the dm's continuation value function allowing for unbounded statespace and unbounded utilities. to do so, fixed-point results are derived for monotone, convex operators that act on a banach space of thin-tailed functions arising naturally from the structure of the continuation value recursion. the fixed-point results are quite general; applications to models with learning and rust-type dynamic discrete choice models are also discussed. for estimation, a perturbation result is derived which provides a necessary and sufficient condition for consistent estimation of continuation values and the worst-case model. the result also allows convergence rates of estimators to be characterized. an empirical application studies an endowment economy where the dm's benchmark model may be interpreted as an aggregate of experts' forecasting models. the application reveals time-variation in the way the dm pessimistically distorts benchmark probabilities. consequences for asset pricing are explored and connections are drawn with the literature on macroeconomic uncertainty.", "categories": "econ.em", "created": "2018-12-28", "updated": "2019-01-28", "authors": ["timothy m. christensen"], "url": "https://arxiv.org/abs/1812.11246"}, {"title": "interdistrict school choice: a theory of student assignment", "id": "1812.11297", "abstract": "interdistrict school choice programs-where a student can be assigned to a school outside of her district-are widespread in the us, yet the market-design literature has not considered such programs. we introduce a model of interdistrict school choice and present two mechanisms that produce stable or efficient assignments. we consider three categories of policy goals on assignments and identify when the mechanisms can achieve them. by introducing a novel framework of interdistrict school choice, we provide a new avenue of research in market design.", "categories": "econ.th", "created": "2018-12-29", "updated": "2019-01-06", "authors": ["isa e. hafalir", "fuhito kojima", "m. bumin yenmez"], "url": "https://arxiv.org/abs/1812.11297"}, {"title": "thought viruses and asset prices", "id": "1812.11417", "abstract": "we use insights from epidemiology, namely the sir model, to study how agents infect each other with \"investment ideas.\" once an investment idea \"goes viral,\" equilibrium prices exhibit the typical \"fever peak,\" which is characteristic for speculative excesses. using our model, we identify a time line of symptoms that indicate whether a boom is in its early or later stages. regarding the market's top, we find that prices start to decline while the number of infected agents, who buy the asset, is still rising. moreover, the presence of fully rational agents (i) accelerates booms (ii) lowers peak prices and (iii) produces broad, drawn-out, market tops.", "categories": "econ.gn q-fin.ec", "created": "2018-12-29", "updated": "", "authors": ["wolfgang kuhle"], "url": "https://arxiv.org/abs/1812.11417"}, {"title": "selling wind", "id": "1812.11420", "abstract": "we offer a parsimonious model to investigate how strategic wind producers sell energy under stochastic production constraints, where the extent of heterogeneity of wind energy availability varies according to wind farm locations. the main insight of our analysis is that increasing heterogeneity in resource availability improves social welfare, as a function of its effects both on improving diversification and on reducing withholding by firms. we show that this insight is quite robust for any concave and downward-sloping inverse demand function. the model is also used to analyze the effect of heterogeneity on firm profits and opportunities for collusion. finally, we analyze the impacts of improving public information and weather forecasting; enhanced public forecasting increases welfare, but it is not always in the best interests of strategic producers.", "categories": "econ.gn q-fin.ec", "created": "2018-12-29", "updated": "", "authors": ["ali kakhbod", "asuman ozdaglar", "ian schneider"], "url": "https://arxiv.org/abs/1812.11420"}, {"title": "e-commerce in hungary: a market analysis", "id": "1812.11488", "abstract": "e-commerce is on the rise in hungary, with significantly growing numbers of customers shopping online. this paper aims to identify the direct and indirect drivers of the double-digit growth rate, including the related macroeconomic indicators and the digital economy and society index (desi). moreover, this study provides a deep insight into industry trends and outlooks, including high industry concentration and top industrial players. it also draws the profile of the typical online shopper and the dominant characteristics of online purchases. development of e-commerce is robust, but there is still plenty of potential for growth and progress in hungary.", "categories": "econ.gn q-fin.ec", "created": "2018-12-30", "updated": "", "authors": ["szabolcs nagy"], "url": "https://arxiv.org/abs/1812.11488"}, {"title": "salvaging falsified instrumental variable models", "id": "1812.11598", "abstract": "what should researchers do when their baseline model is refuted? we provide four constructive answers. first, researchers can measure the extent of falsification. to do this, we consider continuous relaxations of the baseline assumptions of concern. we then define the falsification frontier: the smallest relaxations of the baseline model which are not refuted. this frontier provides a quantitative measure of the extent of falsification. second, researchers can present the identified set for the parameter of interest under the assumption that the true model lies somewhere on this frontier. we call this the falsification adaptive set. this set generalizes the standard baseline estimand to account for possible falsification. third, researchers can present the identified set for a specific point on this frontier. finally, as a sensitivity analysis, researchers can present identified sets for points beyond the frontier. to illustrate these four ways of salvaging falsified models, we study overidentifying restrictions in two instrumental variable models: a homogeneous effects linear model, and heterogeneous effect models with either binary or continuous outcomes. in the linear model, we consider the classical overidentifying restrictions implied when multiple instruments are observed. we generalize these conditions by considering continuous relaxations of the classical exclusion restrictions. by sufficiently weakening the assumptions, a falsified baseline model becomes non-falsified. we obtain analogous results in the heterogeneous effect models, where we derive identified sets for marginal distributions of potential outcomes, falsification frontiers, and falsification adaptive sets under continuous relaxations of the instrument exogeneity assumptions. we illustrate our results in four different empirical applications.", "categories": "econ.em stat.me", "created": "2018-12-30", "updated": "2020-01-06", "authors": ["matthew a. masten", "alexandre poirier"], "url": "https://arxiv.org/abs/1812.11598"}, {"title": "optimal insurance with limited commitment in a finite horizon", "id": "1812.11669", "abstract": "we study a finite horizon optimal contracting problem of a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. the problem involves an infinite number of constraints at each time and each state of the world. miao and zhang (2015) have developed a dual approach to the problem by considering a lagrangian and derived a hamilton-jacobi-bellman equation in an infinite horizon. we consider a similar lagrangian in a finite horizon, but transform the dual problem into an infinite series of optimal stopping problems. for each optimal stopping problem we provide an analytic solution by providing an integral equation representation for the free boundary. we provide a verification theorem that the value function of the original principal's problem is the legender-fenchel transform of the integral of the value functions of the optimal stopping problems. we also provide some numerical simulation results of optimal contracting strategies", "categories": "econ.th", "created": "2018-12-30", "updated": "2019-01-10", "authors": ["junkee jeon", "hyeng keun koo", "kyunghyun park"], "url": "https://arxiv.org/abs/1812.11669"}, {"title": "learning and selfconfirming equilibria in network games", "id": "1812.11775", "abstract": "consider a set of agents who play a network game repeatedly. agents may not know the network. they may even be unaware that they are interacting with other agents in a network. possibly, they just understand that their payoffs depend on an unknown state that in reality is an aggregate of the actions of their neighbors. each time, every agent chooses an action that maximizes her subjective expected payoff and then updates her beliefs according to what she observes. in particular, we assume that each agent only observes her realized payoff. a steady state of such dynamic is a selfconfirming equilibrium given the assumed feedback. we characterize the structure of the set of selfconfirming equilibria in network games and we relate selfconfirming and nash equilibria. thus, we provide conditions on the network under which the nash equilibrium concept has a learning foundation, despite the fact that agents may have incomplete information. in particular, we show that the choice of being active or inactive in a network is crucial to determine whether agents can make correct inferences about the payoff state and hence play the best reply to the truth in a selfconfirming equilibrium. we also study learning dynamics and show how agents can get stuck in non--nash selfconfirming equilibria. in such dynamics, the set of inactive agents can only increase in time, because once an agent finds it optimal to be inactive, she gets no feedback about the payoff state, hence she does not change her beliefs and remains inactive.", "categories": "econ.th cs.gt", "created": "2018-12-31", "updated": "", "authors": ["pierpaolo battigalli", "fabrizio panebianco", "paolo pin"], "url": "https://arxiv.org/abs/1812.11775"}, {"title": "schr\\\"{o}dinger type equation for subjective identification of supply   and demand", "id": "1812.11824", "abstract": "the present authors have put forward a quantum game theory based model of market prices movements. by using fisher information, we present a construction of an equation of schr\\\"{o}dinger type for probability distributions for relationship between demand and supply. various analogies between quantum physics and market phenomena can be found.", "categories": "econ.th quant-ph", "created": "2018-12-20", "updated": "", "authors": ["marcin makowski", "edward w. piotrowski", "jan s\u0142adkowski"], "url": "https://arxiv.org/abs/1812.11824"}, {"title": "approximately optimal mechanism design", "id": "1812.11896", "abstract": "optimal mechanism design enjoys a beautiful and well-developed theory, and also a number of killer applications. rules of thumb produced by the field influence everything from how governments sell wireless spectrum licenses to how the major search engines auction off online advertising. there are, however, some basic problems for which the traditional optimal mechanism design approach is ill-suited---either because it makes overly strong assumptions, or because it advocates overly complex designs. this survey reviews several common issues with optimal mechanisms, including exorbitant communication, computation, and informational requirements; and it presents several examples demonstrating that passing to the relaxed goal of an approximately optimal mechanism allows us to reason about fundamental questions that seem out of reach of the traditional theory.", "categories": "econ.th cs.ds cs.gt", "created": "2018-12-31", "updated": "2020-08-09", "authors": ["tim roughgarden", "inbal talgam-cohen"], "url": "https://arxiv.org/abs/1812.11896"}, {"title": "credit cycles, securitization, and credit default swaps", "id": "1901.00177", "abstract": "we present a limits-to-arbitrage model to study the impact of securitization, leverage and credit risk protection on the cyclicity of bank credit. in a stable bank credit situation, no cycles of credit expansion or contraction appear. unlevered securitization together with mis-pricing of securitized assets increases lending cyclicality, favoring credit booms and busts. leverage changes the state of affairs with respect to the simple securitization. first, the volume of real activity and banking profits increases. second, banks sell securities when markets decline. this selling puts further pressure on falling prices. the mis-pricing of credit risk protection or securitized assets influences the real economy. trading in these contracts reduces the amount of funding available to entrepreneurs, particularly to high-credit-risk borrowers. this trading decreases the liquidity of the securitized assets, and especially those based on investments with high credit risk.", "categories": "econ.th", "created": "2019-01-01", "updated": "", "authors": ["juan ignacio pe\u00f1a"], "url": "https://arxiv.org/abs/1901.00177"}, {"title": "methodological provisions for conducting empirical research of the   availability and implementation of the consumers socially responsible   intentions", "id": "1901.00191", "abstract": "social responsibility of consumers is one of the main conditions for the recoupment of enterprises expenses associated with the implementation of social and ethical marketing tasks. therefore, the enterprises, which plan to act on terms of social and ethical marketing, should monitor the social responsibility of consumers in the relevant markets. at the same time, special attention should be paid to the analysis of factors that prevent consumers from implementing their socially responsible intentions in the regions with a low level of social activity of consumers. the purpose of the article is to develop methodological guidelines that determine the tasks and directions of conducting empirical studies aimed at assessing the gap between the socially responsible intentions of consumers and the actual implementation of these intentions, as well as to identify the causes of this gap. an empirical survey of the sampled consumers in kharkiv was carried out in terms of the proposed methodological provisions. it revealed a rather high level of respondents' willingness to support socially responsible enterprises and a rather low level of implementation of these intentions due to the lack of consumers awareness. to test the proposed methodological guidelines, an empirical study of the consumers social responsibility was conducted in 2017 on a sample of students and professors of the semen kuznets kharkiv national university of economics (120 people). questioning of the respondents was carried out using the google forms. the finding allowed to make conclusion for existence of a high level of respondents' willingness to support socially responsible and socially active enterprises. however, the study also revealed the existence of a significant gap between the intentions and actions of consumers, caused by the lack of awareness.", "categories": "econ.gn q-fin.ec", "created": "2019-01-01", "updated": "", "authors": ["lyudmyla potrashkova", "diana raiko", "leonid tseitlin", "olga savchenko", "szabolcs nagy"], "url": "https://arxiv.org/abs/1901.00191"}, {"title": "multitask learning deep neural networks to combine revealed and stated   preference data", "id": "1901.00227", "abstract": "it is an enduring question how to combine revealed preference (rp) and stated preference (sp) data to analyze travel behavior. this study presents a framework of multitask learning deep neural networks (mtldnns) for this question, and demonstrates that mtldnns are more generic than the traditional nested logit (nl) method, due to its capacity of automatic feature learning and soft constraints. about 1,500 mtldnn models are designed and applied to the survey data that was collected in singapore and focused on the rp of four current travel modes and the sp with autonomous vehicles (av) as the one new travel mode in addition to those in rp. we found that mtldnns consistently outperform six benchmark models and particularly the classical nl models by about 5% prediction accuracy in both rp and sp datasets. this performance improvement can be mainly attributed to the soft constraints specific to mtldnns, including its innovative architectural design and regularization methods, but not much to the generic capacity of automatic feature learning endowed by a standard feedforward dnn architecture. besides prediction, mtldnns are also interpretable. the empirical results show that av is mainly the substitute of driving and av alternative-specific variables are more important than the socio-economic variables in determining av adoption. overall, this study introduces a new mtldnn framework to combine rp and sp, and demonstrates its theoretical flexibility and empirical power for prediction and interpretation. future studies can design new mtldnn architectures to reflect the speciality of rp and sp and extend this work to other behavioral analysis.", "categories": "econ.gn cs.lg q-fin.ec", "created": "2019-01-01", "updated": "2019-08-22", "authors": ["shenhao wang", "qingyi wang", "jinhua zhao"], "url": "https://arxiv.org/abs/1901.00227"}, {"title": "digital economy and society. a cross country comparison of hungary and   ukraine", "id": "1901.00283", "abstract": "we live in the digital age in which both economy and society have been transforming significantly. the internet and the connected digital devices are inseparable parts of our daily life and the engine of the economic growth. in this paper, first i analyzed the status of digital economy and society in hungary, then compared it with ukraine and made conclusions regarding the future development tendencies. using secondary data provided by the european commission i investigated the five components of the digital economy and society index of hungary. i performed cross country analysis to find out the significant differences between ukraine and hungary in terms of access to the internet and device use including smartphones, computers and tablets. based on my findings, i concluded that hungary is more developed in terms of the significant parameters of the digital economy and society than ukraine, but even hungary is an emerging digital nation. considering the high growth rate of internet, tablet and smartphone penetration in both countries, i expect faster progress in the development of the digital economy and society in hungary and ukraine.", "categories": "econ.gn q-fin.ec", "created": "2019-01-02", "updated": "", "authors": ["szabolcs nagy"], "url": "https://arxiv.org/abs/1901.00283"}, {"title": "decomposing changes in the distribution of real hourly wages in the u.s", "id": "1901.00419", "abstract": "we analyze the sources of changes in the distribution of hourly wages in the united states using cps data for the survey years 1976 to 2016. we account for the selection bias from the employment decision by modeling the distribution of annual hours of work and estimating a nonseparable model of wages which uses a control function to account for selection. this allows the inclusion of all individuals working positive hours and thus provides a fuller description of the wage distribution. we decompose changes in the distribution of wages into composition, structural and selection effects. composition effects have increased wages at all quantiles but the patterns of change are generally determined by the structural effects. evidence of changes in the selection effects only appear at the lower quantiles of the female wage distribution. these various components combine to produce a substantial increase in wage inequality.", "categories": "econ.em stat.me", "created": "2018-12-21", "updated": "2019-11-12", "authors": ["iv\u00e1n fern\u00e1ndez-val", "franco peracchi", "aico van vuuren", "francis vella"], "url": "https://arxiv.org/abs/1901.00419"}, {"title": "the institutional economics of collective waste recovery systems: an   empirical investigation", "id": "1901.00495", "abstract": "the main purpose of the study is to develop the model for transaction costs measurement in the collective waste recovery systems. the methodology of new institutional economics is used in the research. the impact of the study is related both to the enlargement of the limits of the theory about the interaction between transaction costs and social costs and to the identification of institutional failures of the european concept for circular economy. a new model for social costs measurement is developed. keywords: circular economy, transaction costs, extended producer responsibility jel: a13, c51, d23, l22, q53", "categories": "econ.gn q-fin.ec", "created": "2019-01-02", "updated": "2019-01-04", "authors": ["shteryo nozharov"], "url": "https://arxiv.org/abs/1901.00495"}, {"title": "modeling dynamic transport network with matrix factor models: with an   application to international trade flow", "id": "1901.00769", "abstract": "international trade research plays an important role to inform trade policy and shed light on wider issues relating to poverty, development, migration, productivity, and economy. with recent advances in information technology, global and regional agencies distribute an enormous amount of internationally comparable trading data among a large number of countries over time, providing a goldmine for empirical analysis of international trade. meanwhile, an array of new statistical methods are recently developed for dynamic network analysis. however, these advanced methods have not been utilized for analyzing such massive dynamic cross-country trading data. international trade data can be viewed as a dynamic transport network because it emphasizes the amount of goods moving across a network. most literature on dynamic network analysis concentrates on the connectivity network that focuses on link formation or deformation rather than the transport moving across the network. we take a different perspective from the pervasive node-and-edge level modeling: the dynamic transport network is modeled as a time series of relational matrices. we adopt a matrix factor model of \\cite{wang2018factor}, with a specific interpretation for the dynamic transport network. under the model, the observed surface network is assumed to be driven by a latent dynamic transport network with lower dimensions. the proposed method is able to unveil the latent dynamic structure and achieve the objective of dimension reduction. we applied the proposed framework and methodology to a data set of monthly trading volumes among 24 countries and regions from 1982 to 2015. our findings shed light on trading hubs, centrality, trends and patterns of international trade and show matching change points to trading policies. the dataset also provides a fertile ground for future research on international trade.", "categories": "econ.em stat.me", "created": "2019-01-02", "updated": "", "authors": ["elynn y. chen", "rong chen"], "url": "https://arxiv.org/abs/1901.00769"}, {"title": "the impact of country of origin in mobile phone choice of generation y   and z", "id": "1901.00793", "abstract": "mobile phones play a very important role in our life. mobile phone sales have been soaring over the last decade due to the growing acceptance of technological innovations, especially by generations y and z. understanding the change in customers' requirement is the key to success in the smartphone business. new, strong mobile phone models will emerge if the voice of the customer can be heard. although it has been widely known that country of origin has serious impact on the attitudes and purchase decisions of mobile phone consumers, there lack substantial studies that investigate the mobile phone preference of young adults aged 18-25, members of late generation y and early generation z. in order to investigate the role of country of origin in mobile phone choice of generations y and z, an online survey with 228 respondents was conducted in hungary in 2016. besides the descriptive statistical methods, crosstabs, anova and pearson correlation are used to analyze the collected data and find out significant relationships. factor analysis (principal component analysis) is used for data reduction to create new factor components. the findings of this exploratory study support the idea that country of origin plays a significant role in many respects related to young adults' mobile phone choice. mobile phone owners with different countries of origin attribute crucial importance to the various product features including technical parameters, price, design, brand name, operating system, and memory size. country of origin has a moderating effect on the price sensitivity of consumers with varied net income levels. it is also found that frequent buyers of mobile phones, especially us brand products, spend the most significant amount of money for their consumption in this aspect.", "categories": "econ.gn q-fin.ec", "created": "2019-01-02", "updated": "", "authors": ["szabolcs nagy"], "url": "https://arxiv.org/abs/1901.00793"}, {"title": "nonparametric instrumental variables estimation under misspecification", "id": "1901.01241", "abstract": "we show that nonparametric instrumental variables (npiv) estimators are highly sensitive to misspecification: an arbitrarily small deviation from instrumental validity can lead to large asymptotic bias for a broad class of estimators. one can mitigate the problem by placing strong restrictions on the structural function in estimation. however, if the true function does not obey the restrictions then imposing them imparts bias. therefore, there is a trade-off between the sensitivity to invalid instruments and bias from imposing excessive restrictions. in light of this trade-off we propose a partial identification approach to estimation in npiv models. we provide a point estimator that minimizes the worst-case asymptotic bias and error-bounds that explicitly account for some degree of misspecification. we apply our methods to the empirical setting of blundell et al. (2007) and horowitz (2011) to estimate shape-invariant engel curves.", "categories": "econ.em", "created": "2019-01-04", "updated": "2019-11-15", "authors": ["ben deaner"], "url": "https://arxiv.org/abs/1901.01241"}, {"title": "conditions for the uniqueness of the gately point for cooperative games", "id": "1901.01485", "abstract": "we are studying the gately point, an established solution concept for cooperative games. we point out that there are superadditive games for which the gately point is not unique, i.e. in general the concept is rather set-valued than an actual point. we derive conditions under which the gately point is guaranteed to be a unique imputation and provide a geometric interpretation. the gately point can be understood as the intersection of a line defined by two points with the set of imputations. our uniqueness conditions guarantee that these two points do not coincide. we provide demonstrative interpretations for negative propensities to disrupt. we briefly show that our uniqueness conditions for the gately point include quasibalanced games and discuss the relation of the gately point to the $\\tau$-value in this context. finally, we point out relations to cost games and the aca method and end upon a few remarks on the implementation of the gately point and an upcoming software package for cooperative game theory.", "categories": "econ.th", "created": "2019-01-05", "updated": "", "authors": ["jochen staudacher", "johannes anwander"], "url": "https://arxiv.org/abs/1901.01485"}, {"title": "invest or exit? optimal decisions in the face of a declining profit   stream", "id": "1901.01486", "abstract": "even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. in order to study this phenomenon, we model the firm's profit stream as a brownian motion with negative drift. at each point in time, the firm can continue operations, or it can stop and exit the project. in addition, there is a one-time option to make an investment which boosts the project's profit rate. using stochastic analysis, we show that the optimal policy always exists and that it is characterized by three thresholds. there are investment and exit thresholds before investment, and there is a threshold for exit after investment. we also effect a comparative statics analysis of the thresholds with respect to the drift and the volatility of the brownian motion. when the profit boost upon investment is sufficiently large, we find a novel result: the investment threshold decreases in volatility.", "categories": "math.oc econ.gn q-fin.ec q-fin.mf", "created": "2019-01-05", "updated": "", "authors": ["h. dharma kwon"], "url": "https://arxiv.org/abs/1901.01486"}, {"title": "shrinkage for categorical regressors", "id": "1901.01898", "abstract": "this paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors, (quasi-)experimental data or panel data models. the loss function is penalized by adding weighted squared l2-norm differences between group location parameters and informative first-stage estimates. under quadratic loss, the penalized estimation problem has a simple interpretable closed-form solution that nests methods established in the literature on ridge regression, discretized support smoothing kernels and model averaging methods. we derive risk-optimal penalty parameters and propose a plug-in approach for estimation. the large sample properties are analyzed in an asymptotic local to zero framework by introducing a class of sequences for close and distant systems of locations that is sufficient for describing a large range of data generating processes. we provide the asymptotic distributions of the shrinkage estimators under different penalization schemes. the proposed plug-in estimator uniformly dominates the ordinary least squares in terms of asymptotic risk if the number of groups is larger than three. monte carlo simulations reveal robust improvements over standard methods in finite samples. real data examples of estimating time trends in a panel and a difference-in-differences study illustrate potential applications.", "categories": "econ.em", "created": "2019-01-07", "updated": "", "authors": ["phillip heiler", "jana mareckova"], "url": "https://arxiv.org/abs/1901.01898"}, {"title": "decision-making and fuzzy temporal logic", "id": "1901.01970", "abstract": "this paper shows that the fuzzy temporal logic can model figures of thought to describe decision-making behaviors. in order to exemplify, some economic behaviors observed experimentally were modeled from problems of choice containing time, uncertainty and fuzziness. related to time preference, it is noted that the subadditive discounting is mandatory in positive rewards situations and, consequently, results in the magnitude effect and time effect, where the last has a stronger discounting for earlier delay periods (as in, one hour, one day), but a weaker discounting for longer delay periods (for instance, six months, one year, ten years). in addition, it is possible to explain the preference reversal (change of preference when two rewards proposed on different dates are shifted in the time). related to the prospect theory, it is shown that the risk seeking and the risk aversion are magnitude dependents, where the risk seeking may disappear when the values to be lost are very high.", "categories": "cs.ai econ.th math.lo", "created": "2019-01-07", "updated": "2019-02-15", "authors": ["jos\u00e9 cl\u00e1udio do nascimento"], "url": "https://arxiv.org/abs/1901.01970"}, {"title": "the interconnected wealth of nations: shock propagation on global   trade-investment multiplex networks", "id": "1901.01976", "abstract": "the increasing integration of world economies, which organize in complex multilayer networks of interactions, is one of the critical factors for the global propagation of economic crises. we adopt the network science approach to quantify shock propagation on the global trade-investment multiplex network. to this aim, we propose a model that couples a susceptible-infected-recovered epidemic spreading dynamics, describing how economic distress propagates between connected countries, with an internal contagion mechanism, describing the spreading of such economic distress within a given country. at the local level, we find that the interplay between trade and financial interactions influences the vulnerabilities of countries to shocks. at the large scale, we find a simple linear relation between the relative magnitude of a shock in a country and its global impact on the whole economic system, albeit the strength of internal contagion is country-dependent and the intercountry propagation dynamics is non-linear. interestingly, this systemic impact can be predicted on the basis of intra-layer and inter-layer scale factors that we name network multipliers, that are independent of the magnitude of the initial shock. our model sets-up a quantitative framework to stress-test the robustness of individual countries and of the world economy to propagating crashes.", "categories": "physics.soc-ph econ.gn q-fin.ec q-fin.gn", "created": "2019-01-08", "updated": "", "authors": ["michele starnini", "mari\u00e1n bogu\u00f1\u00e1", "m. \u00e1ngeles serrano"], "url": "https://arxiv.org/abs/1901.01976"}, {"title": "semi-parametric dynamic contextual pricing", "id": "1901.02045", "abstract": "motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information describing the customer's history and the product's type to predict her valuation of the product. however, her true valuation is unobservable to the seller, only binary outcome in the form of success-failure of a transaction is observed. unlike in usual contextual bandit settings, the optimal price/arm given a covariate in our setting is sensitive to the detailed characteristics of the residual uncertainty distribution. we develop a semi-parametric model in which the residual distribution is non-parametric and provide the first algorithm which learns both regression parameters and residual distribution with $\\tilde o(\\sqrt{n})$ regret. we empirically test a scalable implementation of our algorithm and observe good performance.", "categories": "cs.lg econ.em stat.ml", "created": "2019-01-07", "updated": "2019-08-10", "authors": ["virag shah", "jose blanchet", "ramesh johari"], "url": "https://arxiv.org/abs/1901.02045"}, {"title": "public health and access to medicine. pharmaceutical industry's role", "id": "1901.02384", "abstract": "every year, 10 million people die from lack of access to treatment for curable diseases, specially in developing countries. meanwhile, legal but unsafe drugs cause 130 thousand deaths per year. how can this be happening in 21st century? what role does the pharmaceutical industry play in this tragedy? in this research, who reports are analyzed and primary information gathered so as to answer this questions.", "categories": "econ.gn q-fin.ec", "created": "2019-01-08", "updated": "", "authors": ["juan gonzalez-blanco"], "url": "https://arxiv.org/abs/1901.02384"}, {"title": "modeling tax distribution in metropolitan regions with policyspace", "id": "1901.02391", "abstract": "brazilian executive body has consistently vetoed legislative initiatives easing creation and emancipation of municipalities. the literature lists evidence of the negative results of municipal fragmentation, especially so for metropolitan regions. in order to provide evidences for the argument of metropolitan union, this paper quantifies the quality of life of metropolitan citizens in the face of four alternative rules of distribution of municipal tax collection. methodologically, a validated agent-based spatial model is simulated. on top of that, econometric models are tested using real exogenous variables and simulated data. results suggest two central conclusions. first, the progressiveness of the municipal participation fund and its relevance to a better quality of life in metropolitan municipalities is confirmed. second, municipal financial merging would improve citizens' quality of life, compared to the status quo for 23 brazilian metropolises. further, the paper presents quantitative evidence that allows comparing alternative tax distributions for each of the 40 simulated metropolises, identifying more efficient forms of fiscal distribution and contributing to the literature and to contemporary parliamentary debate.", "categories": "econ.gn cs.ma q-fin.ec", "created": "2018-12-31", "updated": "", "authors": ["bernardo alves furtado"], "url": "https://arxiv.org/abs/1901.02391"}, {"title": "dynamic tail inference with log-laplace volatility", "id": "1901.02419", "abstract": "we propose a family of models that enable predictive estimation of time-varying extreme event probabilities in heavy-tailed and nonlinearly dependent time series. the models are a white noise process with conditionally log-laplace stochastic volatility. in contrast to other, similar stochastic volatility formalisms, this process has analytic expressions for its conditional probabilistic structure that enable straightforward estimation of dynamically changing extreme event probabilities. the process and volatility are conditionally pareto-tailed, with tail exponent given by the reciprocal of the log-volatility's mean absolute innovation. this formalism can accommodate a wide variety of nonlinear dependence, as well as conditional power law-tail behavior ranging from weakly non-gaussian to cauchy-like tails. we provide a computationally straightforward estimation procedure that uses an asymptotic approximation of the process' dynamic large deviation probabilities. we demonstrate the estimator's utility with a simulation study. we then show the method's predictive capabilities on a simulated nonlinear time series where the volatility is driven by the chaotic lorenz system. lastly we provide an empirical application, which shows that this simple modeling method can be effectively used for dynamic and predictive tail inference in financial time series.", "categories": "stat.me econ.em math.st q-fin.rm q-fin.st stat.th", "created": "2019-01-08", "updated": "2019-07-31", "authors": ["gordon v. chavez"], "url": "https://arxiv.org/abs/1901.02419"}, {"title": "efficient minimum distance estimation of pareto exponent from top income   shares", "id": "1901.02471", "abstract": "we propose an efficient estimation method for the income pareto exponent when only certain top income shares are observable. our estimator is based on the asymptotic theory of weighted sums of order statistics and the efficient minimum distance estimator. simulations show that our estimator has excellent finite sample properties. we apply our estimation method to u.s. top income share data and find that the pareto exponent has been stable at around 1.5 since 1985, suggesting that the rise in inequality during the last three decades is mainly driven by redistribution between the rich and poor, not among the rich.", "categories": "math.st econ.gn q-fin.ec stat.ap stat.th", "created": "2019-01-08", "updated": "2020-02-21", "authors": ["alexis akira toda", "yulong wang"], "url": "https://arxiv.org/abs/1901.02471"}, {"title": "blockchain in global supply chains and cross border trade: a critical   synthesis of the state-of-the-art, challenges and opportunities", "id": "1901.02715", "abstract": "blockchain in supply chain management is expected to boom over the next five years. it is estimated that the global blockchain supply chain market would grow at a compound annual growth rate of 87% and increase from \\$45 million in 2018 to \\$3,314.6 million by 2023. blockchain will improve business for all global supply chain stakeholders by providing enhanced traceability, facilitating digitisation, and securing chain-of-custody. this paper provides a synthesis of the existing challenges in global supply chain and trade operations, as well as the relevant capabilities and potential of blockchain. we further present leading pilot initiatives on applying blockchains to supply chains and the logistics industry to fulfill a range of needs. finally, we discuss the implications of blockchain on customs and governmental agencies, summarize challenges in enabling the wide scale deployment of blockchain in global supply chain management, and identify future research directions.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2019-01-05", "updated": "", "authors": ["yanling chang", "eleftherios iakovou", "weidong shi4"], "url": "https://arxiv.org/abs/1901.02715"}, {"title": "estimating population average treatment effects from experiments with   noncompliance", "id": "1901.02991", "abstract": "randomized control trials (rcts) are the gold standard for estimating causal effects, but often use samples that are non-representative of the actual population of interest. we propose a reweighting method for estimating population average treatment effects in settings with noncompliance. simulations show the proposed compliance-adjusted population estimator outperforms its unadjusted counterpart when compliance is relatively low and can be predicted by observed covariates. we apply the method to evaluate the effect of medicaid coverage on health care use for a target population of adults who may benefit from expansions to the medicaid program. we draw rct data from the oregon health insurance experiment, where less than one-third of those randomly selected to receive medicaid benefits actually enrolled.", "categories": "stat.me econ.em stat.ap", "created": "2019-01-09", "updated": "2020-08-08", "authors": ["kellie ottoboni", "jason poulos"], "url": "https://arxiv.org/abs/1901.02991"}, {"title": "when does privatization spur entrepreneurial performance? the moderating   effect of institutional quality in an emerging market", "id": "1901.03356", "abstract": "we explore how institutional quality moderates the effectiveness of privatization on entrepreneurs sales performance. to do this, we blend agency theory and entrepreneurial cognition theory with insights from institutional economics to develop a model of emerging market venture performance. using data from the world banks enterprise survey of entrepreneurs in china, our results suggest that private-owned enterprises (poes) outperform state-owned enterprises (soes) but only in environments with high-quality market institutions. in environments with low-quality market institutions, soes outperform poes. these findings suggest that the effectiveness of privatization on entrepreneurial performance is context-specific, which reveals more nuance than previously has been attributed.", "categories": "econ.gn q-fin.ec", "created": "2019-01-10", "updated": "", "authors": ["christopher boudreaux"], "url": "https://arxiv.org/abs/1901.03356"}, {"title": "community matters: heterogeneous impacts of a sanitation intervention", "id": "1901.03544", "abstract": "we study the effectiveness of a community-level information intervention aimed at improving sanitation using a cluster-randomized controlled trial (rct) in nigerian communities. the intervention, community-led total sanitation (clts), is currently part of national sanitation policy in more than 25 countries. while average impacts are exiguous almost three years after implementation at scale, the results hide important heterogeneity: the intervention has strong and lasting effects on sanitation practices in poorer communities. these are realized through increased sanitation investments. we show that community wealth, widely available in secondary data, is a key statistic for effective intervention targeting. using data from five other similar randomized interventions in various contexts, we find that community-level wealth heterogeneity can rationalize the wide range of impact estimates in the literature. this exercise provides plausible external validity to our findings, with implications for intervention scale-up. jel codes: o12, i12, i15, i18.", "categories": "econ.gn econ.em q-fin.ec", "created": "2019-01-11", "updated": "2020-02-10", "authors": ["laura abramovsky", "britta augsburg", "melanie l\u00fchrmann", "francisco oteiza", "juan pablo rud"], "url": "https://arxiv.org/abs/1901.03544"}, {"title": "non-parametric inference adaptive to intrinsic dimension", "id": "1901.03719", "abstract": "we consider non-parametric estimation and inference of conditional moment models in high dimensions. we show that even when the dimension $d$ of the conditioning variable is larger than the sample size $n$, estimation and inference is feasible as long as the distribution of the conditioning variable has small intrinsic dimension $d$, as measured by locally low doubling measures. our estimation is based on a sub-sampled ensemble of the $k$-nearest neighbors ($k$-nn) $z$-estimator. we show that if the intrinsic dimension of the covariate distribution is equal to $d$, then the finite sample estimation error of our estimator is of order $n^{-1/(d+2)}$ and our estimate is $n^{1/(d+2)}$-asymptotically normal, irrespective of $d$. the sub-sampling size required for achieving these results depends on the unknown intrinsic dimension $d$. we propose an adaptive data-driven approach for choosing this parameter and prove that it achieves the desired rates. we discuss extensions and applications to heterogeneous treatment effect estimation.", "categories": "cs.lg econ.em math.st stat.ml stat.th", "created": "2019-01-11", "updated": "2019-06-17", "authors": ["khashayar khosravi", "greg lewis", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/1901.03719"}, {"title": "mastering panel 'metrics: causal impact of democracy on growth", "id": "1901.03821", "abstract": "the relationship between democracy and economic growth is of long-standing interest. we revisit the panel data analysis of this relationship by acemoglu, naidu, restrepo and robinson (forthcoming) using state of the art econometric methods. we argue that this and lots of other panel data settings in economics are in fact high-dimensional, resulting in principal estimators -- the fixed effects (fe) and arellano-bond (ab) estimators -- to be biased to the degree that invalidates statistical inference. we can however remove these biases by using simple analytical and sample-splitting methods, and thereby restore valid statistical inference. we find that the debiased fe and ab estimators produce substantially higher estimates of the long-run effect of democracy on growth, providing even stronger support for the key hypothesis in acemoglu, naidu, restrepo and robinson (forthcoming). given the ubiquitous nature of panel data, we conclude that the use of debiased panel data estimators should substantially improve the quality of empirical inference in economics.", "categories": "econ.em stat.me", "created": "2019-01-12", "updated": "", "authors": ["shuowen chen", "victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val"], "url": "https://arxiv.org/abs/1901.03821"}, {"title": "fuzzy profit shifting: a model for optimal tax-induced transfer pricing   with fuzzy arm's length parameter", "id": "1901.03843", "abstract": "this paper proposes a model of optimal tax-induced transfer pricing with a fuzzy arm's length parameter. fuzzy numbers provide a suitable structure for modelling the ambiguity that is intrinsic to the arm's length parameter. for the usual conditions regarding the anti-shifting mechanisms, the optimal transfer price becomes a maximising $\\alpha$-cut of the fuzzy arm's length parameter. nonetheless, we show that it is profitable for firms to choose any maximising transfer price if the probability of tax audit is sufficiently low, even if the chosen price is considered a completely non-arm's length price by tax authorities. in this case, we derive the necessary and sufficient conditions to prevent this extreme shifting strategy", "categories": "econ.gn q-fin.ec", "created": "2019-01-12", "updated": "", "authors": ["alex a. t. rathke"], "url": "https://arxiv.org/abs/1901.03843"}, {"title": "how many people microwork in france? estimating the size of a new labor   force", "id": "1901.03889", "abstract": "microwork platforms allocate fragmented tasks to crowds of providers with remunerations as low as few cents. instrumental to the development of today's artificial intelligence, these micro-tasks push to the extreme the logic of casualization already observed in \"uberized\" workers. the present article uses the results of the diplab study to estimate the number of people who microwork in france. we distinguish three categories of microworkers, corresponding to different modes of engagement: a group of 14,903 \"very active\" microworkers, most of whom are present on these platforms at least once a week; a second featuring 52,337 \"routine\" microworkers, more selective and present at least once a month; a third circle of 266,126 \"casual\" microworkers, more heterogeneous and who alternate inactivity and various levels of work practice. our results show that microwork is comparable to, and even larger than, the workforce of ride-sharing and delivery platforms in france. it is therefore not an anecdotal phenomenon and deserves great attention from researchers, unions and policy-makers.", "categories": "econ.gn q-fin.ec", "created": "2019-01-12", "updated": "", "authors": ["cl\u00e9ment le ludec", "paola tubaro", "antonio a. casilli"], "url": "https://arxiv.org/abs/1901.03889"}, {"title": "inequality, mobility and the financial accumulation process: a   computational economic analysis", "id": "1901.03951", "abstract": "our computational economic analysis investigates the relationship between inequality, mobility and the financial accumulation process. extending the baseline model by levy et al., we characterise the economic process through stylised return structures generating alternative evolutions of income and wealth through time. first, we explore the limited heuristic contribution of one and two factors models comprising one single stock (capital wealth) and one single flow factor (labour) as pure drivers of income and wealth generation and allocation over time. second, we introduce heuristic modes of taxation in line with the baseline approach. our computational economic analysis corroborates that the financial accumulation process featuring compound returns plays a significant role as source of inequality, while institutional arrangements including taxation play a significant role in framing and shaping the aggregate economic process that evolves over socioeconomic space and time.", "categories": "econ.gn econ.th physics.soc-ph q-fin.ec", "created": "2019-01-13", "updated": "", "authors": ["simone righi", "yuri biondi"], "url": "https://arxiv.org/abs/1901.03951"}, {"title": "designing an industrial policy for developing countries: a new approach", "id": "1901.04265", "abstract": "in this study, the prevalent methodology for design of the industrial policy in developing countries was critically assessed, and it was shown that the mechanism and content of classical method is fundamentally contradictory to the goals and components of the endogenous growth theories. this study, by proposing a new approach, along settling schumpeter's economic growth theory as a policy framework, designed the process of entering, analyzing and processing data as the mechanism of the industrial policy in order to provide \"theoretical consistency\" and \"technical and statistical requirements\" for targeting the growth stimulant factor effectively.", "categories": "econ.gn q-fin.ec", "created": "2019-01-14", "updated": "", "authors": ["ali haeri", "abbas arabmazar"], "url": "https://arxiv.org/abs/1901.04265"}, {"title": "100+ metrics for software startups - a multi-vocal literature review", "id": "1901.04819", "abstract": "metrics can be used by businesses to make more objective decisions based on data. software startups in particular are characterized by the uncertain or even chaotic nature of the contexts in which they operate. using data in the form of metrics can help software startups to make the right decisions amidst uncertainty and limited resources. however, whereas conventional business metrics and software metrics have been studied in the past, metrics in the spe-cific context of software startup are not widely covered within academic literature. to promote research in this area and to create a starting point for it, we have conducted a multi-vocal literature review focusing on practitioner literature in order to compile a list of metrics used by software startups. said list is intended to serve as a basis for further research in the area, as the metrics in it are based on suggestions made by practitioners and not empirically verified.", "categories": "cs.gl econ.gn q-fin.ec", "created": "2019-01-15", "updated": "", "authors": ["kai-kristian kemell", "xiaofeng wang", "anh nguyen-duc", "jason grendus", "tuure tuunanen", "pekka abrahamsson"], "url": "https://arxiv.org/abs/1901.04819"}, {"title": "inference on functionals under first order degeneracy", "id": "1901.04861", "abstract": "this paper presents a unified second order asymptotic framework for conducting inference on parameters of the form $\\phi(\\theta_0)$, where $\\theta_0$ is unknown but can be estimated by $\\hat\\theta_n$, and $\\phi$ is a known map that admits null first order derivative at $\\theta_0$. for a large number of examples in the literature, the second order delta method reveals a nondegenerate weak limit for the plug-in estimator $\\phi(\\hat\\theta_n)$. we show, however, that the `standard' bootstrap is consistent if and only if the second order derivative $\\phi_{\\theta_0}''=0$ under regularity conditions, i.e., the standard bootstrap is inconsistent if $\\phi_{\\theta_0}''\\neq 0$, and provides degenerate limits unhelpful for inference otherwise. we thus identify a source of bootstrap failures distinct from that in fang and santos (2018) because the problem (of consistently bootstrapping a \\textit{nondegenerate} limit) persists even if $\\phi$ is differentiable. we show that the correction procedure in babu (1984) can be extended to our general setup. alternatively, a modified bootstrap is proposed when the map is \\textit{in addition} second order nondifferentiable. both are shown to provide local size control under some conditions. as an illustration, we develop a test of common conditional heteroskedastic (ch) features, a setting with both degeneracy and nondifferentiability -- the latter is because the jacobian matrix is degenerate at zero and we allow the existence of multiple common ch features.", "categories": "econ.em math.st stat.th", "created": "2019-01-15", "updated": "", "authors": ["qihui chen", "zheng fang"], "url": "https://arxiv.org/abs/1901.04861"}, {"title": "rps(1) preferences", "id": "1901.04995", "abstract": "we consider a model for decision making based on an adaptive, k-period, learning process where the priors are selected according to von neumann-morgenstern expected utility principle. a preference relation between two prospects is introduced, defined by the condition which prospect is selected more often. we show that the new preferences have similarities with the preferences obtained by kahneman and tversky (1979) in the context of the prospect theory. additionally, we establish that in the limit of large learning period, the new preferences coincide with the expected utility principle.", "categories": "econ.th", "created": "2019-01-14", "updated": "2019-02-15", "authors": ["misha perepelitsa"], "url": "https://arxiv.org/abs/1901.04995"}, {"title": "econophysics of asset price, return and multiple expectations", "id": "1901.05024", "abstract": "this paper describes asset price and return disturbances as result of relations between transactions and multiple kinds of expectations. we show that disturbances of expectations can cause fluctuations of trade volume, price and return. we model price disturbances for transactions made under all types of expectations as weighted sum of partial price and trade volume disturbances for transactions made under separate kinds of expectations. relations on price allow present return as weighted sum of partial return and trade volume \"return\" for transactions made under separate expectations. dependence of price disturbances on trade volume disturbances as well as dependence of return on trade volume \"return\" cause dependence of volatility and statistical distributions of price and return on statistical properties of trade volume disturbances and trade volume \"return\" respectively.", "categories": "econ.gn q-fin.ec", "created": "2019-01-15", "updated": "2020-09-08", "authors": ["victor olkhov"], "url": "https://arxiv.org/abs/1901.05024"}, {"title": "an inattention model for traveler behavior with e-coupons", "id": "1901.05070", "abstract": "in this study, we consider traveler coupon redemption behavior from the perspective of an urban mobility service. assuming traveler behavior is in accordance with the principle of utility maximization, we first formulate a baseline dynamical model for traveler's expected future trip sequence under the framework of markov decision processes and from which we derive approximations of the optimal coupon redemption policy. however, we find that this baseline model cannot explain perfectly observed coupon redemption behavior of traveler for a car-sharing service. to resolve this deviation from utility-maximizing behavior, we suggest a hypothesis that travelers may not be aware of all coupons available to them. based on this hypothesis, we formulate an inattention model on unawareness, which is complementary to the existing models of inattention, and incorporate it into the baseline model. estimation results show that the proposed model better explains the coupon redemption dataset than the baseline model. we also conduct a simulation experiment to quantify the negative impact of unawareness on coupons' promotional effects. these results can be used by mobility service operators to design effective coupon distribution schemes in practice.", "categories": "econ.th stat.ap", "created": "2018-12-28", "updated": "", "authors": ["han qiu"], "url": "https://arxiv.org/abs/1901.05070"}, {"title": "lassopack: model selection and prediction with regularized regression in   stata", "id": "1901.05397", "abstract": "this article introduces lassopack, a suite of programs for regularized regression in stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation ols. the methods are suitable for the high-dimensional setting where the number of predictors $p$ may be large and possibly greater than the number of observations, $n$. we offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), $k$-fold cross-validation and $h$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). we discuss the theoretical framework and practical considerations for each approach. we also present monte carlo results to compare the performance of the penalization approaches.", "categories": "econ.em", "created": "2019-01-16", "updated": "", "authors": ["achim ahrens", "christian b. hansen", "mark e. schaffer"], "url": "https://arxiv.org/abs/1901.05397"}, {"title": "relational communication", "id": "1901.05645", "abstract": "we study a communication game between an informed sender and an uninformed receiver with repeated interactions and voluntary transfers. transfers motivate the receiver's decision-making and signal the sender's information. although full separation can always be supported in equilibrium, partial or complete pooling is optimal if the receiver's decision-making is highly responsive to information. in this case, the receiver's decision-making is disciplined by pooling extreme states, where she is most tempted to defect.", "categories": "econ.th", "created": "2019-01-17", "updated": "2020-07-04", "authors": ["anton kolotilin", "hongyi li"], "url": "https://arxiv.org/abs/1901.05645"}, {"title": "preparing millennials as digital citizens and socially and   environmentally responsible business professionals in a socially   irresponsible climate", "id": "1901.06609", "abstract": "as of 2015, a millennial born in the 1990's became the largest population in the workplace and are still growing. studies indicate that a millennial is tech savvy but lag in the exercise of digital responsibility. in addition, they are passive towards environmental sustainability and fail to grasp the importance of social responsibility. this paper provides a review of such findings relating to business communications educators in their classrooms. the literature should enable the development of a millennial as an excellent global citizen through business communications curricula that emphasizes digital citizenship, environmental sustainability and social responsibility. the impetus for this work is to provide guidance in the development of courses and teaching strategies customized to the development of each millennial as a digital, environmental and socially responsible global citizen.", "categories": "econ.gn q-fin.ec", "created": "2019-01-19", "updated": "", "authors": ["barbara burgess-wilkerson", "clovia hamilton", "chlotia garrison", "keith robbins"], "url": "https://arxiv.org/abs/1901.06609"}, {"title": "dancing with donald: polarity in the 2016 presidential election", "id": "1901.07542", "abstract": "in almost every election cycle, the validity of the united states electoral college is brought into question. the 2016 presidential election again brought up the issue of a candidate winning the popular vote but not winning the electoral college, with hillary clinton receiving close to three million more votes than donald trump. however, did the popular vote actually determine the most liked candidate in the election? in this paper, we demonstrate that different voting policies can alter which candidate is elected. additionally, we explore the trade-offs between each of these mechanisms. finally, we introduce two novel mechanisms with the intent of electing the least polarizing candidate.", "categories": "econ.gn econ.th q-fin.ec", "created": "2019-01-21", "updated": "", "authors": ["robert chuchro", "kyle d'souza", "darren mei"], "url": "https://arxiv.org/abs/1901.07542"}, {"title": "a noncooperative model of contest network formation", "id": "1901.07605", "abstract": "in this paper we study a model of weighted network formation. the bilateral interaction is modeled as a tullock contest game with the possibility of a draw. we describe stable networks under different concepts of stability. we show that a nash stable network is either the empty network or the complete network. the complete network is not immune to bilateral deviations. when we allow for limited farsightedness, stable networks immune to bilateral deviations must be complete $m$-partite networks, with partitions of different sizes. the empty network is the efficient network. we provide several comparative statics results illustrating the importance of network structure in mediating the effects of shocks and interventions. in particular, we show that an increase in the likelihood of a draw has a non-monotonic effect on the level of wasteful contest spending in the society. to the best of our knowledge, this paper is the first attempt to model weighted network formation when the actions of individuals are neither strategic complements nor strategic substitutes.", "categories": "econ.th", "created": "2019-01-22", "updated": "2020-05-19", "authors": ["kenan huremovic"], "url": "https://arxiv.org/abs/1901.07605"}, {"title": "academic engagement and commercialization in an institutional transition   environment: evidence from shanghai maritime university", "id": "1901.07725", "abstract": "does academic engagement accelerate or crowd out the commercialization of university knowledge? research on this topic seldom considers the impact of the institutional environment, especially when a formal institution for encouraging the commercial activities of scholars has not yet been established. this study investigates this question in the context of china, which is in the institutional transition stage. based on a survey of scholars from shanghai maritime university, we demonstrate that academic engagement has a positive impact on commercialization and that this impact is greater for risk-averse scholars than for other risk-seeking scholars. our results suggest that in an institutional transition environment, the government should consider encouraging academic engagement to stimulate the commercialization activities of conservative scholars.", "categories": "econ.gn q-fin.ec", "created": "2019-01-23", "updated": "", "authors": ["dongbo shi", "yeyanran ge"], "url": "https://arxiv.org/abs/1901.07725"}, {"title": "the economic complexity of us metropolitan areas", "id": "1901.08112", "abstract": "we calculate measures of economic complexity for us metropolitan areas for the years 2007-2015 based on industry employment data. we show that the concept of economic complexity translates well from the cross-country to the regional setting, and is able to incorporate local as well as traded industries. the largest cities and the northeast of the us have the highest average complexity, while traded industries are more complex than local-serving ones on average, but with some exceptions. on average, regions with higher complexity have a higher income per capita, but those regions also were more affected by the financial crisis. finally, economic complexity is a significant predictor of within-decreases in income per capita and population. our findings highlight the importance of subnational regions, and particularly metropolitan areas, as units of economic geography.", "categories": "econ.gn q-fin.ec", "created": "2019-01-23", "updated": "", "authors": ["benedikt s. l. fritz", "robert a. manduca"], "url": "https://arxiv.org/abs/1901.08112"}, {"title": "the wisdom of a kalman crowd", "id": "1901.08133", "abstract": "the kalman filter has been called one of the greatest inventions in statistics during the 20th century. its purpose is to measure the state of a system by processing the noisy data received from different electronic sensors. in comparison, a useful resource for managers in their effort to make the right decisions is the wisdom of crowds. this phenomenon allows managers to combine judgments by different employees to get estimates that are often more accurate and reliable than estimates, which managers produce alone. since harnessing the collective intelligence of employees, and filtering signals from multiple noisy sensors appear related, we looked at the possibility of using the kalman filter on estimates by people. our predictions suggest, and our findings based on the survey of professional forecasters reveal, that the kalman filter can help managers solve their decision-making problems by giving them stronger signals before they choose. indeed, when used on a subset of forecasters identified by the contribution weighted model, the kalman filter beat that rule clearly, across all the forecasting horizons in the survey.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-01-23", "updated": "", "authors": ["ulrik w. nash"], "url": "https://arxiv.org/abs/1901.08133"}, {"title": "optimal reduction of public debt under partial observation of the   economic growth", "id": "1901.08356", "abstract": "we consider a government that aims at reducing the debt-to-gross domestic product (gdp) ratio of a country. the government observes the level of the debt-to-gdp ratio and an indicator of the state of the economy, but does not directly observe the development of the underlying macroeconomic conditions. the government's criterion is to minimize the sum of the total expected costs of holding debt and of debt's reduction policies. we model this problem as a singular stochastic control problem under partial observation. the contribution of the paper is twofold. firstly, we provide a general formulation of the model in which the level of debt-to-gdp ratio and the value of the macroeconomic indicator evolve as a diffusion and a jump-diffusion, respectively, with coefficients depending on the regimes of the economy. these are described through a finite-state continuous-time markov chain. we reduce via filtering techniques the original problem to an equivalent one with full information (the so-called separated problem), and we provide a general verification result in terms of a related optimal stopping problem under full information. secondly, we specialize to a case study in which the economy faces only two regimes, and the macroeconomic indicator has a suitable diffusive dynamics. in this setting we provide the optimal debt reduction policy. this is given in terms of the continuous free boundary arising in an auxiliary fully two-dimensional optimal stopping problem.", "categories": "math.oc econ.gn q-fin.ec", "created": "2019-01-24", "updated": "2019-01-25", "authors": ["giorgia callegaro", "claudia ceci", "giorgio ferrari"], "url": "https://arxiv.org/abs/1901.08356"}, {"title": "theories and practice of agent based modeling: some practical   implications for economic planners", "id": "1901.08932", "abstract": "nowadays, we are surrounded by a large number of complex phenomena ranging from rumor spreading, social norms formation to rise of new economic trends and disruption of traditional businesses. to deal with such phenomena,complex adaptive system (cas) framework has been found very influential among social scientists,especially economists. as the most powerful methodology of cas modeling, agent-based modeling (abm) has gained a growing application among academicians and practitioners. abms show how simple behavioral rules of agents and local interactions among them at micro-scale can generate surprisingly complex patterns at macro-scale. despite a growing number of abm publications, those researchers unfamiliar with this methodology have to study a number of works to understand (1) the why and what of abms and (2) the ways they are rigorously developed. therefore, the major focus of this paper is to help social sciences researchers,especially economists get a big picture of abms and know how to develop them both systematically and rigorously.", "categories": "econ.th", "created": "2019-01-23", "updated": "", "authors": ["hossein sabzian", "mohammad ali shafia", "ali maleki", "seyeed mostapha seyeed hashemi", "ali baghaei", "hossein gharib"], "url": "https://arxiv.org/abs/1901.08932"}, {"title": "orthogonal statistical learning", "id": "1901.09036", "abstract": "we provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. we analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target parameter and one for the nuisance parameter. we show that if the population risk satisfies a condition called neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. this enables the use of a plethora of existing results from statistical learning and machine learning to give new guarantees for learning with a nuisance component. moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. we provide conditions on the metric entropy of the nuisance and target classes such that oracle rates---rates of the same order as if we knew the nuisance parameter---are achieved. we also derive new rates for specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. we highlight the applicability of our results in four settings of central importance: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.", "categories": "math.st cs.lg econ.em stat.ml stat.th", "created": "2019-01-24", "updated": "2020-09-23", "authors": ["dylan j. foster", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/1901.09036"}, {"title": "technological parasitism", "id": "1901.09073", "abstract": "technological parasitism is a new theory to explain the evolution of technology in society. in this context, this study proposes a model to analyze the interaction between a host technology (system) and a parasitic technology (subsystem) to explain evolutionary pathways of technologies as complex systems. the coefficient of evolutionary growth of the model here indicates the typology of evolution of parasitic technology in relation to host technology: i.e., underdevelopment, growth and development. this approach is illustrated with realistic examples using empirical data of product and process technologies. overall, then, the theory of technological parasitism can be useful for bringing a new perspective to explain and generalize the evolution of technology and predict which innovations are likely to evolve rapidly in society.", "categories": "econ.gn q-fin.ec", "created": "2019-01-25", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1901.09073"}, {"title": "volatility models applied to geophysics and high frequency financial   market data", "id": "1901.09145", "abstract": "this work is devoted to the study of modeling geophysical and financial time series. a class of volatility models with time-varying parameters is presented to forecast the volatility of time series in a stationary environment. the modeling of stationary time series with consistent properties facilitates prediction with much certainty. using the garch and stochastic volatility model, we forecast one-step-ahead suggested volatility with +/- 2 standard prediction errors, which is enacted via maximum likelihood estimation. we compare the stochastic volatility model relying on the filtering technique as used in the conditional volatility with the garch model. we conclude that the stochastic volatility is a better forecasting tool than garch (1, 1), since it is less conditioned by autoregressive past information.", "categories": "stat.ap econ.em q-fin.st", "created": "2019-01-25", "updated": "", "authors": ["maria c mariani", "md al masum bhuiyan", "osei k tweneboah", "hector gonzalez-huizar", "ionut florescu"], "url": "https://arxiv.org/abs/1901.09145"}, {"title": "may's instability in large economies", "id": "1901.09629", "abstract": "will a large economy be stable? building on robert may's original argument for large ecosystems, we conjecture that evolutionary and behavioural forces conspire to drive the economy towards marginal stability. we study networks of firms in which inputs for production are not easily substitutable, as in several real-world supply chains. relying on results from random matrix theory, we argue that such networks generically become dysfunctional when their size increases, when the heterogeneity between firms becomes too strong or when substitutability of their production inputs is reduced. at marginal stability and for large heterogeneities, we find that the distribution of firm sizes develops a power-law tail, as observed empirically. crises can be triggered by small idiosyncratic shocks, which lead to \"avalanches\" of defaults characterized by a power-law distribution of total output losses. this scenario would naturally explain the well-known \"small shocks, large business cycles\" puzzle, as anticipated long ago by bak, chen, scheinkman and woodford.", "categories": "physics.soc-ph cond-mat.stat-mech econ.gn q-fin.ec", "created": "2019-01-28", "updated": "2019-09-21", "authors": ["jos\u00e9 moran", "jean-philippe bouchaud"], "url": "https://arxiv.org/abs/1901.09629"}, {"title": "estimation and simulation of the transaction arrival process in intraday   electricity markets", "id": "1901.09729", "abstract": "we examine the novel problem of the estimation of transaction arrival processes in the intraday electricity markets. we model the inter-arrivals using multiple time-varying parametric densities based on the generalized f distribution estimated by maximum likelihood. we analyse both the in-sample characteristics and the probabilistic forecasting performance. in a rolling window forecasting study, we simulate many trajectories to evaluate the forecasts and gain significant insights into the model fit. the prediction accuracy is evaluated by a functional version of the mae (mean absolute error), rmse (root mean squared error) and crps (continuous ranked probability score) for the simulated count processes. this paper fills the gap in the literature regarding the intensity estimation of transaction arrivals and is a major contribution to the topic, yet leaves much of the field for further development. the study presented in this paper is conducted based on the german intraday continuous electricity market data, but this method can be easily applied to any other continuous intraday electricity market. for the german market, a specific generalized gamma distribution setup explains the overall behaviour significantly best, especially as the tail behaviour of the process is well covered.", "categories": "econ.gn q-fin.ec q-fin.st stat.ap", "created": "2019-01-28", "updated": "2019-12-02", "authors": ["micha\u0142 narajewski", "florian ziel"], "url": "https://arxiv.org/abs/1901.09729"}, {"title": "lost in diversification", "id": "1901.09795", "abstract": "as financial instruments grow in complexity more and more information is neglected by risk optimization practices. this brings down a curtain of opacity on the origination of risk, that has been one of the main culprits in the 2007-2008 global financial crisis. we discuss how the loss of transparency may be quantified in bits, using information theoretic concepts. we find that {\\em i)} financial transformations imply large information losses, {\\em ii)} portfolios are more information sensitive than individual stocks only if fundamental analysis is sufficiently informative on the co-movement of assets, that {\\em iii)} securitisation, in the relevant range of parameters, yields assets that are less information sensitive than the original stocks and that {\\em iv)} when diversification (or securitisation) is at its best (i.e. when assets are uncorrelated) information losses are maximal. we also address the issue of whether pricing schemes can be introduced to deal with information losses. this is relevant for the transmission of incentives to gather information on the risk origination side. within a simple mean variance scheme, we find that market incentives are not generally sufficient to make information harvesting sustainable.", "categories": "q-fin.gn econ.th physics.soc-ph", "created": "2019-01-28", "updated": "", "authors": ["marco bardoscia", "daniele d'arienzo", "matteo marsili", "valerio volpati"], "url": "https://arxiv.org/abs/1901.09795"}, {"title": "a review on energy, environmental, and sustainability implications of   connected and automated vehicles", "id": "1901.10581", "abstract": "connected and automated vehicles (cavs) are poised to reshape transportation and mobility by replacing humans as the driver and service provider. while the primary stated motivation for vehicle automation is to improve safety and convenience of road mobility, this transformation also provides a valuable opportunity to improve vehicle energy efficiency and reduce emissions in the transportation sector. progress in vehicle efficiency and functionality, however, does not necessarily translate to net positive environmental outcomes. here we examine the interactions between cav technology and the environment at four levels of increasing complexity: vehicle, transportation system, urban system, and society. we find that environmental impacts come from cav-facilitated transformations at all four levels, rather than from cav technology directly. we anticipate net positive environmental impacts at the vehicle, transportation system, and urban system levels, but expect greater vehicle utilization and shifts in travel patterns at the society level to offset some of these benefits. focusing on the vehicle-level improvements associated with cav technology is likely to yield excessively optimistic estimates of environmental benefits. future research and policy efforts should strive to clarify the extent and possible synergetic effects from a systems level in order to envisage and address concerns regarding the short- and long-term sustainable adoption of cav technology.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2019-01-22", "updated": "2019-02-17", "authors": ["morteza taiebat", "austin l. brown", "hannah r. safford", "shen qu", "ming xu"], "url": "https://arxiv.org/abs/1901.10581"}, {"title": "learning choice functions: concepts and architectures", "id": "1901.10860", "abstract": "we study the problem of learning choice functions, which play an important role in various domains of application, most notably in the field of economics. formally, a choice function is a mapping from sets to sets: given a set of choice alternatives as input, a choice function identifies a subset of most preferred elements. learning choice functions from suitable training data comes with a number of challenges. for example, the sets provided as input and the subsets produced as output can be of any size. moreover, since the order in which alternatives are presented is irrelevant, a choice function should be symmetric. perhaps most importantly, choice functions are naturally context-dependent, in the sense that the preference in favor of an alternative may depend on what other options are available. we formalize the problem of learning choice functions and present two general approaches based on two representations of context-dependent utility functions. both approaches are instantiated by means of appropriate neural network architectures, and their performance is demonstrated on suitable benchmark tasks.", "categories": "cs.lg cs.ne econ.gn q-fin.ec stat.ml", "created": "2019-01-29", "updated": "2019-05-24", "authors": ["karlson pfannschmidt", "pritha gupta", "eyke h\u00fcllermeier"], "url": "https://arxiv.org/abs/1901.10860"}, {"title": "a dynamic factor model approach to incorporate big data in state space   models for official statistics", "id": "1901.11355", "abstract": "in this paper we consider estimation of unobserved components in state space models using a dynamic factor approach to incorporate auxiliary information from high-dimensional data sources. we apply the methodology to unemployment estimation as done by statistics netherlands, who uses a multivariate state space model to produce monthly figures for the unemployment using series observed with the labour force survey (lfs). we extend the model by including auxiliary series of google trends about job-search and economic uncertainty, and claimant counts, partially observed at higher frequencies. our factor model allows for nowcasting the variable of interest, providing reliable unemployment estimates in real-time before lfs data become available.", "categories": "econ.em stat.me", "created": "2019-01-31", "updated": "2020-02-14", "authors": ["caterina schiavoni", "franz palm", "stephan smeekes", "jan van den brakel"], "url": "https://arxiv.org/abs/1901.11355"}, {"title": "modelling transfer profits as externalities in a cooperative   game-theoretic model of natural gas networks", "id": "1901.11435", "abstract": "existing cooperative game theoretic studies of bargaining power in gas pipeline systems are based on the so called characteristic function form (cff). this approach is potentially misleading if some pipelines fall under regulated third party access (tpa). tpa, which is by now the norm in the eu, obliges the owner of a pipeline to transport gas for others, provided they pay a regulated transport fee. from a game theoretic perspective, this institutional setting creates so called \"externalities,\" the description of which requires partition function form (pff) games. in this paper we propose a method to compute payoffs, reflecting the power structure, for a pipeline system with regulated tpa. the method is based on an iterative flow mechanism to determine gas flows and transport fees for individual players and uses the recursive core and the minimal claim function to convert the ppf game back into a cff game, which can be solved by standard methods. we illustrate the approach with a simple stylized numerical example of the gas network in central eastern europe with a focus on ukraine's power index as a major transit country.", "categories": "econ.th", "created": "2019-01-31", "updated": "2019-02-01", "authors": ["d\u00e1vid csercsik", "franz hubert", "bal\u00e1zs r. sziklai", "l\u00e1szl\u00f3 \u00e1. k\u00f3czy"], "url": "https://arxiv.org/abs/1901.11435"}, {"title": "approaches toward the bayesian estimation of the stochastic volatility   model with leverage", "id": "1901.11491", "abstract": "the sampling efficiency of mcmc methods in bayesian inference for stochastic volatility (sv) models is known to highly depend on the actual parameter values, and the effectiveness of samplers based on different parameterizations varies significantly. we derive novel algorithms for the centered and the non-centered parameterizations of the practically highly relevant sv model with leverage, where the return process and innovations of the volatility process are allowed to correlate. moreover, based on the idea of ancillarity-sufficiency interweaving (asis), we combine the resulting samplers in order to guarantee stable sampling efficiency irrespective of the baseline parameterization.we carry out an extensive comparison to already existing sampling methods for this model using simulated as well as real world data.", "categories": "stat.co econ.em q-fin.st stat.me", "created": "2019-01-31", "updated": "2019-11-28", "authors": ["darjus hosszejni", "gregor kastner"], "url": "https://arxiv.org/abs/1901.11491"}, {"title": "forecasting the impact of connected and automated vehicles on energy use   a microeconomic study of induced travel and energy rebound", "id": "1902.00382", "abstract": "connected and automated vehicles (cavs) are expected to yield significant improvements in safety, energy efficiency, and time utilization. however, their net effect on energy and environmental outcomes is unclear. higher fuel economy reduces the energy required per mile of travel, but it also reduces the fuel cost of travel, incentivizing more travel and causing an energy \"rebound effect.\" moreover, cavs are predicted to vastly reduce the time cost of travel, inducing further increases in travel and energy use. in this paper, we forecast the induced travel and rebound from cavs using data on existing travel behavior. we develop a microeconomic model of vehicle miles traveled (vmt) choice under income and time constraints; then we use it to estimate elasticities of vmt demand with respect to fuel and time costs, with fuel cost data from the 2017 united states national household travel survey (nhts) and wage-derived predictions of travel time cost. our central estimate of the combined price elasticity of vmt demand is -0.4, which differs substantially from previous estimates. we also find evidence that wealthier households have more elastic demand, and that households at all income levels are more sensitive to time costs than to fuel costs. we use our estimated elasticities to simulate vmt and energy use impacts of full, private cav adoption under a range of possible changes to the fuel and time costs of travel. we forecast a 2-47% increase in travel demand for an average household. our results indicate that backfire - i.e., a net rise in energy use - is a possibility, especially in higher income groups. this presents a stiff challenge to policy goals for reductions in not only energy use but also traffic congestion and local and global air pollution, as cav use increases.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-01-31", "updated": "2019-05-09", "authors": ["morteza taiebat", "samuel stolper", "ming xu"], "url": "https://arxiv.org/abs/1902.00382"}, {"title": "does better governance guarantee less corruption? evidence of loss in   effectiveness of the rule of law", "id": "1902.00428", "abstract": "corruption is an endemic societal problem with profound implications in the development of nations. in combating this issue, cross-national evidence supporting the effectiveness of the rule of law seems at odds with poorly realized outcomes from reforms inspired in such literature. this paper provides an explanation for such contradiction. by taking a computational approach, we develop two methodological novelties into the empirical study of corruption: (1) generating large within-country variation by means of simulation (instead of cross-national data pooling), and (2) accounting for interactions between covariates through a spillover network. the latter (the network), seems responsible for a significant reduction in the effectiveness of the rule of law; especially among the least developed countries. we also find that effectiveness can be boosted by improving complementary policy issues that may lie beyond the governance agenda. moreover, our simulations suggest that improvements to the rule of law are a necessary yet not sufficient condition to curve corruption.", "categories": "econ.gn q-fin.ec", "created": "2019-02-01", "updated": "", "authors": ["omar a. guerrero", "gonzalo casta\u00f1eda"], "url": "https://arxiv.org/abs/1902.00428"}, {"title": "the importance of social and government learning in ex ante policy   evaluation", "id": "1902.00429", "abstract": "we provide two methodological insights on \\emph{ex ante} policy evaluation for macro models of economic development. first, we show that the problems of parameter instability and lack of behavioral constancy can be overcome by considering learning dynamics. hence, instead of defining social constructs as fixed exogenous parameters, we represent them through stable functional relationships such as social norms. second, we demonstrate how agent computing can be used for this purpose. by deploying a model of policy prioritization with endogenous government behavior, we estimate the performance of different policy regimes. we find that, while strictly adhering to policy recommendations increases efficiency, the nature of such recipes has a bigger effect. in other words, while it is true that lack of discipline is detrimental to prescription outcomes (a common defense of failed recommendations), it is more important that such prescriptions consider the systemic and adaptive nature of the policymaking process (something neglected by traditional technocratic advice).", "categories": "econ.gn q-fin.ec", "created": "2019-02-01", "updated": "", "authors": ["gonzalo castaeda", "omar a. guerrero"], "url": "https://arxiv.org/abs/1902.00429"}, {"title": "quantifying the coherence of development policy priorities", "id": "1902.00430", "abstract": "over the last 30 years, the concept of policy coherence for development has received especial attention among academics, practitioners and international organizations. however, its quantification and measurement remain elusive. to address this challenge, we develop a theoretical and empirical framework to measure the coherence of policy priorities for development. our procedure takes into account the country-specific constraints that governments face when trying to reach specific development goals. hence, we put forward a new definition of policy coherence where context-specific efficient resource allocations are employed as the baseline to construct an index. to demonstrate the usefulness and validity of our index, we analyze the cases of mexico, korea and estonia, three developing countries that, arguably, joined the oecd with the aim of coherently establishing policies that could enable a catch-up process. we find that korea shows significant signs of policy coherence, estonia seems to be in the process of achieving it, and mexico has unequivocally failed. furthermore, our results highlight the limitations of assessing coherence in terms of naive benchmark comparisons using development-indicator data. altogether, our framework sheds new light in a promising direction to develop bespoke analytic tools to meet the 2030 agenda.", "categories": "econ.gn q-fin.ec", "created": "2019-02-01", "updated": "", "authors": ["omar a. guerrero", "gonzalo casta\u00f1eda"], "url": "https://arxiv.org/abs/1902.00430"}, {"title": "how do governments determine policy priorities? studying development   strategies through spillover networks", "id": "1902.00432", "abstract": "determining policy priorities is a challenging task for any government because there may be, for example, a multiplicity of objectives to be simultaneously attained, a multidimensional policy space to be explored, inefficiencies in the implementation of public policies, interdependencies between policy issues, etc. altogether, these factor s generate a complex landscape that governments need to navigate in order to reach their goals. to address this problem, we develop a framework to model the evolution of development indicators as a political economy game on a network. our approach accounts for the --recently documented-- network of spillovers between policy issues, as well as the well-known political economy problem arising from budget assignment. this allows us to infer not only policy priorities, but also the effective use of resources in each policy issue. using development indicators data from more than 100 countries over 11 years, we show that the country-specific context is a central determinant of the effectiveness of policy priorities. in addition, our model explains well-known aggregate facts about the relationship between corruption and development. finally, this framework provides a new analytic tool to generate bespoke advice on development strategies.", "categories": "econ.gn q-fin.ec", "created": "2019-02-01", "updated": "", "authors": ["omar a. guerrero", "gonzalo casta\u00f1eda", "florian ch\u00e1vez-ju\u00e1rez"], "url": "https://arxiv.org/abs/1902.00432"}, {"title": "robust productivity analysis: an application to german fadn data", "id": "1902.00678", "abstract": "sources of bias in empirical studies can be separated in those coming from the modelling domain (e.g. multicollinearity) and those coming from outliers. we propose a two-step approach to counter both issues. first, by decontaminating data with a multivariate outlier detection procedure and second, by consistently estimating parameters of the production function. we apply this approach to a panel of german field crop data. results show that the decontamination procedure detects multivariate outliers. in general, multivariate outlier control delivers more reasonable results with a higher precision in the estimation of some parameters and seems to mitigate the effects of multicollinearity.", "categories": "econ.gn q-fin.ec", "created": "2019-02-02", "updated": "2019-02-13", "authors": ["mathias kloss", "thomas kirschstein", "steffen liebscher", "martin petrick"], "url": "https://arxiv.org/abs/1902.00678"}, {"title": "bayesian elicitation", "id": "1902.00976", "abstract": "how can a receiver design an information structure in order to elicit information from a sender? we study how a decision-maker can acquire more information from an agent by reducing her own ability to observe what the agent transmits. intuitively, when the two parties' preferences are not perfectly aligned, this garbling relaxes the sender's concern that the receiver will use her information to the sender's disadvantage. we characterize the optimal information structure for the receiver. the main result is that under broad conditions, the receiver can do just as well as if she could commit to a rule mapping the sender's message to actions: information design is just as good as full commitment. similarly, we show that these conditions guarantee that ex ante information acquisition always benefits the receiver, even though this learning might actually lower the receiver's expected payoff in the absence of garbling. we illustrate these effects in a range of economically relevant examples.", "categories": "econ.th", "created": "2019-02-03", "updated": "2020-01-26", "authors": ["mark whitmeyer"], "url": "https://arxiv.org/abs/1902.00976"}, {"title": "factor investing: a bayesian hierarchical approach", "id": "1902.01015", "abstract": "this paper investigates asset allocation problems when returns are predictable. we introduce a market-timing bayesian hierarchical (bh) approach that adopts heterogeneous time-varying coefficients driven by lagged fundamental characteristics. our approach includes a joint estimation of conditional expected returns and covariance matrix and considers estimation risk for portfolio analysis. the hierarchical prior allows modeling different assets separately while sharing information across assets. we demonstrate the performance of the u.s. equity market. though the bayesian forecast is slightly biased, our bh approach outperforms most alternative methods in point and interval prediction. our bh approach in sector investment for the recent twenty years delivers a 0.92\\% average monthly returns and a 0.32\\% significant jensen`s alpha. we also find technology, energy, and manufacturing are important sectors in the past decade, and size, investment, and short-term reversal factors are heavily weighted. finally, the stochastic discount factor constructed by our bh approach explains most anomalies.", "categories": "econ.em stat.ap", "created": "2019-02-03", "updated": "2020-09-17", "authors": ["guanhao feng", "jingyu he"], "url": "https://arxiv.org/abs/1902.01015"}, {"title": "surprised by the hot hand fallacy? a truth in the law of small numbers", "id": "1902.01265", "abstract": "we prove that a subtle but substantial bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. the magnitude of this streak selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work. we observe that the canonical study in the influential hot hand fallacy literature, along with replications, are vulnerable to the bias. upon correcting for the bias we find that the long-standing conclusions of the canonical study are reversed.", "categories": "econ.gn q-fin.ec", "created": "2019-02-04", "updated": "", "authors": ["joshua b. miller", "adam sanjurjo"], "url": "https://arxiv.org/abs/1902.01265"}, {"title": "how on earth: flourishing in a not-for-profit world by 2050", "id": "1902.01398", "abstract": "in this book, we outline a model of a non-capitalist market economy based on not-for-profit forms of business. this work presents both a critique of the current economic system and a vision of a more socially, economically, and ecologically sustainable economy. the point of departure is the purpose and profit-orientation embedded in the legal forms used by businesses (e.g., for-profit or not-for-profit) and the ramifications of this for global sustainability challenges such as environmental pollution, resource use, climate change, and economic inequality. we document the rapid rise of not-for-profit forms of business in the global economy and offer a conceptual framework and an analytical lens through which to view these relatively new economic actors and their potential for transforming the economy. the book explores how a market consisting of only or mostly not-for-profit forms of business might lead to better financial circulation, economic equality, social well-being, and environmental regeneration as compared to for-profit markets.", "categories": "econ.gn q-fin.ec", "created": "2019-02-04", "updated": "", "authors": ["jennifer hinton", "donnie maclurcan"], "url": "https://arxiv.org/abs/1902.01398"}, {"title": "a sieve-smm estimator for dynamic models", "id": "1902.01456", "abstract": "this paper proposes a sieve simulated method of moments (sieve-smm) estimator for the parameters and the distribution of the shocks in nonlinear dynamic models where the likelihood and the moments are not tractable. an important concern with smm, which matches sample with simulated moments, is that a parametric distribution is required. however, economic quantities that depend on this distribution, such as welfare and asset-prices, can be sensitive to misspecification. the sieve-smm estimator addresses this issue by flexibly approximating the distribution of the shocks with a gaussian and tails mixture sieve. the asymptotic framework provides consistency, rate of convergence and asymptotic normality results, extending existing results to a new framework with more general dynamics and latent variables. an application to asset pricing in a production economy shows a large decline in the estimates of relative risk-aversion, highlighting the empirical relevance of misspecification bias.", "categories": "econ.em math.st stat.th", "created": "2019-02-04", "updated": "2020-07-22", "authors": ["jean-jacques forneron"], "url": "https://arxiv.org/abs/1902.01456"}, {"title": "asymptotic theory for clustered samples", "id": "1902.01497", "abstract": "we provide a complete asymptotic distribution theory for clustered data with a large number of independent groups, generalizing the classic laws of large numbers, uniform laws, central limit theory, and clustered covariance matrix estimation. our theory allows for clustered observations with heterogeneous and unbounded cluster sizes. our conditions cleanly nest the classical results for i.n.i.d. observations, in the sense that our conditions specialize to the classical conditions under independent sampling. we use this theory to develop a full asymptotic distribution theory for estimation based on linear least-squares, 2sls, nonlinear mle, and nonlinear gmm.", "categories": "econ.em", "created": "2019-02-04", "updated": "", "authors": ["bruce e. hansen", "seojeong lee"], "url": "https://arxiv.org/abs/1902.01497"}, {"title": "a general framework for prediction in time series models", "id": "1902.01622", "abstract": "in this paper we propose a general framework to analyze prediction in time series models and show how a wide class of popular time series models satisfies this framework. we postulate a set of high-level assumptions, and formally verify these assumptions for the aforementioned time series models. our framework coincides with that of beutner et al. (2019, arxiv:1710.00643) who establish the validity of conditional confidence intervals for predictions made in this framework. the current paper therefore complements the results in beutner et al. (2019, arxiv:1710.00643) by providing practically relevant applications of their theory.", "categories": "econ.em", "created": "2019-02-05", "updated": "", "authors": ["eric beutner", "alexander heinemann", "stephan smeekes"], "url": "https://arxiv.org/abs/1902.01622"}, {"title": "a bootstrap test for the existence of moments for garch processes", "id": "1902.01808", "abstract": "this paper studies the joint inference on conditional volatility parameters and the innovation moments by means of bootstrap to test for the existence of moments for garch(p,q) processes. we propose a residual bootstrap to mimic the joint distribution of the quasi-maximum likelihood estimators and the empirical moments of the residuals and also prove its validity. a bootstrap-based test for the existence of moments is proposed, which provides asymptotically correctly-sized tests without losing its consistency property. it is simple to implement and extends to other garch-type settings. a simulation study demonstrates the test's size and power properties in finite samples and an empirical application illustrates the testing approach.", "categories": "econ.em", "created": "2019-02-05", "updated": "2019-07-10", "authors": ["alexander heinemann"], "url": "https://arxiv.org/abs/1902.01808"}, {"title": "a lifestyle-based model of household neighbourhood location and   individual travel mode choice behaviours", "id": "1902.01986", "abstract": "issues such as urban sprawl, congestion, oil dependence, climate change and public health, are prompting urban and transportation planners to turn to land use and urban design to rein in automobile use. one of the implicit beliefs in this effort is that the right land-use policies will, in fact, help to reduce automobile use and increase the use of alternative modes of transportation. thus, planners and transport engineers are increasingly viewing land use policies and lifestyle patterns as a way to manage transportation demand. while a substantial body of work has looked at the relationship between the built environment and travel behaviour, as well as the influence of lifestyles and lifestyle-related decisions on using different travel modes and activity behaviours, limited work has been done in capturing these effects simultaneously and also in exploring the effect of intra-household interaction on individual attitudes and beliefs towards travel and activity behavior, and their subsequent influence on lifestyles and modality styles. therefore, for this study we proposed a framework that captures the concurrent influence of lifestyles and modality styles on both household-level decisions, such as neighbourhood location, and individual-level decisions, such as travel mode choices using a hierarchical latent class choice model.", "categories": "econ.gn q-fin.ec", "created": "2019-02-05", "updated": "2019-11-28", "authors": ["ali ardeshiri", "akshay vij"], "url": "https://arxiv.org/abs/1902.01986"}, {"title": "conservation or deterioration in heritage sites? estimating willingness   to pay for preservation", "id": "1902.02418", "abstract": "a significant part of the united nations world heritage sites (whss) is located in developing countries. these sites attract an increasing number of tourist and income to these countries. unfortunately, many of these whss are in a poor condition due to climatic and environmental impacts; war and tourism pressure, requiring the urgent need for restoration and preservation (tuan & navrud, 2007). in this study, we characterise residents from shiraz city (visitors and non-visitors) willingness to invest in the management of the heritage sites through models for the preservation of heritage and development of tourism as a local resource. the research looks at different categories of heritage sites within shiraz city, iran. the measurement instrument is a stated preference referendum task administered state-wide to a sample of 489 respondents, with the payment mechanism defined as a purpose-specific incremental levy of a fixed amount over a set period of years. a latent class binary logit model, using parametric constraints is used innovatively to deal with any strategic voting such as yea-sayers and nay-sayers, as well as revealing the latent heterogeneity among sample members. results indicate that almost 14% of the sampled population is unwilling to be levied any amount (nay-sayers) to preserve any heritage sites. not recognizing the presence of nay-sayers in the data or recognizing them but eliminating them from the estimation will result in biased willingness to pay (wtp) results and, consequently, biased policy propositions by authorities. moreover, it is found that the type of heritage site is a driver of wtp. the results from this study provide insights into the wtp of heritage site visitors and non-visitors with respect to avoiding the impacts of future erosion and destruction and contributing to heritage management and maintenance policies.", "categories": "econ.gn q-fin.ec", "created": "2019-02-06", "updated": "", "authors": ["ali ardeshiri", "roya etminani ghasrodashti", "taha hossein rashidi", "mahyar ardeshiri", "ken willis"], "url": "https://arxiv.org/abs/1902.02418"}, {"title": "seasonality effects on consumers preferences over quality attributes of   different beef products", "id": "1902.02419", "abstract": "using discrete choice modelling, the study investigates 946 american consumers willingness-to-pay and preferences for diverse beef products. a novel experiment was used to elicit the number of beef products that each consumer would purchase. the range of products explored in this study included ground, diced, roast, and six cuts of steaks (sirloin, tenderloin, flank, flap, new york and cowboy or rib-eye). the outcome of the study suggests that us consumers vary in their preferences for beef products by season. the presence of a usda certification logo is by far the most important factor affecting consumers willingness to pay for all beef cuts, which is also heavily dependent on season. in relation to packaging, us consumers have mixed preference for different beef products by season. the results from a scaled adjusted ordered logit model showed that after price, safety-related attributes such as certification logos, types of packaging, and antibiotic free and organic products are a stronger influence on american consumers choice. furthermore, us consumers on average purchase diced and roast products more often in winter slow cooking season, than in summer, whereas new york strip and flank steak are more popular in the summer grilling season. this study provides valuable insights for businesses as well as policymakers to make inform decisions while considering how consumers relatively value among different labelling and product attributes by season and better address any ethical, safety and aesthetic concerns that consumers might have.", "categories": "econ.gn q-fin.ec", "created": "2019-02-06", "updated": "", "authors": ["ali ardeshiri", "spring sampson", "joffre swait"], "url": "https://arxiv.org/abs/1902.02419"}, {"title": "crowdfunding public projects: collaborative governance for achieving   citizen co-funding of public goods", "id": "1902.02480", "abstract": "this study explores the potential of crowdfunding as a tool for achieving citizen co-funding of public projects. focusing on philanthropic crowdfunding, we examine whether collaborative projects between public and private organizations are more successful in fundraising than projects initiated solely by private organizations. we argue that government involvement in crowdfunding provides some type of accreditation or certification that attests to a project aim to achieve public rather than private goals, thereby mitigating information asymmetry and improving mutual trust between creators (i.e., private sector organizations) and funders (i.e., crowd). to support this argument, we show that crowdfunding projects with government involvement achieved a greater success rate and attracted a greater amount of funding than comparable projects without government involvement. this evidence shows that governments may take advantage of crowdfunding to co-fund public projects with the citizenry for addressing the complex challenges that we face in the twenty-first century.", "categories": "econ.gn q-fin.ec", "created": "2019-02-07", "updated": "", "authors": ["sounman hong", "jungmin ryu"], "url": "https://arxiv.org/abs/1902.02480"}, {"title": "persuasion meets delegation", "id": "1902.02628", "abstract": "a principal can restrict an agent's information (the persuasion problem) or restrict an agent's discretion (the delegation problem). we show that these problems are generally equivalent - solving one solves the other. we use tools from the persuasion literature to generalize and extend many results in the delegation literature, as well as to address novel delegation problems, such as monopoly regulation with a participation constraint.", "categories": "econ.th", "created": "2019-02-07", "updated": "", "authors": ["anton kolotilin", "andriy zapechelnyuk"], "url": "https://arxiv.org/abs/1902.02628"}, {"title": "testing the order of multivariate normal mixture models", "id": "1902.02920", "abstract": "finite mixtures of multivariate normal distributions have been widely used in empirical applications in diverse fields such as statistical genetics and statistical finance. testing the number of components in multivariate normal mixture models is a long-standing challenge even in the most important case of testing homogeneity. this paper develops likelihood-based tests of the null hypothesis of $m_0$ components against the alternative hypothesis of $m_0 + 1$ components for a general $m_0 \\geq 1$. for heteroscedastic normal mixtures, we propose an em test and derive the asymptotic distribution of the em test statistic. for homoscedastic normal mixtures, we derive the asymptotic distribution of the likelihood ratio test statistic. we also derive the asymptotic distribution of the likelihood ratio test statistic and em test statistic under local alternatives and show the validity of parametric bootstrap. the simulations show that the proposed test has good finite sample size and power properties.", "categories": "math.st econ.em stat.th", "created": "2019-02-07", "updated": "", "authors": ["hiroyuki kasahara", "katsumi shimotsu"], "url": "https://arxiv.org/abs/1902.02920"}, {"title": "expressive mechanisms for equitable rent division on a budget", "id": "1902.02935", "abstract": "we study the incentive properties of envy-free mechanisms for the allocation of rooms and payments of rent among financially constrained roommates. each agent reports her values for rooms, her housing earmark (soft budget), and an index that reflects the difficulty the agent experiences from having to pay over this amount. then an envy-free allocation for these reports is recommended. the complete information non-cooperative outcomes of each of these mechanisms are exactly the envy-free allocations with respect to true preferences if and only if the admissible budget violation indices have a bound.", "categories": "econ.th cs.ai cs.gt", "created": "2019-02-07", "updated": "2020-04-17", "authors": ["rodrigo a. velez"], "url": "https://arxiv.org/abs/1902.02935"}, {"title": "preserve or retreat? willingness-to-pay for coastline protection in new   south wales", "id": "1902.03310", "abstract": "coastal erosion is a global and pervasive phenomenon that predicates a need for a strategic approach to the future management of coastal values and assets (both built and natural), should we invest in protective structures like seawalls that aim to preserve specific coastal features, or allow natural coastline retreat to preserve sandy beaches and other coastal ecosystems. determining the most suitable management approach in a specific context requires a better understanding of the full suite of economic values the populations holds for coastal assets, including non-market values. in this study, we characterise new south wales residents willingness to pay to maintain sandy beaches (width and length). we use an innovative application of a latent class binary logit model to deal with yea-sayers and nay-sayers, as well as revealing the latent heterogeneity among sample members. we find that 65% of the population would be willing to pay some amount of levy, dependent on the policy setting. in most cases, there is no effect of degree of beach deterioration characterised as loss of width and length of sandy beaches of between 5% and 100% on respondents willingness to pay for a management levy. this suggests that respondents who agreed to pay a management levy were motivated to preserve sandy beaches in their current state irrespective of the severity of sand loss likely to occur as a result of coastal erosion. willingness to pay also varies according to beach type (amongst iconic, main, bay and surf beaches) a finding that can assist with spatial prioritisation of coastal management. not recognizing the presence of nay-sayers in the data or recognizing them but eliminating them from the estimation will result in biased wtp results and, consequently, biased policy propositions by coastal managers.", "categories": "econ.gn q-fin.ec", "created": "2019-02-06", "updated": "", "authors": ["ali ardeshiri", "joffre swait", "elizabeth c. heagney", "mladen kovac"], "url": "https://arxiv.org/abs/1902.03310"}, {"title": "censored quantile regression forests", "id": "1902.03327", "abstract": "random forests are powerful non-parametric regression method but are severely limited in their usage in the presence of randomly censored observations, and naively applied can exhibit poor predictive performance due to the incurred biases. based on a local adaptive representation of random forests, we develop its regression adjustment for randomly censored regression quantile models. regression adjustment is based on new estimating equations that adapt to censoring and lead to quantile score whenever the data do not exhibit censoring. the proposed procedure named censored quantile regression forest, allows us to estimate quantiles of time-to-event without any parametric modeling assumption. we establish its consistency under mild model specifications. numerical studies showcase a clear advantage of the proposed procedure.", "categories": "stat.ml cs.lg econ.em stat.me", "created": "2019-02-08", "updated": "", "authors": ["alexander hanbo li", "jelena bradic"], "url": "https://arxiv.org/abs/1902.03327"}, {"title": "the fair reward problem: the illusion of success and how to solve it", "id": "1902.04940", "abstract": "humanity has been fascinated by the pursuit of fortune since time immemorial, and many successful outcomes benefit from strokes of luck. but success is subject to complexity, uncertainty, and change - and at times becoming increasingly unequally distributed. this leads to tension and confusion over to what extent people actually get what they deserve (i.e., fairness/meritocracy). moreover, in many fields, humans are over-confident and pervasively confuse luck for skill (i win, it's skill; i lose, it's bad luck). in some fields, there is too much risk taking; in others, not enough. where success derives in large part from luck - and especially where bailouts skew the incentives (heads, i win; tails, you lose) - it follows that luck is rewarded too much. this incentivizes a culture of gambling, while downplaying the importance of productive effort. and, short term success is often rewarded, irrespective, and potentially at the detriment, of the long-term system fitness. however, much success is truly meritocratic, and the problem is to discern and reward based on merit. we call this the fair reward problem. to address this, we propose three different measures to assess merit: (i) raw outcome; (ii) risk adjusted outcome, and (iii) prospective. we emphasize the need, in many cases, for the deductive prospective approach, which considers the potential of a system to adapt and mutate in novel futures. this is formalized within an evolutionary system, comprised of five processes, inter alia handling the exploration-exploitation trade-off. several human endeavors - including finance, politics, and science -are analyzed through these lenses, and concrete solutions are proposed to support a prosperous and meritocratic society.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-02-03", "updated": "2019-04-18", "authors": ["didier sornette", "spencer wheatley", "peter cauwels"], "url": "https://arxiv.org/abs/1902.04940"}, {"title": "partial identification and inference in nonparametric one-to-one   matching models", "id": "1902.05610", "abstract": "when the analyst has data on one large market, we study partial identification of the preference parameters in models of one-to-one matching with transfers without imposing parametric distributional restrictions on the agents' unobserved characteristics. we provide a tractable characterisation of the sharp identified set and discuss inference, under various classes of nonparametric distributional assumptions on the agents' unobserved characteristics. we use our methodology to test if the variations in marriage matching patterns observed over time in the u.s. are caused by changes in the agents' preferences for education assortativeness or by a shift in the proportion of educated women.", "categories": "econ.em", "created": "2019-02-14", "updated": "2019-10-05", "authors": ["cristina gualdani", "shruti sinha"], "url": "https://arxiv.org/abs/1902.05610"}, {"title": "the shapley taylor interaction index", "id": "1902.05622", "abstract": "the attribution problem, that is the problem of attributing a model's prediction to its base features, is well-studied. we extend the notion of attribution to also apply to feature interactions.   the shapley value is a commonly used method to attribute a model's prediction to its base features. we propose a generalization of the shapley value called shapley-taylor index that attributes the model's prediction to interactions of subsets of features up to some size k. the method is analogous to how the truncated taylor series decomposes the function value at a certain point using its derivatives at a different point. in fact, we show that the shapley taylor index is equal to the taylor series of the multilinear extension of the set-theoretic behavior of the model.   we axiomatize this method using the standard shapley axioms -- linearity, dummy, symmetry and efficiency -- and an additional axiom that we call the interaction distribution axiom. this new axiom explicitly characterizes how interactions are distributed for a class of functions that model pure interaction.   we contrast the shapley-taylor index against the previously proposed shapley interaction index (cf. [9]) from the cooperative game theory literature. we also apply the shapley taylor index to three models and identify interesting qualitative insights.", "categories": "cs.gt econ.th", "created": "2019-02-14", "updated": "2020-02-07", "authors": ["kedar dhamdhere", "ashish agarwal", "mukund sundararajan"], "url": "https://arxiv.org/abs/1902.05622"}, {"title": "a comparison of economic agent-based model calibration methods", "id": "1902.05938", "abstract": "interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of empirically-observed stylised facts that are not easily recovered by more traditional modelling approaches. nevertheless, the agent-based modelling paradigm faces mounting criticism, focused particularly on the rigour of current validation and calibration practices, most of which remain qualitative and stylised fact-driven. while the literature on quantitative and data-driven approaches has seen significant expansion in recent years, most studies have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. we therefore compare a number of prominent abm calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach and the overall quality of the resultant parameter estimates. we find that bayesian estimation, though less popular in the literature, consistently outperforms frequentist, objective function-based approaches and results in reasonable parameter estimates in many contexts. despite this, we also find that agent-based model calibration techniques require further development in order to definitively calibrate large-scale models. we therefore make suggestions for future research.", "categories": "q-fin.cp econ.gn q-fin.ec", "created": "2019-02-15", "updated": "", "authors": ["donovan platt"], "url": "https://arxiv.org/abs/1902.05938"}, {"title": "weak identification and estimation of social interaction models", "id": "1902.06143", "abstract": "the identification of the network effect is based on either group size variation, the structure of the network or the relative position in the network. i provide easy-to-verify necessary conditions for identification of undirected network models based on the number of distinct eigenvalues of the adjacency matrix. identification of network effects is possible; although in many empirical situations existing identification strategies may require the use of many instruments or instruments that could be strongly correlated with each other. the use of highly correlated instruments or many instruments may lead to weak identification or many instruments bias. this paper proposes regularized versions of the two-stage least squares (2sls) estimators as a solution to these problems. the proposed estimators are consistent and asymptotically normal. a monte carlo study illustrates the properties of the regularized estimators. an empirical application, assessing a local government tax competition model, shows the empirical relevance of using regularization methods.", "categories": "econ.em", "created": "2019-02-16", "updated": "", "authors": ["guy tchuente"], "url": "https://arxiv.org/abs/1902.06143"}, {"title": "semiparametric correction for endogenous truncation bias with vox populi   based participation decision", "id": "1902.06286", "abstract": "we synthesize the knowledge present in various scientific disciplines for the development of semiparametric endogenous truncation-proof algorithm, correcting for truncation bias due to endogenous self-selection. this synthesis enriches the algorithm's accuracy, efficiency and applicability. improving upon the covariate shift assumption, data are intrinsically affected and largely generated by their own behavior (cognition). refining the concept of vox populi (wisdom of crowd) allows data points to sort themselves out depending on their estimated latent reference group opinion space. monte carlo simulations, based on 2,000,000 different distribution functions, practically generating 100 million realizations, attest to a very high accuracy of our model.", "categories": "econ.em cs.lg", "created": "2019-02-17", "updated": "", "authors": ["nir billfeld", "moshe kim"], "url": "https://arxiv.org/abs/1902.06286"}, {"title": "market fragmentation and market consolidation: multiple steady states in   systems of adaptive traders choosing where to trade", "id": "1902.06549", "abstract": "technological progress is leading to proliferation and diversification of trading venues, thus increasing the relevance of the long-standing question of market fragmentation versus consolidation. to address this issue quantitatively, we analyse systems of adaptive traders that choose where to trade based on their previous experience. we demonstrate that only based on aggregate parameters about trading venues, such as the demand to supply ratio, we can assess whether a population of traders will prefer fragmentation or specialization towards a single venue. we investigate what conditions lead to market fragmentation for populations with a long memory and analyse the stability and other properties of both fragmented and consolidated steady states. finally we investigate the dynamics of populations with finite memory; when this memory is long the true long-time steady states are consolidated but fragmented states are strongly metastable, dominating the behaviour out to long times.", "categories": "q-fin.tr econ.gn nlin.ao q-fin.ec", "created": "2019-02-18", "updated": "2019-06-24", "authors": ["aleksandra alori\u0107", "peter sollich"], "url": "https://arxiv.org/abs/1902.06549"}, {"title": "existence of solutions to principal-agent problems with adverse   selection under minimal assumptions", "id": "1902.06552", "abstract": "we prove an existence result for the principal-agent problem with adverse selection under general assumptions on preferences and allocation spaces. instead of assuming that the allocation space is finite-dimensional or compact, we consider a more general coercivity condition which takes into account the principal's cost and the agents' preferences. our existence proof is simple and flexible enough to adapt to partial participation models as well as to the case of type-dependent budget constraints.", "categories": "math.oc econ.th", "created": "2019-02-18", "updated": "2020-03-10", "authors": ["guillaume carlier", "kelvin shuangjian zhang"], "url": "https://arxiv.org/abs/1902.06552"}, {"title": "discrete choice under risk with limited consideration", "id": "1902.06629", "abstract": "this paper is concerned with learning decision makers' preferences using data on observed choices from a finite set of risky alternatives. we propose a discrete choice model with unobserved heterogeneity in consideration sets and in standard risk aversion. we obtain sufficient conditions for the model's semi-nonparametric point identification, including in cases where consideration depends on preferences and on some of the exogenous variables. our method yields an estimator that is easy to compute and is applicable in markets with large choice sets. we illustrate its properties using a dataset on property insurance purchases.", "categories": "econ.em", "created": "2019-02-18", "updated": "2020-06-06", "authors": ["levon barseghyan", "francesca molinari", "matthew thirkettle"], "url": "https://arxiv.org/abs/1902.06629"}, {"title": "estimating network effects using naturally occurring peer notification   queue counterfactuals", "id": "1902.07133", "abstract": "randomized experiments, or a/b tests are used to estimate the causal impact of a feature on the behavior of users by creating two parallel universes in which members are simultaneously assigned to treatment and control. however, in social network settings, members interact, such that the impact of a feature is not always contained within the treatment group. researchers have developed a number of experimental designs to estimate network effects in social settings. alternatively, naturally occurring exogenous variation, or 'natural experiments,' allow researchers to recover causal estimates of peer effects from observational data in the absence of experimental manipulation. natural experiments trade off the engineering costs and some of the ethical concerns associated with network randomization with the search costs of finding situations with natural exogenous variation. to mitigate the search costs associated with discovering natural counterfactuals, we identify a common engineering requirement used to scale massive online systems, in which natural exogenous variation is likely to exist: notification queueing. we identify two natural experiments on the linkedin platform based on the order of notification queues to estimate the causal impact of a received message on the engagement of a recipient. we show that receiving a message from another member significantly increases a member's engagement, but that some popular observational specifications, such as fixed-effects estimators, overestimate this effect by as much as 2.7x. we then apply the estimated network effect coefficients to a large body of past experiments to quantify the extent to which it changes our interpretation of experimental results. the study points to the benefits of using messaging queues to discover naturally occurring counterfactuals for the estimation of causal effects without experimenter intervention.", "categories": "cs.si cs.cy econ.em stat.ap", "created": "2019-02-19", "updated": "", "authors": ["craig tutterow", "guillaume saint-jacques"], "url": "https://arxiv.org/abs/1902.07133"}, {"title": "the preference lattice", "id": "1902.07260", "abstract": "most comparisons of preferences have the structure of single-crossing dominance. we examine the lattice structure of single-crossing dominance, proving characterisation, existence and uniqueness results for minimum upper bounds of arbitrary sets of preferences. we apply these theorems to monotone comparative statics, ambiguity- and risk-aversion, social choice, and politically correct discourse.", "categories": "econ.th", "created": "2019-02-19", "updated": "2020-07-09", "authors": ["gregorio curello", "ludvig sinander"], "url": "https://arxiv.org/abs/1902.07260"}, {"title": "estimation and inference for synthetic control methods with spillover   effects", "id": "1902.07343", "abstract": "the synthetic control method is often used in treatment effect estimation with panel data where only a few units are treated and a small number of post-treatment periods are available. current estimation and inference procedures for synthetic control methods do not allow for the existence of spillover effects, which are plausible in many applications. in this paper, we consider estimation and inference for synthetic control methods, allowing for spillover effects. we propose estimators for both direct treatment effects and spillover effects and show they are asymptotically unbiased. in addition, we propose an inferential procedure and show it is asymptotically unbiased. our estimation and inference procedure applies to cases with multiple treated units or periods, and where the underlying factor model is either stationary or cointegrated. in simulations, we confirm that the presence of spillovers renders current methods biased and have distorted sizes, whereas our methods yield properly sized tests and retain reasonable power. we apply our method to a classic empirical example that investigates the effect of california's tobacco control program as in abadie et al. (2010) and find evidence of spillovers.", "categories": "econ.em", "created": "2019-02-19", "updated": "2019-11-22", "authors": ["jianfei cao", "connor dowd"], "url": "https://arxiv.org/abs/1902.07343"}, {"title": "combining outcome-based and preference-based matching: a constrained   priority mechanism", "id": "1902.07355", "abstract": "we introduce a constrained priority mechanism that combines outcome-based matching from machine-learning with preference-based allocation schemes common in market design. using real-world data, we illustrate how our mechanism could be applied to the assignment of refugee families to host country locations, and kindergarteners to schools. our mechanism allows a planner to first specify a threshold $\\bar g$ for the minimum acceptable average outcome score that should be achieved by the assignment. in the refugee matching context, this score corresponds to the predicted probability of employment, while in the student assignment context it corresponds to standardized test scores. the mechanism is a priority mechanism that considers both outcomes and preferences by assigning agents (refugee families, students) based on their preferences, but subject to meeting the planner's specified threshold. the mechanism is both strategy-proof and constrained efficient in that it always generates a matching that is not pareto dominated by any other matching that respects the planner's threshold.", "categories": "econ.gn cs.lg q-fin.ec stat.ap", "created": "2019-02-19", "updated": "2020-08-11", "authors": ["avidit acharya", "kirk bansak", "jens hainmueller"], "url": "https://arxiv.org/abs/1902.07355"}, {"title": "elicitation of ambiguous beliefs with mixing bets", "id": "1902.07447", "abstract": "i consider the elicitation of ambiguous beliefs about an event and show how to identify the interval of relevant probabilities (representing ambiguity perception) for several classes of ambiguity averse preferences. the agent reveals her preference for mixing binarized bets on the uncertain event and its complement under varying betting odds. under ambiguity aversion, mixing is informative about the interval of beliefs. in particular, the mechanism allows to distinguish ambiguous beliefs from point beliefs, and identifies the belief interval for maxmin preferences. for ambiguity averse smooth second order and variational preferences, the mechanism reveals inner bounds for the belief interval, which are sharp under additional assumptions. in an experimental study, participants perceive almost as much ambiguity for natural events (generated by the stock exchange and by a prisoners dilemma game) as for the ellsberg urn, indicating that ambiguity may play a role in real-world decision making.", "categories": "econ.em", "created": "2019-02-20", "updated": "2019-10-22", "authors": ["patrick schmidt"], "url": "https://arxiv.org/abs/1902.07447"}, {"title": "have econometric analyses of happiness data been futile? a simple truth   about happiness scales", "id": "1902.07696", "abstract": "econometric analyses in the happiness literature typically use subjective well-being (swb) data to compare the mean of observed or latent happiness across samples. recent critiques show that comparing the mean of ordinal data is only valid under strong assumptions that are usually rejected by swb data. this leads to an open question whether much of the empirical studies in the economics of happiness literature have been futile. in order to salvage some of the prior results and avoid future issues, we suggest regression analysis of swb (and other ordinal data) should focus on the median rather than the mean. median comparisons using parametric models such as the ordered probit and logit can be readily carried out using familiar statistical softwares like stata. we also show a previously assumed impractical task of estimating a semiparametric median ordered-response model is also possible by using a novel constrained mixed integer optimization technique. we use gss data to show the famous easterlin paradox from the happiness literature holds for the us independent of any parametric assumption.", "categories": "econ.em", "created": "2019-02-20", "updated": "", "authors": ["le-yu chen", "ekaterina oparina", "nattavudh powdthavee", "sorawoot srisuma"], "url": "https://arxiv.org/abs/1902.07696"}, {"title": "nonparametric counterfactuals in random utility models", "id": "1902.08350", "abstract": "we bound features of counterfactual choices in the nonparametric random utility model of demand, i.e. if observable choices are repeated cross-sections and one allows for unrestricted, unobserved heterogeneity. in this setting, tight bounds are developed on counterfactual discrete choice probabilities and on the expectation and c.d.f. of (functionals of) counterfactual stochastic demand.", "categories": "econ.em", "created": "2019-02-21", "updated": "2019-05-19", "authors": ["yuichi kitamura", "j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1902.08350"}, {"title": "controlling systemic risk - network structures that minimize it and node   properties to calculate it", "id": "1902.08483", "abstract": "evaluation of systemic risk in networks of financial institutions in general requires information of inter-institution financial exposures. in the framework of debt rank algorithm, we introduce an approximate method of systemic risk evaluation which requires only node properties, such as total assets and liabilities, as inputs. we demonstrate that this approximation captures a large portion of systemic risk measured by debt rank. furthermore, using monte carlo simulations, we investigate network structures that can amplify systemic risk. indeed, while no topology in general sense is {\\em a priori} more stable if the market is liquid [1], a larger complexity is detrimental for the overall stability [2]. here we find that the measure of scalar assortativity correlates well with level of systemic risk. in particular, network structures with high systemic risk are scalar assortative, meaning that risky banks are mostly exposed to other risky banks. network structures with low systemic risk are scalar disassortative, with interactions of risky banks with stable banks.", "categories": "q-fin.rm econ.th physics.soc-ph", "created": "2019-02-22", "updated": "", "authors": ["sebastian m. krause", "hrvoje \u0161tefan\u010di\u0107", "vinko zlati\u0107", "guido caldarelli"], "url": "https://arxiv.org/abs/1902.08483"}, {"title": "counterfactual inference in duration models with random censoring", "id": "1902.08502", "abstract": "we propose a counterfactual kaplan-meier estimator that incorporates exogenous covariates and unobserved heterogeneity of unrestricted dimensionality in duration models with random censoring. under some regularity conditions, we establish the joint weak convergence of the proposed counterfactual estimator and the unconditional kaplan-meier (1958) estimator. applying the functional delta method, we make inference on the cumulative hazard policy effect, that is, the change of duration dependence in response to a counterfactual policy. we also evaluate the finite sample performance of the proposed counterfactual estimation method in a monte carlo study.", "categories": "econ.em", "created": "2019-02-22", "updated": "", "authors": ["jiun-hua su"], "url": "https://arxiv.org/abs/1902.08502"}, {"title": "influencing factors that determine the usage of the crowd-shipping   services", "id": "1902.08681", "abstract": "the objective of this study is to understand how senders choose shipping services for different products, given the availability of both emerging crowd-shipping (cs) and traditional carriers in a logistics market. using data collected from a us survey, random utility maximization (rum) and random regret minimization (rrm) models have been employed to reveal factors that influence the diversity of decisions made by senders. shipping costs, along with additional real-time services such as courier reputations, tracking info, e-notifications, and customized delivery time and location, have been found to have remarkable impacts on senders' choices. interestingly, potential senders were willing to pay more to ship grocery items such as food, beverages, and medicines by cs services. moreover, the real-time services have low elasticities, meaning that only a slight change in those services will lead to a change in sender-behavior. finally, data-science techniques were used to assess the performance of the rum and rrm models and found to have similar accuracies. the findings from this research will help logistics firms address potential market segments, prepare service configurations to fulfill senders' expectations, and develop effective business operations strategies.", "categories": "econ.gn q-fin.ec", "created": "2019-02-22", "updated": "", "authors": ["tho v. le", "satish v. ukkusuri"], "url": "https://arxiv.org/abs/1902.08681"}, {"title": "robust principal component analysis with non-sparse errors", "id": "1902.08735", "abstract": "we show that when a high-dimensional data matrix is the sum of a low-rank matrix and a random error matrix with independent entries, the low-rank component can be consistently estimated by solving a convex minimization problem. we develop a new theoretical argument to establish consistency without assuming sparsity or the existence of any moments of the error matrix, so that fat-tailed continuous random errors such as cauchy are allowed. the results are illustrated by simulations.", "categories": "econ.em", "created": "2019-02-22", "updated": "2019-11-13", "authors": ["jushan bai", "junlong feng"], "url": "https://arxiv.org/abs/1902.08735"}, {"title": "diversity and its decomposition into variety, balance and disparity", "id": "1902.09167", "abstract": "diversity is a central concept in many fields. despite its importance, there is no unified methodological framework to measure diversity and its three components of variety, balance and disparity. current approaches take into account disparity of the types by considering their pairwise similarities. pairwise similarities between types do not adequately capture total disparity, since they fail to take into account in which way pairs are similar. hence, pairwise similarities do not discriminate between similarity of types in terms of the same feature and similarity of types in terms of different features. this paper presents an alternative approach which is based similarities of features between types over the whole set. the proposed measure of diversity properly takes into account the aspects of variety, balance and disparity, and without having to set an arbitrary weight for each aspect of diversity. based on this measure, the 'abc decomposition' is introduced, which provides separate measures for the variety, balance and disparity, allowing them to enter analysis separately. the method is illustrated by analyzing the industrial diversity from 1850 to present while taking into account the overlap in occupations they employ. finally, the framework is extended to take into account disparity considering multiple features, providing a helpful tool in analysis of high-dimensional data.", "categories": "q-bio.pe econ.gn physics.data-an physics.soc-ph q-fin.ec", "created": "2019-02-25", "updated": "2019-02-26", "authors": ["alje van dam"], "url": "https://arxiv.org/abs/1902.09167"}, {"title": "climate change and agriculture: subsistence farmers' response to extreme   heat", "id": "1902.09204", "abstract": "this paper examines how subsistence farmers respond to extreme heat. using micro-data from peruvian households, we find that high temperatures reduce agricultural productivity, increase area planted, and change crop mix. these findings are consistent with farmers using input adjustments as a short-term mechanism to attenuate the effect of extreme heat on output. this response seems to complement other coping strategies, such as selling livestock, but exacerbates the drop in yields, a standard measure of agricultural productivity. using our estimates, we show that accounting for land adjustments is important to quantify damages associated with climate change.", "categories": "econ.gn q-fin.ec", "created": "2019-02-25", "updated": "2019-02-28", "authors": ["fernando m. arag\u00f3n", "francisco oteiza", "juan pablo rud"], "url": "https://arxiv.org/abs/1902.09204"}, {"title": "on binscatter", "id": "1902.09608", "abstract": "binscatter is very popular in applied microeconomics. it provides a flexible, yet parsimonious way of visualizing and summarizing large data sets in regression settings, and it is often used for informal evaluation of substantive hypotheses such as linearity or monotonicity of the regression function. this paper presents a foundational, thorough analysis of binscatter: we give an array of theoretical and practical results that aid both in understanding current practices (i.e., their validity or lack thereof) and in offering theory-based guidance for future applications. our main results include principled number of bins selection, confidence intervals and bands, hypothesis tests for parametric and shape restrictions of the regression function, and several other new methods, applicable to canonical binscatter as well as higher-order polynomial, covariate-adjusted and smoothness-restricted extensions thereof. in particular, we highlight important methodological problems related to covariate adjustment methods used in current practice. we also discuss extensions to clustered data. our results are illustrated with simulated and real data throughout. companion general-purpose software packages for \\texttt{stata} and \\texttt{r} are provided. finally, from a technical perspective, new theoretical results for partitioning-based series estimation are obtained that may be of independent interest.", "categories": "econ.em stat.me stat.ml", "created": "2019-02-25", "updated": "", "authors": ["matias d. cattaneo", "richard k. crump", "max h. farrell", "yingjie feng"], "url": "https://arxiv.org/abs/1902.09608"}, {"title": "binscatter regressions", "id": "1902.09615", "abstract": "we introduce the \\texttt{stata} (and \\texttt{r}) package \\textsf{binsreg}, which implements the binscatter methods developed in \\citet*{cattaneo-crump-farrell-feng_2019_binscatter}. the package includes the commands \\texttt{binsreg}, \\texttt{binsregtest}, and \\texttt{binsregselect}. the first command (\\texttt{binsreg}) implements binscatter for the regression function and its derivatives, offering several point estimation, confidence intervals and confidence bands procedures, with particular focus on constructing binned scatter plots. the second command (\\texttt{binsregtest}) implements hypothesis testing procedures for parametric specification and for nonparametric shape restrictions of the unknown regression function. finally, the third command (\\texttt{binsregselect}) implements data-driven number of bins selectors for binscatter implementation using either quantile-spaced or evenly-spaced binning/partitioning. all the commands allow for covariate adjustment, smoothness restrictions, weighting and clustering, among other features. a companion \\texttt{r} package with the same capabilities is also available.", "categories": "econ.em stat.co", "created": "2019-02-25", "updated": "", "authors": ["matias d. cattaneo", "richard k. crump", "max h. farrell", "yingjie feng"], "url": "https://arxiv.org/abs/1902.09615"}, {"title": "semiparametric estimation of heterogeneous treatment effects under the   nonignorable assignment condition", "id": "1902.09978", "abstract": "we propose a semiparametric two-stage least square estimator for the heterogeneous treatment effects (hte). hte is the solution to certain integral equation which belongs to the class of fredholm integral equations of the first kind, which is known to be ill-posed problem. naive semi/nonparametric methods do not provide stable solution to such problems. then we propose to approximate the function of interest by orthogonal series under the constraint which makes the inverse mapping of integral to be continuous and eliminates the ill-posedness. we illustrate the performance of the proposed estimator through simulation experiments.", "categories": "econ.em stat.me", "created": "2019-02-26", "updated": "", "authors": ["keisuke takahata", "takahiro hoshino"], "url": "https://arxiv.org/abs/1902.09978"}, {"title": "analytic solutions in a continuous-time financial market model", "id": "1902.09999", "abstract": "we propose a heterogeneous agent market model (ham) in continuous time. the market is populated by fundamental traders and chartists, who both use simple linear trading rules. most of the related literature explores stability, price dynamics and profitability either within deterministic models or by simulation. our novel formulation lends itself to analytic treatment even in the stochastic case. we prove conditions for the (stochastic) stability of the price process, and also for the price to mean-revert to the fundamental value. assuming stability, we derive analytic formulae on how the population ratios influence price dynamics and the profitability of the strategies. our results suggest that whichever trader type is more present in the market will achieve higher returns.", "categories": "econ.gn q-fin.ec q-fin.tr", "created": "2019-02-26", "updated": "", "authors": ["zsolt bihary", "attila andr\u00e1s v\u00edg"], "url": "https://arxiv.org/abs/1902.09999"}, {"title": "gig economy: a dynamic principal-agent model", "id": "1902.10021", "abstract": "the gig economy, where employees take short-term, project-based jobs, is increasingly spreading all over the world. in this paper, we investigate the employer's and the worker's behavior in the gig economy with a dynamic principal-agent model. in our proposed model the worker's previous decisions influence his later decisions through his dynamically changing participation constraint. he accepts the contract offered by the employer when his expected utility is higher than the irrational valuation of his effort's worth. this reference point is based on wages he achieved in previous rounds. we formulate the employer's stochastic control problem and derive the solution in the deterministic limit. we obtain the feasible net wage of the worker, and the profit of the employer. workers who can afford to go unemployed and need not take a gig at all costs will realize high net wages. conversely, far-sighted employers who can afford to stall production will obtain high profits.", "categories": "econ.gn q-fin.ec", "created": "2019-02-26", "updated": "", "authors": ["zsolt bihary", "p\u00e9ter ker\u00e9nyi"], "url": "https://arxiv.org/abs/1902.10021"}, {"title": "meeting global cooling demand with photovoltaics during the 21st century", "id": "1902.10080", "abstract": "space conditioning, and cooling in particular, is a key factor in human productivity and well-being across the globe. during the 21st century, global cooling demand is expected to grow significantly due to the increase in wealth and population in sunny nations across the globe and the advance of global warming. the same locations that see high demand for cooling are also ideal for electricity generation via photovoltaics (pv). despite the apparent synergy between cooling demand and pv generation, the potential of the cooling sector to sustain pv generation has not been assessed on a global scale. here, we perform a global assessment of increased pv electricity adoption enabled by the residential cooling sector during the 21st century. already today, utilizing pv production for cooling could facilitate an additional installed pv capacity of approximately 540 gw, more than the global pv capacity of today. using established scenarios of population and income growth, as well as accounting for future global warming, we further project that the global residential cooling sector could sustain an added pv capacity between 20-200 gw each year for most of the 21st century, on par with the current global manufacturing capacity of 100 gw. furthermore, we find that without storage, pv could directly power approximately 50% of cooling demand, and that this fraction is set to increase from 49% to 56% during the 21st century, as cooling demand grows in locations where pv and cooling have a higher synergy. with this geographic shift in demand, the potential of distributed storage also grows. we simulate that with a 1 m$^3$ water-based latent thermal storage per household, the fraction of cooling demand met with pv would increase from 55% to 70% during the century. these results show that the synergy between cooling and pv is notable and could significantly accelerate the growth of the global pv industry.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-02-24", "updated": "", "authors": ["hannu s. laine", "jyri salpakari", "erin e. looney", "hele savin", "ian marius peters", "tonio buonassisi"], "url": "https://arxiv.org/abs/1902.10080"}, {"title": "penalized sieve gel for weighted average derivatives of nonparametric   quantile iv regressions", "id": "1902.10100", "abstract": "this paper considers estimation and inference for a weighted average derivative (wad) of a nonparametric quantile instrumental variables regression (npqiv). npqiv is a non-separable and nonlinear ill-posed inverse problem, which might be why there is no published work on the asymptotic properties of any estimator of its wad. we first characterize the semiparametric efficiency bound for a wad of a npqiv, which, unfortunately, depends on an unknown conditional derivative operator and hence an unknown degree of ill-posedness, making it difficult to know if the information bound is singular or not. in either case, we propose a penalized sieve generalized empirical likelihood (gel) estimation and inference procedure, which is based on the unconditional wad moment restriction and an increasing number of unconditional moments that are implied by the conditional npqiv restriction, where the unknown quantile function is approximated by a penalized sieve. under some regularity conditions, we show that the self-normalized penalized sieve gel estimator of the wad of a npqiv is asymptotically standard normal. we also show that the quasi likelihood ratio statistic based on the penalized sieve gel criterion is asymptotically chi-square distributed regardless of whether or not the information bound is singular.", "categories": "math.st econ.em stat.th", "created": "2019-02-26", "updated": "", "authors": ["xiaohong chen", "demian pouzo", "james l. powell"], "url": "https://arxiv.org/abs/1902.10100"}, {"title": "estimation of dynamic panel threshold model using stata", "id": "1902.10318", "abstract": "we develop a stata command xthenreg to implement the first-differenced gmm estimation of the dynamic panel threshold model, which seo and shin (2016, journal of econometrics 195: 169-186) have proposed. furthermore, we derive the asymptotic variance formula for a kink constrained gmm estimator of the dynamic threshold model and include an estimation algorithm. we also propose a fast bootstrap algorithm to implement the bootstrap for the linearity test. the use of the command is illustrated through a monte carlo simulation and an economic application.", "categories": "econ.em", "created": "2019-02-26", "updated": "", "authors": ["myung hwan seo", "sueyoul kim", "young-joo kim"], "url": "https://arxiv.org/abs/1902.10318"}, {"title": "mean-field moral hazard for optimal energy demand response management", "id": "1902.10405", "abstract": "we study the problem of demand response contracts in electricity markets by quantifying the impact of considering a mean-field of consumers, whose consumption is impacted by a common noise. we formulate the problem as a principal-agent problem with moral hazard in which the principal - she - is an electricity producer who observes continuously the consumption of a continuum of risk-averse consumers, and designs contracts in order to reduce her production costs. more precisely, the producer incentivises the consumers to reduce the average and the volatility of their consumption in different usages, without observing the efforts they make. we prove that the producer can benefit from considering the mean-field of consumers by indexing contracts on the consumption of one agent and aggregate consumption statistics from the distribution of the entire population of consumers. in the case of linear energy valuation, we provide closed-form expression for this new type of optimal contracts that maximises the utility of the producer. in most cases, we show that this new type of contracts allows the principal to choose the risks she wants to bear, and to reduce the problem at hand to an uncorrelated one.", "categories": "math.pr econ.gn math.oc q-fin.ec", "created": "2019-02-27", "updated": "2020-03-24", "authors": ["romuald elie", "emma hubert", "thibaut mastrolia", "dylan possama\u00ef"], "url": "https://arxiv.org/abs/1902.10405"}, {"title": "on the monotonicity of the eigenvector method", "id": "1902.10790", "abstract": "pairwise comparisons are used in a wide variety of decision situations where the importance of alternatives should be measured on a numerical scale. one popular method to derive the priorities is based on the right eigenvector of a multiplicative pairwise comparison matrix. we consider two monotonicity axioms in this setting. first, increasing an arbitrary entry of a pairwise comparison matrix is not allowed to result in a counter-intuitive rank reversal, that is, the favoured alternative in the corresponding row cannot be ranked lower than any other alternative if this was not the case before the change (rank monotonicity). second, the same modification should not decrease the normalised weight of the favoured alternative (weight monotonicity). both properties are satisfied by the geometric mean method but violated by the eigenvector method. the axioms do not uniquely determine the geometric mean. the relationship between the two monotonicity properties and the saaty inconsistency index are investigated for the eigenvector method via simulations. even though their violation turns out not to be a usual problem even for heavily inconsistent matrices, all decision-makers should be informed about the possible occurrence of such unexpected consequences of increasing a matrix entry.", "categories": "math.oc econ.gn q-fin.ec", "created": "2019-02-27", "updated": "2020-06-25", "authors": ["l\u00e1szl\u00f3 csat\u00f3", "d\u00f3ra gr\u00e9ta petr\u00f3czy"], "url": "https://arxiv.org/abs/1902.10790"}, {"title": "granger causality testing in high-dimensional vars: a   post-double-selection procedure", "id": "1902.10991", "abstract": "we develop an lm test for granger causality in high-dimensional var models based on penalized least squares estimations. to obtain a test retaining the appropriate size after the variable selection done by the lasso, we propose a post-double-selection procedure to partial out effects of nuisance variables and establish its uniform asymptotic validity. we conduct an extensive set of monte-carlo simulations that show our tests perform well under different data generating processes, even without sparsity. we apply our testing procedure to find networks of volatility spillovers and we find evidence that causal relationships become clearer in high-dimensional compared to standard low-dimensional vars.", "categories": "econ.em stat.me", "created": "2019-02-28", "updated": "2019-10-29", "authors": ["alain hecq", "luca margaritella", "stephan smeekes"], "url": "https://arxiv.org/abs/1902.10991"}, {"title": "the empirical content of binary choice models", "id": "1902.11012", "abstract": "an important goal of empirical demand analysis is choice and welfare prediction on counterfactual budget sets arising from potential policy-interventions. such predictions are more credible when made without arbitrary functional-form/distributional assumptions, and instead based solely on economic rationality, i.e. that choice is consistent with utility maximization by a heterogeneous population. this paper investigates nonparametric economic rationality in the empirically important context of binary choice. we show that under general unobserved heterogeneity, economic rationality is equivalent to a pair of slutsky-like shape-restrictions on choice-probability functions. the forms of these restrictions differ from slutsky-inequalities for continuous goods. unlike mcfadden-richter's stochastic revealed preference, our shape-restrictions (a) are global, i.e. their forms do not depend on which and how many budget-sets are observed, (b) are closed-form, hence easy to impose on parametric/semi/non-parametric models in practical applications, and (c) provide computationally simple, theory-consistent bounds on demand and welfare predictions on counterfactual budget-sets.", "categories": "econ.em", "created": "2019-02-28", "updated": "2020-10-01", "authors": ["debopam bhattacharya"], "url": "https://arxiv.org/abs/1902.11012"}, {"title": "income effects and rationalizability in multinomial choice models", "id": "1902.11017", "abstract": "mcfadden's random-utility model of multinomial choice has long been the workhorse of applied research. we establish shape-restrictions under which multinomial choice-probability functions can be rationalized via random-utility models with nonparametric heterogeneity and general income-effects. when combined with an additional restriction, the above conditions are shown to be necessary and sufficient for the canonical additive random utility model (arum). the sufficiency-proof is constructive, and facilitates nonparametric identification of preference-distributions. a corollary establishes that slutsky-symmetry, a key condition for previous rationalizability results (daly-zachary 1978, armstrong-vickers 2015), is equivalent to absence of income-effects. the theory of partial differential equations plays a key role in our analys", "categories": "econ.em", "created": "2019-02-28", "updated": "2020-10-01", "authors": ["debopam bhattacharya"], "url": "https://arxiv.org/abs/1902.11017"}, {"title": "robust nearly-efficient estimation of large panels with factor   structures", "id": "1902.11181", "abstract": "this paper studies estimation of linear panel regression models with heterogeneous coefficients, when both the regressors and the residual contain a possibly common, latent, factor structure. our theory is (nearly) efficient, because based on the gls principle, and also robust to the specification of such factor structure because it does not require any information on the number of factors nor estimation of the factor structure itself. we first show how the unfeasible gls estimator not only affords an efficiency improvement but, more importantly, provides a bias-adjusted estimator with the conventional limiting distribution, for situations where the ols is affected by a first-order bias. the technical challenge resolved in the paper is to show how these properties are preserved for a class of feasible gls estimators in a double-asymptotics setting. our theory is illustrated by means of monte carlo exercises and, then, with an empirical application using individual asset returns and firms' characteristics data.", "categories": "econ.em", "created": "2019-02-28", "updated": "", "authors": ["marco avarucci", "paolo zaffaroni"], "url": "https://arxiv.org/abs/1902.11181"}, {"title": "persuading part of an audience", "id": "1903.00129", "abstract": "i propose a cheap-talk model in which the sender can use private messages and only cares about persuading a subset of her audience. for example, a candidate only needs to persuade a majority of the electorate in order to win an election. i find that senders can gain credibility by speaking truthfully to some receivers while lying to others. in general settings, the model admits information transmission in equilibrium for some prior beliefs. the sender can approximate her preferred outcome when the fraction of the audience she needs to persuade is sufficiently small. i characterize the sender-optimal equilibrium and the benefit of not having to persuade your whole audience in separable environments. i also analyze different applications and verify that the results are robust to some perturbations of the model, including non-transparent motives as in crawford and sobel (1982), and full commitment as in kamenica and gentzkow (2011).", "categories": "econ.th", "created": "2019-02-28", "updated": "", "authors": ["bruno salcedo"], "url": "https://arxiv.org/abs/1903.00129"}, {"title": "identifying bid leakage in procurement auctions: machine learning   approach", "id": "1903.00261", "abstract": "we propose a novel machine-learning-based approach to detect bid leakage in first-price sealed-bid auctions. we extract and analyze the data on more than 1.4 million russian procurement auctions between 2014 and 2018. as bid leakage in each particular auction is tacit, the direct classification is impossible. instead, we reduce the problem of bid leakage detection to positive-unlabeled classification. the key idea is to regard the losing participants as fair and the winners as possibly corrupted. this allows us to estimate the prior probability of bid leakage in the sample, as well as the posterior probability of bid leakage for each specific auction. we find that at least 16\\% of auctions are exposed to bid leakage. bid leakage is more likely in auctions with a higher reserve price, lower number of bidders and lower price fall, and where the winning bid is received in the last hour before the deadline.", "categories": "econ.gn q-fin.ec", "created": "2019-03-01", "updated": "", "authors": ["dmitry i. ivanov", "alexander s. nesterov"], "url": "https://arxiv.org/abs/1903.00261"}, {"title": "approximation properties of variational bayes for vector autoregressions", "id": "1903.00617", "abstract": "variational bayes (vb) is a recent approximate method for bayesian inference. it has the merit of being a fast and scalable alternative to markov chain monte carlo (mcmc) but its approximation error is often unknown. in this paper, we derive the approximation error of vb in terms of mean, mode, variance, predictive density and kl divergence for the linear gaussian multi-equation regression. our results indicate that vb approximates the posterior mean perfectly. factors affecting the magnitude of underestimation in posterior variance and mode are revealed. importantly, we demonstrate that vb estimates predictive densities accurately.", "categories": "stat.ml cs.lg econ.em stat.co", "created": "2019-03-01", "updated": "", "authors": ["reza hajargasht"], "url": "https://arxiv.org/abs/1903.00617"}, {"title": "optimal investment-consumption-insurance with durable and perishable   consumption goods in a jump diffusion market", "id": "1903.00631", "abstract": "we investigate an optimal investment-consumption and optimal level of insurance on durable consumption goods with a positive loading in a continuous-time economy. we assume that the economic agent invests in the financial market and in durable as well as perishable consumption goods to derive utilities from consumption over time in a jump-diffusion market. assuming that the financial assets and durable consumption goods can be traded without transaction costs, we provide a semi-explicit solution for the optimal insurance coverage for durable goods and financial asset. with transaction costs for trading the durable good proportional to the total value of the durable good, we formulate the agent's optimization problem as a combined stochastic and impulse control problem, with an implicit intervention value function. we solve this problem numerically using stopping time iteration, and analyze the numerical results using illustrative examples.", "categories": "econ.gn q-fin.cp q-fin.ec", "created": "2019-03-02", "updated": "", "authors": ["jin sun", "ryle s. perera", "pavel v. shevchenko"], "url": "https://arxiv.org/abs/1903.00631"}, {"title": "using artificial intelligence to recapture norms: did #metoo change   gender norms in sweden?", "id": "1903.00690", "abstract": "norms are challenging to define and measure, but this paper takes advantage of text data and the recent development in machine learning to create an encompassing measure of norms. an lstm neural network is trained to detect gendered language. the network functions as a tool to create a measure on how gender norms changes in relation to the metoo movement on swedish twitter. this paper shows that gender norms on average are less salient half a year after the date of the first appearance of the hashtag #metoo. previous literature suggests that gender norms change over generations, but the current result suggests that norms can change in the short run.", "categories": "econ.gn q-fin.ec", "created": "2019-03-02", "updated": "", "authors": ["sara moricz"], "url": "https://arxiv.org/abs/1903.00690"}, {"title": "model selection in utility-maximizing binary prediction", "id": "1903.00716", "abstract": "the maximum utility estimation proposed by elliott and lieli (2013) can be viewed as cost-sensitive binary classification; thus, its in-sample overfitting issue is similar to that of perceptron learning. a utility-maximizing prediction rule (umpr) is constructed to alleviate the in-sample overfitting of the maximum utility estimation. we establish non-asymptotic upper bounds on the difference between the maximal expected utility and the generalized expected utility of the umpr. simulation results show that the umpr with an appropriate data-dependent penalty achieves larger generalized expected utility than common estimators in the binary classification if the conditional probability of the binary outcome is misspecified.", "categories": "econ.em", "created": "2019-03-02", "updated": "2020-07-28", "authors": ["jiun-hua su"], "url": "https://arxiv.org/abs/1903.00716"}, {"title": "cover's rebalancing option with discrete hindsight optimization", "id": "1903.00829", "abstract": "we study t. cover's rebalancing option (ordentlich and cover 1998) under discrete hindsight optimization in continuous time. the payoff in question is equal to the final wealth that would have accrued to a $\\$1$ deposit into the best of some finite set of (perhaps levered) rebalancing rules determined in hindsight. a rebalancing rule (or fixed-fraction betting scheme) amounts to fixing an asset allocation (i.e. $200\\%$ stocks and $-100\\%$ bonds) and then continuously executing rebalancing trades to counteract allocation drift. restricting the hindsight optimization to a small number of rebalancing rules (i.e. 2) has some advantages over the pioneering approach taken by cover $\\&$ company in their brilliant theory of universal portfolios (1986, 1991, 1996, 1998), where one's on-line trading performance is benchmarked relative to the final wealth of the best unlevered rebalancing rule of any kind in hindsight. our approach lets practitioners express an a priori view that one of the favored asset allocations (\"bets\") $b\\in\\{b_1,...,b_n\\}$ will turn out to have performed spectacularly well in hindsight. in limiting our robustness to some discrete set of asset allocations (rather than all possible asset allocations) we reduce the price of the rebalancing option and guarantee to achieve a correspondingly higher percentage of the hindsight-optimized wealth at the end of the planning period. a practitioner who lives to delta-hedge this variant of cover's rebalancing option through several decades is guaranteed to see the day that his realized compound-annual capital growth rate is very close to that of the best $b_i$ in hindsight. hence the point of the rock-bottom option price.", "categories": "q-fin.pm econ.gn econ.th q-fin.ec q-fin.mf q-fin.pr", "created": "2019-03-02", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1903.00829"}, {"title": "artificial counselor system for stock investment", "id": "1903.00955", "abstract": "this paper proposes a novel trading system which plays the role of an artificial counselor for stock investment. in this paper, the stock future prices (technical features) are predicted using support vector regression. thereafter, the predicted prices are used to recommend which portions of the budget an investor should invest in different existing stocks to have an optimum expected profit considering their level of risk tolerance. two different methods are used for suggesting best portions, which are markowitz portfolio theory and fuzzy investment counselor. the first approach is an optimization-based method which considers merely technical features, while the second approach is based on fuzzy logic taking into account both technical and fundamental features of the stock market. the experimental results on new york stock exchange (nyse) show the effectiveness of the proposed system.", "categories": "q-fin.gn cs.ce econ.gn q-fin.ec", "created": "2019-03-03", "updated": "", "authors": ["hadi nekoeiqachkanloo", "benyamin ghojogh", "ali saheb pasand", "mark crowley"], "url": "https://arxiv.org/abs/1903.00955"}, {"title": "limit theorems for network dependent random variables", "id": "1903.01059", "abstract": "this paper is concerned with cross-sectional dependence arising because observations are interconnected through an observed network. following doukhan and louhichi (1999), we measure the strength of dependence by covariances of nonlinearly transformed variables. we provide a law of large numbers and central limit theorem for network dependent variables. we also provide a method of calculating standard errors robust to general forms of network dependence. for that purpose, we rely on a network heteroskedasticity and autocorrelation consistent (hac) variance estimator, and show its consistency. the results rely on conditions characterized by tradeoffs between the rate of decay of dependence across a network and network's denseness. our approach can accommodate data generated by network formation models, random fields on graphs, conditional dependency graphs, and large functional-causal systems of equations.", "categories": "econ.em stat.me", "created": "2019-03-03", "updated": "2020-10-03", "authors": ["denis kojevnikov", "vadim marmer", "kyungchul song"], "url": "https://arxiv.org/abs/1903.01059"}, {"title": "exact solution for the portfolio diversification problem based on   maximizing the risk adjusted return", "id": "1903.01082", "abstract": "the potential benefits of portfolio diversification have been known to investors for a long time. markowitz (1952) suggested the seminal approach for optimizing the portfolio problem based on finding the weights as budget shares that minimize the variance of the underlying portfolio. hatemi-j and el-khatib (2015) suggested finding the weights that will result in maximizing the risk adjusted return of the portfolio. this approach seems to be preferred by the rational investors since it combines risk and return when the optimal budget shares are sought for. the current paper provides a general solution for this risk adjusted return problem that can be utilized for any potential number of assets that are included in the portfolio.", "categories": "econ.th", "created": "2019-03-04", "updated": "", "authors": ["abdulnasser hatemi-j", "mohamed ali hajji", "youssef el-khatib"], "url": "https://arxiv.org/abs/1903.01082"}, {"title": "finite sample inference for the maximum score estimand", "id": "1903.01511", "abstract": "we provide a finite sample inference method for the structural parameters of a semiparametric binary response model under a conditional median restriction originally studied by manski (1975, 1985). our inference method is valid for any sample size and irrespective of whether the structural parameters are point identified or partially identified, for example due to the lack of a continuously distributed covariate with large support. our inference approach exploits distributional properties of observable outcomes conditional on the observed sequence of exogenous variables. moment inequalities conditional on this size n sequence of exogenous covariates are constructed, and the test statistic is a monotone function of violations of sample moment inequalities. the critical value used for inference is provided by the appropriate quantile of a known function of n independent rademacher random variables. we investigate power properties of the underlying test and provide simulation studies to support the theoretical findings.", "categories": "econ.em", "created": "2019-03-04", "updated": "2020-05-08", "authors": ["adam m. rosen", "takuya ura"], "url": "https://arxiv.org/abs/1903.01511"}, {"title": "verifying the existence of maximum likelihood estimates for generalized   linear models", "id": "1903.01633", "abstract": "a fundamental problem with nonlinear estimation models is that estimates are not guaranteed to exist. however, while non-existence is a well-studied issue for binary choice models, it presents significant challenges for other models as well and is not as well understood in more general settings. these challenges are only magnified for models that feature many fixed effects and other high-dimensional parameters. we address the current ambiguity surrounding this topic by studying the conditions that govern the existence of estimates for a wide class of generalized linear models (glms). we show that some, but not all, glms can still deliver consistent estimates of at least some of the linear parameters when these conditions fail to hold. we also demonstrate how to verify these conditions in the presence of high-dimensional fixed effects, as are often recommended in the international trade literature and in other common panel settings", "categories": "econ.em", "created": "2019-03-04", "updated": "2019-08-01", "authors": ["sergio correia", "paulo guimar\u00e3es", "thomas zylkin"], "url": "https://arxiv.org/abs/1903.01633"}, {"title": "econometric analysis of potential outcomes time series: instruments,   shocks, linearity and the causal response function", "id": "1903.01637", "abstract": "bojinov & shephard (2019) defined potential outcome time series to nonparametrically measure dynamic causal effects in time series experiments. four innovations are developed in this paper: \"instrumental paths,\" treatments which are \"shocks,\" \"linear potential outcomes\" and the \"causal response function.\" potential outcome time series are then used to provide a nonparametric causal interpretation of impulse response functions, generalized impulse response functions, local projections and lp-iv.", "categories": "econ.em", "created": "2019-03-04", "updated": "2020-02-25", "authors": ["ashesh rambachan", "neil shephard"], "url": "https://arxiv.org/abs/1903.01637"}, {"title": "ppmlhdfe: fast poisson estimation with high-dimensional fixed effects", "id": "1903.01690", "abstract": "in this paper we present ppmlhdfe, a new stata command for estimation of (pseudo) poisson regression models with multiple high-dimensional fixed effects (hdfe). estimation is implemented using a modified version of the iteratively reweighted least-squares (irls) algorithm that allows for fast estimation in the presence of hdfe. because the code is built around the reghdfe package, it has similar syntax, supports many of the same functionalities, and benefits from reghdfe's fast convergence properties for computing high-dimensional least squares problems.   performance is further enhanced by some new techniques we introduce for accelerating hdfe-irls estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo) maximum likelihood estimates.", "categories": "econ.em", "created": "2019-03-05", "updated": "2019-08-02", "authors": ["sergio correia", "paulo guimar\u00e3es", "thomas zylkin"], "url": "https://arxiv.org/abs/1903.01690"}, {"title": "externalities in knowledge production: evidence from a randomized field   experiment", "id": "1903.01861", "abstract": "are there positive or negative externalities in knowledge production? do current contributions to knowledge production increase or decrease the future growth of knowledge? we use a randomized field experiment, which added relevant content to some pages in wikipedia while leaving similar pages unchanged. we find that the addition of content has a negligible impact on the subsequent long-run growth of content. our results have implications for information seeding and incentivizing contributions, implying that additional content does not generate sizable externalities by inspiring nor discouraging future contributions.", "categories": "econ.gn cs.cy cs.gt q-fin.ec", "created": "2019-03-02", "updated": "", "authors": ["marit hinnosaar", "toomas hinnosaar", "michael kummer", "olga slivko"], "url": "https://arxiv.org/abs/1903.01861"}, {"title": "elusive longer-run impacts of head start: replications within and across   cohorts", "id": "1903.01954", "abstract": "using an additional decade of cnlsy data, this study replicated and extended deming's (2009) evaluation of head start's life-cycle skill formation impacts in three ways. extending the measurement interval for deming's adulthood outcomes, we found no statistically significant impacts on earnings and mixed evidence of impacts on other adult outcomes. applying deming's sibling comparison framework to more recent birth cohorts born to cnlsy mothers revealed mostly negative head start impacts. combining all cohorts shows generally null impacts on school-age and early adulthood outcomes.", "categories": "econ.gn q-fin.ec", "created": "2019-03-05", "updated": "2020-02-02", "authors": ["remy j. -c. pages", "dylan j. lukes", "drew h. bailey", "greg j. duncan"], "url": "https://arxiv.org/abs/1903.01954"}, {"title": "optimal climate strategy with mitigation, carbon removal, and solar   geoengineering", "id": "1903.02043", "abstract": "until recently, analysis of optimal global climate policy has focused on mitigation. exploration of policies to meet the 1.5{\\deg}c target have brought carbon dioxide removal (cdr), a second instrument, into the climate policy mainstream. far less agreement exists regarding the role of solar geoengineering (sg), a third instrument to limit global climate risk. integrated assessment modelling (iam) studies offer little guidance on trade-offs between these three instruments because they have dealt with cdr and sg in isolation. here, i extend the dynamic integrated model of climate and economy (dice) to include both cdr and sg to explore the temporal ordering of the three instruments. contrary to implicit assumptions that sg would be employed only after mitigation and cdr are exhausted, i find that sg is introduced parallel to mitigation temporary reducing climate risks during the era of peak co2 concentrations. cdr reduces concentrations after mitigation is exhausted, enabling sg phasing out.", "categories": "econ.gn q-fin.ec", "created": "2019-03-05", "updated": "", "authors": ["mariia belaia"], "url": "https://arxiv.org/abs/1903.02043"}, {"title": "experimenting in equilibrium", "id": "1903.02124", "abstract": "classical approaches to experimental design assume that intervening on one unit does not affect other units. there are many important settings, however, where this non-interference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. in this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobstrusive randomization with lightweight modeling, all while remaining in equilibrium. we can then use these estimates to optimize the system by gradient descent. concretely, we focus on the problem of a platform that seeks to optimize supply-side payments p in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and show that our approach enables the platform to optimize p in large systems using vanishingly small perturbations.", "categories": "math.oc econ.em stat.me", "created": "2019-03-05", "updated": "2020-06-30", "authors": ["stefan wager", "kuang xu"], "url": "https://arxiv.org/abs/1903.02124"}, {"title": "mean field equilibrium: uniqueness, existence, and comparative statics", "id": "1903.02273", "abstract": "the standard solution concept for stochastic games is markov perfect equilibrium (mpe); however, its computation becomes intractable as the number of players increases. instead, we consider mean field equilibrium (mfe) that has been popularized in the recent literature. mfe takes advantage of averaging effects in models with a large number of players. we make three main contributions. first, our main result provides conditions that ensure the uniqueness of an mfe. we believe this uniqueness result is the first of its nature in the class of models we study. second, we generalize previous mfe existence results. third, we provide general comparative statics results. we apply our results to dynamic oligopoly models and to heterogeneous agent macroeconomic models commonly used in previous work in economics and operations.", "categories": "econ.th cs.ma math.oc math.pr", "created": "2019-03-06", "updated": "2020-06-04", "authors": ["bar light", "gabriel weintraub"], "url": "https://arxiv.org/abs/1903.02273"}, {"title": "the africa-dummy: gone with the millennium?", "id": "1903.02357", "abstract": "a fixed effects regression estimator is introduced that can directly identify and estimate the africa-dummy in one regression step so that its correct standard errors as well as correlations to other coefficients can easily be estimated. we can estimate the nickel bias and found it to be negligibly tiny. semiparametric extensions check whether the africa-dummy is simply a result of misspecification of the functional form. in particular, we show that the returns to growth factors are different for sub-saharan african countries compared to the rest of the world. for example, returns to population growth are positive and beta-convergence is faster. when extending the model to identify the development of the africa-dummy over time we see that it has been changing dramatically over time and that the punishment for sub-saharan african countries has been decreasing incrementally to reach insignificance around the turn of the millennium.", "categories": "econ.em", "created": "2019-03-06", "updated": "", "authors": ["max k\u00f6hler", "stefan sperlich"], "url": "https://arxiv.org/abs/1903.02357"}, {"title": "a varying coefficient model for assessing the returns to growth to   account for poverty and inequality", "id": "1903.02390", "abstract": "various papers demonstrate the importance of inequality, poverty and the size of the middle class for economic growth. when explaining why these measures of the income distribution are added to the growth regression, it is often mentioned that poor people behave different which may translate to the economy as a whole. however, simply adding explanatory variables does not reflect this behavior. by a varying coefficient model we show that the returns to growth differ a lot depending on poverty and inequality. furthermore, we investigate how these returns differ for the poorer and for the richer part of the societies. we argue that the differences in the coefficients impede, on the one hand, that the means coefficients are informative, and, on the other hand, challenge the credibility of the economic interpretation. in short, we show that, when estimating mean coefficients without accounting for poverty and inequality, the estimation is likely to suffer from a serious endogeneity bias.", "categories": "econ.em", "created": "2019-03-06", "updated": "", "authors": ["max k\u00f6hler", "stefan sperlich", "jisu yoon"], "url": "https://arxiv.org/abs/1903.02390"}, {"title": "the interdependence of hierarchical institutions: federal regulation,   job creation, and the moderating effect of state economic freedom", "id": "1903.02924", "abstract": "regulation is commonly viewed as a hindrance to entrepreneurship, but heterogeneity in the effects of regulation is rarely explored. we focus on regional variation in the effects of national-level regulations by developing a theory of hierarchical institutional interdependence. using the political science theory of market-preserving federalism, we argue that regional economic freedom attenuates the negative influence of national regulation on net job creation. using u.s. data, we find that regulation destroys jobs on net, but regional economic freedom moderates this effect. in regions with average economic freedom, a one percent increase in regulation results in 14 fewer jobs created on net. however, a standard deviation increase in economic freedom attenuates this relationship by four fewer jobs. interestingly, this moderation accrues strictly to older firms; regulation usually harms young firm job creation, and economic freedom does not attenuate this relationship.", "categories": "econ.gn q-fin.ec", "created": "2019-03-07", "updated": "", "authors": ["david s. lucas", "christopher j. boudreaux"], "url": "https://arxiv.org/abs/1903.02924"}, {"title": "entrepreneurship, institutions, and economic growth: does the level of   development matter?", "id": "1903.02934", "abstract": "entrepreneurship is often touted for its ability to generate economic growth. through the creative-destructive process, entrepreneurs are often able to innovate and outperform incumbent organizations, all of which is supposed to lead to higher employment and economic growth. although some empirical evidence supports this logic, it has also been the subject of recent criticisms. specifically, entrepreneurship does not lead to growth in developing countries; it only does in more developed countries with higher income levels. using global entrepreneurship monitor data for a panel of 83 countries from 2002 to 2014, we examine the contribution of entrepreneurship towards economic growth. our evidence validates earlier studies findings but also exposes previously undiscovered findings. that is, we find that entrepreneurship encourages economic growth but not in developing countries. in addition, our evidence finds that the institutional environment of the country, as measured by gem entrepreneurial framework conditions, only contributes to economic growth in more developed countries but not in developing countries. these findings have important policy implications. namely, our evidence contradicts policy proposals that suggest entrepreneurship and the adoption of pro-market institutions that support it to encourage economic growth in developing countries. our evidence suggests these policy proposals will be unlikely to generate the economic growth desired.", "categories": "econ.gn q-fin.ec", "created": "2019-03-07", "updated": "", "authors": ["christopher j. boudreaux"], "url": "https://arxiv.org/abs/1903.02934"}, {"title": "nowcasting recessions using the svm machine learning algorithm", "id": "1903.03202", "abstract": "we introduce a novel application of support vector machines (svm), an important machine learning algorithm, to determine the beginning and end of recessions in real time. nowcasting, \"forecasting\" a condition about the present time because the full information about it is not available until later, is key for recessions, which are only determined months after the fact. we show that svm has excellent predictive performance for this task, and we provide implementation details to facilitate its use in similar problems in economics and finance.", "categories": "q-fin.gn cs.lg econ.gn q-fin.ec stat.ap stat.ml", "created": "2019-02-17", "updated": "2019-06-27", "authors": ["alexander james", "yaser s. abu-mostafa", "xiao qiao"], "url": "https://arxiv.org/abs/1903.03202"}, {"title": "economic resilience from input-output susceptibility improves   predictions of economic growth and recovery", "id": "1903.03203", "abstract": "modern macroeconomic theories were unable to foresee the last great recession and could neither predict its prolonged duration nor the recovery rate. they are based on supply-demand equilibria that do not exist during recessionary shocks. here we focus on resilience as a nonequilibrium property of networked production systems and develop a linear response theory for input-output economics. by calibrating the framework to data from 56 industrial sectors in 43 countries between 2000 and 2014, we find that the susceptibility of individual industrial sectors to economic shocks varies greatly across countries, sectors, and time. we show that susceptibility-based predictions that take sector- and country-specific recovery into account, outperform--by far--standard econometric growth-models. our results are analytically rigorous, empirically testable, and flexible enough to address policy-relevant scenarios. we illustrate the latter by estimating the impact of recently imposed tariffs on us imports (steel and aluminum) on specific sectors across european countries.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-02-27", "updated": "", "authors": ["peter klimek", "sebastian poledna", "stefan thurner"], "url": "https://arxiv.org/abs/1903.03203"}, {"title": "heterogeneous impact of the minimum wage: implications for changes in   between- and within-group inequality", "id": "1903.03925", "abstract": "workers who earn at or below the minimum wage in the united states are mostly either less educated, young, or female. little is known, however, concerning the extent to which the minimum wage influences wage differentials among workers with different observed characteristics and among workers with the same observed characteristics. this paper shows that changes in the real value of the minimum wage over recent decades have affected the relationship of hourly wages with education, experience, and gender. the results suggest that changes in the real value of the minimum wage account in part for the patterns of changes in education, experience, and gender wage differentials and mostly for the patterns of changes in within-group wage differentials among female workers with lower levels of experience.", "categories": "econ.gn q-fin.ec", "created": "2019-03-10", "updated": "2019-07-19", "authors": ["tatsushi oka", "ken yamada"], "url": "https://arxiv.org/abs/1903.03925"}, {"title": "price competition with uncertain quality and cost", "id": "1903.03987", "abstract": "consumers in many markets are uncertain about firms' qualities and costs, so buy based on both the price and the quality inferred from it. optimal pricing depends on consumer heterogeneity only when firms with higher quality have higher costs, regardless of whether costs and qualities are private or public. if better quality firms have lower costs, then good quality is sold cheaper than bad under private costs and qualities, but not under public. however, if higher quality is costlier, then price weakly increases in quality under both informational environments.", "categories": "econ.th", "created": "2019-03-10", "updated": "2019-04-10", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1903.03987"}, {"title": "stackelberg independence", "id": "1903.04060", "abstract": "the standard model of sequential capacity choices is the stackelberg quantity leadership model with linear demand. i show that under the standard assumptions, leaders' actions are informative about market conditions and independent of leaders' beliefs about the arrivals of followers. however, this stackelberg independence property relies on all standard assumptions being satisfied. it fails to hold whenever the demand function is non-linear, marginal cost is not constant, goods are differentiated, firms are non-identical, or there are any externalities. i show that small deviations from the linear demand assumption may make the leaders' choices completely uninformative.", "categories": "cs.gt econ.gn econ.th q-fin.ec", "created": "2019-03-10", "updated": "", "authors": ["toomas hinnosaar"], "url": "https://arxiv.org/abs/1903.04060"}, {"title": "parametric inference with universal function approximators", "id": "1903.04209", "abstract": "universal function approximators, such as artificial neural networks, can learn a large variety of target functions arbitrarily well given sufficient training data. this flexibility comes at the cost of the ability to perform parametric inference. we address this gap by proposing a generic framework based on the shapley-taylor decomposition of a model. a surrogate parametric regression analysis is performed in the space spanned by the shapley value expansion of a model. this allows for the testing of standard hypotheses of interest. at the same time, the proposed approach provides novel insights into statistical learning processes themselves derived from the consistency and bias properties of the nonparametric estimators. we apply the framework to the estimation of heterogeneous treatment effects in simulated and real-world randomised experiments. we introduce an explicit treatment function based on higher-order shapley-taylor indices. this can be used to identify potentially complex treatment channels and help the generalisation of findings from experimental settings. more generally, the presented approach allows for a standardised use and communication of results from machine learning models.", "categories": "stat.ml cs.lg econ.em", "created": "2019-03-11", "updated": "2020-10-04", "authors": ["andreas joseph"], "url": "https://arxiv.org/abs/1903.04209"}, {"title": "a fractional-order difference cournot duopoly game with long memory", "id": "1903.04305", "abstract": "we reconsider the cournot duopoly problem in light of the theory for long memory. we introduce the caputo fractional-order difference calculus to classical duopoly theory to propose a fractional-order discrete cournot duopoly game model, which allows participants to make decisions while making full use of their historical information. then we discuss nash equilibria and local stability by using linear approximation. finally, we detect the chaos of the model by employing a 0-1 test algorithm.", "categories": "econ.gn q-fin.ec", "created": "2019-03-11", "updated": "", "authors": ["baogui xin", "wei peng", "yekyung kwon"], "url": "https://arxiv.org/abs/1903.04305"}, {"title": "frictional unemployment on labor flow networks", "id": "1903.04954", "abstract": "we develop an alternative theory to the aggregate matching function in which workers search for jobs through a network of firms: the labor flow network. the lack of an edge between two companies indicates the impossibility of labor flows between them due to high frictions. in equilibrium, firms' hiring behavior correlates through the network, generating highly disaggregated local unemployment. hence, aggregation depends on the topology of the network in non-trivial ways. this theory provides new micro-foundations for the beveridge curve, wage dispersion, and the employer-size premium. we apply our model to employer-employee matched records and find that network topologies with pareto-distributed connections cause disproportionately large changes on aggregate unemployment under high labor supply elasticity.", "categories": "econ.gn cs.ma physics.soc-ph q-fin.ec", "created": "2019-03-01", "updated": "", "authors": ["robert l. axtell", "omar a. guerrero", "eduardo l\u00f3pez"], "url": "https://arxiv.org/abs/1903.04954"}, {"title": "science quality and the value of inventions", "id": "1903.05020", "abstract": "despite decades of research, the relationship between the quality of science and the value of inventions has remained unclear. we present the result of a large-scale matching exercise between 4.8 million patent families and 43 million publication records. we find a strong positive relationship between quality of scientific contributions referenced in patents and the value of the respective inventions. we rank patents by the quality of the science they are linked to. strikingly, high-rank patents are twice as valuable as low-rank patents, which in turn are about as valuable as patents without direct science link. we show this core result for various science quality and patent value measures. the effect of science quality on patent value remains relevant even when science is linked indirectly through other patents. our findings imply that what is considered \"excellent\" within the science sector also leads to outstanding outcomes in the technological or commercial realm.", "categories": "econ.gn cs.dl q-fin.ec", "created": "2019-03-12", "updated": "2019-04-03", "authors": ["felix poege", "dietmar harhoff", "fabian gaessler", "stefano baruffaldi"], "url": "https://arxiv.org/abs/1903.05020"}, {"title": "a statistical analysis of time trends in atmospheric ethane", "id": "1903.05403", "abstract": "ethane is the most abundant non-methane hydrocarbon in the earth's atmosphere and an important precursor of tropospheric ozone through various chemical pathways. ethane is also an indirect greenhouse gas (global warming potential), influencing the atmospheric lifetime of methane through the consumption of the hydroxyl radical (oh). understanding the development of trends and identifying trend reversals in atmospheric ethane is therefore crucial. our dataset consists of four series of daily ethane columns obtained from ground-based ftir measurements. as many other decadal time series, our data are characterized by autocorrelation, heteroskedasticity, and seasonal effects. additionally, missing observations due to instrument failure or unfavorable measurement conditions are common in such series. the goal of this paper is therefore to analyze trends in atmospheric ethane with statistical tools that correctly address these data features. we present selected methods designed for the analysis of time trends and trend reversals. we consider bootstrap inference on broken linear trends and smoothly varying nonlinear trends. in particular, for the broken trend model, we propose a bootstrap method for inference on the break location and the corresponding changes in slope. for the smooth trend model we construct simultaneous confidence bands around the nonparametrically estimated trend. our autoregressive wild bootstrap approach, combined with a seasonal filter, is able to handle all issues mentioned above.", "categories": "stat.ap econ.em", "created": "2019-03-13", "updated": "2020-06-17", "authors": ["marina friedrich", "eric beutner", "hanno reuvers", "stephan smeekes", "jean-pierre urbain", "whitney bader", "bruno franco", "bernard lejeune", "emmanuel mahieu"], "url": "https://arxiv.org/abs/1903.05403"}, {"title": "a micro-simulation model of irrigation farms in the southern   murray-darling basin", "id": "1903.05781", "abstract": "this paper presents a farm level irrigation microsimulation model of the southern murray-darling basin. the model leverages detailed abares survey data to estimate a series of input demand and output supply equations, derived from a normalised quadratic profit function. the parameters from this estimation are then used to simulate the impact on total cost, revenue and profit of a hypothetical 30 per cent increase in the price of water. the model is still under development, with several potential improvements suggested in the conclusion. this is a working paper, provided for the purpose of receiving feedback on the analytical approach to improve future iterations of the microsimulation model.", "categories": "econ.gn q-fin.ec", "created": "2019-03-13", "updated": "", "authors": ["huong dinh", "manannan donoghoe", "neal hughes", "tim goesch"], "url": "https://arxiv.org/abs/1903.05781"}, {"title": "inference for first-price auctions with guerre, perrigne, and vuong's   estimator", "id": "1903.06401", "abstract": "we consider inference on the probability density of valuations in the first-price sealed-bid auctions model within the independent private value paradigm. we show the asymptotic normality of the two-step nonparametric estimator of guerre, perrigne, and vuong (2000) (gpv), and propose an easily implementable and consistent estimator of the asymptotic variance. we prove the validity of the pointwise percentile bootstrap confidence intervals based on the gpv estimator. lastly, we use the intermediate gaussian approximation approach to construct bootstrap-based asymptotically valid uniform confidence bands for the density of the valuations.", "categories": "econ.em", "created": "2019-03-15", "updated": "", "authors": ["jun ma", "vadim marmer", "artyom shneyerov"], "url": "https://arxiv.org/abs/1903.06401"}, {"title": "estimating dynamic conditional spread densities to optimise daily   storage trading of electricity", "id": "1903.06668", "abstract": "this paper formulates dynamic density functions, based upon skewed-t and similar representations, to model and forecast electricity price spreads between different hours of the day. this supports an optimal day ahead storage and discharge schedule, and thereby facilitates a bidding strategy for a merchant arbitrage facility into the day-ahead auctions for wholesale electricity. the four latent moments of the density functions are dynamic and conditional upon exogenous drivers, thereby permitting the mean, variance, skewness and kurtosis of the densities to respond hourly to such factors as weather and demand forecasts. the best specification for each spread is selected based on the pinball loss function, following the closed form analytical solutions of the cumulative density functions. those analytical properties also allow the calculation of risk associated with the spread arbitrages. from these spread densities, the optimal daily operation of a battery storage facility is determined.", "categories": "stat.ap cs.lg econ.em q-fin.tr stat.ml", "created": "2019-03-09", "updated": "", "authors": ["ekaterina abramova", "derek bunn"], "url": "https://arxiv.org/abs/1903.06668"}, {"title": "bulow-klemperer-style results for welfare maximization in two-sided   markets", "id": "1903.06696", "abstract": "we consider the problem of welfare maximization in two-sided markets using simple mechanisms that are prior-independent. the myerson-satterthwaite impossibility theorem shows that even for bilateral trade, there is no feasible (ir, truthful, budget balanced) mechanism that has welfare as high as the optimal-yet-infeasible vcg mechanism, which attains maximal welfare but runs a deficit. on the other hand, the optimal feasible mechanism needs to be carefully tailored to the bayesian prior, and is extremely complex, eluding a precise description.   we present bulow-klemperer-style results to circumvent these hurdles in double-auction markets. we suggest using the buyer trade reduction (btr) mechanism, a variant of mcafee's mechanism, which is feasible and simple (in particular, deterministic, truthful, prior-independent, anonymous). first, in the setting where buyers' and sellers' values are sampled i.i.d. from the same distribution, we show that for any such market of any size, btr with one additional buyer whose value is sampled from the same distribution has expected welfare at least as high as the optimal in the original market.   we then move to a more general setting where buyers' values are sampled from one distribution and sellers' from another, focusing on the case where the buyers' distribution first-order stochastically dominates the sellers'. we present bounds on the number of buyers that, when added, guarantees that btr in the augmented market have welfare at least as high as the optimal in the original market. our lower bounds extend to a large class of mechanisms, and all of our results extend to adding sellers instead of buyers. in addition, we present positive results about the usefulness of pricing at a sample for welfare maximization in two-sided markets under the above two settings, which to the best of our knowledge are the first sampling results in this context.", "categories": "cs.gt econ.th", "created": "2019-03-15", "updated": "2019-12-23", "authors": ["moshe babaioff", "kira goldner", "yannai a. gonczarowski"], "url": "https://arxiv.org/abs/1903.06696"}, {"title": "deciding with judgment", "id": "1903.06980", "abstract": "a decision maker starts from a judgmental decision and moves to the closest boundary of the confidence interval. this statistical decision rule is admissible and does not perform worse than the judgmental decision with a probability equal to the confidence level, which is interpreted as a coefficient of statistical risk aversion. the confidence level is related to the decision maker's aversion to uncertainty and can be elicited with laboratory experiments using urns a la ellsberg. the decision rule is applied to a problem of asset allocation for an investor whose judgmental decision is to keep all her wealth in cash.", "categories": "stat.me econ.em", "created": "2019-03-16", "updated": "", "authors": ["simone manganelli"], "url": "https://arxiv.org/abs/1903.06980"}, {"title": "the prosumer economy -- being like a forest", "id": "1903.07615", "abstract": "planetary life support systems are collapsing due to climate change and the biodiversity crisis. the root cause is the existing consumer economy, coupled with profit maximisation based on ecological and social externalities. trends can be reversed, civilisation may be saved by transforming the profit maximising consumer economy into an ecologically and socially just economy, which we call the prosumer economy. prosumer economy is a macro scale circular economy with minimum negative or positive ecological and social impact, an ecosystem of producers and prosumers, who have synergistic and circular relationships with deepened circular supply chains, networks, where leakage of wealth out of the system is minimised. in a prosumer economy there is no waste, no lasting negative impacts on the ecology and no social exploitation. the prosumer economy is like a lake or a forest, an economic ecosystem that is productive and supportive of the planet. we are already planting this forest through good4trust.org, started in turkey. good4trust is a community platform bringing together ecologically and socially just producers and prosumers. prosumers come together around a basic ethical tenet the golden rule and share on the platform their good deeds. the relationship are already deepening and circularity is forming to create a prosumer economy. the platforms software to structure the economy is open source, and is available to be licenced to start good4trust anywhere on the planet. complexity theory tells us that if enough agents in a given system adopt simple rules which they all follow, the system may shift. the shift from a consumer economy to a prosumer economy has already started, the future is either ecologically and socially just or bust.", "categories": "econ.gn q-fin.ec", "created": "2019-03-16", "updated": "", "authors": ["uygar ozesmi"], "url": "https://arxiv.org/abs/1903.07615"}, {"title": "j. s. mill's liberal principle and unanimity", "id": "1903.07769", "abstract": "the broad concept of an individual's welfare is actually a cluster of related specific concepts that bear a \"family resemblance\" to one another. one might care about how a policy will affect people both in terms of their subjective preferences and also in terms of some notion of their objective interests. this paper provides a framework for evaluation of policies in terms of welfare criteria that combine these two considerations. sufficient conditions are provided for such a criterion to imply the same ranking of social states as does pareto's unanimity criterion. sufficiency is proved via study of a community of agents with interdependent ordinal preferences.", "categories": "econ.th", "created": "2019-03-18", "updated": "", "authors": ["edward j. green"], "url": "https://arxiv.org/abs/1903.07769"}, {"title": "the effects of institutional quality on formal and informal borrowing   across high-, middle-, and low-income countries", "id": "1903.07866", "abstract": "this paper examines the effects of institutional quality on financing choice of individual using a large dataset of 137,160 people from 131 countries. we classify borrowing activities into three categories, including formal, constructive informal, and underground borrowing. although the result shows that better institutions aids the uses of formal borrowing, the impact of institutions on constructive informal and underground borrowing among three country sub-groups differs. higher institutional quality improves constructive informal borrowing in middle-income countries but reduces the use of underground borrowing in high- and low-income countries.", "categories": "econ.gn q-fin.ec", "created": "2019-03-19", "updated": "", "authors": ["lan chu khanh"], "url": "https://arxiv.org/abs/1903.07866"}, {"title": "an integrated panel data approach to modelling economic growth", "id": "1903.07948", "abstract": "empirical growth analysis has three major problems --- variable selection, parameter heterogeneity and cross-sectional dependence --- which are addressed independently from each other in most studies. the purpose of this study is to propose an integrated framework that extends the conventional linear growth regression model to allow for parameter heterogeneity and cross-sectional error dependence, while simultaneously performing variable selection. we also derive the asymptotic properties of the estimator under both low and high dimensions, and further investigate the finite sample performance of the estimator through monte carlo simulations. we apply the framework to a dataset of 89 countries over the period from 1960 to 2014. our results reveal some cross-country patterns not found in previous studies (e.g., \"middle income trap hypothesis\", \"natural resources curse hypothesis\", \"religion works via belief, not practice\", etc.).", "categories": "econ.em", "created": "2019-03-19", "updated": "", "authors": ["guohua feng", "jiti gao", "bin peng"], "url": "https://arxiv.org/abs/1903.07948"}, {"title": "variety, complexity and economic development", "id": "1903.07997", "abstract": "we propose a combinatorial model of economic development. an economy develops by acquiring new capabilities allowing for the production of an ever greater variety of products of increasingly complex products. taking into account that economies abandon the least complex products as they develop over time, we show that variety first increases and then decreases in the course of economic development. this is consistent with the empirical pattern known as 'the hump'. our results question the common association of variety with complexity. we further discuss the implications of our model for future research.", "categories": "econ.gn q-fin.ec", "created": "2019-03-19", "updated": "", "authors": ["alje van dam", "koen frenken"], "url": "https://arxiv.org/abs/1903.07997"}, {"title": "bayesian midas penalized regressions: estimation, selection, and   prediction", "id": "1903.08025", "abstract": "we propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to group lasso penalization and bayesian techniques for estimation and inference. in particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a group lasso with a spike-and-slab prior. penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive mcmc algorithm. we establish good frequentist asymptotic properties of the posterior of the in-sample and out-of-sample prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. when applied to forecasting u.s. gdp, our penalized regressions can outperform many strong competitors. results suggest that financial variables may have some, although very limited, short-term predictive content.", "categories": "econ.em", "created": "2019-03-19", "updated": "2020-06-11", "authors": ["matteo mogliani", "anna simoni"], "url": "https://arxiv.org/abs/1903.08025"}, {"title": "state-building through public land disposal? an application of matrix   completion for counterfactual prediction", "id": "1903.08028", "abstract": "how would the frontier have evolved in the absence of homestead policies? i apply a matrix completion method to predict the counterfactual time-series of frontier state capacity had there been no homesteading. in placebo tests, the matrix completion method outperforms synthetic controls and other regression-based estimators in terms of minimizing prediction error. causal estimates signify that homestead policies had significant and long-lasting negative impacts on state government expenditure and revenue. these results are similar to difference-in-difference estimates that exploit variation in the timing and intensity of homestead entries aggregated from 1.46 million individual land patent records.", "categories": "econ.gn econ.em q-fin.ec stat.ap", "created": "2019-03-19", "updated": "", "authors": ["jason poulos"], "url": "https://arxiv.org/abs/1903.08028"}, {"title": "markov chain models of refugee migration data", "id": "1903.08255", "abstract": "the application of markov chains to modelling refugee crises is explored, focusing on local migration of individuals at the level of cities and days. as an explicit example we apply the markov chains migration model developed here to unhcr data on the burundi refugee crisis. we compare our method to a state-of-the-art `agent-based' model of burundi refugee movements, and highlight that markov chain approaches presented here can improve the match to data while simultaneously being more algorithmically efficient.", "categories": "physics.soc-ph cs.cy econ.gn q-fin.ec", "created": "2019-03-19", "updated": "", "authors": ["vincent huang", "james unwin"], "url": "https://arxiv.org/abs/1903.08255"}, {"title": "the impact of sex education on sexual activity, pregnancy, and abortion", "id": "1903.08307", "abstract": "the purpose of this study is to find a relation between sex education and abortion in the united states. accordingly, multivariate logistic regression is employed to study the relation between abortion and frequency of sex, pre-marriage sex, and pregnancy by rape. the finding shows the odds of abortion among those who have had premarital sex, more frequent sex before marriage, and been the victim of rape is higher than those who have not experienced any of these incidents. the output identified with one unit increase in pre-marriage sex the log-odds of abortion increases by 0.47. similarly, it shows by one unit increase in the frequency of sex, the log-odds of abortion increases by 0.39. also, for every additional pregnancy by rape, there is an expectation of a 3.17 increase in the log-odds of abortion. the findings of this study also suggests abortion is associated with sex education. despite previous findings, this study shows the factors of age, having children, and social standing is not considered a burden to parents and thereby do not have a causal relation to abortion.", "categories": "econ.gn q-fin.ec", "created": "2019-03-19", "updated": "", "authors": ["nima khodakarami"], "url": "https://arxiv.org/abs/1903.08307"}, {"title": "the role of strategic load participants in two-stage settlement   electricity markets", "id": "1903.08341", "abstract": "two-stage electricity market clearing is designed to maintain market efficiency under ideal conditions, e.g., perfect forecast and nonstrategic generation. this work demonstrates that the individual strategic behavior of inelastic load participants in a two-stage settlement electricity market can deteriorate efficiency. our analysis further implies that virtual bidding can play a role in alleviating this loss of efficiency by mitigating the market power of strategic load participants. we use real-world market data from new york iso to validate our theory.", "categories": "math.oc econ.th", "created": "2019-03-20", "updated": "2019-09-15", "authors": ["pengcheng you", "dennice f. gayme", "enrique mallada"], "url": "https://arxiv.org/abs/1903.08341"}, {"title": "feature quantization for parsimonious and interpretable predictive   models", "id": "1903.08920", "abstract": "for regulatory and interpretability reasons, logistic regression is still widely used. to improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized and, if numerous, levels of categorical features are grouped. an even better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. but doing so, the predictive loss has to be optimized on a huge set. to overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network. the good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the uci library and cr\\'edit agricole consumer finance (a major european historic player in the consumer credit market).", "categories": "stat.me econ.em", "created": "2019-03-21", "updated": "", "authors": ["adrien ehrhardt", "christophe biernacki", "vincent vandewalle", "philippe heinrich"], "url": "https://arxiv.org/abs/1903.08920"}, {"title": "strategic research funding", "id": "1903.09055", "abstract": "we study a dynamic game in which information arrives gradually as long as a principal funds research, and an agent takes an action in each period. in equilibrium, the principal's patience is the key determinant of her information provision: the lower her discount rate, the more eagerly she funds. when she is sufficiently patient, her information provision and value function are well-approximated by the 'bayesian persuasion' model. if the conflict of interest is purely belief-based and information is valuable, then she provides full information if she is patient. we also obtain a sharp characterisation of the principal's value function. our proofs rely on a novel dynamic programming principle rooted in the theory of viscosity solutions of differential equations.", "categories": "econ.th", "created": "2019-03-21", "updated": "", "authors": ["matteo escud\u00e9", "ludvig sinander"], "url": "https://arxiv.org/abs/1903.09055"}, {"title": "unravelling the forces underlying urban industrial agglomeration", "id": "1903.09279", "abstract": "as early as the 1920's marshall suggested that firms co-locate in cities to reduce the costs of moving goods, people, and ideas. these 'forces of agglomeration' have given rise, for example, to the high tech clusters of san francisco and boston, and the automobile cluster in detroit. yet, despite its importance for city planners and industrial policy-makers, until recently there has been little success in estimating the relative importance of each marshallian channel to the location decisions of firms.   here we explore a burgeoning literature that aims to exploit the co-location patterns of industries in cities in order to disentangle the relationship between industry co-agglomeration and customer/supplier, labour and idea sharing. building on previous approaches that focus on across- and between-industry estimates, we propose a network-based method to estimate the relative importance of each marshallian channel at a meso scale. specifically, we use a community detection technique to construct a hierarchical decomposition of the full set of industries into clusters based on co-agglomeration patterns, and show that these industry clusters exhibit distinct patterns in terms of their relative reliance on individual marshallian channels.", "categories": "econ.gn q-fin.ec", "created": "2019-03-21", "updated": "2019-06-05", "authors": ["neave o'clery", "samuel heroy", "francois hulot", "mariano beguerisse-d\u00edaz"], "url": "https://arxiv.org/abs/1903.09279"}, {"title": "the impact of renewable energy forecasts on intraday electricity prices", "id": "1903.09641", "abstract": "in this paper we study the impact of errors in wind and solar power forecasts on intraday electricity prices. we develop a novel econometric model which is based on day-ahead wholesale auction curves data and errors in wind and solar power forecasts. the model shifts day-ahead supply curves to calculate intraday prices. we apply our model to the german epex spot se data. our model outperforms both linear and non-linear benchmarks. our study allows us to conclude that errors in renewable energy forecasts exert a non-linear impact on intraday prices. we demonstrate that additional wind and solar power capacities induce non-linear changes in the intraday price volatility. finally, we comment on economical and policy implications of our findings.", "categories": "econ.gn q-fin.ec", "created": "2019-03-22", "updated": "2019-11-18", "authors": ["sergei kulakov", "florian ziel"], "url": "https://arxiv.org/abs/1903.09641"}, {"title": "identification and estimation of a partially linear regression model   using network data", "id": "1903.09679", "abstract": "i study a regression model in which one covariate is an unknown function of a latent driver of link formation in a network. rather than specify and fit a parametric network formation model, i introduce a new method based on matching pairs of agents with similar columns of the squared adjacency matrix, the ijth entry of which contains the number of other agents linked to both agents i and j. the intuition behind this approach is that for a large class of network formation models the columns of the squared adjacency matrix characterize all of the identifiable information about individual linking behavior. in this paper, i describe the model, formalize this intuition, and provide consistent estimators for the parameters of the regression model. large sample approximations suitable for inference, potential extensions to the framework, simulation evidence, and an application to network peer effects can be found in an online appendix.", "categories": "econ.em", "created": "2019-03-22", "updated": "2020-08-10", "authors": ["eric auerbach"], "url": "https://arxiv.org/abs/1903.09679"}, {"title": "on the core of normal form games with a continuum of players : a   correction", "id": "1903.09819", "abstract": "we study the core of normal form games with a continuum of players and without side payments. we consider the weak-core concept, which is an approximation of the core, introduced by weber, shapley and shubik. for payoffs depending on the players' strategy profile, we prove that the weak-core is nonempty. the existence result establishes a weak-core element as a limit of elements in weak-cores of appropriate finite games. we establish by examples that our regularity hypotheses are relevant in the continuum case and the weak-core can be strictly larger than the aumann's $\\alpha$-core. for games where payoffs depend on the distribution of players' strategy profile, we prove that analogous regularity conditions ensuring the existence of pure strategy nash equilibria are irrelevant for the non-vacuity of the weak-core.", "categories": "econ.th", "created": "2019-03-23", "updated": "", "authors": ["youcef askoura"], "url": "https://arxiv.org/abs/1903.09819"}, {"title": "an interim core for normal form games and exchange economies with   incomplete information: a correction", "id": "1903.09867", "abstract": "we consider the interim core of normal form cooperative games and exchange economies with incomplete information based on the partition model. we develop a solution concept that we can situate roughly between wilson's coarse core and yannelis's private core. we investigate the interim negotiation of contracts and address the two situations of contract delivery: interim and ex post. our solution differs from wilson's concept because the measurability of strategies in our solution is postponed until the consumption date (assumed with respect to the information that will be known by the players at the consumption date). for interim consumption, our concept differs from yannelis's private core because players can negotiate conditional on proper common knowledge events in our solution, which strengthens the interim aspect of the game, as we will illustrate with examples.", "categories": "econ.th", "created": "2019-03-23", "updated": "", "authors": ["youcef askoura"], "url": "https://arxiv.org/abs/1903.09867"}, {"title": "machine learning methods economists should know about", "id": "1903.10075", "abstract": "we discuss the relevance of the recent machine learning (ml) literature for economics and econometrics. first we discuss the differences in goals, methods and settings between the ml literature and the traditional econometrics and statistics literatures. then we discuss some specific methods from the machine learning literature that we view as important for empirical researchers in economics. these include supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. finally, we highlight newly developed methods at the intersection of ml and econometrics, methods that typically perform better than either off-the-shelf ml or more traditional econometric methods when applied to particular classes of problems, problems that include causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.", "categories": "econ.em stat.ml", "created": "2019-03-24", "updated": "", "authors": ["susan athey", "guido imbens"], "url": "https://arxiv.org/abs/1903.10075"}, {"title": "ensemble methods for causal effects in panel data settings", "id": "1903.10079", "abstract": "this paper studies a panel data setting where the goal is to estimate causal effects of an intervention by predicting the counterfactual values of outcomes for treated units, had they not received the treatment. several approaches have been proposed for this problem, including regression methods, synthetic control methods and matrix completion methods. this paper considers an ensemble approach, and shows that it performs better than any of the individual methods in several economic datasets. matrix completion methods are often given the most weight by the ensemble, but this clearly depends on the setting. we argue that ensemble methods present a fruitful direction for further research in the causal panel data setting.", "categories": "econ.em", "created": "2019-03-24", "updated": "", "authors": ["susan athey", "mohsen bayati", "guido imbens", "zhaonan qu"], "url": "https://arxiv.org/abs/1903.10079"}, {"title": "on the fair division of a random object", "id": "1903.10361", "abstract": "ann likes oranges and dislikes apples; bob likes apples and dislikes oranges. tomorrow they will receive one fruit that will be an orange or an apple with equal probability 0.5. giving to each half of that fruit is fair for each realisation of the fruit; but agreeing that whatever fruit appears will go to the agent who likes it more gives a higher expected utility to each agent and is fair in the average sense: in expectation, each agent prefers his allocation to the equal division of the object, he gets a fair share.   we turn this familiar observation into an economic design problem: upon drawing a random object (the fruit), we learn the realised utility of each agent and can compare it to the mean of his distribution of utilities; no other statistical information about the distribution is available. we fully characterize the division rules that use only this sparse information in the most efficient possible way, while giving everyone a fair share. although the probability distribution of individual utilities is arbitrary and mostly unknown to the designer, these rules perform in the same range as the best fair rule having full knowledge of this distribution.", "categories": "cs.gt econ.th", "created": "2019-03-25", "updated": "2020-07-05", "authors": ["anna bogomolnaia", "herve moulin", "fedor sandomirskiy"], "url": "https://arxiv.org/abs/1903.10361"}, {"title": "r\\'eint\\'egration des refus\\'es en credit scoring", "id": "1903.10855", "abstract": "the granting process of all credit institutions rejects applicants who seem risky regarding the repayment of their debt. a credit score is calculated and associated with a cut-off value beneath which an applicant is rejected. developing a new score implies having a learning dataset in which the response variable good/bad borrower is known, so that rejects are de facto excluded from the learning process. we first introduce the context and some useful notations. then we formalize if this particular sampling has consequences on the score's relevance. finally, we elaborate on methods that use not-financed clients' characteristics and conclude that none of these methods are satisfactory in practice using data from cr\\'edit agricole consumer finance.   -----   un syst\\`eme d'octroi de cr\\'edit peut refuser des demandes de pr\\^et jug\\'ees trop risqu\\'ees. au sein de ce syst\\`eme, le score de cr\\'edit fournit une valeur mesurant un risque de d\\'efaut, valeur qui est compar\\'ee \\`a un seuil d'acceptabilit\\'e. ce score est construit exclusivement sur des donn\\'ees de clients financ\\'es, contenant en particulier l'information `bon ou mauvais payeur', alors qu'il est par la suite appliqu\\'e \\`a l'ensemble des demandes. un tel score est-il statistiquement pertinent ? dans cette note, nous pr\\'ecisons et formalisons cette question et \\'etudions l'effet de l'absence des non-financ\\'es sur les scores \\'elabor\\'es. nous pr\\'esentons ensuite des m\\'ethodes pour r\\'eint\\'egrer les non-financ\\'es et concluons sur leur inefficacit\\'e en pratique, \\`a partir de donn\\'ees issues de cr\\'edit agricole consumer finance.", "categories": "econ.gn q-fin.ec", "created": "2019-03-21", "updated": "", "authors": ["adrien ehrhardt", "christophe biernacki", "vincent vandewalle", "philippe heinrich", "s\u00e9bastien beben"], "url": "https://arxiv.org/abs/1903.10855"}, {"title": "improving the scalability of a prosumer cooperative game with k-means   clustering", "id": "1903.10965", "abstract": "among the various market structures under peer-to-peer energy sharing, one model based on cooperative game theory provides clear incentives for prosumers to collaboratively schedule their energy resources. the computational complexity of this model, however, increases exponentially with the number of participants. to address this issue, this paper proposes the application of k-means clustering to the energy profiles following the grand coalition optimization. the cooperative model is run with the \"clustered players\" to compute their payoff allocations, which are then further distributed among the prosumers within each cluster. case studies show that the proposed method can significantly improve the scalability of the cooperative scheme while maintaining a high level of financial incentives for the prosumers.", "categories": "cs.ce cs.gt cs.lg econ.gn math.oc q-fin.ec", "created": "2019-03-26", "updated": "2020-07-31", "authors": ["liyang han", "thomas morstyn", "constance crozier", "malcolm mcculloch"], "url": "https://arxiv.org/abs/1903.10965"}, {"title": "on the effect of imputation on the 2sls variance", "id": "1903.11004", "abstract": "endogeneity and missing data are common issues in empirical research. we investigate how both jointly affect inference on causal parameters. conventional methods to estimate the variance, which treat the imputed data as if it was observed in the first place, are not reliable. we derive the asymptotic variance and propose a heteroskedasticity robust variance estimator for two-stage least squares which accounts for the imputation. monte carlo simulations support our theoretical findings.", "categories": "econ.em", "created": "2019-03-26", "updated": "", "authors": ["helmut farbmacher", "alexander kann"], "url": "https://arxiv.org/abs/1903.11004"}, {"title": "estimation of the shapley value of a peer-to-peer energy sharing game   using coalitional stratified random sampling", "id": "1903.11047", "abstract": "various peer-to-peer energy markets have emerged in recent years in an attempt to manage distributed energy resources in a more efficient way. one of the main challenges these models face is how to create and allocate incentives to participants. cooperative game theory offers a methodology to financially reward prosumers based on their contributions made to the local energy coalition using the shapley value, but its high computational complexity limits the size of the game. this paper explores a stratified sampling method proposed in existing literature for shapley value estimation, and modifies the method for a peer-to-peer cooperative game to improve its scalability. finally, selected case studies verify the effectiveness of the proposed coalitional stratified random sampling method and demonstrate results from large games.", "categories": "cs.gt cs.ce econ.gn math.oc q-fin.ec", "created": "2019-03-26", "updated": "", "authors": ["liyang han", "thomas morstyn", "malcolm mcculloch"], "url": "https://arxiv.org/abs/1903.11047"}, {"title": "measuring differences in stochastic network structure", "id": "1903.11117", "abstract": "how can one determine whether a community-level treatment, such as the introduction of a social program or trade shock, alters agents' incentives to form links in a network? this paper proposes analogues of a two-sample kolmogorov-smirnov test, widely used in the literature to test the null hypothesis of \"no treatment effects\", for network data. it first specifies a testing problem in which the null hypothesis is that two networks are drawn from the same random graph model. it then describes two randomization tests based on the magnitude of the difference between the networks' adjacency matrices as measured by the $2\\to2$ and $\\infty\\to1$ operator norms. power properties of the tests are examined analytically, in simulation, and through two real-world applications. a key finding is that the test based on the $\\infty\\to1$ norm can be substantially more powerful than that based on the $2\\to2$ norm for the kinds of sparse and degree-heterogeneous networks common in economics.", "categories": "econ.em", "created": "2019-03-26", "updated": "2020-02-06", "authors": ["eric auerbach"], "url": "https://arxiv.org/abs/1903.11117"}, {"title": "why understanding multiplex social network structuring processes will   help us better understand the evolution of human behavior", "id": "1903.11183", "abstract": "social scientists have long appreciated that relationships between individuals cannot be described from observing a single domain, and that the structure across domains of interaction can have important effects on outcomes of interest (e.g., cooperation).1 one debate explicitly about this surrounds food sharing. some argue that failing to find reciprocal food sharing means that some process other than reciprocity must be occurring, whereas others argue for models that allow reciprocity to span domains in the form of trade.2 multilayer networks, high-dimensional networks that allow us to consider multiple sets of relationships at the same time, are ubiquitous and have consequences, so processes giving rise to them are important social phenomena. the analysis of multi-dimensional social networks has recently garnered the attention of the network science community.3 recent models of these processes show how ignoring layer interdependencies can lead one to miss why a layer formed the way it did, and/or draw erroneous conclusions.6 understanding the structuring processes that underlie multiplex networks will help understand increasingly rich datasets, giving more accurate and complete pictures of social interactions.", "categories": "econ.gn cs.si physics.soc-ph q-bio.pe q-fin.ec", "created": "2019-03-26", "updated": "2020-05-27", "authors": ["curtis atkisson", "piotr j. g\u00f3rski", "matthew o. jackson", "janusz a. ho\u0142yst", "raissa m. d'souza"], "url": "https://arxiv.org/abs/1903.11183"}, {"title": "parallel experimentation in a competitive advertising marketplace", "id": "1903.11198", "abstract": "when multiple firms are simultaneously running experiments on a platform, the treatment effects for one firm may depend on the experimentation policies of others. this paper presents a set of causal estimands that are relevant to such an environment. we also present an experimental design that is suitable for facilitating experimentation across multiple competitors in such an environment. together, these can be used by a platform to run experiments \"as a service,\" on behalf of its participating firms. we show that the causal estimands we develop are identified nonparametrically by the variation induced by the design, and present two scalable estimators that help measure them in typical high-dimensional situations. we implement the design on the advertising platform of jd.com, an ecommerce company, which is also a publisher of digital ads in china. we discuss how the design is engineered within the platform's auction-driven ad-allocation system, which is typical of modern, digital advertising marketplaces. finally, we present results from a parallel experiment involving 16 advertisers and millions of jd.com users. these results showcase the importance of accommodating a role for interactions across experimenters and demonstrates the viability of the framework.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-03-26", "updated": "2019-05-28", "authors": ["xiliang lin", "harikesh s. nair", "navdeep s. sahni", "caio waisman"], "url": "https://arxiv.org/abs/1903.11198"}, {"title": "determining fundamental supply and demand curves in a wholesale   electricity market", "id": "1903.11383", "abstract": "in this paper we develop a novel method of wholesale electricity market modeling. our optimization-based model decomposes wholesale supply and demand curves into buy and sell orders of individual market participants. in doing so, the model detects and removes arbitrage orders. as a result, we construct an innovative fundamental model of a wholesale electricity market. first, our fundamental demand curve has a unique composition. the demand curve lies in between the wholesale demand curve and a perfectly inelastic demand curve. second, our fundamental supply and demand curves contain only actual (i.e. non-arbitrage) transactions with physical assets on buy and sell sides. third, these transactions are designated to one of the three groups of wholesale electricity market participants: retailers, suppliers, or utility companies. to evaluate the performance of our model, we use the german wholesale market data. our fundamental model yields a more precise approximation of the actual load values than a model with perfectly inelastic demand. moreover, we conduct a study of wholesale demand elasticities. the obtained conclusions regarding wholesale demand elasticity are consistent with the existing academic literature.", "categories": "econ.gn q-fin.ec", "created": "2019-03-22", "updated": "2019-11-15", "authors": ["sergei kulakov", "florian ziel"], "url": "https://arxiv.org/abs/1903.11383"}, {"title": "time series models for realized covariance matrices based on the   matrix-f distribution", "id": "1903.12077", "abstract": "we propose a new conditional bekk matrix-f (cbf) model for the time-varying realized covariance (rcov) matrices. this cbf model is capable of capturing heavy-tailed rcov, which is an important stylized fact but could not be handled adequately by the wishart-based models. to further mimic the long memory feature of the rcov, a special cbf model with the conditional heterogeneous autoregressive (har) structure is introduced. moreover, we give a systematical study on the probabilistic properties and statistical inferences of the cbf model, including exploring its stationarity, establishing the asymptotics of its maximum likelihood estimator, and giving some new inner-product-based tests for its model checking. in order to handle a large dimensional rcov matrix, we construct two reduced cbf models -- the variance-target cbf model (for moderate but fixed dimensional rcov matrix) and the factor cbf model (for high dimensional rcov matrix). for both reduced models, the asymptotic theory of the estimated parameters is derived. the importance of our entire methodology is illustrated by simulation results and two real examples.", "categories": "math.st econ.em stat.me stat.th", "created": "2019-03-26", "updated": "2020-07-09", "authors": ["jiayuan zhou", "feiyu jiang", "ke zhu", "wai keung li"], "url": "https://arxiv.org/abs/1903.12077"}, {"title": "inference in high-dimensional set-identified affine models", "id": "1904.00111", "abstract": "this paper proposes both point-wise and uniform confidence sets (cs) for an element $\\theta_{1}$ of a parameter vector $\\theta\\in\\mathbb{r}^{d}$ that is partially identified by affine moment equality and inequality conditions. the method is based on an estimator of a regularized support function of the identified set. this estimator is \\emph{half-median unbiased} and has an \\emph{asymptotic linear representation} which provides closed form standard errors and enables optimization-free multiplier bootstrap. the proposed cs can be computed as a solution to a finite number of linear and convex quadratic programs, which leads to a substantial decrease in \\emph{computation time} and \\emph{guarantee of global optimum}. as a result, the method provides uniformly valid inference in applications with the dimension of the parameter space, $d$, and the number of inequalities, $k$, that were previously computationally unfeasible ($d,k >100$). the proposed approach is then extended to construct polygon-shaped joint cs for multiple components of $\\theta$. inference for coefficients in the linear iv regression model with interval outcome is used as an illustrative example.   key words: affine moment inequalities; asymptotic linear representation; delta\\textendash method; interval data; intersection bounds; partial identification; regularization; strong approximation; stochastic programming; subvector inference; uniform inference.", "categories": "econ.em", "created": "2019-03-29", "updated": "", "authors": ["bulat gafarov"], "url": "https://arxiv.org/abs/1904.00111"}, {"title": "post-selection inference in three-dimensional panel data", "id": "1904.00211", "abstract": "three-dimensional panel models are widely used in empirical analysis. researchers use various combinations of fixed effects for three-dimensional panels. when one imposes a parsimonious model and the true model is rich, then it incurs mis-specification biases. when one employs a rich model and the true model is parsimonious, then it incurs larger standard errors than necessary. it is therefore useful for researchers to know correct models. in this light, lu, miao, and su (2018) propose methods of model selection. we advance this literature by proposing a method of post-selection inference for regression parameters. despite our use of the lasso technique as means of model selection, our assumptions allow for many and even all fixed effects to be nonzero. simulation studies demonstrate that the proposed method is more precise than under-fitting fixed effect estimators, is more efficient than over-fitting fixed effect estimators, and allows for as accurate inference as the oracle estimator.", "categories": "econ.em", "created": "2019-03-30", "updated": "2019-04-30", "authors": ["harold d. chiang", "joel rodrigue", "yuya sasaki"], "url": "https://arxiv.org/abs/1904.00211"}, {"title": "herding driven by the desire to differ", "id": "1904.00454", "abstract": "observational learning often involves congestion: an agent gets lower payoff from an action when more predecessors have taken that action. this preference to act differently from previous agents may paradoxically increase all but one agent's probability of matching the actions of the predecessors. the reason is that when previous agents conform to their predecessors despite the preference to differ, their actions become more informative. the desire to match predecessors' actions may reduce herding by a similar reasoning.", "categories": "econ.th", "created": "2019-03-31", "updated": "", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1904.00454"}, {"title": "game of variable contributions to the common good under uncertainty", "id": "1904.00500", "abstract": "we consider a stochastic game of contribution to the common good in which the players have continuous control over the degree of contribution, and we examine the gradualism arising from the free rider effect. this game belongs to the class of variable concession games which generalize wars of attrition. previously known examples of variable concession games in the literature yield equilibria characterized by singular control strategies without any delay of concession. however, these no-delay equilibria are in contrast to mixed strategy equilibria of canonical wars of attrition in which each player delays concession by a randomized time. we find that a variable contribution game with a single state variable, which extends the nerlove-arrow model, possesses an equilibrium characterized by regular control strategies that result in a gradual concession. this equilibrium naturally generalizes the mixed strategy equilibria from the canonical wars of attrition. stochasticity of the problem accentuates the qualitative difference between a singular control solution and a regular control equilibrium solution. we also find that asymmetry between the players can mitigate the inefficiency caused by the gradualism.", "categories": "math.oc econ.th q-fin.mf", "created": "2019-03-31", "updated": "", "authors": ["h. dharma kwon"], "url": "https://arxiv.org/abs/1904.00500"}, {"title": "an alternative set model of cognitive jump", "id": "1904.00613", "abstract": "when we enumerate numbers up to some specific value, or, even if we do not specify the number, we know at the same time that there are much greater numbers which should be reachable by the same enumeration, but indeed we also congnize them without practical enumeration. namely, if we deem enumeration to be a way of reaching a number without any \"jump\", there is a \"jump'' in our way of cognition of such greater numbers. in this article, making use of a set theoretical framework by vop\\v{e}nka (1979) (alternative set theory) which describes such structure, we attempt to shed light on an analogous sturucture in human and social phenomenon. as an example, we examine a problem of common knowledge in electronic mail game presented by rubinstein (1989). we show an event comes to common knowledge by a \"cognitive jump\".", "categories": "cs.gt econ.th", "created": "2019-04-01", "updated": "", "authors": ["kiri sakahara", "takashi sato"], "url": "https://arxiv.org/abs/1904.00613"}, {"title": "counterfactual sensitivity and robustness", "id": "1904.00989", "abstract": "researchers frequently make parametric assumptions about the distribution of unobservables when formulating structural models. such assumptions are typically motived by computational convenience rather than economic theory and are often untestable. counterfactuals can be particularly sensitive to such assumptions, threatening the credibility of structural modeling exercises. to address this issue, we leverage insights from the literature on ambiguity and model uncertainty to propose a tractable econometric framework for characterizing the sensitivity of counterfactuals with respect to a researcher's assumptions about the distribution of unobservables in a class of structural models. in particular, we show how to construct the smallest and largest values of the counterfactual as the distribution of unobservables spans nonparametric neighborhoods of the researcher's assumed specification while other `structural' features of the model, e.g. equilibrium conditions, are maintained. our methods are computationally simple to implement, with the nuisance distribution effectively profiled out via a low-dimensional convex program. our procedure delivers sharp bounds for the identified set of counterfactuals (i.e. without parametric assumptions about the distribution of unobservables) as the neighborhoods become large. over small neighborhoods, we relate our procedure to a measure of local sensitivity which is further characterized using an influence function representation. we provide a suitable sampling theory for plug-in estimators and apply our procedure to models of strategic interaction and dynamic discrete choice.", "categories": "econ.em", "created": "2019-04-01", "updated": "2019-04-15", "authors": ["timothy christensen", "benjamin connault"], "url": "https://arxiv.org/abs/1904.00989"}, {"title": "dynamically optimal treatment allocation using reinforcement learning", "id": "1904.01047", "abstract": "devising guidance on how to assign individuals to treatment is an important goal in empirical research. in practice, individuals often arrive sequentially, and the planner faces various constraints such as limited budget/capacity, or borrowing constraints, or the need to place people in a queue. for instance, a governmental body may receive a budget outlay at the beginning of a year, and it may need to decide how best to allocate resources within the year to individuals who arrive sequentially. in this and other examples involving inter-temporal trade-offs, previous work on devising optimal policy rules in a static context is either not applicable, or sub-optimal. here we show how one can use offline observational data to estimate an optimal policy rule that maximizes expected welfare in this dynamic context. we allow the class of policy rules to be restricted for legal, ethical or incentive compatibility reasons. the problem is equivalent to one of optimal control under a constrained policy class, and we exploit recent developments in reinforcement learning (rl) to propose an algorithm to solve this. the algorithm is easily implementable with speedups achieved through multiple rl agents learning in parallel processes. we also characterize the statistical regret from using our estimated policy rule by casting the evolution of the value function under each policy in a partial differential equation (pde) form and using the theory of viscosity solutions to pdes. we find that the policy regret decays at a $n^{-1/2}$ rate in most examples; this is the same rate as in the static case.", "categories": "econ.em cs.lg", "created": "2019-04-01", "updated": "2020-08-30", "authors": ["karun adusumilli", "friedrich geiecke", "claudio schilter"], "url": "https://arxiv.org/abs/1904.01047"}, {"title": "matching points: supplementing instruments with covariates in triangular   models", "id": "1904.01159", "abstract": "models with a discrete endogenous variable are typically underidentified when the instrument takes on too few values. this paper presents a new method that matches pairs of covariates and instruments to restore point identification in this scenario in a triangular model. the model consists of a structural function for a continuous outcome and a selection model for the discrete endogenous variable. the structural outcome function must be continuous and monotonic in a scalar disturbance, but it can be nonseparable. the selection model allows for unrestricted heterogeneity. global identification is obtained under weak conditions. the paper also provides estimators of the structural outcome function. two empirical examples of the return to education and selection into head start illustrate the value and limitations of the method.", "categories": "econ.em", "created": "2019-04-01", "updated": "2020-07-27", "authors": ["junlong feng"], "url": "https://arxiv.org/abs/1904.01159"}, {"title": "synthetic learner: model-free inference on treatments over time", "id": "1904.01490", "abstract": "understanding of the effect of a particular treatment or a policy pertains to many areas of interest -- ranging from political economics, marketing to health-care and personalized treatment studies. in this paper, we develop a non-parametric, model-free test for detecting the effects of treatment over time that extends widely used synthetic control tests. the test is built on counterfactual predictions arising from many learning algorithms. in the neyman-rubin potential outcome framework with possible carry-over effects, we show that the proposed test is asymptotically consistent for stationary, beta mixing processes. we do not assume that class of learners captures the correct model necessarily. we also discuss estimates of the average treatment effect, and we provide regret bounds on the predictive performance. to the best of our knowledge, this is the first set of results that allow for example any random forest to be useful for provably valid statistical inference in the synthetic control setting. in experiments, we show that our synthetic learner is substantially more powerful than classical methods based on synthetic control or difference-in-differences, especially in the presence of non-linear outcome models.", "categories": "stat.me cs.lg econ.em stat.ml", "created": "2019-04-02", "updated": "", "authors": ["davide viviano", "jelena bradic"], "url": "https://arxiv.org/abs/1904.01490"}, {"title": "do hospital data breaches reduce patient care quality?", "id": "1904.02058", "abstract": "objective: to estimate the relationship between a hospital data breach and hospital quality outcome   materials and methods: hospital data breaches reported to the u.s. department of health and human services breach portal and the privacy rights clearinghouse database were merged with the medicare hospital compare data to assemble a panel of non-federal acutecare inpatient hospitals for years 2011 to 2015. the study panel included 2,619 hospitals. changes in 30-day ami mortality rate following a hospital data breach were estimated using a multivariate regression model based on a difference-in-differences approach.   results: a data breach was associated with a 0.338[95% ci, 0.101-0.576] percentage point increase in the 30-day ami mortality rate in the year following the breach and a 0.446[95% ci, 0.164-0.729] percentage point increase two years after the breach. for comparison, the median 30-day ami mortality rate has been decreasing about 0.4 percentage points annually since 2011 due to progress in care. the magnitude of the breach impact on hospitals' ami mortality rates was comparable to a year's worth historical progress in reducing ami mortality rates.   conclusion: hospital data breaches significantly increased the 30-day mortality rate for ami. data breaches may disrupt the processes of care that rely on health information technology. financial costs to repair a breach may also divert resources away from patient care. thus breached hospitals should carefully focus investments in security procedures, processes, and health information technology that jointly lead to better data security and improved patient outcomes.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-04-03", "updated": "", "authors": ["sung j. choi", "m. eric johnson"], "url": "https://arxiv.org/abs/1904.02058"}, {"title": "prudent case-based prediction when experience is lacking", "id": "1904.02934", "abstract": "an inexperienced predictor is asked to qualitatively rank eventualities according to their plausibility, given past cases. inexperience means that, resampling past cases (with replacement) fails to generate a suitably diverse set of rankings. (4-diversity requires that each of the 4! strict rankings of four eventualities arises for some sample.) along with other essential consistency requirements, 4-diversity yields a matrix representation that may be viewed as an empirical likelihood function (gilboa and schmeidler, 2003). we impose 2-diversity and derive a similar representation: provided the predictor is prudent enough to ensure that the arrival of novel cases will not force her into being dogmatic, intransitive or into revising her existing rankings. we build on this to establish a formal tradeoff between inexperience and the cognitive or computational cost of more abstract resampling.", "categories": "econ.th econ.em", "created": "2019-04-05", "updated": "", "authors": ["patrick h. o'callaghan"], "url": "https://arxiv.org/abs/1904.02934"}, {"title": "anticipated impacts of brexit scenarios on uk food prices and   implications for policies on poverty and health: a structured expert   judgement approach", "id": "1904.03053", "abstract": "food insecurity is associated with increased risk for several health conditions and with poor chronic disease management. key determinants for household food insecurity are income and food costs. whereas short-term household incomes are likely to remain static, increased food prices would be a significant driver of food insecurity. to investigate food price drivers for household food security and its health consequences in the uk under scenarios of deal and no deal for brexit . to estimate the 5\\% and 95\\% quantiles of the projected price distributions. structured expert judgement elicitation, a well-established method for quantifying uncertainty, using experts. in july 2018, each expert estimated the median, 5\\% and 95\\% quantiles of changes in price for ten food categories under brexit deal and no-deal to june 2020 assuming brexit had taken place on 29th march 2019. these were aggregated based on the accuracy and informativeness of the experts on calibration questions. ten specialists in food procurement, retail, agriculture, economics, statistics and household food security. results: when combined in proportions used to calculate consumer prices index food basket costs, median food price change for brexit with a deal is expected to be +6.1\\% [90\\% credible interval:-3\\%, +17\\%] and with no deal +22.5\\% [+1\\%, +52\\%]. the number of households experiencing food insecurity and its severity are likely to increase because of expected sizeable increases in median food prices after brexit. higher increases are more likely than lower rises and towards the upper limits, these would entail severe impacts. research showing a low food budget leads to increasingly poor diet suggests that demand for health services in both the short and longer term is likely to increase due to the effects of food insecurity on the incidence and management of diet-sensitive conditions.", "categories": "econ.gn q-fin.ec", "created": "2019-04-05", "updated": "2020-01-22", "authors": ["martine j barons", "willy aspinall"], "url": "https://arxiv.org/abs/1904.03053"}, {"title": "bayesian estimation of mixed multinomial logit models: advances and   simulation-based evaluations", "id": "1904.03647", "abstract": "variational bayes (vb) methods have emerged as a fast and computationally-efficient alternative to markov chain monte carlo (mcmc) methods for scalable bayesian estimation of mixed multinomial logit (mmnl) models. it has been established that vb is substantially faster than mcmc at practically no compromises in predictive accuracy. in this paper, we address two critical gaps concerning the usage and understanding of vb for mmnl. first, extant vb methods are limited to utility specifications involving only individual-specific taste parameters. second, the finite-sample properties of vb estimators and the relative performance of vb, mcmc and maximum simulated likelihood estimation (msle) are not known. to address the former, this study extends several vb methods for mmnl to admit utility specifications including both fixed and random utility parameters. to address the latter, we conduct an extensive simulation-based evaluation to benchmark the extended vb methods against mcmc and msle in terms of estimation times, parameter recovery and predictive accuracy. the results suggest that all vb variants with the exception of the ones relying on an alternative variational lower bound constructed with the help of the modified jensen's inequality perform as well as mcmc and msle at prediction and parameter recovery. in particular, vb with nonconjugate variational message passing and the delta-method (vb-ncvmp-delta) is up to 16 times faster than mcmc and msle. thus, vb-ncvmp-delta can be an attractive alternative to mcmc and msle for fast, scalable and accurate estimation of mmnl models.", "categories": "stat.ml cs.lg econ.em stat.me", "created": "2019-04-07", "updated": "2019-12-12", "authors": ["prateek bansal", "rico krueger", "michel bierlaire", "ricardo a. daziano", "taha h. rashidi"], "url": "https://arxiv.org/abs/1904.03647"}, {"title": "binary choice models with high-dimensional individual and time fixed   effects", "id": "1904.04217", "abstract": "empirical economists are often deterred from the application of nonlinear fixed effects models mainly for two reasons: the incidental parameter problem and the computational challenge even in moderately large panels. using the example of binary choice models with individual and time fixed effects, we show how both issues can be alleviated by combining the asymptotic bias corrections of fern\\'andez-val and weidner (2016) with an estimation algorithm proposed by stammann (2018). we conduct extensive simulation experiments to analyze the statistical properties of various (bias-corrected) estimators in large and even unbalanced panels. our simulation results provide new insights that are relevant for the application of bias corrections in empirical work. overall, we find that analytically bias corrected estimators are clearly preferable, especially for unbalanced panels.", "categories": "econ.em", "created": "2019-04-08", "updated": "2019-08-01", "authors": ["daniel czarnowske", "amrei stammann"], "url": "https://arxiv.org/abs/1904.04217"}, {"title": "fdi, banking crisis and growth: direct and spill over effects", "id": "1904.04911", "abstract": "this study suggests a new decomposition of the effect of foreign direct investment (fdi) on long-term growth in developing countries. it reveals that fdi not only have a positive direct effect on growth, but also increase the latter by reducing the recessionary effect resulting from a banking crisis. even more, they reduce its occurrence. jel: f65, f36, g01, g15", "categories": "econ.gn q-fin.ec", "created": "2019-04-08", "updated": "", "authors": ["brahim gaies", "khaled guesmi", "st\u00e9phane goutte"], "url": "https://arxiv.org/abs/1904.04911"}, {"title": "robust mathematical formulation of agent-based computational economic   market models", "id": "1904.04951", "abstract": "in science and especially in the economic literature, agent-based modeling has become a widely used modeling approach. these models are often formulated as a large system of difference equations. in this study, we discuss two aspects of numerical modeling for two agent-based computational economic market models: the levy-levy-solomon model and the franke-westerhoff model. we derive time-continuous formulations of both models and, for the levy-levy-solomon model, we discuss the impact of the time-scaling on the model behavior. for the franke-westerhoff model, we proof that a constraint required in the original model is not necessary for stability of the time-continuous model. it is shown that a semi-implicit discretization of the time-continuous system preserves this unconditional stability. in addition, this semi-implicit discretization can be computed at cost comparable to the original model.", "categories": "q-fin.tr econ.gn q-fin.ec q-fin.gn q-fin.st", "created": "2019-04-09", "updated": "2019-11-08", "authors": ["maximilian beikirch", "simon cramer", "martin frank", "philipp otte", "emma pabich", "torsten trimborn"], "url": "https://arxiv.org/abs/1904.04951"}, {"title": "a normative dual-value theory for bitcoin and other cryptocurrencies", "id": "1904.05028", "abstract": "bitcoin as well as other cryptocurrencies are all plagued by the impact from bifurcation. since the marginal cost of bifurcation is theoretically zero, it causes the coin holders to doubt on the existence of the coin's intrinsic value. this paper suggests a normative dual-value theory to assess the fundamental value of bitcoin. we draw on the experience from the art market, where similar replication problems are prevalent. the idea is to decompose the total value of a cryptocurrency into two parts: one is its art value and the other is its use value. the tradeoff between these two values is also analyzed, which enlightens our proposal of an image coin for bitcoin so as to elevate its use value without sacrificing its art value. to show the general validity of the dual-value theory, we also apply it to evaluate the prospects of four major cryptocurrencies. we find this framework is helpful for both the investors and the exchanges to examine a new coin's value when it first appears in the market.", "categories": "econ.gn q-fin.ec", "created": "2019-04-10", "updated": "", "authors": ["zhiyong tu", "lan ju"], "url": "https://arxiv.org/abs/1904.05028"}, {"title": "local polynomial estimation of time-varying parameters in nonlinear   models", "id": "1904.05209", "abstract": "we develop a novel asymptotic theory for local polynomial (quasi-) maximum-likelihood estimators of time-varying parameters in a broad class of nonlinear time series models. under weak regularity conditions, we show the proposed estimators are consistent and follow normal distributions in large samples. our conditions impose weaker smoothness and moment conditions on the data-generating process and its likelihood compared to existing theories. furthermore, the bias terms of the estimators take a simpler form. we demonstrate the usefulness of our general results by applying our theory to local (quasi-)maximum-likelihood estimators of a time-varying var's, arch and garch, and poisson autogressions. for the first three models, we are able to substantially weaken the conditions found in the existing literature. for the poisson autogression, existing theories cannot be be applied while our novel approach allows us to analyze it.", "categories": "econ.em math.st stat.th", "created": "2019-04-10", "updated": "", "authors": ["dennis kristensen", "young jun lee"], "url": "https://arxiv.org/abs/1904.05209"}, {"title": "solving dynamic discrete choice models using smoothing and sieve methods", "id": "1904.05232", "abstract": "we propose to combine smoothing, simulations and sieve approximations to solve for either the integrated or expected value function in a general class of dynamic discrete choice (ddc) models. we use importance sampling to approximate the bellman operators defining the two functions. the random bellman operators, and therefore also the corresponding solutions, are generally non-smooth which is undesirable. to circumvent this issue, we introduce a smoothed version of the random bellman operator and solve for the corresponding smoothed value function using sieve methods. we show that one can avoid using sieves by generalizing and adapting the `self-approximating' method of rust (1997) to our setting. we provide an asymptotic theory for the approximate solutions and show that they converge with root-n-rate, where $n$ is number of monte carlo draws, towards gaussian processes. we examine their performance in practice through a set of numerical experiments and find that both methods perform well with the sieve method being particularly attractive in terms of computational speed and accuracy.", "categories": "econ.em math.st stat.th", "created": "2019-04-10", "updated": "2020-02-28", "authors": ["dennis kristensen", "patrick k. mogensen", "jong myun moon", "bertel schjerning"], "url": "https://arxiv.org/abs/1904.05232"}, {"title": "stochastic comparative statics in markov decision processes", "id": "1904.05481", "abstract": "in multi-period stochastic optimization problems, the future optimal decision is a random variable whose distribution depends on the parameters of the optimization problem. we analyze how the expected value of this random variable changes as a function of the dynamic optimization parameters in the context of markov decision processes. we call this analysis \\emph{stochastic comparative statics}. we derive both \\emph{comparative statics} results and \\emph{stochastic comparative statics} results showing how the current and future optimal decisions change in response to changes in the single-period payoff function, the discount factor, the initial state of the system, and the transition probability function. we apply our results to various models from the economics and operations research literature, including investment theory, dynamic pricing models, controlled random walks, and comparisons of stationary distributions.", "categories": "math.oc econ.gn q-fin.ec", "created": "2019-04-10", "updated": "2020-01-25", "authors": ["bar light"], "url": "https://arxiv.org/abs/1904.05481"}, {"title": "understanding consumer demand for new transport technologies and   services, and implications for the future of mobility", "id": "1904.05554", "abstract": "the transport sector is witnessing unprecedented levels of disruption. privately owned cars that operate on internal combustion engines have been the dominant modes of passenger transport for much of the last century. however, recent advances in transport technologies and services, such as the development of autonomous vehicles, the emergence of shared mobility services, and the commercialization of alternative fuel vehicle technologies, promise to revolutionise how humans travel. the implications are profound: some have predicted the end of private car dependent western societies, others have portended greater suburbanization than has ever been observed before. if transport systems are to fulfil current and future needs of different subpopulations, and satisfy short and long-term societal objectives, it is imperative that we comprehend the many factors that shape individual behaviour. this chapter introduces the technologies and services most likely to disrupt prevailing practices in the transport sector. we review past studies that have examined current and future demand for these new technologies and services, and their likely short and long-term impacts on extant mobility patterns. we conclude with a summary of what these new technologies and services might mean for the future of mobility.", "categories": "econ.gn q-fin.ec", "created": "2019-04-11", "updated": "", "authors": ["akshay vij"], "url": "https://arxiv.org/abs/1904.05554"}, {"title": "pricing under fairness concerns", "id": "1904.05656", "abstract": "this paper proposes a theory of pricing premised upon the assumptions that customers dislike unfair prices---those marked up steeply over cost---and that firms take these concerns into account when setting prices. since they do not observe firms' costs, customers must extract costs from prices. the theory assumes that customers infer less than rationally: when a price rises due to a cost increase, customers partially misattribute the higher price to a higher markup---which they find unfair. firms anticipate this response and trim their price increases, which drives the passthrough of costs into prices below one: prices are somewhat rigid. embedded in a new keynesian model as a replacement for the usual pricing frictions, our theory produces monetary nonneutrality: when monetary policy loosens and inflation rises, customers misperceive markups as higher and feel unfairly treated; firms mitigate this perceived unfairness by reducing their markups; in general equilibrium, employment rises. the theory also features a hybrid short-run phillips curve, realistic impulse responses of output and employment to monetary and technology shocks, and an upward-sloping long-run phillips curve.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-04-11", "updated": "2020-08-01", "authors": ["erik eyster", "kristof madarasz", "pascal michaillat"], "url": "https://arxiv.org/abs/1904.05656"}, {"title": "identification of noncausal models by quantile autoregressions", "id": "1904.05952", "abstract": "we propose a model selection criterion to detect purely causal from purely noncausal models in the framework of quantile autoregressions (qar). we also present asymptotics for the i.i.d. case with regularly varying distributed innovations in qar. this new modelling perspective is appealing for investigating the presence of bubbles in economic and financial time series, and is an alternative to approximate maximum likelihood methods. we illustrate our analysis using hyperinflation episodes in latin american countries.", "categories": "econ.em", "created": "2019-04-11", "updated": "", "authors": ["alain hecq", "li sun"], "url": "https://arxiv.org/abs/1904.05952"}, {"title": "distribution regression in duration analysis: an application to   unemployment spells", "id": "1904.06185", "abstract": "this article proposes estimation and inference procedures for distribution regression models with randomly right-censored data. the proposal generalizes classical duration models to a situation where slope coefficients can vary with the elapsed duration, and is suitable for discrete, continuous or mixed outcomes. given that in general distribution regression coefficients do not have clear economic interpretation, we also propose consistent and asymptotically normal estimators for the average distribution marginal effects. finite sample properties of the proposed method are studied by means of monte carlo experiments. finally, we apply our proposed tools to study the effect of unemployment benefits on unemployment duration. our results suggest that, on average, an increase in unemployment benefits is associated with a nonlinear, non-monotone effect on the unemployment duration distribution and that such an effect is more pronounced for workers subjected to liquidity constraints.", "categories": "econ.em", "created": "2019-04-12", "updated": "", "authors": ["miguel a. delgado", "andr\u00e9s garc\u00eda-suaza", "pedro h. c. sant'anna"], "url": "https://arxiv.org/abs/1904.06185"}, {"title": "journal ranking should depend on the level of aggregation", "id": "1904.06300", "abstract": "journal ranking is becoming more important in assessing the quality of academic research. several indices have been suggested for this purpose, typically on the basis of a citation graph between the journals. we follow an axiomatic approach and find an impossibility theorem: any self-consistent ranking method, which satisfies a natural monotonicity property, should depend on the level of aggregation. our result presents a trade-off between two axiomatic properties and reveals a dilemma of aggregation.", "categories": "cs.dl cs.gt econ.th", "created": "2019-04-08", "updated": "2019-09-02", "authors": ["l\u00e1szl\u00f3 csat\u00f3"], "url": "https://arxiv.org/abs/1904.06300"}, {"title": "optimal behaviour in solar renewable energy certificate (srec) markets", "id": "1904.06337", "abstract": "srec markets are a relatively novel market-based system to incentivize the production of energy from solar means. a regulator imposes a floor on the amount of energy each regulated firm must generate from solar power in a given period and provides them with certificates for each generated mwh. firms offset these certificates against the floor and pay a penalty for any lacking certificates. certificates are tradable assets, allowing firms to purchase/sell them freely. in this work, we formulate a stochastic control problem for generating and trading in srec markets from a regulated firm's perspective. we account for generation and trading costs, the impact both have on srec prices, provide a characterization of the optimal strategy, and develop a numerical algorithm to solve this control problem. through numerical experiments, we explore how a firm who acts optimally behaves under various conditions. we find that an optimal firm's generation and trading behaviour can be separated into various regimes, based on the marginal benefit of obtaining an additional srec, and validate our theoretical characterization of the optimal strategy. we also conduct parameter sensitivity experiments and conduct comparisons of the optimal strategy to other candidate strategies.", "categories": "q-fin.mf econ.gn q-fin.ec q-fin.tr", "created": "2019-04-12", "updated": "2020-04-06", "authors": ["arvind shrivats", "sebastian jaimungal"], "url": "https://arxiv.org/abs/1904.06337"}, {"title": "rational inattention and retirement puzzles", "id": "1904.06520", "abstract": "i present evidence incorporating costly thought solves three puzzles in the retirement literature. the first puzzle is, given incentives, the extent of bunching of labour market exits at legislated state pension ages (spa) seems incompatible with rational expectations. adding to the evidence for this puzzle, i include an empirical analysis focusing on whether liquidity constraints can explain this bunching and find they cannot. the nature of this puzzle is clarified by exploring a life-cycle model with rational agents that matches aggregate profiles. this model succeeds in matching aggregates by overestimating the impact of the spa on poorer individuals whilst underestimating its impact on wealthier people. the second puzzle is people are often mistaken about their own pension provisions. concerning the second puzzle, i incorporate rational inattention to the spa into the aforementioned life-cycle model, allowing for mistaken beliefs. to the best of my knowledge, this paper is the first not only to incorporate rational inattention into a life-cycle model but also to assess a rationally inattentive model against non-experimental individual choice data. this facilitates another important contribution: discipling the cost of attention with subjective belief data. preliminary results indicate rational inattention improves the aggregate fit and better matches the response of participation to the spa across the wealth distribution, hence offering a resolution to the first puzzle. the third puzzle is despite actuarially advantageous options to defer receipt of pension benefits, take up is extremely low. an extension of the model generates an explanation of this last puzzle: the actuarial calculations implying deferral is preferable ignore the utility cost of tracking your pension which can be avoided by claiming. these puzzles are researched in the context of the reform to the uk female spa.", "categories": "econ.gn q-fin.ec", "created": "2019-04-13", "updated": "", "authors": ["jamie hentall maccuish"], "url": "https://arxiv.org/abs/1904.06520"}, {"title": "nash bargaining over margin loans to kelly gamblers", "id": "1904.06628", "abstract": "i derive practical formulas for optimal arrangements between sophisticated stock market investors (namely, continuous-time kelly gamblers or, more generally, crra investors) and the brokers who lend them cash for leveraged bets on a high sharpe asset (i.e. the market portfolio). rather than, say, the broker posting a monopoly price for margin loans, the gambler agrees to use a greater quantity of margin debt than he otherwise would in exchange for an interest rate that is lower than the broker would otherwise post. the gambler thereby attains a higher asymptotic capital growth rate and the broker enjoys a greater rate of intermediation profit than would obtain under non-cooperation. if the threat point represents a vicious breakdown of negotiations (resulting in zero margin loans), then we get an elegant rule of thumb: $r_l^*=(3/4)r+(1/4)(\\nu-\\sigma^2/2)$, where $r$ is the broker's cost of funds, $\\nu$ is the compound-annual growth rate of the market index, and $\\sigma$ is the annual volatility. we show that, regardless of the particular threat point, the gambler will negotiate to size his bets as if he himself could borrow at the broker's call rate.", "categories": "econ.gn econ.th q-fin.ec q-fin.gn q-fin.mf q-fin.pm", "created": "2019-04-14", "updated": "2019-08-30", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1904.06628"}, {"title": "complex network construction of internet financial risk", "id": "1904.06640", "abstract": "internet finance is a new financial model that applies internet technology to payment, capital borrowing and lending and transaction processing. in order to study the internal risks, this paper uses the internet financial risk elements as the network node to construct the complex network of internet financial risk system. different from the study of macroeconomic shocks and financial institution data, this paper mainly adopts the perspective of complex system to analyze the systematic risk of internet finance. by dividing the entire financial system into internet financial subnet, regulatory subnet and traditional financial subnet, the paper discusses the relationship between contagion and contagion among different risk factors, and concludes that risks are transmitted externally through the internal circulation of internet finance, thus discovering potential hidden dangers of systemic risks. the results show that the nodes around the center of the whole system are the main objects of financial risk contagion in the internet financial network. in addition, macro-prudential regulation plays a decisive role in the control of the internet financial system, and points out the reasons why the current regulatory measures are still limited. this paper summarizes a research model which is still in its infancy, hoping to open up new prospects and directions for us to understand the cascading behaviors of internet financial risks.", "categories": "econ.em", "created": "2019-04-14", "updated": "2019-08-30", "authors": ["runjie xu", "chuanmin mi", "rafal mierzwiak", "runyu meng"], "url": "https://arxiv.org/abs/1904.06640"}, {"title": "eliciting preferences of ridehailing users and drivers: evidence from   the united states", "id": "1904.06695", "abstract": "transportation network companies (tncs) are changing the transportation ecosystem, but micro-decisions of drivers and users need to be better understood to assess the system-level impacts of tncs. in this regard, we contribute to the literature by estimating a) individuals' preferences of being a rider, a driver, or a non-user of tnc services; b) preferences of ridehailing users for ridepooling; c) tnc drivers' choice to switch to vehicles with better fuel economy, and also d) the drivers' decision to buy, rent or lease new vehicles with driving for tncs being a major consideration. elicitation of drivers' preferences using a unique sample (n=11,902) of the u.s. population residing in tnc-served areas is the key feature of this study. the statistical analysis indicates that ridehailing services are mainly attracting personal vehicle users as riders, without substantially affecting demand for transit. moreover, around 10% of ridehailing users reported postponing the purchase of a new car due to the availability of tnc services. the model estimation results indicate that the likelihood of being a tnc user increases with the increase in age for someone younger than 44 years, but the pattern is reversed post 44 years. this change in direction of the marginal effect of age is insightful as the previous studies have reported a negative association. we also find that postgraduate drivers who live in metropolitan regions are more likely to switch to fuel-efficient vehicles. these findings would inform transportation planners and tncs in developing policies to improve the fuel economy of the fleet.", "categories": "econ.gn q-fin.ec", "created": "2019-04-14", "updated": "", "authors": ["prateek bansal", "akanksha sinha", "rubal dua", "ricardo daziano"], "url": "https://arxiv.org/abs/1904.06695"}, {"title": "boomerang: rebounding the consequences of reputation feedback on   crowdsourcing platforms", "id": "1904.06722", "abstract": "paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. these inflated scores, which make high-quality work and workers difficult to find, stem from social pressure to avoid giving negative feedback. we introduce boomerang, a reputation system for crowdsourcing that elicits more accurate feedback by rebounding the consequences of feedback directly back onto the person who gave it. with boomerang, requesters find that their highly-rated workers gain earliest access to their future tasks, and workers find tasks from their highly-rated requesters at the top of their task feed. field experiments verify that boomerang causes both workers and requesters to provide feedback that is more closely aligned with their private opinions. inspired by a game-theoretic notion of incentive-compatibility, boomerang opens opportunities for interaction design to incentivize honest reporting over strategic dishonesty.", "categories": "cs.cy cs.hc econ.gn q-fin.ec", "created": "2019-04-14", "updated": "", "authors": ["n/a snehalkumar", "s. gaikwad", "durim morina", "adam ginzberg", "catherine mullings", "shirish goyal", "dilrukshi gamage", "christopher diemert", "mathias burton", "sharon zhou", "mark whiting", "karolina ziulkoski", "alipta ballav", "aaron gilbee", "senadhipathige s. niranga", "vibhor sehgal", "jasmine lin", "leonardy kristianto", "angela richmond-fuller", "jeff regino", "nalin chhibber", "dinesh majeti", "sachin sharma", "kamila mananova", "dinesh dhakal", "william dai", "victoria purynova", "samarth sandeep", "varshine chandrakanthan", "tejas sarma", "sekandar matin", "ahmed nasser", "rohit nistala", "alexander stolzoff", "kristy milland", "vinayak mathur", "rajan vaish", "michael s. bernstein"], "url": "https://arxiv.org/abs/1904.06722"}, {"title": "peer effects in random consideration sets", "id": "1904.06742", "abstract": "this paper develops a dynamic model of discrete choice that incorporates peer effects into random consideration sets. we characterize the equilibrium behavior and study the empirical content of the model. in our setup, changes in the choices of friends induce changes in the distribution of the consideration sets. we exploit this variation to recover the ranking of preferences, the attention mechanisms, and the network connections. these identification results allow unrestricted heterogeneity across people and do not rely on variation of either covariates or the set of available options. our methodology leads to a maximum-likelihood estimator that performs well in simulations.", "categories": "econ.em", "created": "2019-04-14", "updated": "2020-09-01", "authors": ["nail kashaev", "natalia lazzati"], "url": "https://arxiv.org/abs/1904.06742"}, {"title": "price setting on a network", "id": "1904.06757", "abstract": "most products are produced and sold by supply chain networks, where an interconnected network of producers and intermediaries set prices to maximize their profits. i show that there exists a unique equilibrium in a price-setting game on a network. the key distortion reducing both total profits and social welfare is multiple-marginalization, which is magnified by strategic interactions. individual profits are proportional to influentiality, which is a new measure of network centrality defined by the equilibrium characterization. the results emphasize the importance of the network structure when considering policy questions such as mergers or trade tariffs.", "categories": "cs.gt econ.gn econ.th q-fin.ec", "created": "2019-04-14", "updated": "", "authors": ["toomas hinnosaar"], "url": "https://arxiv.org/abs/1904.06757"}, {"title": "estimation of cross-sectional dependence in large panels", "id": "1904.06843", "abstract": "accurate estimation for extent of cross{sectional dependence in large panel data analysis is paramount to further statistical analysis on the data under study. grouping more data with weak relations (cross{sectional dependence) together often results in less efficient dimension reduction and worse forecasting. this paper describes cross-sectional dependence among a large number of objects (time series) via a factor model and parameterizes its extent in terms of strength of factor loadings. a new joint estimation method, benefiting from unique feature of dimension reduction for high dimensional time series, is proposed for the parameter representing the extent and some other parameters involved in the estimation procedure. moreover, a joint asymptotic distribution for a pair of estimators is established. simulations illustrate the effectiveness of the proposed estimation method in the finite sample performance. applications in cross-country macro-variables and stock returns from s&p 500 are studied.", "categories": "econ.em stat.me", "created": "2019-04-15", "updated": "", "authors": ["jiti gao", "guangming pan", "yanrong yang", "bo zhang"], "url": "https://arxiv.org/abs/1904.06843"}, {"title": "subgeometrically ergodic autoregressions", "id": "1904.07089", "abstract": "in this paper we discuss how the notion of subgeometric ergodicity in markov chain theory can be exploited to study stationarity and ergodicity of nonlinear time series models. subgeometric ergodicity means that the transition probability measures converge to the stationary measure at a rate slower than geometric. specifically, we consider suitably defined higher-order nonlinear autoregressions that behave similarly to a unit root process for large values of the observed series but we place almost no restrictions on their dynamics for moderate values of the observed series. results on the subgeometric ergodicity of nonlinear autoregressions have previously appeared only in the first-order case. we provide an extension to the higher-order case and show that the autoregressions we consider are, under appropriate conditions, subgeometrically ergodic. as useful implications we also obtain stationarity and $\\beta$-mixing with subgeometrically decaying mixing coefficients.", "categories": "econ.em math.pr math.st stat.th", "created": "2019-04-15", "updated": "2020-03-06", "authors": ["mika meitz", "pentti saikkonen"], "url": "https://arxiv.org/abs/1904.07089"}, {"title": "subgeometric ergodicity and $\\beta$-mixing", "id": "1904.07103", "abstract": "it is well known that stationary geometrically ergodic markov chains are $\\beta$-mixing (absolutely regular) with geometrically decaying mixing coefficients. furthermore, for initial distributions other than the stationary one, geometric ergodicity implies $\\beta$-mixing under suitable moment assumptions. in this note we show that similar results hold also for subgeometrically ergodic markov chains. in particular, for both stationary and other initial distributions, subgeometric ergodicity implies $\\beta$-mixing with subgeometrically decaying mixing coefficients. although this result is simple it should prove very useful in obtaining rates of mixing in situations where geometric ergodicity can not be established. to illustrate our results we derive new subgeometric ergodicity and $\\beta$-mixing results for the self-exciting threshold autoregressive model.", "categories": "econ.em math.pr math.st stat.th", "created": "2019-04-15", "updated": "2019-04-16", "authors": ["mika meitz", "pentti saikkonen"], "url": "https://arxiv.org/abs/1904.07103"}, {"title": "on the construction of confidence intervals for ratios of expectations", "id": "1904.07111", "abstract": "in econometrics, many parameters of interest can be written as ratios of expectations. the main approach to construct confidence intervals for such parameters is the delta method. however, this asymptotic procedure yields intervals that may not be relevant for small sample sizes or, more generally, in a sequence-of-model framework that allows the expectation in the denominator to decrease to $0$ with the sample size. in this setting, we prove a generalization of the delta method for ratios of expectations and the consistency of the nonparametric percentile bootstrap. we also investigate finite-sample inference and show a partial impossibility result: nonasymptotic uniform confidence intervals can be built for ratios of expectations but not at every level. based on this, we propose an easy-to-compute index to appraise the reliability of the intervals based on the delta method. simulations and an application illustrate our results and the practical usefulness of our rule of thumb.", "categories": "math.st econ.em stat.me stat.th", "created": "2019-04-10", "updated": "", "authors": ["alexis derumigny", "lucas girard", "yannick guyonvarch"], "url": "https://arxiv.org/abs/1904.07111"}, {"title": "equilibria in a large production economy with an infinite dimensional   commodity space and price dependent preferences", "id": "1904.07444", "abstract": "we prove the existence of a competitive equilibrium in a production economy with infinitely many commodities and a measure space of agents whose preferences are price dependent. we employ a saturated measure space for the set of agents and apply recent results for an infinite dimensional separable banach space such as lyapunov's convexity theorem and an exact fatou's lemma to obtain the result.", "categories": "econ.th math.oc", "created": "2019-04-15", "updated": "2020-02-04", "authors": ["hyo seok jang", "sangjik lee"], "url": "https://arxiv.org/abs/1904.07444"}, {"title": "optimal mechanism for the sale of a durable good", "id": "1904.07456", "abstract": "we show that posted prices are the optimal mechanism to sell a durable good to a privately informed buyer when the seller has limited commitment in an infinite horizon setting. we provide a methodology for mechanism design with limited commitment and transferable utility. whereas in the case of commitment, subject to the buyer's truthtelling and participation constraints, the seller's problem is a decision problem, in the case of limited commitment, the seller's problem corresponds to an intrapersonal game, where different \"incarnations\" of the seller represent the different beliefs he may have about the buyer's valuation.", "categories": "econ.th", "created": "2019-04-16", "updated": "2020-08-25", "authors": ["laura doval", "vasiliki skreta"], "url": "https://arxiv.org/abs/1904.07456"}, {"title": "consumer privacy and serial monopoly", "id": "1904.07644", "abstract": "we examine the implications of consumer privacy when preferences today depend upon past consumption choices, and consumers shop from different sellers in each period. although consumers are ex ante identical, their initial consumption choices cannot be deterministic. thus ex post heterogeneity in preferences arises endogenously. consumer privacy improves social welfare, consumer surplus and the profits of the second-period seller, while reducing the profits of the first period seller, relative to the situation where consumption choices are observed by the later seller.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-04-16", "updated": "", "authors": ["v. bhaskar", "nikita roketskiy"], "url": "https://arxiv.org/abs/1904.07644"}, {"title": "multiple-interaction kinetic modelling of a virtual-item gambling   economy", "id": "1904.07660", "abstract": "in recent years, there has been a proliferation of online gambling sites, which made gambling more accessible with a consequent rise in related problems, such as addiction. hence, the analysis of the gambling behaviour at both the individual and the aggregate levels has become the object of several investigations. in this paper, resorting to classical methods of the kinetic theory, we describe the behaviour of a multi-agent system of gamblers participating in lottery-type games on a virtual-item gambling market. the comparison with previous, often empirical, results highlights the ability of the kinetic approach to explain how the simple microscopic rules of a gambling-type game produce complex collective trends, which might be difficult to interpret precisely by looking only at the available data.", "categories": "physics.soc-ph econ.gn nlin.ao q-fin.ec", "created": "2019-04-16", "updated": "", "authors": ["giuseppe toscani", "andrea tosin", "mattia zanella"], "url": "https://arxiv.org/abs/1904.07660"}, {"title": "p\\'olygamma data augmentation to address non-conjugacy in the bayesian   estimation of mixed multinomial logit models", "id": "1904.07688", "abstract": "the standard gibbs sampler of mixed multinomial logit (mmnl) models involves sampling from conditional densities of utility parameters using metropolis-hastings (mh) algorithm due to unavailability of conjugate prior for logit kernel. to address this non-conjugacy concern, we propose the application of p\\'olygamma data augmentation (pg-da) technique for the mmnl estimation. the posterior estimates of the augmented and the default gibbs sampler are similar for two-alternative scenario (binary choice), but we encounter empirical identification issues in the case of more alternatives ($j \\geq 3$).", "categories": "stat.ml cs.lg econ.em stat.ap", "created": "2019-04-13", "updated": "", "authors": ["prateek bansal", "rico krueger", "michel bierlaire", "ricardo a. daziano", "taha h. rashidi"], "url": "https://arxiv.org/abs/1904.07688"}, {"title": "can mobility-on-demand services do better after discerning reliability   preferences of riders?", "id": "1904.07987", "abstract": "we formalize one aspect of reliability in the context of mobility-on-demand (mod) systems by acknowledging the uncertainty in the pick-up time of these services. this study answers two key questions: i) how the difference between the stated and actual pick-up times affect the propensity of a passenger to choose an mod service? ii) how an mod service provider can leverage this information to increase its ridership? we conduct a discrete choice experiment in new york to answer the former question and adopt a micro-simulation-based optimization method to answer the latter question. in our experiments, the ridership of an mod service could be increased by up to 10\\% via displaying the predicted wait time strategically.", "categories": "econ.gn q-fin.ec", "created": "2019-04-16", "updated": "", "authors": ["prateek bansal", "yang liu", "ricardo daziano", "samitha samaranayake"], "url": "https://arxiv.org/abs/1904.07987"}, {"title": "averaging plus learning in financial markets", "id": "1904.08131", "abstract": "this paper develops original models to study interacting agents in financial markets. the key feature of these models is how interactions are formulated and analysed. agents learn from their observations and learning ability to interpret news or private information. central limit theorems are developed but they arise rather unexpectedly. under certain type of conditions governing the learning, agents beliefs converge in distribution that can be even fractal. the underlying randomness in the systems is not restricted to be of a certain class. fresh insights are gained not only from developing new non-linear social learning models but also from using different techniques to study discrete time random linear dynamical systems.", "categories": "q-fin.mf econ.th", "created": "2019-04-17", "updated": "2019-06-04", "authors": ["ionel popescu", "tushar vaidya"], "url": "https://arxiv.org/abs/1904.08131"}, {"title": "a pyramid scheme model based on \"consumption rebate\" frauds", "id": "1904.08136", "abstract": "there are various types of pyramid schemes which have inflicted or are inflicting losses on many people in the world. we propose a pyramid scheme model which has the principal characters of many pyramid schemes appeared in recent years: promising high returns, rewarding the participants recruiting the next generation of participants, and the organizer will take all the money away when he finds the money from the new participants is not enough to pay the previous participants interest and rewards. we assume the pyramid scheme carries on in the tree network, er random network, sw small-world network or ba scale-free network respectively, then give the analytical results of how many generations the pyramid scheme can last in these cases. we also use our model to analyse a pyramid scheme in the real world and we find the connections between participants in the pyramid scheme may constitute a sw small-world network.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-04-17", "updated": "2019-06-26", "authors": ["yong shi", "bo li", "wen long"], "url": "https://arxiv.org/abs/1904.08136"}, {"title": "a generalized continuous-multinomial response model with a t-distributed   error kernel", "id": "1904.08332", "abstract": "in multinomial response models, idiosyncratic variations in the indirect utility are generally modeled using gumbel or normal distributions. this study makes a strong case to substitute these thin-tailed distributions with a t-distribution. first, we demonstrate that a model with a t-distributed error kernel better estimates and predicts preferences, especially in class-imbalanced datasets. our proposed specification also implicitly accounts for decision-uncertainty behavior, i.e. the degree of certainty that decision-makers hold in their choices relative to the variation in the indirect utility of any alternative. second, after applying a t-distributed error kernel in a multinomial response model for the first time, we extend this specification to a generalized continuous-multinomial (gcm) model and derive its full-information maximum likelihood estimator. the likelihood involves an open-form expression of the cumulative density function of the multivariate t-distribution, which we propose to compute using a combination of the composite marginal likelihood method and the separation-of-variables approach. third, we establish finite sample properties of the gcm model with a t-distributed error kernel (gcm-t) and highlight its superiority over the gcm model with a normally-distributed error kernel (gcm-n) in a monte carlo study. finally, we compare gcm-t and gcm-n in an empirical setting related to preferences for electric vehicles (evs). we observe that accounting for decision-uncertainty behavior in gcm-t results in lower elasticity estimates and a higher willingness to pay for improving the ev attributes than those of the gcm-n model. these differences are relevant in making policies to expedite the adoption of evs.", "categories": "econ.em", "created": "2019-04-17", "updated": "2020-01-18", "authors": ["subodh dubey", "prateek bansal", "ricardo a. daziano", "erick guerra"], "url": "https://arxiv.org/abs/1904.08332"}, {"title": "sharp bounds for the marginal treatment effect with sample selection", "id": "1904.08522", "abstract": "i analyze treatment effects in situations when agents endogenously select into the treatment group and into the observed sample. as a theoretical contribution, i propose pointwise sharp bounds for the marginal treatment effect (mte) of interest within the always-observed subpopulation under monotonicity assumptions. moreover, i impose an extra mean dominance assumption to tighten the previous bounds. i further discuss how to identify those bounds when the support of the propensity score is either continuous or discrete. using these results, i estimate bounds for the mte of the job corps training program on hourly wages for the always-employed subpopulation and find that it is decreasing in the likelihood of attending the program within the non-hispanic group. for example, the average treatment effect on the treated is between \\$.33 and \\$.99 while the average treatment effect on the untreated is between \\$.71 and \\$3.00.", "categories": "econ.em econ.gn q-fin.ec stat.ap", "created": "2019-04-17", "updated": "", "authors": ["vitor possebom"], "url": "https://arxiv.org/abs/1904.08522"}, {"title": "asymptotic behavior of bayesian learners with misspecified models", "id": "1904.08551", "abstract": "we consider an agent who represents uncertainty about the environment via a possibly misspecified model. each period, the agent takes an action, observes a consequence, and uses bayes' rule to update her belief about the environment. this framework has become increasingly popular in economics to study behavior driven by incorrect or biased beliefs. current literature has characterized asymptotic behavior under fairly specific assumptions. by first showing that the key element to predict the agent's behavior is the frequency of her past actions, we are able to characterize asymptotic behavior in general settings in terms of the solutions of a generalization of a differential equation that describes the evolution of the frequency of actions. we then present a series of implications that can be readily applied to economic applications, thus providing off-the-shelf tools that can be used to characterize behavior under misspecified learning.", "categories": "econ.th math.st stat.th", "created": "2019-04-17", "updated": "2019-10-22", "authors": ["ignacio esponda", "demian pouzo", "yuichi yamamoto"], "url": "https://arxiv.org/abs/1904.08551"}, {"title": "ridge regularization for mean squared error reduction in regression with   weak instruments", "id": "1904.08580", "abstract": "in this paper, i show that classic two-stage least squares (2sls) estimates are highly unstable with weak instruments. i propose a ridge estimator (ridge iv) and show that it is asymptotically normal even with weak instruments, whereas 2sls is severely distorted and un-bounded. i motivate the ridge iv estimator as a convex optimization problem with a gmm objective function and an l2 penalty. i show that ridge iv leads to sizable mean squared error reductions theoretically and validate these results in a simulation study inspired by data designs of papers published in the american economic review.", "categories": "econ.em math.st stat.th", "created": "2019-04-17", "updated": "", "authors": ["karthik rajkumar"], "url": "https://arxiv.org/abs/1904.08580"}, {"title": "location-sector analysis of international profit shifting on a   multilayer ownership-tax network", "id": "1904.09165", "abstract": "currently all countries including developing countries are expected to utilize their own tax revenues and carry out their own development for solving poverty in their countries. however, developing countries cannot earn tax revenues like developed countries partly because they do not have effective countermeasures against international tax avoidance. our analysis focuses on treaty shopping among various ways to conduct international tax avoidance because tax revenues of developing countries have been heavily damaged through treaty shopping. to analyze the location and sector of conduit firms likely to be used for treaty shopping, we constructed a multilayer ownership-tax network and proposed multilayer centrality. because multilayer centrality can consider not only the value owing in the ownership network but also the withholding tax rate, it is expected to grasp precisely the locations and sectors of conduit firms established for the purpose of treaty shopping. our analysis shows that firms in the sectors of finance & insurance and wholesale & retail trade etc. are involved with treaty shopping. we suggest that developing countries make a clause focusing on these sectors in the tax treaties they conclude.", "categories": "econ.em", "created": "2019-04-19", "updated": "", "authors": ["tembo nakamoto", "odile rouhban", "yuichi ikeda"], "url": "https://arxiv.org/abs/1904.09165"}, {"title": "average density estimators: efficiency and bootstrap consistency", "id": "1904.09372", "abstract": "this paper highlights a tension between semiparametric efficiency and bootstrap consistency in the context of a canonical semiparametric estimation problem. it is shown that although simple plug-in estimators suffer from bias problems preventing them from achieving semiparametric efficiency under minimal smoothness conditions, the nonparametric bootstrap automatically corrects for this bias and that, as a result, these seemingly inferior estimators achieve bootstrap consistency under minimal smoothness conditions. in contrast, \"debiased\" estimators that achieve semiparametric efficiency under minimal smoothness conditions do not achieve bootstrap consistency under those same conditions.", "categories": "econ.em math.st stat.th", "created": "2019-04-19", "updated": "", "authors": ["matias d. cattaneo", "michael jansson"], "url": "https://arxiv.org/abs/1904.09372"}, {"title": "limits to green growth and the dynamics of innovation", "id": "1904.09586", "abstract": "central to the official \"green growth\" discourse is the conjecture that absolute decoupling can be achieved with certain market instruments. this paper evaluates this claim focusing on the role of technology, while changes in gdp composition are treated elsewhere. some fundamental difficulties for absolute decoupling, referring specifically to thermodynamic costs, are identified through a stylized model based on empirical knowledge on innovation and learning. normally, monetary costs decrease more slowly than production grows, and this is unlikely to change should monetary costs align with thermodynamic costs, except, potentially, in the transition after the price reform. furthermore, thermodynamic efficiency must eventually saturate for physical reasons. while this model, as usual, introduces technological innovation just as a source of efficiency, innovation also creates challenges: therefore, attempts to sustain growth by ever-accelerating innovation collide also with the limited reaction capacity of people and institutions. information technology could disrupt innovation dynamics in the future, permitting quicker gains in eco-efficiency, but only up to saturation and exacerbating the downsides of innovation. these observations suggest that long-term sustainability requires much deeper transformations than the green growth discourse presumes, exposing the need to rethink scales, tempos and institutions, in line with ecological economics and the degrowth literature.", "categories": "econ.th", "created": "2019-04-21", "updated": "2019-05-03", "authors": ["salvador pueyo"], "url": "https://arxiv.org/abs/1904.09586"}, {"title": "ict capital-skill complementarity and wage inequality: evidence from   oecd countries", "id": "1904.09857", "abstract": "although wage inequality has evolved in advanced countries over recent decades, it is unknown the extent to which the evolution of wage inequality is attributed to observed factors such as capital and labor quantities or unobserved factors such as labor-augmenting technology. to examine this issue, we estimate aggregate production functions extended to allow for capital-skill complementarity and skill-biased technological change using cross-country panel data and the shift-share instrument. our results indicate that most of the changes in the skill premium are attributed to observed factors including ict equipment in some major countries.", "categories": "econ.gn q-fin.ec", "created": "2019-04-22", "updated": "2020-05-25", "authors": ["hiroya taniguchi", "ken yamada"], "url": "https://arxiv.org/abs/1904.09857"}, {"title": "penney's game odds from no-arbitrage", "id": "1904.09888", "abstract": "penney's game is a two player zero-sum game in which each player chooses a three-flip pattern of heads and tails and the winner is the player whose pattern occurs first in repeated tosses of a fair coin. because the players choose sequentially, the second mover has the advantage. in fact, for any three-flip pattern, there is another three-flip pattern that is strictly more likely to occur first. this paper provides a novel no-arbitrage argument that generates the winning odds corresponding to any pair of distinct patterns. the resulting odds formula is equivalent to that generated by conway's \"leading number\" algorithm. the accompanying betting odds intuition adds insight into why conway's algorithm works. the proof is simple and easy to generalize to games involving more than two outcomes, unequal probabilities, and competing patterns of various length. additional results on the expected duration of penney's game are presented. code implementing and cross-validating the algorithms is included.", "categories": "math.oc econ.th", "created": "2019-03-28", "updated": "2019-04-23", "authors": ["joshua b. miller"], "url": "https://arxiv.org/abs/1904.09888"}, {"title": "investment in ev charging spots for parking", "id": "1904.09967", "abstract": "as demand for electric vehicles (evs) is expanding, meeting the need for charging infrastructure, especially in urban areas, has become a critical issue. one method of adding charging stations is to install them at parking spots. this increases the value of these spots to ev drivers needing to charge their vehicles. however, there is a cost to constructing these spots and such spots may preclude drivers not needing to charge from using them, reducing the parking options for such drivers\\color{black}. we look at two models for how decisions surrounding investment in charging stations on existing parking spots may be undertaken. first, we analyze two firms who compete over installing stations under government set mandates or subsidies. given the cost of constructing spots and the competitiveness of the markets, we find it is ambiguous whether setting higher mandates or higher subsidies for spot construction leads to better aggregate outcomes. second, we look at a system operator who faces uncertainty on the size of the ev market. if they are risk neutral, we find a relatively small change in the uncertainty of the ev market can lead to large changes in the optimal charging capacity.", "categories": "cs.gt econ.gn q-fin.ec", "created": "2019-04-22", "updated": "", "authors": ["brendan badia", "randall berry", "ermin wei"], "url": "https://arxiv.org/abs/1904.09967"}, {"title": "observing actions in bayesian games", "id": "1904.10744", "abstract": "we study bayesian coordination games where agents receive noisy private information over the game's payoff structure, and over each others' actions. if private information over actions is precise, we find that agents can coordinate on multiple equilibria. if private information over actions is of low quality, equilibrium uniqueness obtains like in a standard global games setting. the current model, with its flexible information structure, can thus be used to study phenomena such as bank-runs, currency crises, recessions, riots, and revolutions, where agents rely on information over each others' actions.", "categories": "econ.gn q-fin.ec", "created": "2019-04-24", "updated": "", "authors": ["dominik grafenhofer", "wolfgang kuhle"], "url": "https://arxiv.org/abs/1904.10744"}, {"title": "normal approximation in large network models", "id": "1904.11060", "abstract": "we develop a methodology for proving central limit theorems in network models with strategic interactions and homophilous agents. we consider an asymptotic framework in which the size of the network tends to infinity, which is useful for inference in the typical setting in which the sample consists of a single large network. in the presence of strategic interactions, network moments are generally complex functions of network components, where a node's component consists of all alters to which it is directly or indirectly connected. we find that a modification of \"exponential stabilization\" conditions from the stochastic geometry literature provides a useful formulation of weak dependence for moments of this type. our first contribution is to prove a clt for a large class of network moments satisfying stabilization and a moment condition. our second contribution is a methodology for deriving primitive sufficient conditions for stabilization using results in branching process theory. we apply the methodology to static and dynamic models of network formation and discuss how it can be used more broadly.", "categories": "econ.em math.st stat.th", "created": "2019-04-24", "updated": "2020-04-14", "authors": ["michael p. leung", "hyungsik roger moon"], "url": "https://arxiv.org/abs/1904.11060"}, {"title": "identification of regression models with a misclassified and endogenous   binary regressor", "id": "1904.11143", "abstract": "we study identification in nonparametric regression models with a misclassified and endogenous binary regressor when an instrument is correlated with misclassification error. we show that the regression function is nonparametrically identified if one binary instrument variable and one binary covariate that satisfy the following conditions are present. the instrumental variable (iv) corrects endogeneity; the iv must be correlated with the unobserved true underlying binary variable, must be uncorrelated with the error term in the outcome equation, and is allowed to be correlated with the misclassification error. the covariate corrects misclassification; this variable can be one of the regressors in the outcome equation, must be correlated with the unobserved true underlying binary variable, and must be uncorrelated with the misclassification error. we also propose a mixture-based framework for modeling unobserved heterogeneous treatment effects with a misclassified and endogenous binary regressor and show that treatment effects can be identified if the true treatment effect is related to an observed regressor and another observable variable.", "categories": "econ.em", "created": "2019-04-24", "updated": "", "authors": ["hiroyuki kasahara", "katsumi shimotsu"], "url": "https://arxiv.org/abs/1904.11143"}, {"title": "forecasting in big data environments: an adaptable and automated   shrinkage estimation of neural networks (aashnet)", "id": "1904.11145", "abstract": "this paper considers improved forecasting in possibly nonlinear dynamic settings, with high-dimension predictors (\"big data\" environments). to overcome the curse of dimensionality and manage data and model complexity, we examine shrinkage estimation of a back-propagation algorithm of a deep neural net with skip-layer connections. we expressly include both linear and nonlinear components. this is a high-dimensional learning approach including both sparsity l1 and smoothness l2 penalties, allowing high-dimensionality and nonlinearity to be accommodated in one step. this approach selects significant predictors as well as the topology of the neural network. we estimate optimal values of shrinkage hyperparameters by incorporating a gradient-based optimization technique resulting in robust predictions with improved reproducibility. the latter has been an issue in some approaches. this is statistically interpretable and unravels some network structure, commonly left to a black box. an additional advantage is that the nonlinear part tends to get pruned if the underlying process is linear. in an application to forecasting equity returns, the proposed approach captures nonlinear dynamics between equities to enhance forecast performance. it offers an appreciable improvement over current univariate and multivariate models by rmse and actual portfolio performance.", "categories": "econ.em cs.lg q-fin.st stat.ml", "created": "2019-04-24", "updated": "", "authors": ["ali habibnia", "esfandiar maasoumi"], "url": "https://arxiv.org/abs/1904.11145"}, {"title": "nonparametric estimation and inference in economic and psychological   experiments", "id": "1904.11156", "abstract": "the goal of this paper is to provide some tools for nonparametric estimation and inference in psychological and economic experiments. we consider an experimental framework in which each of $n$subjects provides $t$ responses to a vector of $t$ stimuli. we propose to estimate the unknown function $f$ linking stimuli to responses through a nonparametric sieve estimator. we give conditions for consistency when either $n$ or $t$ or both diverge. the rate of convergence depends upon the error covariance structure, that is allowed to differ across subjects. with these results we derive the optimal divergence rate of the dimension of the sieve basis with both $n$ and $t$. we provide guidance about the optimal balance between the number of subjects and questions in a laboratory experiment and argue that a large $n$is often better than a large $t$. we derive conditions for asymptotic normality of functionals of the estimator of $t$ and apply them to obtain the asymptotic distribution of the wald test when the number of constraints under the null is finite and when it diverges along with other asymptotic parameters. lastly, we investigate the previous properties when the conditional covariance matrix is replaced by an estimator.", "categories": "econ.em stat.ap", "created": "2019-04-25", "updated": "2019-12-06", "authors": ["raffaello seri", "samuele centorrino", "michele bernasconi"], "url": "https://arxiv.org/abs/1904.11156"}, {"title": "risk-neutral pricing for apt", "id": "1904.11252", "abstract": "we consider infinite dimensional optimization problems motivated by the financial model called arbitrage pricing theory. using probabilistic and functional analytic tools, we provide a dual characterization of the super-replication cost. then, we show the existence of optimal strategies for investors maximizing their expected utility and the convergence of their reservation prices to the super-replication cost as their risk-aversion tends to infinity.", "categories": "econ.gn q-fin.ec q-fin.mf", "created": "2019-04-25", "updated": "2020-10-02", "authors": ["laurence carassus", "miklos rasonyi"], "url": "https://arxiv.org/abs/1904.11252"}, {"title": "shared factory: a new production node for social manufacturing in the   context of sharing economy", "id": "1904.11377", "abstract": "manufacturing industry is heading towards socialization, interconnection, and platformization. motivated by the infiltration of sharing economy usage in manufacturing, this paper addresses a new factory model -- shared factory -- and provides a theoretical architecture and some actual cases for manufacturing sharing. concepts related to three kinds of shared factories which deal respectively with sharing production-orders, manufacturing-resources and manufacturing-capabilities, are defined accordingly. these three kinds of shared factory modes can be used for building correspondent sharing manufacturing ecosystems. on the basis of sharing economic analysis, we identify feasible key enabled technologies for configuring and running a shared factory. at the same time, opportunities and challenges of enabling the shared factory are also analyzed in detail. in fact, shared factory, as a new production node, enhances the sharing nature of social manufacturing paradigm, fits the needs of light assets and gives us a new chance to use socialized manufacturing resources. it can be drawn that implementing a shared factory would reach a win-win way through production value-added transformation and social innovation.", "categories": "econ.gn q-fin.ec", "created": "2019-04-12", "updated": "", "authors": ["pingyu jiang", "pulin li"], "url": "https://arxiv.org/abs/1904.11377"}, {"title": "multiple benefits through smart home energy management solutions -- a   simulation-based case study of a single-family house in algeria and germany", "id": "1904.11496", "abstract": "from both global and local perspectives, there are strong reasons to promote energy efficiency. these reasons have prompted leaders in the european union (eu) and countries of the middle east and north africa (mena) to adopt policies to move their citizenry toward more efficient energy consumption. energy efficiency policy is typically framed at the national, or transnational level. policy makers then aim to incentivize microeconomic actors to align their decisions with macroeconomic policy. we suggest another path towards greater energy efficiency: highlighting individual benefits at microeconomic level. by simulating lighting, heating and cooling operations in a model single-family home equipped with modest automation, we show that individual actors can be led to pursue energy efficiency out of enlightened self-interest. we apply simple-to-use, easily, scalable impact indicators that can be made available to homeowners and serve as intrinsic economic, environmental and social motivators for pursuing energy efficiency. the indicators reveal tangible homeowner benefits realizable under both the market-based pricing structure for energy in germany and the state-subsidized pricing structure in algeria. benefits accrue under both the continental climate regime of germany and the mediterranean regime of algeria, notably in the case that cooling energy needs are considered. our findings show that smart home technology provides an attractive path for advancing energy efficiency goals. the indicators we assemble can help policy makers both to promote tangible benefits of energy efficiency to individual homeowners, and to identify those investments of public funds that best support individual pursuit of national and transnational energy goals.", "categories": "econ.gn cs.ni q-fin.ec", "created": "2019-04-25", "updated": "", "authors": ["marc ringel", "roufaida laidi", "djamel djenouri"], "url": "https://arxiv.org/abs/1904.11496"}, {"title": "a game theoretic setting of capitation versus fee-for-service payment   systems", "id": "1904.11604", "abstract": "we aim to determine whether a game-theoretic model between an insurer and a healthcare practice yields a predictive equilibrium that incentivizes either player to deviate from a fee-for-service to capitation payment system. using united states data from various primary care surveys, we find that non-extreme equilibria (i.e., shares of patients, or shares of patient visits, seen under a fee-for-service payment system) can be derived from a stackelberg game if insurers award a non-linear bonus to practices based on performance. overall, both insurers and practices can be incentivized to embrace capitation payments somewhat, but potentially at the expense of practice performance.", "categories": "econ.gn q-fin.ec", "created": "2019-04-25", "updated": "2019-09-30", "authors": ["allison koenecke"], "url": "https://arxiv.org/abs/1904.11604"}, {"title": "a multicriteria decision making approach to study the barriers to the   adoption of autonomous vehicles", "id": "1904.12051", "abstract": "the automation technology is emerging, but the adoption rate of autonomous vehicles (av) will largely depend upon how policymakers and the government address various challenges such as public acceptance and infrastructure development. this study proposes a five-step method to understand these barriers to av adoption. first, based on a literature review followed by discussions with experts, ten barriers are identified. second, the opinions of eighteen experts from industry and academia regarding inter-relations between these barriers are recorded. third, a multicriteria decision making (mcdm) technique, the grey-based decision-making trial and evaluation laboratory (grey-dematel), is applied to characterize the structure of relationships between the barriers. fourth, robustness of the results is tested using sensitivity analysis. fifth, the key results are depicted in a causal loop diagram (cld), a systems thinking approach, to comprehend cause-and-effect relationships between the barriers. the results indicate that the lack of customer acceptance (lca) is the most prominent barrier, the one which should be addressed at the highest priority. the cld suggests that lca can be rather mitigated by addressing two other prominent, yet more tangible, barriers -- lack of industry standards and the absence of regulations and certifications. the study's overarching contribution thus lies in bringing to fore multiple barriers to av adoption and their potential influences on each other. moreover, the insights from this study can help associations related to avs prioritize their endeavors to expedite av adoption. from the methodological perspective, this is the first study in transportation literature that integrates grey-dematel with systems thinking.", "categories": "econ.gn q-fin.ec", "created": "2019-04-26", "updated": "2019-12-27", "authors": ["alok raj", "j ajith kumar", "prateek bansal"], "url": "https://arxiv.org/abs/1904.12051"}, {"title": "the category of node-and-choice forms, with subcategories for   choice-sequence forms and choice-set forms", "id": "1904.12085", "abstract": "the literature specifies extensive-form games in many styles, and eventually i hope to formally translate games across those styles. toward that end, this paper defines $\\mathbf{ncf}$, the category of node-and-choice forms. the category's objects are extensive forms in essentially any style, and the category's isomorphisms are made to accord with the literature's small handful of ad hoc style equivalences.   further, this paper develops two full subcategories: $\\mathbf{csqf}$ for forms whose nodes are choice-sequences, and $\\mathbf{csetf}$ for forms whose nodes are choice-sets. i show that $\\mathbf{ncf}$ is \"isomorphically enclosed\" in $\\mathbf{csqf}$ in the sense that each $\\mathbf{ncf}$ form is isomorphic to a $\\mathbf{csqf}$ form. similarly, i show that $\\mathbf{csqf_{\\tilde a}}$ is isomorphically enclosed in $\\mathbf{csetf}$ in the sense that each $\\mathbf{csqf}$ form with no-absentmindedness is isomorphic to a $\\mathbf{csetf}$ form. the converses are found to be almost immediate, and the resulting equivalences unify and simplify two ad hoc style equivalences in kline and luckraz 2016 and streufert 2019.   aside from the larger agenda, this paper already makes three practical contributions. style equivalences are made easier to derive by [1] a natural concept of isomorphic invariance and [2] the composability of isomorphic enclosures. in addition, [3] some new consequences of equivalence are systematically deduced.", "categories": "econ.th cs.lo math.ct", "created": "2019-04-26", "updated": "", "authors": ["peter a. streufert"], "url": "https://arxiv.org/abs/1904.12085"}, {"title": "regulating ai: do we need new tools?", "id": "1904.12134", "abstract": "the artificial intelligence paradigm (hereinafter referred to as \"ai\") builds on the analysis of data able, among other things, to snap pictures of the individuals' behaviors and preferences. such data represent the most valuable currency in the digital ecosystem, where their value derives from their being a fundamental asset in order to train machines with a view to developing ai applications. in this environment, online providers attract users by offering them services for free and getting in exchange data generated right through the usage of such services. this swap, characterized by an implicit nature, constitutes the focus of the present paper, in the light of the disequilibria, as well as market failures, that it may bring about. we use mobile apps and the related permission system as an ideal environment to explore, via econometric tools, those issues. the results, stemming from a dataset of over one million observations, show that both buyers and sellers are aware that access to digital services implicitly implies an exchange of data, although this does not have a considerable impact neither on the level of downloads (demand), nor on the level of the prices (supply). in other words, the implicit nature of this exchange does not allow market indicators to work efficiently. we conclude that current policies (e.g. transparency rules) may be inherently biased and we put forward suggestions for a new approach.", "categories": "econ.gn cs.ai q-fin.ec", "created": "2019-04-27", "updated": "", "authors": ["otello ardovino", "jacopo arpetti", "marco delmastro"], "url": "https://arxiv.org/abs/1904.12134"}, {"title": "identification of key companies for international profit shifting in the   global ownership network", "id": "1904.12397", "abstract": "in the global economy, the intermediate companies owned by multinational corporations are becoming an important policy issue as they are likely to cause international profit shifting and diversion of foreign direct investments. the purpose of this analysis is to call the intermediate companies with high risk of international profit shifting as key firms and to identifying and clarify them. for this aim, we propose a model that focuses on each affiliate's position on the ownership structure of each multinational corporation. based on the information contained in the orbis database, we constructed the global ownership network, reflecting the relationship that can give significant influence to a firm, and analyzed for large multinational corporations listed in fortune global 500. in this analysis, first, we confirmed the validity of this model by identifying affiliates playing an important role in international tax avoidance at a certain degree. secondly, intermediate companies are mainly found in the netherlands and the united kingdom, etc., and tended to be located in jurisdictions favorable to treaty shopping. and it was found that such key firms are concentrated on the in component of the bow-tie structure that the giant weakly connected component of the global ownership network consist of. therefore, it clarifies that the key firms are geographically located in specific jurisdictions, and concentrates on specific components in the global ownership network. the location of key firms are related with the ease of treaty shopping, and there is a difference in the jurisdiction where key firms are located depending on the location of the multinational corporations.", "categories": "econ.gn q-fin.ec", "created": "2019-04-28", "updated": "", "authors": ["tembo nakamoto", "abhijit chakraborty", "yuichi ikeda"], "url": "https://arxiv.org/abs/1904.12397"}, {"title": "efficiency in truthful auctions via a social network", "id": "1904.12422", "abstract": "in this paper, we study efficiency in truthful auctions via a social network, where a seller can only spread the information of an auction to the buyers through the buyers' network. in single-item auctions, we show that no mechanism is strategy-proof, individually rational, efficient, and weakly budget balanced. in addition, we propose $\\alpha$-apg mechanisms, a class of mechanisms which operate a trade-off between efficiency and weakly budget balancedness. in multi-item auctions, there already exists a strategy-proof mechanism when all buyers need only one item. however, we indicate a counter-example to strategy-proofness in this mechanism, and to the best of our knowledge, the question of finding a strategy-proof mechanism remains open. we assume that all buyers have decreasing marginal utility and propose a generalized apg mechanism that is strategy-proof and individually rational but not efficient. importantly, we show that this mechanism achieves the largest efficiency measure among all strategy-proof mechanisms.", "categories": "econ.th cs.gt", "created": "2019-04-28", "updated": "", "authors": ["seiji takanashi", "takehiro kawasaki", "taiki todo", "makoto yokoo"], "url": "https://arxiv.org/abs/1904.12422"}, {"title": "exact testing of many moment inequalities against multiple violations", "id": "1904.12775", "abstract": "this paper considers the problem of testing many moment inequalities, where the number of moment inequalities ($p$) is possibly larger than the sample size ($n$). chernozhukov et al. (2019) proposed asymptotic tests for this problem using the maximum $t$ statistic. we observe that such tests can have low power if multiple inequalities are violated. as an alternative, we propose novel randomization tests based on a maximum non-negatively weighted combination of $t$ statistics. we provide a condition guaranteeing size control in large samples. simulations show that the tests control size in small samples ($n = 30$, $p = 1000$), and often has substantially higher power against alternatives with multiple violations than tests based on the maximum $t$ statistic.", "categories": "math.st econ.em stat.th", "created": "2019-04-29", "updated": "2020-06-08", "authors": ["nick koning", "paul bekker"], "url": "https://arxiv.org/abs/1904.12775"}, {"title": "fast mesh refinement in pseudospectral optimal control", "id": "1904.12992", "abstract": "mesh refinement in pseudospectral (ps) optimal control is embarrassingly easy --- simply increase the order $n$ of the lagrange interpolating polynomial and the mathematics of convergence automates the distribution of the grid points. unfortunately, as $n$ increases, the condition number of the resulting linear algebra increases as $n^2$; hence, spectral efficiency and accuracy are lost in practice. in this paper, we advance birkhoff interpolation concepts over an arbitrary grid to generate well-conditioned ps optimal control discretizations. we show that the condition number increases only as $\\sqrt{n}$ in general, but is independent of $n$ for the special case of one of the boundary points being fixed. hence, spectral accuracy and efficiency are maintained as $n$ increases. the effectiveness of the resulting fast mesh refinement strategy is demonstrated by using \\underline{polynomials of over a thousandth order} to solve a low-thrust, long-duration orbit transfer problem.", "categories": "math.oc cs.ce cs.na econ.em", "created": "2019-04-29", "updated": "", "authors": ["n. koeppen", "i. m. ross", "l. c. wilcox", "r. j. proulx"], "url": "https://arxiv.org/abs/1904.12992"}, {"title": "a factor-augmented markov switching (fams) model", "id": "1904.13194", "abstract": "this paper investigates the role of high-dimensional information sets in the context of markov switching models with time varying transition probabilities. markov switching models are commonly employed in empirical macroeconomic research and policy work. however, the information used to model the switching process is usually limited drastically to ensure stability of the model. increasing the number of included variables to enlarge the information set might even result in decreasing precision of the model. moreover, it is often not clear a priori which variables are actually relevant when it comes to informing the switching behavior. building strongly on recent contributions in the field of factor analysis, we introduce a general type of markov switching autoregressive models for non-linear time series analysis. large numbers of time series are allowed to inform the switching process through a factor structure. this factor-augmented markov switching (fams) model overcomes estimation issues that are likely to arise in previous assessments of the modeling framework. more accurate estimates of the switching behavior as well as improved model fit result. the performance of the fams model is illustrated in a simulated data example as well as in an us business cycle application.", "categories": "econ.em stat.ap", "created": "2019-04-30", "updated": "2019-05-03", "authors": ["gregor zens", "maximilian b\u00f6ck"], "url": "https://arxiv.org/abs/1904.13194"}, {"title": "tax mechanisms and gradient flows", "id": "1904.13276", "abstract": "we demonstrate how a static optimal income taxation problem can be analyzed using dynamical methods. specifically, we show that the taxation problem is intimately connected to the heat equation. our first result is a new property of the optimal tax which we call the fairness principle. the optimal tax at any income is invariant under a family of properly adjusted gaussian averages (the heat kernel) of the optimal taxes at other incomes. that is, the optimal tax at a given income is equal to the weighted by the heat kernels average of optimal taxes at other incomes and income densities. moreover, this averaging happens at every scale tightly linked to each other providing a unified weighting scheme at all income ranges. the fairness principle arises not due to equality considerations but rather it represents an efficient way to smooth the burden of taxes and generated revenues across incomes. just as nature wants to distribute heat evenly, the optimal way for a government to raise revenues is to distribute the tax burden and raised revenues evenly among individuals. we then construct a gradient flow of taxes -- a dynamic process changing the existing tax system in the direction of the increase in tax revenues -- and show that it takes the form of a heat equation. the fairness principle holds also for the short-term asymptotics of the gradient flow, where the averaging is done over the current taxes. the gradient flow we consider can be viewed as a continuous process of a reform of the nonlinear income tax schedule and thus unifies the variational approach to taxation and optimal taxation. we present several other characteristics of the gradient flow focusing on its smoothing properties.", "categories": "econ.th", "created": "2019-04-30", "updated": "", "authors": ["stefan steinerberger", "aleh tsyvinski"], "url": "https://arxiv.org/abs/1904.13276"}, {"title": "supervised machine learning for eliciting individual demand", "id": "1904.13329", "abstract": "direct elicitation, guided by theory, is the standard method for eliciting latent preferences. the canonical direct-elicitation approach for measuring individuals' valuations for goods is the becker-degroot-marschak procedure, which generates willingness-to-pay (wtp) values that are imprecise and systematically biased by understating valuations. we show that enhancing elicited wtp values with supervised machine learning (sml) can substantially improve estimates of peoples' out-of-sample purchase behavior. furthermore, swapping wtp data with choice data generated from a simple task, two-alternative forced choice, leads to comparable performance. combining all the data with the best-performing sml methods yields large improvements in predicting out-of-sample purchases. we quantify the benefit of using various sml methods in conjunction with using different types of data. our results suggest that prices set by sml would increase revenue by 28% over using the stated wtp, with the same data.", "categories": "econ.gn q-fin.ec", "created": "2019-04-30", "updated": "2020-08-24", "authors": ["john a. clithero", "jae joon lee", "joshua tasoff"], "url": "https://arxiv.org/abs/1904.13329"}, {"title": "boosting: why you can use the hp filter", "id": "1905.00175", "abstract": "the hodrick-prescott (hp) filter is one of the most widely used econometric methods in applied macroeconomic research. like all nonparametric methods, the hp filter depends critically on a tuning parameter that controls the degree of smoothing. yet in contrast to modern nonparametric methods and applied work with these procedures, empirical practice with the hp filter almost universally relies on standard settings for the tuning parameter that have been suggested largely by experimentation with macroeconomic data and heuristic reasoning. as recent research (phillips and jin, 2015) has shown, standard settings may not be adequate in removing trends, particularly stochastic trends, in economic data.   this paper proposes an easy-to-implement practical procedure of iterating the hp smoother that is intended to make the filter a smarter smoothing device for trend estimation and trend elimination. we call this iterated hp technique the boosted hp filter in view of its connection to $l_{2}$-boosting in machine learning. the paper develops limit theory to show that the boosted hp (bhp) filter asymptotically recovers trend mechanisms that involve unit root processes, deterministic polynomial drifts, and polynomial drifts with structural breaks. a stopping criterion is used to automate the iterative hp algorithm, making it a data-determined method that is ready for modern data-rich environments in economic research. the methodology is illustrated using three real data examples that highlight the differences between simple hp filtering, the data-determined boosted filter, and an alternative autoregressive approach. these examples show that the bhp filter is helpful in analyzing a large collection of heterogeneous macroeconomic time series that manifest various degrees of persistence, trend behavior, and volatility.", "categories": "econ.em stat.ml", "created": "2019-04-30", "updated": "2019-11-08", "authors": ["peter c. b. phillips", "zhentao shi"], "url": "https://arxiv.org/abs/1905.00175"}, {"title": "compactification of extensive forms and belief in the opponents' future   rationality", "id": "1905.00355", "abstract": "we introduce an operation, called compactification, to reduce an extensive form to a compact one where each decision node in the game tree can be assigned to more than one player. motivated by thompson (1952)'s interchange of decision nodes, we attempt to capture the notion of a faithful representation of the chronological order of the moves in a dynamic game which plays a vital role in fields like epistemic game theory. the compactification process preserves perfect recall and the unambiguity of the order among information sets. we specify an algorithm, called leaves-to-root process, which compactifies at least as many information sets as any other compactification process. the compact extensive form provides an approach to avoid problems in dynamic game theory due to the vague definition of the chronological order of the moves, for example, belief in the opponents' future rationality (perea (2014))'s sensitivity to the specific extensive form representation. we show that any strategy which can rationally be chosen under common belief in future rationality in a minimal compact game if and only if it satisfies this property in every extensive form game which is related to it via some compactification process.", "categories": "econ.th", "created": "2019-05-01", "updated": "2019-05-02", "authors": ["shuige liu"], "url": "https://arxiv.org/abs/1905.00355"}, {"title": "matching for the israeli \"mechinot\" gap-year programs: handling rich   diversity requirements", "id": "1905.00364", "abstract": "we describe our experience with designing and running a matching market for the israeli \"mechinot\" gap-year programs. the main conceptual challenge in the design of this market was the rich set of diversity considerations, which necessitated the development of an appropriate preference-specification language along with corresponding choice-function semantics, which we also theoretically analyze. our contribution extends the existing toolbox for two-sided matching with soft constraints. this market was run for the first time in january 2018 and matched 1,607 candidates (out of a total of 3,120 candidates) to 35 different programs, has been run twice more since, and has been adopted by the joint council of the \"mechinot\" gap-year programs for the foreseeable future.", "categories": "cs.gt econ.th", "created": "2019-05-01", "updated": "2020-08-25", "authors": ["yannai a. gonczarowski", "lior kovalio", "noam nisan", "assaf romm"], "url": "https://arxiv.org/abs/1905.00364"}, {"title": "variational bayesian inference for mixed logit models with unobserved   inter- and intra-individual heterogeneity", "id": "1905.00419", "abstract": "variational bayes (vb), a method originating from machine learning, enables fast and scalable estimation of complex probabilistic models. thus far, applications of vb in discrete choice analysis have been limited to mixed logit models with unobserved inter-individual taste heterogeneity. however, such a model formulation may be too restrictive in panel data settings, since tastes may vary both between individuals as well as across choice tasks encountered by the same individual. in this paper, we derive a vb method for posterior inference in mixed logit models with unobserved inter- and intra-individual heterogeneity. in a simulation study, we benchmark the performance of the proposed vb method against maximum simulated likelihood (msl) and markov chain monte carlo (mcmc) methods in terms of parameter recovery, predictive accuracy and computational efficiency. the simulation study shows that vb can be a fast, scalable and accurate alternative to msl and mcmc estimation, especially in applications in which fast predictions are paramount. vb is observed to be between 2.8 and 17.7 times faster than the two competing methods, while affording comparable or superior accuracy. besides, the simulation study demonstrates that a parallelised implementation of the msl estimator with analytical gradients is a viable alternative to mcmc in terms of both estimation accuracy and computational efficiency, as the msl estimator is observed to be between 0.9 and 2.1 times faster than mcmc.", "categories": "stat.me econ.em", "created": "2019-05-01", "updated": "2020-01-16", "authors": ["rico krueger", "prateek bansal", "michel bierlaire", "ricardo a. daziano", "taha h. rashidi"], "url": "https://arxiv.org/abs/1905.00419"}, {"title": "data analytics in operations management: a review", "id": "1905.00556", "abstract": "research in operations management has traditionally focused on models for understanding, mostly at a strategic level, how firms should operate. spurred by the growing availability of data and recent advances in machine learning and optimization methodologies, there has been an increasing application of data analytics to problems in operations management. in this paper, we review recent applications of data analytics to operations management, in three major areas -- supply chain management, revenue management and healthcare operations -- and highlight some exciting directions for the future.", "categories": "econ.gn q-fin.ec", "created": "2019-05-01", "updated": "", "authors": ["velibor v. mi\u0161i\u0107", "georgia perakis"], "url": "https://arxiv.org/abs/1905.00556"}, {"title": "sparsity double robust inference of average treatment effects", "id": "1905.00744", "abstract": "many popular methods for building confidence intervals on causal effects under high-dimensional confounding require strong \"ultra-sparsity\" assumptions that may be difficult to validate in practice. to alleviate this difficulty, we here study a new method for average treatment effect estimation that yields asymptotically exact confidence intervals assuming that either the conditional response surface or the conditional probability of treatment allows for an ultra-sparse representation (but not necessarily both). this guarantee allows us to provide valid inference for average treatment effect in high dimensions under considerably more generality than available baselines. in addition, we showcase that our results are semi-parametrically efficient.", "categories": "math.st econ.em stat.me stat.th", "created": "2019-05-02", "updated": "", "authors": ["jelena bradic", "stefan wager", "yinchu zhu"], "url": "https://arxiv.org/abs/1905.00744"}, {"title": "the declining price anomaly is not universal in multi-buyer sequential   auctions (but almost is)", "id": "1905.00853", "abstract": "the declining price anomaly states that the price weakly decreases when multiple copies of an item are sold sequentially over time. the anomaly has been observed in a plethora of practical applications. on the theoretical side, gale and stegeman proved that the anomaly is guaranteed to hold in full information sequential auctions with exactly two buyers. we prove that the declining price anomaly is not guaranteed in full information sequential auctions with three or more buyers. this result applies to both first-price and second-price sequential auctions. moreover, it applies regardless of the tie-breaking rule used to generate equilibria in these sequential auctions. to prove this result we provide a refined treatment of subgame perfect equilibria that survive the iterative deletion of weakly dominated strategies and use this framework to experimentally generate a very large number of random sequential auction instances. in particular, our experiments produce an instance with three bidders and eight items that, for a specific tie-breaking rule, induces a non-monotonic price trajectory. theoretic analyses are then applied to show that this instance can be used to prove that for every possible tie-breaking rule there is a sequential auction on which it induces a non-monotonic price trajectory. on the other hand, our experiments show that non-monotonic price trajectories are extremely rare. in over six million experiments only a 0.000183 proportion of the instances violated the declining price anomaly.", "categories": "cs.gt econ.th", "created": "2019-05-02", "updated": "", "authors": ["vishnu v. narayan", "enguerrand prebet", "adrian vetta"], "url": "https://arxiv.org/abs/1905.00853"}, {"title": "a uniform bound on the operator norm of sub-gaussian random matrices and   its applications", "id": "1905.01096", "abstract": "for an $n \\times t$ random matrix $x(\\beta)$ with weakly dependent uniformly sub-gaussian entries $x_{it}(\\beta)$ that may depend on a possibly infinite-dimensional parameter $\\beta\\in \\bbf$, we obtain a uniform bound on its operator norm of the form $\\e \\sup_{\\beta \\in \\bbf} ||x(\\beta)|| \\leq ck \\left(\\sqrt{\\max(n,t)} + \\gamma_2(\\bbf,d_\\bbf)\\right)$, where $c$ is an absolute constant, $k$ controls the tail behavior of (the increments of) $x_{it}(\\cdot)$, and $\\gamma_2(\\bbf,d_\\bbf)$ is talagrand's functional, a measure of multi-scale complexity of the metric space $(\\bbf,d_\\bbf)$. we illustrate how this result may be used for estimation that seeks to minimize the operator norm of moment conditions as well as for estimation of the maximal number of factors with functional data.", "categories": "econ.em math.st stat.th", "created": "2019-05-03", "updated": "2020-10-02", "authors": ["grigory franguridi", "hyungsik roger moon"], "url": "https://arxiv.org/abs/1905.01096"}, {"title": "do informational cascades happen with non-myopic agents?", "id": "1905.01327", "abstract": "we consider an environment where players need to decide whether to buy a certain product (or adopt a technology) or not. the product is either good or bad but its true value is not known to the players. instead, each player has her own private information on its quality. each player can observe the previous actions of other players and estimate the quality of the product. a classic result in the literature shows that in similar settings information cascades occur where learning stops for the whole network and players repeat the actions of their predecessors. in contrast to the existing literature on informational cascades, in this work, players get more than one opportunity to act. in each turn, a player is chosen uniformly at random and can decide to buy the product and leave the market or to wait. we provide a characterization of structured perfect bayesian equilibria (spbe) with forward-looking strategies through a fixed-point equation of dimensionality that grows only quadratically with the number of players. in particular, a sufficient state for players' strategies at each time instance is a pair of two integers, the first corresponding to the estimated quality of the good and the second indicating the number of players that cannot offer additional information about the good to the rest of the players. based on this characterization we study informational cascades in two regimes. first, we show that for a discount factor strictly smaller than one, informational cascades happen with high probability as the number of players increases. furthermore, only a small portion of the total information in the system is revealed before a cascade occurs. secondly, and more surprisingly, we show that for a fixed number of players, as the discount factor approaches one, bad informational cascades are benign when the product is bad, and are completely eliminated when the discount factor equals one.", "categories": "econ.gn q-fin.ec", "created": "2019-05-03", "updated": "2020-03-09", "authors": ["ilai bistritz", "nasimeh heydaribeni", "achilleas anastasopoulos"], "url": "https://arxiv.org/abs/1905.01327"}, {"title": "evidence for gross domestic product growth time delay dependence over   foreign direct investment. a time-lag dependent correlation study", "id": "1905.01617", "abstract": "this paper considers an often forgotten relationship, the time delay between a cause and its effect in economies and finance. we treat the case of foreign direct investment (fdi) and economic growth, - measured through a country gross domestic product (gdp). the pertinent data refers to 43 countries, over 1970-2015, - for a total of 4278 observations. when countries are grouped   according to the inequality-adjusted human development index (ihdi), it is found that a time lag dependence effect exists in fdi-gdp correlations.   this is established through a time-dependent pearson 's product-moment correlation coefficient matrix.   moreover, such a pearson correlation coefficient is observed to evolve from positive   to negative values depending on the ihdi, from low to high. it is \"politically and policy   \"relevant\" that   the correlation is statistically significant providing the time lag is less than 3 years. a \"rank-size\" law is demonstrated.   it is recommended to reconsider such a time lag effect when discussing previous analyses whence conclusions on international business, and thereafter on forecasting.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2019-05-05", "updated": "", "authors": ["marcel ausloos", "ali eskandary", "parmjit kaur", "gurjeet dhesi"], "url": "https://arxiv.org/abs/1905.01617"}, {"title": "netflix games: local public goods with capacity constraints", "id": "1905.01693", "abstract": "this paper considers incentives to provide goods that are partially excludable along social links. individuals face a capacity constraint in that, conditional upon providing, they may nominate only a subset of neighbours as co-beneficiaries. our model has two typically incompatible ingredients: (i) a graphical game (individuals decide how much of the good to provide), and (ii) graph formation (individuals decide which subset of neighbours to nominate as co-beneficiaries). for any capacity constraints and any graph, we show the existence of specialised pure strategy nash equilibria - those in which some individuals (the drivers, d) contribute while the remaining individuals (the passengers, p) free ride. the proof is constructive and corresponds to showing, for a given capacity, the existence of a new kind of spanning bipartite subgraph, a dp-subgraph, with partite sets d and p. we consider how the number of drivers in equilibrium changes as the capacity constraints are relaxed and show a weak monotonicity result. finally, we introduce dynamics and show that only specialised equilibria are stable against individuals unilaterally changing their provision level.", "categories": "econ.th", "created": "2019-05-05", "updated": "2019-08-26", "authors": ["stefanie gerke", "gregory gutin", "sung-ha hwang", "philip neary"], "url": "https://arxiv.org/abs/1905.01693"}, {"title": "non-standard inference for augmented double autoregressive models with   null volatility coefficients", "id": "1905.01798", "abstract": "this paper considers an augmented double autoregressive (dar) model, which allows null volatility coefficients to circumvent the over-parameterization problem in the dar model. since the volatility coefficients might be on the boundary, the statistical inference methods based on the gaussian quasi-maximum likelihood estimation (gqmle) become non-standard, and their asymptotics require the data to have a finite sixth moment, which narrows applicable scope in studying heavy-tailed data. to overcome this deficiency, this paper develops a systematic statistical inference procedure based on the self-weighted gqmle for the augmented dar model. except for the lagrange multiplier test statistic, the wald, quasi-likelihood ratio and portmanteau test statistics are all shown to have non-standard asymptotics. the entire procedure is valid as long as the data is stationary, and its usefulness is illustrated by simulation studies and one real example.", "categories": "econ.em stat.me", "created": "2019-05-05", "updated": "", "authors": ["feiyu jiang", "dong li", "ke zhu"], "url": "https://arxiv.org/abs/1905.01798"}, {"title": "computing a data dividend", "id": "1905.01805", "abstract": "quality data is a fundamental contributor to success in statistics and machine learning. if a statistical assessment or machine learning leads to decisions that create value, data contributors may want a share of that value. this paper presents methods to assess the value of individual data samples, and of sets of samples, to apportion value among different data contributors. we use shapley values for individual samples and owen values for combined samples, and show that these values can be computed in polynomial time in spite of their definitions having numbers of terms that are exponential in the number of samples.", "categories": "cs.gt cs.cy econ.gn q-fin.ec stat.co", "created": "2019-05-05", "updated": "2019-06-27", "authors": ["eric bax"], "url": "https://arxiv.org/abs/1905.01805"}, {"title": "estimation of high-dimensional factor models and its application in   power data analysis", "id": "1905.02061", "abstract": "in dealing with high-dimensional data, factor models are often used for reducing dimensions and extracting relevant information. the spectrum of covariance matrices from power data exhibits two aspects: 1) bulk, which arises from random noise or fluctuations and 2) spikes, which represents factors caused by anomaly events. in this paper, we propose a new approach to the estimation of high-dimensional factor models, minimizing the distance between the empirical spectral density (esd) of covariance matrices of the residuals of power data that are obtained by subtracting principal components and the limiting spectral density (lsd) from a multiplicative covariance structure model. the free probability theory (fpt) is used to derive the spectral density of the multiplicative covariance model, which efficiently solves the computational difficulties. the proposed approach connects the estimation of the number of factors to the lsd of covariance matrices of the residuals, which provides estimators of the number of factors and the correlation structure information in the residuals. considering a lot of measurement noise is contained in the power data and the correlation structure is complex for the residuals, the approach prefers approaching the esd of covariance matrices of the residuals through a multiplicative covariance model, which avoids making crude assumptions or simplifications on the complex structure of the data. theoretical studies show the proposed approach is robust against noise and sensitive to the presence of weak factors. the synthetic data from ieee 118-bus power system is used to validate the effectiveness of the approach. furthermore, the application to the analysis of the real-world online monitoring data in a power grid shows that the estimators in the approach can be used to indicate the system behavior.", "categories": "stat.ap econ.em eess.sp", "created": "2019-05-06", "updated": "2019-10-19", "authors": ["xin shi", "robert qiu"], "url": "https://arxiv.org/abs/1905.02061"}, {"title": "when abstinence increases prevalence", "id": "1905.02073", "abstract": "in the pool of people seeking partners, a uniformly greater preference for abstinence increases the prevalence of infection and worsens everyone's welfare. in contrast, prevention and treatment reduce prevalence and improve payoffs. the results are driven by adverse selection: people who prefer more partners are likelier disease carriers. a given decrease in the number of matches is a smaller proportional reduction for people with many partners, thus increases the fraction of infected in the pool. the greater disease risk further decreases partner-seeking and payoffs.", "categories": "econ.th", "created": "2019-05-06", "updated": "", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1905.02073"}, {"title": "impact of artificial intelligence on businesses: from research,   innovation, market deployment to future shifts in business models", "id": "1905.02092", "abstract": "the fast pace of artificial intelligence (ai) and automation is propelling strategists to reshape their business models. this is fostering the integration of ai in the business processes but the consequences of this adoption are underexplored and need attention. this paper focuses on the overall impact of ai on businesses - from research, innovation, market deployment to future shifts in business models. to access this overall impact, we design a three-dimensional research model, based upon the neo-schumpeterian economics and its three forces viz. innovation, knowledge, and entrepreneurship. the first dimension deals with research and innovation in ai. in the second dimension, we explore the influence of ai on the global market and the strategic objectives of the businesses and finally, the third dimension examines how ai is shaping business contexts. additionally, the paper explores ai implications on actors and its dark sides.", "categories": "econ.gn cs.ai cs.cy q-fin.ec", "created": "2019-05-03", "updated": "", "authors": ["neha soni", "enakshi khular sharma", "narotam singh", "amita kapoor"], "url": "https://arxiv.org/abs/1905.02092"}, {"title": "lasso under multi-way clustering: estimation and post-selection   inference", "id": "1905.02107", "abstract": "this paper studies high-dimensional regression models with lasso when data is sampled under multi-way clustering. first, we establish convergence rates for the lasso and post-lasso estimators. second, we propose a novel inference method based on a post-double-selection procedure and show its asymptotic validity. our procedure can be easily implemented with existing statistical packages. simulation results demonstrate that the proposed procedure works well in finite sample. we illustrate the proposed method with a couple of empirical applications to development and growth economics.", "categories": "econ.em", "created": "2019-05-06", "updated": "2019-08-21", "authors": ["harold d. chiang", "yuya sasaki"], "url": "https://arxiv.org/abs/1905.02107"}, {"title": "where does active travel fit within local community narratives of   mobility space and place?", "id": "1905.02674", "abstract": "encouraging sustainable mobility patterns is at the forefront of policymaking at all scales of governance as the collective consciousness surrounding climate change continues to expand. not every community, however, possesses the necessary economic or socio-cultural capital to encourage modal shifts away from private motorized vehicles towards active modes. the current literature on `soft' policy emphasizes the importance of tailoring behavior change campaigns to individual or geographic context. yet, there is a lack of insight and appropriate tools to promote active mobility and overcome transport disadvantage from the local community perspective. the current study investigates the promotion of walking and cycling adoption using a series of focus groups with local residents in two geographic communities, namely chicago's (1) humboldt park neighborhood and (2) suburb of evanston. the research approach combines traditional qualitative discourse analysis with quantitative text-mining tools, namely topic modeling and sentiment analysis. the analysis uncovers the local mobility culture, embedded norms and values associated with acceptance of active travel modes in different communities. we observe that underserved populations within diverse communities view active mobility simultaneously as a necessity and as a symbol of privilege that is sometimes at odds with the local culture. the mixed methods approach to analyzing community member discourses is translated into policy findings that are either tailored to local context or broadly applicable to curbing automobile dominance. overall, residents of both humboldt park and evanston envision a society in which multimodalism replaces car-centrism, but differences in the local physical and social environments would and should influence the manner in which overarching policy objectives are met.", "categories": "econ.gn q-fin.ec stat.ml", "created": "2019-05-07", "updated": "", "authors": ["alec biehl", "ying chen", "karla sanabria-veaz", "david uttal", "amanda stathopoulos"], "url": "https://arxiv.org/abs/1905.02674"}, {"title": "decision making with machine learning and roc curves", "id": "1905.02810", "abstract": "the receiver operating characteristic (roc) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data science. this paper studies the statistical properties of roc curves and its implication on model selection. we analyze the implications of different models of incentive heterogeneity and information asymmetry on the relation between human decisions and the roc curves. our theoretical discussion is illustrated in the context of a large data set of pregnancy outcomes and doctor diagnosis from the pre-pregnancy checkups of reproductive age couples in henan province provided by the chinese ministry of health.", "categories": "stat.me cs.ai econ.gn q-fin.ec stat.ml", "created": "2019-05-05", "updated": "", "authors": ["kai feng", "han hong", "ke tang", "jingyuan wang"], "url": "https://arxiv.org/abs/1905.02810"}, {"title": "conditions for stable equilibrium in cournot duopoly models with tax   evasion and time delay", "id": "1905.02817", "abstract": "we provide conditions for stable equilibrium in cournot duopoly models with tax evasion and time delay. we prove that our conditions actually imply asymptotically stable equilibrium and delay independence. conditions include the same marginal cost and equal probability for evading taxes. we give examples of cost and inverse demand functions satisfying the proposed conditions. some economic interpretations of our results are also included.", "categories": "math.oc econ.th math.ds", "created": "2019-05-07", "updated": "2019-10-07", "authors": ["raul villafuerte-segura", "eduardo alvarado-santos", "benjamin a. itza-ortiz"], "url": "https://arxiv.org/abs/1905.02817"}, {"title": "does environmental economics lead to patentable research?", "id": "1905.02875", "abstract": "in this feasibility study, the impact of academic research from social sciences and humanities on technological innovation is explored through a study of citations patterns of journal articles in patents. specifically we focus on citations of journals from the field of environmental economics in patents included in an american patent database (uspto). three decades of patents have led to a small set of journal articles (85) that are being cited from the field of environmental economics. while this route of measuring how academic research is validated through its role in stimulating technological progress may be rather limited (based on this first exploration), it may still point to a valuable and interesting topic for further research.", "categories": "cs.dl econ.gn q-fin.ec", "created": "2019-05-07", "updated": "", "authors": ["xiaojun hu", "ronald rousseau", "sandra rousseau"], "url": "https://arxiv.org/abs/1905.02875"}, {"title": "spherical preferences", "id": "1905.02917", "abstract": "we introduce and study the property of orthogonal independence, a restricted additivity axiom applying when alternatives are orthogonal. the axiom requires that the preference for one marginal change over another should be maintained after each marginal change has been shifted in a direction that is orthogonal to both.   we show that continuous preferences satisfy orthogonal independence if and only if they are spherical: their indifference curves are spheres with the same center, with preference being \"monotone\" either away or towards the center. spherical preferences include linear preferences as a special (limiting) case. we discuss different applications to economic and political environments. our result delivers euclidean preferences in models of spatial voting, quadratic welfare aggregation in social choice, and expected utility in models of choice under uncertainty.", "categories": "econ.th", "created": "2019-05-08", "updated": "2020-02-12", "authors": ["christopher p. chambers", "federico echenique"], "url": "https://arxiv.org/abs/1905.02917"}, {"title": "economic performance through time: a dynamical theory", "id": "1905.02956", "abstract": "the central problems of development economics are the explanation of the gross disparities in the global distribution, $\\cal{d}$, of economic performance, $\\cal{e}$, and the persistence, $\\cal{p}$, of said distribution. douglass north argued, epigrammatically, that institutions, $\\cal{i}$, are the rules of the game, meaning that $\\cal{i}$ determines or at least constrains $\\cal{e}$. this promised to explain $\\cal{d}$. 65,000 citations later, the central problems remain unsolved. north's institutions are informal, slowly changing cultural norms as well as roads, guilds, and formal legislation that may change overnight. this definition, mixing the static and the dynamic, is unsuited for use in a necessarily time dependent theory of developing economies. we offer here a suitably precise definition of $\\cal{i}$, a dynamical theory of economic development, a new measure of the economy, an explanation of $\\cal{p}$, a bivariate model that explains half of $\\cal{d}$, and a critical reconsideration of north's epigram.", "categories": "econ.gn q-fin.ec", "created": "2019-05-08", "updated": "", "authors": ["daniel seligson", "anne mccants"], "url": "https://arxiv.org/abs/1905.02956"}, {"title": "the mitigating role of regulation on the concentric patterns of   broadband diffusion. the case of finland", "id": "1905.03002", "abstract": "this article analyzes the role of finnish regulation in achieving the broadband penetration goals defined by the national regulatory authority. it is well known that in the absence of regulatory mitigation the population density has a positive effect on broadband diffusion. hence, we measure the effect of the population density on the determinants of broadband diffusion throughout the postal codes of finland via geographically weighted regression. we suggest that the main determinants of broadband diffusion and the population density follow a spatial pattern that is either concentric with a weak/medium/strong strength or non-concentric convex/concave. based on 10 patterns, we argue that the finnish spectrum policy encouraged mobile network operators to satisfy ambitious universal service obligations without the need for a universal service fund. spectrum auctions facilitated infrastructure-based competition via equitable spectrum allocation and coverage obligation delivery via low-fee licenses. however, state subsidies for fiber deployment did not attract investment from nationwide operators due to mobile preference. these subsidies encouraged demand-driven investment, leading to the emergence of fiber consumer cooperatives. to explain this emergence, we show that when population density decreases, the level of mobile service quality decreases and community commitment increases. hence, we recommend regulators implementing market-driven strategies for 5g to stimulate local investment. for example, by allocating the 3.5 ghz and higher bands partly through local light licensing.", "categories": "econ.gn q-fin.ec", "created": "2019-05-08", "updated": "2019-07-29", "authors": ["jaume benseny", "juuso t\u00f6yli", "heikki h\u00e4mm\u00e4inen", "andr\u00e9s arcia-moret"], "url": "https://arxiv.org/abs/1905.03002"}, {"title": "working women and caste in india: a study of social disadvantage using   feature attribution", "id": "1905.03092", "abstract": "women belonging to the socially disadvantaged caste-groups in india have historically been engaged in labour-intensive, blue-collar work. we study whether there has been any change in the ability to predict a woman's work-status and work-type based on her caste by interpreting machine learning models using feature attribution. we find that caste is now a less important determinant of work for the younger generation of women compared to the older generation. moreover, younger women from disadvantaged castes are now more likely to be working in white-collar jobs.", "categories": "econ.em cs.lg stat.ap stat.ml", "created": "2019-04-27", "updated": "2020-01-03", "authors": ["kuhu joshi", "chaitanya k. joshi"], "url": "https://arxiv.org/abs/1905.03092"}, {"title": "from sicilian mafia to chinese \"scam villages\"", "id": "1905.03108", "abstract": "inspired by gambetta's theory on the origins of the mafia in sicily, we report a geo-concentrating phenomenon of scams in china, and propose a novel economic explanation. our analysis has some policy implications.", "categories": "cs.cr cs.cy econ.gn q-fin.ec", "created": "2019-05-06", "updated": "", "authors": ["jeff yan"], "url": "https://arxiv.org/abs/1905.03108"}, {"title": "online reviews can predict long-term returns of individual stocks", "id": "1905.03189", "abstract": "online reviews are feedback voluntarily posted by consumers about their consumption experiences. this feedback indicates customer attitudes such as affection, awareness and faith towards a brand or a firm and demonstrates inherent connections with a company's future sales, cash flow and stock pricing. however, the predicting power of online reviews for long-term returns on stocks, especially at the individual level, has received little research attention, making a comprehensive exploration necessary to resolve existing debates. in this paper, which is based exclusively on online reviews, a methodology framework for predicting long-term returns of individual stocks with competent performance is established. specifically, 6,246 features of 13 categories inferred from more than 18 million product reviews are selected to build the prediction models. with the best classifier selected from cross-validation tests, a satisfactory increase in accuracy, 13.94%, was achieved compared to the cutting-edge solution with 10 technical indicators being features, representing an 18.28% improvement relative to the random value. the robustness of our model is further evaluated and testified in realistic scenarios. it is thus confirmed for the first time that long-term returns of individual stocks can be predicted by online reviews. this study provides new opportunities for investors with respect to long-term investments in individual stocks.", "categories": "econ.gn cs.si q-fin.cp q-fin.ec", "created": "2019-04-29", "updated": "", "authors": ["junran wu", "ke xu", "jichang zhao"], "url": "https://arxiv.org/abs/1905.03189"}, {"title": "dependencies and systemic risk in the european insurance sector: some   new evidence based on copula-dcc-garch model and selected clustering methods", "id": "1905.03273", "abstract": "the subject of the present article is the study of correlations between large insurance companies and their contribution to systemic risk in the insurance sector. our main goal is to analyze the conditional structure of the correlation on the european insurance market and to compare systemic risk in different regimes of this market. these regimes are identified by monitoring the weekly rates of returns of eight of the largest insurers (five from europe and the biggest insurers from the usa, canada and china) during the period january 2005 to december 2018. to this aim we use statistical clustering methods for time units (weeks) to which we assigned the conditional variances obtained from the estimated copula-dcc-garch model. the advantage of such an approach is that there is no need to assume a priori a number of market regimes, since this number has been identified by means of clustering quality validation. in each of the identified market regimes we determined the commonly now used covar systemic risk measure. from the performed analysis we conclude that all the considered insurance companies are positively correlated and this correlation is stronger in times of turbulences on global markets which shows an increased exposure of the european insurance sector to systemic risk during crisis. moreover, in times of turbulences on global markets the value level of the covar systemic risk index is much higher than in \"normal conditions\".", "categories": "econ.gn q-fin.ec q-fin.st", "created": "2019-05-08", "updated": "", "authors": ["anna denkowska", "stanis\u0142aw wanat"], "url": "https://arxiv.org/abs/1905.03273"}, {"title": "the implications of pricing on social learning", "id": "1905.03452", "abstract": "we study the implications of endogenous pricing for learning and welfare in the classic herding model . when prices are determined exogenously, it is known that learning occurs if and only if signals are unbounded. by contrast, we show that learning can occur when signals are bounded as long as non-conformism among consumers is scarce. more formally, learning happens if and only if signals exhibit the vanishing likelihood property introduced bellow. we discuss the implications of our results for potential market failure in the context of schumpeterian growth with uncertainty over the value of innovations.", "categories": "econ.th cs.gt", "created": "2019-05-09", "updated": "", "authors": ["itai arieli", "moran koren", "rann smorodinsky"], "url": "https://arxiv.org/abs/1905.03452"}, {"title": "the likelihood of mixed hitting times", "id": "1905.03463", "abstract": "we present a method for computing the likelihood of a mixed hitting-time model that specifies durations as the first time a latent l\\'evy process crosses a heterogeneous threshold. this likelihood is not generally known in closed form, but its laplace transform is. our approach to its computation relies on numerical methods for inverting laplace transforms that exploit special properties of the first passage times of l\\'evy processes. we use our method to implement a maximum likelihood estimator of the mixed hitting-time model in matlab. we illustrate the application of this estimator with an analysis of kennan's (1985) strike data.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-05-09", "updated": "", "authors": ["jaap h. abbring", "tim salimans"], "url": "https://arxiv.org/abs/1905.03463"}, {"title": "on the kolkata index as a measure of income inequality", "id": "1905.03615", "abstract": "we study the mathematical and economic structure of the kolkata (k) index of income inequality. we show that the k-index always exists and is a unique fixed point of the complementary lorenz function, where the lorenz function itself gives the fraction of cumulative income possessed by the cumulative fraction of population (when arranged from poorer to richer). we show that the k-index generalizes pareto's 80/20 rule. although the k and pietra indices both split the society into two groups, we show that k-index is a more intensive measure for the poor-rich split. we compare the normalized k-index with the gini coefficient and the pietra index and discuss when they coincide. we establish that for any income distribution the value of gini coefficient is no less than that of the pietra index and the value of the pietra index is no less than that of the normalized k-index. while the gini coefficient and the pietra index are affected by transfers exclusively among the rich or among the poor, the k-index is only affected by transfers across the two groups.", "categories": "econ.th cond-mat.stat-mech", "created": "2019-04-30", "updated": "2019-10-18", "authors": ["suchismita banerjee", "bikas k. chakrabarti", "manipushpak mitra", "suresh mutuswami"], "url": "https://arxiv.org/abs/1905.03615"}, {"title": "empirical bias and efficiency of alpha-auctions: experimental evidence", "id": "1905.03876", "abstract": "we experimentally evaluate the comparative performance of the winner-bid, average-bid, and loser-bid auctions for the dissolution of a partnership. the analysis of these auctions based on the empirical equilibrium refinement of velez and brown (2020) arxiv:1907.12408 reveals that as long as behavior satisfies weak payoff monotonicity, winner-bid and loser-bid auctions necessarily exhibit a form of bias when empirical distributions of play approximate best responses (velez and brown, 2020 arxiv:1905.08234). we find support for both weak payoff monotonicity and the form of bias predicted by the theory for these two auctions. consistently with the theory, the average-bid auction does not exhibit this form of bias. it has lower efficiency that the winner-bid auction, however.", "categories": "econ.gn q-fin.ec", "created": "2019-05-09", "updated": "2020-07-13", "authors": ["alexander l. brown", "rodrigo a. velez"], "url": "https://arxiv.org/abs/1905.03876"}, {"title": "identifying present-bias from the timing of choices", "id": "1905.03959", "abstract": "timing decisions are common: when to file your taxes, finish a referee report, or complete a task at work. we ask whether time preferences can be inferred when \\textsl{only} task completion is observed. to answer this question, we analyze the following model: each period a decision maker faces the choice whether to complete the task today or to postpone it to later. cost and benefits of task completion cannot be directly observed by the analyst, but the analyst knows that net benefits are drawn independently between periods from a time-invariant distribution and that the agent has time-separable utility. furthermore, we suppose the analyst can observe the agent's exact stopping probability. we establish that for any agent with quasi-hyperbolic $\\beta,\\delta$-preferences and given level of partial naivete $\\hat{\\beta}$, the probability of completing the task conditional on not having done it earlier increases towards the deadline. and conversely, for any given preference parameters $\\beta,\\delta$ and (weakly increasing) profile of task completion probability, there exists a stationary payoff distribution that rationalizes her behavior as long as the agent is either sophisticated or fully naive. an immediate corollary being that, without parametric assumptions, it is impossible to rule out time-consistency even when imposing an a priori assumption on the permissible long-run discount factor. we also provide an exact partial identification result when the analyst can, in addition to the stopping probability, observe the agent's continuation value.", "categories": "econ.th econ.em", "created": "2019-05-10", "updated": "", "authors": ["paul heidhues", "philipp strack"], "url": "https://arxiv.org/abs/1905.03959"}, {"title": "demand and welfare analysis in discrete choice models with social   interactions", "id": "1905.04028", "abstract": "many real-life settings of consumer-choice involve social interactions, causing targeted policies to have spillover-effects. this paper develops novel empirical tools for analyzing demand and welfare-effects of policy-interventions in binary choice settings with social interactions. examples include subsidies for health-product adoption and vouchers for attending a high-achieving school. we establish the connection between econometrics of large games and brock-durlauf-type interaction models, under both i.i.d. and spatially correlated unobservables. we develop new convergence results for associated beliefs and estimates of preference-parameters under increasing-domain spatial asymptotics. next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand-prediction under interactions, are insufficient for welfare-calculations. this is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare-effects and deadweight-loss from a policy-intervention. standard index-restrictions imply distribution-free bounds on welfare. we illustrate our results using experimental data on mosquito-net adoption in rural kenya.", "categories": "econ.em econ.gn q-fin.ec stat.ap", "created": "2019-05-10", "updated": "", "authors": ["debopam bhattacharya", "pascaline dupas", "shin kanaya"], "url": "https://arxiv.org/abs/1905.04028"}, {"title": "particulate air pollution, birth outcomes, and infant mortality:   evidence from japan's automobile emission control law of 1992", "id": "1905.04417", "abstract": "this study investigates the impacts of the automobile nox law of 1992 on ambient air pollutants and fetal and infant health outcomes in japan. using panel data taken from more than 1,500 monitoring stations between 1987 and 1997, we find that nox and so2 levels reduced by 87% and 52%, respectively in regulated areas following the 1992 regulation. in addition, using a municipal-level vital statistics panel dataset and adopting the regression differences-in-differences method, we find that the enactment of the regulation explained most of the improvements in the fetal death rate between 1991 and 1993. this study is the first to provide evidence on the positive impacts of this large-scale automobile regulation policy on fetal health.", "categories": "econ.gn q-fin.ec", "created": "2019-05-10", "updated": "2019-12-09", "authors": ["tatsuki inoue", "nana nunokawa", "daisuke kurisu", "kota ogasawara"], "url": "https://arxiv.org/abs/1905.04417"}, {"title": "the role of pawnshops in risk coping in early twentieth-century japan", "id": "1905.04419", "abstract": "this study examines the role of pawnshops as a risk-coping device in prewar japan. using data on pawnshop loans for more than 250 municipalities and exploiting the 1918-1920 influenza pandemic as a natural experiment, we find that the adverse health shock increased the total amount of loans from pawnshops. this is because those who regularly relied on pawnshops borrowed more money from them than usual to cope with the adverse health shock, and not because the number of people who used pawnshops increased.", "categories": "econ.gn q-fin.ec", "created": "2019-05-10", "updated": "2019-08-25", "authors": ["tatsuki inoue"], "url": "https://arxiv.org/abs/1905.04419"}, {"title": "regression discontinuity design with multiple groups for heterogeneous   causal effect estimation", "id": "1905.04443", "abstract": "we propose a new estimation method for heterogeneous causal effects which utilizes a regression discontinuity (rd) design for multiple datasets with different thresholds. the standard rd design is frequently used in applied researches, but the result is very limited in that the average treatment effects is estimable only at the threshold on the running variable. in application studies it is often the case that thresholds are different among databases from different regions or firms. for example thresholds for scholarship differ with states. the proposed estimator based on the augmented inverse probability weighted local linear estimator can estimate the average effects at an arbitrary point on the running variable between the thresholds under mild conditions, while the method adjust for the difference of the distributions of covariates among datasets. we perform simulations to investigate the performance of the proposed estimator in the finite samples.", "categories": "econ.em", "created": "2019-05-11", "updated": "", "authors": ["takayuki toda", "ayako wakano", "takahiro hoshino"], "url": "https://arxiv.org/abs/1905.04443"}, {"title": "exogenous rewards for promoting cooperation in scale-free networks", "id": "1905.04964", "abstract": "the design of mechanisms that encourage pro-social behaviours in populations of self-regarding agents is recognised as a major theoretical challenge within several areas of social, life and engineering sciences. when interference from external parties is considered, several heuristics have been identified as capable of engineering a desired collective behaviour at a minimal cost. however, these studies neglect the diverse nature of contexts and social structures that characterise real-world populations. here we analyse the impact of diversity by means of scale-free interaction networks with high and low levels of clustering, and test various interference mechanisms using simulations of agents facing a cooperative dilemma. our results show that interference on scale-free networks is not trivial and that distinct levels of clustering react differently to each interference mechanism. as such, we argue that no tailored response fits all scale-free networks and present which mechanisms are more efficient at fostering cooperation in both types of networks. finally, we discuss the pitfalls of considering reckless interference mechanisms.", "categories": "cs.gt cs.ma cs.si econ.th physics.soc-ph", "created": "2019-05-13", "updated": "2019-05-17", "authors": ["theodor cimpeanu", "the anh han", "francisco c. santos"], "url": "https://arxiv.org/abs/1905.04964"}, {"title": "mixtures of mean-preserving contractions", "id": "1905.05157", "abstract": "given a purely atomic probability measure with support on n points, p, any mean-preserving contraction (mpc) of p, q, with support on m > n points is a mixture of mpcs of p, each with support on most n points. we illustrate an application of this result in economics.", "categories": "econ.th math.pr", "created": "2019-05-13", "updated": "2020-09-20", "authors": ["joseph whitmeyer", "mark whitmeyer"], "url": "https://arxiv.org/abs/1905.05157"}, {"title": "sustainable investing and the cross-section of maximum drawdown", "id": "1905.05237", "abstract": "we use supervised learning to identify factors that predict the cross-section of maximum drawdown for stocks in the us equity market. our data run from january 1980 to june 2018 and our analysis includes ordinary least squares, penalized linear regressions, tree-based models, and neural networks. we find that the most important predictors tended to be consistent across models, and that non-linear models had better predictive power than linear models. predictive power was higher in calm periods than stressed periods, and environmental, social, and governance indicators augmented predictive power for non-linear models.", "categories": "q-fin.st econ.em", "created": "2019-05-13", "updated": "", "authors": ["lisa r. goldberg", "saad mouti"], "url": "https://arxiv.org/abs/1905.05237"}, {"title": "the paradox of monotone structural qre", "id": "1905.05814", "abstract": "mckelvey and palfrey (1995)'s monotone structural quantal response equilibrium theory may be misspecified for the study of monotone behavior.", "categories": "econ.th", "created": "2019-05-14", "updated": "2019-07-29", "authors": ["rodrigo a. velez", "alexander l. brown"], "url": "https://arxiv.org/abs/1905.05814"}, {"title": "analyzing subjective well-being data with misclassification", "id": "1905.06037", "abstract": "we use novel nonparametric techniques to test for the presence of non-classical measurement error in reported life satisfaction (ls) and study the potential effects from ignoring it. our dataset comes from wave 3 of the uk understanding society that is surveyed from 35,000 british households. our test finds evidence of measurement error in reported ls for the entire dataset as well as for 26 out of 32 socioeconomic subgroups in the sample. we estimate the joint distribution of reported and latent ls nonparametrically in order to understand the mis-reporting behavior. we show this distribution can then be used to estimate parametric models of latent ls. we find measurement error bias is not severe enough to distort the main drivers of ls. but there is an important difference that is policy relevant. we find women tend to over-report their latent ls relative to men. this may help explain the gender puzzle that questions why women are reportedly happier than men despite being worse off on objective outcomes such as income and employment.", "categories": "econ.em", "created": "2019-05-15", "updated": "", "authors": ["ekaterina oparina", "sorawoot srisuma"], "url": "https://arxiv.org/abs/1905.06037"}, {"title": "computational socioeconomics", "id": "1905.06166", "abstract": "uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. the understanding of socioeconomic processes provides foundations to quantify global economic development, to map regional industrial structure, and to infer individual socioeconomic status. in this review, we will make a brief manifesto about a new interdisciplinary research field named computational socioeconomics, followed by detailed introduction about data resources, computational tools, data-driven methods, theoretical models and novel applications at multiple resolutions, including the quantification of global economic inequality and complexity, the map of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. this review, together with pioneering works we have highlighted, will draw increasing interdisciplinary attentions and induce a methodological shift in future socioeconomic studies.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-05-15", "updated": "", "authors": ["jian gao", "yi-cheng zhang", "tao zhou"], "url": "https://arxiv.org/abs/1905.06166"}, {"title": "mrsc: multi-dimensional robust synthetic control", "id": "1905.06400", "abstract": "when evaluating the impact of a policy on a metric of interest, it may not be possible to conduct a randomized control trial. in settings where only observational data is available, synthetic control (sc) methods provide a popular data-driven approach to estimate a \"synthetic\" control by combining measurements of \"similar\" units (donors). recently, robust sc (rsc) was proposed as a generalization of sc to overcome the challenges of missing data high levels of noise, while removing the reliance on domain knowledge for selecting donors. however, sc, rsc, and their variants, suffer from poor estimation when the pre-intervention period is too short. as the main contribution, we propose a generalization of unidimensional rsc to multi-dimensional rsc, mrsc. our proposed mechanism incorporates multiple metrics to estimate a synthetic control, thus overcoming the challenge of poor inference from limited pre-intervention data. we show that the mrsc algorithm with $k$ metrics leads to a consistent estimator of the synthetic control for the target unit under any metric. our finite-sample analysis suggests that the prediction error decays to zero at a rate faster than the rsc algorithm by a factor of $k$ and $\\sqrt{k}$ for the training and testing periods (pre- and post-intervention), respectively. additionally, we provide a diagnostic test that evaluates the utility of including additional metrics. moreover, we introduce a mechanism to validate the performance of mrsc: time series prediction. that is, we propose a method to predict the future evolution of a time series based on limited data when the notion of time is relative and not absolute, i.e., we have access to a donor pool that has undergone the desired future evolution. finally, we conduct experimentation to establish the efficacy of mrsc on synthetic data and two real-world case studies (retail and cricket).", "categories": "stat.me econ.em", "created": "2019-05-15", "updated": "2019-09-23", "authors": ["muhummad amjad", "vishal misra", "devavrat shah", "dennis shen"], "url": "https://arxiv.org/abs/1905.06400"}, {"title": "parallel search for information", "id": "1905.06485", "abstract": "we consider the problem of a decision-maker searching for information on multiple alternatives when information is learned on all alternatives simultaneously. the decision-maker has a running cost of searching for information, and has to decide when to stop searching for information and choose one alternative. the expected payoff of each alternative evolves as a diffusion process when information is being learned. we present necessary and sufficient conditions for the solution, establishing existence and uniqueness. we show that the optimal boundary where search is stopped (free boundary) is star-shaped, and present an asymptotic characterization of the value function and the free boundary. we show properties of how the distance between the free boundary and the diagonal varies with the number of alternatives, and how the free boundary under parallel search relates to the one under sequential search, with and without economies of scale on the search costs.", "categories": "econ.th math.ap math.pr", "created": "2019-05-15", "updated": "2020-04-09", "authors": ["t. tony ke", "wenpin tang", "j. miguel villas-boas", "yuming zhang"], "url": "https://arxiv.org/abs/1905.06485"}, {"title": "inference in a class of optimization problems: confidence regions and   finite sample bounds on errors in coverage probabilities", "id": "1905.06491", "abstract": "this paper describes a method for carrying out inference on partially identified parameters that are solutions to a class of optimization problems. the optimization problems arise in applications in which grouped data are used for estimation of a model's structural parameters. the parameters are characterized by restrictions that involve the unknown population means of observed random variables in addition to the structural parameters of interest. inference consists of finding confidence intervals for the structural parameters. our theory provides a finite-sample bound on the difference between the true and nominal probabilities with which a confidence interval contains the true but unknown value of a parameter. we contrast our method with an alternative inference method based on the median-of-means estimator of minsker (2015). the results of monte carlo experiments and empirical examples illustrate the usefulness of our method.", "categories": "stat.me econ.em", "created": "2019-05-15", "updated": "2020-09-23", "authors": ["joel l. horowitz", "sokbae lee"], "url": "https://arxiv.org/abs/1905.06491"}, {"title": "improving regression-based event study analysis using a topological   machine-learning method", "id": "1905.06536", "abstract": "this paper introduces a new correction scheme to a conventional regression-based event study method: a topological machine-learning approach with a self-organizing map (som).we use this new scheme to analyze a major market event in japan and find that the factors of abnormal stock returns can be easily can be easily identified and the event-cluster can be depicted.we also find that a conventional event study method involves an empirical analysis mechanism that tends to derive bias due to its mechanism, typically in an event-clustered market situation. we explain our new correction scheme and apply it to an event in the japanese market --- the holding disclosure of the government pension investment fund (gpif) on july 31, 2015.", "categories": "econ.gn q-fin.ec q-fin.st", "created": "2019-05-16", "updated": "", "authors": ["takashi yamashita", "ryozo miura"], "url": "https://arxiv.org/abs/1905.06536"}, {"title": "playing with ghosts in a dynkin game", "id": "1905.06564", "abstract": "we study a class of optimal stopping games (dynkin games) of preemption type, with uncertainty about the existence of competitors. the set-up is well-suited to model, for example, real options in the context of investors who do not want to publicly reveal their interest in a certain business opportunity. we show that there exists a nash equilibrium in randomized stopping times which is described explicitly in terms of the corresponding one-player game.", "categories": "math.pr econ.gn math.oc q-fin.ec", "created": "2019-05-16", "updated": "", "authors": ["tiziano de angelis", "erik ekstr\u00f6m"], "url": "https://arxiv.org/abs/1905.06564"}, {"title": "the empirical saddlepoint estimator", "id": "1905.06977", "abstract": "we define a moment-based estimator that maximizes the empirical saddlepoint (esp) approximation of the distribution of solutions to empirical moment conditions. we call it the esp estimator. we prove its existence, consistency and asymptotic normality, and we propose novel test statistics. we also show that the esp estimator corresponds to the mm (method of moments) estimator shrunk toward parameter values with lower estimated variance, so it reduces the documented instability of existing moment-based estimators. in the case of just-identified moment conditions, which is the case we focus on, the esp estimator is different from the mm estimator, unlike the recently proposed alternatives, such as the empirical-likelihood-type estimators.", "categories": "math.st econ.em stat.th", "created": "2019-05-16", "updated": "", "authors": ["benjamin holcblat", "fallaw sowell"], "url": "https://arxiv.org/abs/1905.06977"}, {"title": "a comment on \"estimating dynamic discrete choice models with hyperbolic   discounting\" by hanming fang and yang wang", "id": "1905.07048", "abstract": "the recent literature often cites fang and wang (2015) for analyzing the identification of time preferences in dynamic discrete choice under exclusion restrictions (e.g. yao et al., 2012; lee, 2013; ching et al., 2013; norets and tang, 2014; dub\\'e et al., 2014; gordon and sun, 2015; bajari et al., 2016; chan, 2017; gayle et al., 2018). fang and wang's proposition 2 claims generic identification of a dynamic discrete choice model with hyperbolic discounting. this claim uses a definition of \"generic\" that does not preclude the possibility that a generically identified model is nowhere identified. to illustrate this point, we provide two simple examples of models that are generically identified in fang and wang's sense, but that are, respectively, everywhere and nowhere identified. we conclude that proposition 2 is void: it has no implications for identification of the dynamic discrete choice model. we show that its proof is incorrect and incomplete and suggest alternative approaches to identification.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-05-16", "updated": "2019-07-03", "authors": ["jaap h. abbring", "\u00f8ystein daljord"], "url": "https://arxiv.org/abs/1905.07048"}, {"title": "cointegration in high frequency data", "id": "1905.07081", "abstract": "in this paper, we consider a framework adapting the notion of cointegration when two asset prices are generated by a driftless it\\^{o}-semimartingale featuring jumps with infinite activity, observed synchronously and regularly at high frequency. we develop a regression based estimation of the cointegrated relations method and show the related consistency and central limit theory when there is cointegration within that framework. we also provide a dickey-fuller type residual based test for the null of no cointegration against the alternative of cointegration, along with its limit theory. under no cointegration, the asymptotic limit is the same as that of the original dickey-fuller residual based test, so that critical values can be easily tabulated in the same way. finite sample indicates adequate size and good power properties in a variety of realistic configurations, outperforming original dickey-fuller and phillips-perron type residual based tests, whose sizes are distorted by non ergodic time-varying variance and power is altered by price jumps. two empirical examples consolidate the monte-carlo evidence that the adapted tests can be rejected while the original tests are not, and vice versa.", "categories": "q-fin.st econ.em", "created": "2019-05-16", "updated": "", "authors": ["simon clinet", "yoann potiron"], "url": "https://arxiv.org/abs/1905.07081"}, {"title": "driver surge pricing", "id": "1905.07544", "abstract": "ride-hailing marketplaces like uber and lyft use dynamic pricing, often called surge, to balance the supply of available drivers with the demand for rides. we study driver-side payment mechanisms for such marketplaces, presenting the theoretical foundation that has informed the design of uber's new additive driver surge mechanism. we present a dynamic stochastic model to capture the impact of surge pricing on driver earnings and their strategies to maximize such earnings. in this setting, some time periods (surge) are more valuable than others (non-surge), and so trips of different time lengths vary in the induced driver opportunity cost.   first, we show that multiplicative surge, historically the standard on ride-hailing platforms, is not incentive compatible in a dynamic setting. we then propose a structured, incentive-compatible pricing mechanism. this closed-form mechanism has a simple form and is well-approximated by uber's new additive surge mechanism. finally, through both numerical analysis and real data from a ride-hailing marketplace, we show that additive surge is more incentive compatible in practice than is multiplicative surge.", "categories": "cs.gt econ.gn q-fin.ec", "created": "2019-05-18", "updated": "2020-07-11", "authors": ["nikhil garg", "hamid nazerzadeh"], "url": "https://arxiv.org/abs/1905.07544"}, {"title": "nestedness in complex networks: observation, emergence, and implications", "id": "1905.07593", "abstract": "the observed architecture of ecological and socio-economic networks differs significantly from that of random networks. from a network science standpoint, non-random structural patterns observed in real networks call for an explanation of their emergence and an understanding of their potential systemic consequences. this article focuses on one of these patterns: nestedness. given a network of interacting nodes, nestedness can be described as the tendency for nodes to interact with subsets of the interaction partners of better-connected nodes. known since more than $80$ years in biogeography, nestedness has been found in systems as diverse as ecological mutualistic organizations, world trade, inter-organizational relations, among many others. this review article focuses on three main pillars: the existing methodologies to observe nestedness in networks; the main theoretical mechanisms conceived to explain the emergence of nestedness in ecological and socio-economic networks; the implications of a nested topology of interactions for the stability and feasibility of a given interacting system. we survey results from variegated disciplines, including statistical physics, graph theory, ecology, and theoretical economics. nestedness was found to emerge both in bipartite networks and, more recently, in unipartite ones; this review is the first comprehensive attempt to unify both streams of studies, usually disconnected from each other. we believe that the truly interdisciplinary endeavour -- while rooted in a complex systems perspective -- may inspire new models and algorithms whose realm of application will undoubtedly transcend disciplinary boundaries.", "categories": "physics.soc-ph econ.th nlin.ao physics.data-an q-bio.pe", "created": "2019-05-18", "updated": "", "authors": ["manuel sebastian mariani", "zhuo-ming ren", "jordi bascompte", "claudio juan tessone"], "url": "https://arxiv.org/abs/1905.07593"}, {"title": "nonparametric instrumental regressions with (potentially discrete)   instruments independent of the error term", "id": "1905.07812", "abstract": "we consider a nonparametric instrumental regression model with continuous endogenous regressor where instruments are fully independent of the error term. this assumption allows us to extend the reach of this model to cases where the instrumental variable is discrete, and therefore to substantially enlarge its potential empirical applications. under our assumptions, the regression function becomes solution to a nonlinear integral equation. we contribute to existing literature by providing an exhaustive analysis of identification and a simple iterative estimation procedure. details on the implementation and on the asymptotic properties of this estimation algorithm are given. we conclude the paper with a simulation experiment for a binary instrument and an empirical application to the estimation of the engel curve for food, where we show that our estimator delivers results that are consistent with existing evidence under several discretizations of the instrumental variable.", "categories": "econ.em stat.me", "created": "2019-05-19", "updated": "", "authors": ["samuele centorrino", "fr\u00e9d\u00e9rique f\u00e8ve", "jean-pierre florens"], "url": "https://arxiv.org/abs/1905.07812"}, {"title": "time series analysis and forecasting of the us housing starts using   econometric and machine learning model", "id": "1905.07848", "abstract": "in this research paper, i have performed time series analysis and forecasted the monthly value of housing starts for the year 2019 using several econometric methods - arima(x), varx, (g)arch and machine learning algorithms - artificial neural networks, ridge regression, k-nearest neighbors, and support vector regression, and created an ensemble model. the ensemble model stacks the predictions from various individual models, and gives a weighted average of all predictions. the analyses suggest that the ensemble model has performed the best among all the models as the prediction errors are the lowest, while the econometric models have higher error rates.", "categories": "econ.em", "created": "2019-05-19", "updated": "", "authors": ["sudiksha joshi"], "url": "https://arxiv.org/abs/1905.07848"}, {"title": "conformal prediction interval estimations with an application to   day-ahead and intraday power markets", "id": "1905.07886", "abstract": "we discuss a concept denoted as conformal prediction (cp) in this paper. while initially stemming from the world of machine learning, it was never applied or analyzed in the context of short-term electricity price forecasting. therefore, we elaborate the aspects that render conformal prediction worthwhile to know and explain why its simple yet very efficient idea has worked in other fields of application and why its characteristics are promising for short-term power applications as well. we compare its performance with different state-of-the-art electricity price forecasting models such as quantile regression averaging (qra) in an empirical out-of-sample study for three short-term electricity time series. we combine conformal prediction with various underlying point forecast models to demonstrate its versatility and behavior under changing conditions. our findings suggest that conformal prediction yields sharp and reliable prediction intervals in short-term power markets. we further inspect the effect each of conformal prediction's model components has and provide a path-based guideline on how to find the best cp model for each market.", "categories": "econ.em q-fin.pm q-fin.tr stat.ap stat.ml", "created": "2019-05-20", "updated": "2020-09-17", "authors": ["christopher kath", "florian ziel"], "url": "https://arxiv.org/abs/1905.07886"}, {"title": "demand forecasting techniques for build-to-order lean manufacturing   supply chains", "id": "1905.07902", "abstract": "build-to-order (bto) supply chains have become common-place in industries such as electronics, automotive and fashion. they enable building products based on individual requirements with a short lead time and minimum inventory and production costs. due to their nature, they differ significantly from traditional supply chains. however, there have not been studies dedicated to demand forecasting methods for this type of setting. this work makes two contributions. first, it presents a new and unique data set from a manufacturer in the bto sector. second, it proposes a novel data transformation technique for demand forecasting of bto products. results from thirteen forecasting methods show that the approach compares well to the state-of-the-art while being easy to implement and to explain to decision-makers.", "categories": "cs.lg econ.em stat.ap stat.ml", "created": "2019-05-20", "updated": "", "authors": ["rodrigo rivera-castro", "ivan nazarov", "yuke xiang", "alexander pletneev", "ivan maksimov", "evgeny burnaev"], "url": "https://arxiv.org/abs/1905.07902"}, {"title": "empirical bias of extreme-price auctions: analysis", "id": "1905.08234", "abstract": "we advance empirical equilibrium analysis (velez and brown, 2020, arxiv:1907.12408) of the winner-bid and loser-bid auctions for the dissolution of a partnership. we show, in a complete information environment, that even though these auctions are essentially equivalent for the nash equilibrium prediction, they can be expected to differ in fundamental ways when they are operated. besides the direct policy implications, two general consequences follow. first, a mechanism designer who accounts for the empirical plausibility of equilibria may not be constrained by maskin invariance. second, a mechanism designer who does not account for the empirical plausibility of equilibria may inadvertently design biased mechanisms.", "categories": "econ.th", "created": "2019-05-12", "updated": "2020-07-13", "authors": ["rodrigo a. velez", "alexander l. brown"], "url": "https://arxiv.org/abs/1905.08234"}, {"title": "smoothing quantile regressions", "id": "1905.08535", "abstract": "we propose to smooth the entire objective function, rather than only the check function, in a linear quantile regression context. not only does the resulting smoothed quantile regression estimator yield a lower mean squared error and a more accurate bahadur-kiefer representation than the standard estimator, but it is also asymptotically differentiable. we exploit the latter to propose a quantile density estimator that does not suffer from the curse of dimensionality. this means estimating the conditional density function without worrying about the dimension of the covariate vector. it also allows for two-stage efficient quantile regression estimation. our asymptotic theory holds uniformly with respect to the bandwidth and quantile level. finally, we propose a rule of thumb for choosing the smoothing bandwidth that should approximate well the optimal bandwidth. simulations confirm that our smoothed quantile regression estimator indeed performs very well in finite samples.", "categories": "econ.em stat.me", "created": "2019-05-21", "updated": "2019-08-15", "authors": ["marcelo fernandes", "emmanuel guerre", "eduardo horta"], "url": "https://arxiv.org/abs/1905.08535"}, {"title": "the perils of automated fitting of datasets: the case of a wind turbine   cost model", "id": "1905.08870", "abstract": "rinne et al. conduct an interesting analysis of the impact of wind turbine technology and land-use on wind power potentials, which allows profound insights into each factors contribution to overall potentials. the paper presents a detailed model of site-specific wind turbine investment cost (i.e. road- and grid access costs) complemented by a model used to estimate site-independent costs. we believe that propose a cutting edge model of site-specific investment costs. however, the site-independent cost model is flawed in our opinion. this flaw most likely does not impact the results presented in the paper, although we expect a considerable generalization error. thus the application of the wind turbine cost model in other contexts may lead to unreasonable results. more generally, the derivation of the wind turbine cost model serves as an example of how applications of automated regression analysis can go wrong.", "categories": "stat.ap cs.sy econ.gn q-fin.ec", "created": "2019-05-21", "updated": "", "authors": ["claude kl\u00f6ckl", "katharina gruber", "peter regner", "sebastian wehrle", "johannes schmidt"], "url": "https://arxiv.org/abs/1905.08870"}, {"title": "cheating in ranking systems", "id": "1905.09116", "abstract": "consider an application sold on an on-line platform, with the app paying a commission fee and, henceforth, offered for sale on the platform. the ability to sell the application depends on its customer ranking. therefore, developers may have an incentive to promote their applications ranking in a dishonest manner. one way to do this is by faking positive customer reviews. however, the platform is able to detect dishonest behavior (cheating) with some probability and then proceeds to decide whether to ban the application. we provide an analysis and find the equilibrium behaviors of both the applications developers (cheat or not) and the platform (setting of the commission fee). we provide initial insights into how the platforms detection accuracy affects the incentives of the app developers.", "categories": "econ.th cs.cy", "created": "2019-05-22", "updated": "", "authors": ["lihi dery", "dror hermel", "artyom jelnov"], "url": "https://arxiv.org/abs/1905.09116"}, {"title": "technological learning and innovation gestation lags at the frontier of   science: from cern procurement to patent", "id": "1905.09552", "abstract": "this paper contributes to the literature on the impact of big science centres on technological innovation. we exploit a unique dataset with information on cern's procurement orders to study the collaborative innovation process between cern and its industrial partners. after a qualitative discussion of case studies, survival and count data models are estimated; the impact of cern procurement on suppliers' innovation is captured by the number of patent applications. the fact that firms in our sample received their first order over a long time span (1995-2008) delivers a natural partition of industrial partners into \"suppliers\" and \"not yet suppliers\". this allows estimating the impact of cern on the hazard to file a patent for the first time and on the number of patent applications, as well as the time needed for these effects to show up. we find that a \"cern effect\" does exist: being an industrial partner of cern is associated with an increase in the hazard to file a patent for the first time and in the number of patent applications. these effects require a significant \"gestation lag\" in the range of five to eight years, pointing to a relatively slow process of absorption of new ideas.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-05-23", "updated": "", "authors": ["andrea bastianin", "paolo castelnovo", "massimo florio", "anna giunta"], "url": "https://arxiv.org/abs/1905.09552"}, {"title": "semi-parametric efficient policy learning with continuous actions", "id": "1905.10116", "abstract": "we consider off-policy evaluation and optimization with continuous action spaces. we focus on observational data where the data collection policy is unknown and needs to be estimated. we take a semi-parametric approach where the value function takes a known parametric form in the treatment, but we are agnostic on how it depends on the observed contexts. we propose a doubly robust off-policy estimate for this setting and show that off-policy optimization based on this estimate is robust to estimation errors of the policy function or the regression model. our results also apply if the model does not satisfy our semi-parametric form, but rather we measure regret in terms of the best projection of the true value function to this functional space. our work extends prior approaches of policy optimization from observational data that only considered discrete actions. we provide an experimental evaluation of our method in a synthetic data example motivated by optimal personalized pricing and costly resource allocation.", "categories": "econ.em cs.lg math.st stat.ml stat.th", "created": "2019-05-24", "updated": "2019-07-20", "authors": ["mert demirer", "vasilis syrgkanis", "greg lewis", "victor chernozhukov"], "url": "https://arxiv.org/abs/1905.10116"}, {"title": "machine learning estimation of heterogeneous treatment effects with   instruments", "id": "1905.10176", "abstract": "we consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. such settings arise in a/b tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. we develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). the reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). we show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a neyman orthogonality criterion. our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. when these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. we applied our method to estimate the effect of membership on downstream webpage engagement on tripadvisor, using as an instrument an intent-to-treat a/b test among 4 million tripadvisor users, where some users received an easier membership sign-up process. we also validate our method on synthetic data and on public datasets for the effects of schooling on income.", "categories": "econ.em cs.lg stat.ap stat.ml", "created": "2019-05-24", "updated": "2019-06-05", "authors": ["vasilis syrgkanis", "victor lei", "miruna oprescu", "maggie hei", "keith battocchi", "greg lewis"], "url": "https://arxiv.org/abs/1905.10176"}, {"title": "pure nash equilibria and best-response dynamics in random games", "id": "1905.10758", "abstract": "in finite games mixed nash equilibria always exist, but pure equilibria may fail to exist. to assess the relevance of this nonexistence, we consider games where the payoffs are drawn at random. in particular, we focus on games where a large number of players can each choose one of two possible strategies, and the payoffs are i.i.d. with the possibility of ties. we provide asymptotic results about the random number of pure nash equilibria, such as fast growth and a central limit theorem, with bounds for the approximation error. moreover, by using a new link between percolation models and game theory, we describe in detail the geometry of nash equilibria and show that, when the probability of ties is small, a best-response dynamics reaches a nash equilibrium with a probability that quickly approaches one as the number of players grows. we show that a multitude of phase transitions depend only on a single parameter of the model, that is, the probability of having ties.", "categories": "cs.gt econ.th math.co math.pr", "created": "2019-05-26", "updated": "2020-06-16", "authors": ["ben amiet", "andrea collevecchio", "marco scarsini", "ziwen zhong"], "url": "https://arxiv.org/abs/1905.10758"}, {"title": "inducing sparsity and shrinkage in time-varying parameter models", "id": "1905.10787", "abstract": "time-varying parameter (tvp) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. global-local priors are increasingly used to induce shrinkage in such models. but the estimates produced by these priors can still have appreciable uncertainty. sparsification has the potential to reduce this uncertainty and improve forecasts. in this paper, we develop computationally simple methods which both shrink and sparsify tvp models. in a simulated data exercise we show the benefits of our shrink-then-sparsify approach in a variety of sparse and dense tvp regressions. in a macroeconomic forecasting exercise, we find our approach to substantially improve forecast performance relative to shrinkage alone.", "categories": "econ.em", "created": "2019-05-26", "updated": "2019-12-16", "authors": ["florian huber", "gary koop", "luca onorante"], "url": "https://arxiv.org/abs/1905.10787"}, {"title": "score-driven exponential random graphs: a new class of time-varying   parameter models for dynamical networks", "id": "1905.10806", "abstract": "motivated by the evidence that real-world networks evolve in time and may exhibit non-stationary features, we propose an extension of the exponential random graph models (ergms) accommodating the time variation of network parameters. within the ergm framework, a network realization is sampled from a static probability distribution defined parametrically in terms of network statistics. inspired by the fast growing literature on dynamic conditional score-driven models, in our approach, each parameter evolves according to an updating rule driven by the score of the conditional distribution. we demonstrate the flexibility of the score-driven ergms, both as data generating processes and as filters, and we prove the advantages of the dynamic version with respect to the static one. our method captures dynamical network dependencies, that emerge from the data, and allows for a test discriminating between static or time-varying parameters. finally, we corroborate our findings with the application to networks from real financial and political systems exhibiting non stationary dynamics.", "categories": "stat.ap econ.em econ.gn q-fin.ec", "created": "2019-05-26", "updated": "", "authors": ["domenico di gangi", "giacomo bormetti", "fabrizio lillo"], "url": "https://arxiv.org/abs/1905.10806"}, {"title": "contest architecture with public disclosures", "id": "1905.11004", "abstract": "i study optimal disclosure policies in sequential contests. a contest designer chooses at which periods to publicly disclose the efforts of previous contestants. i provide results for a wide range of possible objectives for the contest designer. while different objectives involve different trade-offs, i show that under many circumstances the optimal contest is one of the three basic contest structures widely studied in the literature: simultaneous, first-mover, or sequential contest.", "categories": "econ.th cs.gt econ.gn q-fin.ec", "created": "2019-05-27", "updated": "2019-05-28", "authors": ["toomas hinnosaar"], "url": "https://arxiv.org/abs/1905.11004"}, {"title": "local asymptotic equivalence of the bai and ng (2004) and moon and   perron (2004) frameworks for panel unit root testing", "id": "1905.11184", "abstract": "this paper considers unit-root tests in large n and large t heterogeneous panels with cross-sectional dependence generated by unobserved factors. we reconsider the two prevalent approaches in the literature, that of moon and perron (2004) and the panic setup proposed in bai and ng (2004). while these have been considered as completely different setups, we show that, in case of gaussian innovations, the frameworks are asymptotically equivalent in the sense that both experiments are locally asymptotically normal (lan) with the same central sequence. using le cam's theory of statistical experiments we determine the local asymptotic power envelope and derive an optimal test jointly in both setups. we show that the popular moon and perron (2004) and bai and ng (2010) tests only attain the power envelope in case there is no heterogeneity in the long-run variance of the idiosyncratic components. the new test is asymptotically uniformly most powerful irrespective of possible heterogeneity. moreover, it turns out that for any test, satisfying a mild regularity condition, the size and local asymptotic power are the same under both data generating processes. thus, applied researchers do not need to decide on one of the two frameworks to conduct unit root tests. monte-carlo simulations corroborate our asymptotic results and document significant gains in finite-sample power if the variances of the idiosyncratic shocks differ substantially among the cross sectional units.", "categories": "econ.em", "created": "2019-05-27", "updated": "", "authors": ["oliver wichert", "i. gaia becheri", "feike c. drost", "ramon van den akker"], "url": "https://arxiv.org/abs/1905.11184"}, {"title": "autonomous driving and residential location preferences: evidence from a   stated choice survey", "id": "1905.11486", "abstract": "the literature suggests that autonomous vehicles (avs) may drastically change the user experience of private automobile travel by allowing users to engage in productive or relaxing activities while travelling. as a consequence, the generalised cost of car travel may decrease, and car users may become less sensitive to travel time. by facilitating private motorised mobility, avs may eventually impact land use and households' residential location choices. this paper seeks to advance the understanding of the potential impacts of avs on travel behaviour and land use by investigating stated preferences for combinations of residential locations and travel options for the commute in the context of autonomous automobile travel. our analysis draws from a stated preference survey, which was completed by 512 commuters from the sydney metropolitan area in australia and provides insights into travel time valuations in a long-term decision-making context. for the analysis of the stated choice data, mixed logit models are estimated. based on the empirical results, no changes in the valuation of travel time due to the advent of avs should be expected. however, given the hypothetical nature of the stated preference survey, the results may be affected by methodological limitations.", "categories": "econ.gn q-fin.ec", "created": "2019-05-27", "updated": "2019-09-25", "authors": ["rico krueger", "taha h. rashidi", "vinayak v. dixit"], "url": "https://arxiv.org/abs/1905.11486"}, {"title": "perceived advantage in perspective application of integrated choice and   latent variable model to capture electric vehicles perceived advantage from   consumers perspective", "id": "1905.11606", "abstract": "relative advantage, or the degree to which a new technology is perceived to be better over the existing technology it supersedes, has a significant impact on individuals decision of adopting to the new technology. this paper investigates the impact of electric vehicles perceived advantage over the conventional internal combustion engine vehicles, from consumers perspective, on their decision to select electric vehicles. data is obtained from a stated preference survey from 1176 residents in new south wales, australia. the collected data is used to estimate an integrated choice and latent variable model of electric vehicle choice, which incorporates the perceived advantage of electric vehicles in the form of latent variables in the utility function. the design of the electric vehicle, impact on the environment, and safety are three identified advantages from consumers point of view. the model is used to simulate the effectiveness of various policies to promote electric vehicles on different cohorts. rebate on the purchase price is found to be the most effective strategy to promote electric vehicles adoption.", "categories": "econ.gn q-fin.ec", "created": "2019-05-28", "updated": "2019-11-28", "authors": ["milad ghasri", "ali ardeshiri", "taha rashidi"], "url": "https://arxiv.org/abs/1905.11606"}, {"title": "credit scoring by incorporating dynamic networked information", "id": "1905.11795", "abstract": "in this paper, the credit scoring problem is studied by incorporating networked information, where the advantages of such incorporation are investigated theoretically in two scenarios. firstly, a bayesian optimal filter is proposed to provide risk prediction for lenders assuming that published credit scores are estimated merely from structured financial data. such prediction can then be used as a monitoring indicator for the risk management in lenders' future decisions. secondly, a recursive bayes estimator is further proposed to improve the precision of credit scoring by incorporating the dynamic interaction topology of clients. it is shown that under the proposed evolution framework, the designed estimator has a higher precision than any efficient estimator, and the mean square errors are strictly smaller than the cram\\'er-rao lower bound for clients within a certain range of scores. finally, simulation results for a special case illustrate the feasibility and effectiveness of the proposed algorithms.", "categories": "econ.th", "created": "2019-05-28", "updated": "2019-10-31", "authors": ["yibei li", "ximei wang", "boualem djehiche", "xiaoming hu"], "url": "https://arxiv.org/abs/1905.11795"}, {"title": "graph-based era segmentation of international financial integration", "id": "1905.11842", "abstract": "assessing world-wide financial integration constitutes a recurrent challenge in macroeconometrics, often addressed by visual inspections searching for data patterns. econophysics literature enables us to build complementary, data-driven measures of financial integration using graphs. the present contribution investigates the potential and interests of a novel 3-step approach that combines several state-of-the-art procedures to i) compute graph-based representations of the multivariate dependence structure of asset prices time series representing the financial states of 32 countries world-wide (1955-2015); ii) compute time series of 5 graph-based indices that characterize the time evolution of the topologies of the graph; iii) segment these time evolutions in piece-wise constant eras, using an optimization framework constructed on a multivariate multi-norm total variation penalized functional. the method shows first that it is possible to find endogenous stable eras of world-wide financial integration. then, our results suggest that the most relevant globalization eras would be based on the historical patterns of global capital flows, while the major regulatory events of the 1970s would only appear as a cause of sub-segmentation.", "categories": "q-fin.gn econ.em physics.data-an", "created": "2019-05-28", "updated": "", "authors": ["c\u00e9cile bastidon", "antoine parent", "pablo jensen", "patrice abry", "pierre borgnat"], "url": "https://arxiv.org/abs/1905.11842"}, {"title": "matching on what matters: a pseudo-metric learning approach to matching   estimation in high dimensions", "id": "1905.12020", "abstract": "when pre-processing observational data via matching, we seek to approximate each unit with maximally similar peers that had an alternative treatment status--essentially replicating a randomized block design. however, as one considers a growing number of continuous features, a curse of dimensionality applies making asymptotically valid inference impossible (abadie and imbens, 2006). the alternative of ignoring plausibly relevant features is certainly no better, and the resulting trade-off substantially limits the application of matching methods to \"wide\" datasets. instead, li and fu (2017) recasts the problem of matching in a metric learning framework that maps features to a low-dimensional space that facilitates \"closer matches\" while still capturing important aspects of unit-level heterogeneity. however, that method lacks key theoretical guarantees and can produce inconsistent estimates in cases of heterogeneous treatment effects. motivated by straightforward extension of existing results in the matching literature, we present alternative techniques that learn latent matching features through either mlps or through siamese neural networks trained on a carefully selected loss function. we benchmark the resulting alternative methods in simulations as well as against two experimental data sets--including the canonical nsw worker training program data set--and find superior performance of the neural-net-based methods.", "categories": "econ.em cs.lg stat.ml", "created": "2019-05-28", "updated": "", "authors": ["gentry johnson", "brian quistorff", "matt goldman"], "url": "https://arxiv.org/abs/1905.12020"}, {"title": "heuristics in multi-winner approval voting", "id": "1905.12104", "abstract": "in many real world situations, collective decisions are made using voting. moreover, scenarios such as committee or board elections require voting rules that return multiple winners. in multi-winner approval voting (av), an agent may vote for as many candidates as they wish. winners are chosen by tallying up the votes and choosing the top-$k$ candidates receiving the most votes. an agent may manipulate the vote to achieve a better outcome by voting in a way that does not reflect their true preferences. in complex and uncertain situations, agents may use heuristics to strategize, instead of incurring the additional effort required to compute the manipulation which most favors them. in this paper, we examine voting behavior in multi-winner approval voting scenarios with complete information. we show that people generally manipulate their vote to obtain a better outcome, but often do not identify the optimal manipulation. instead, voters tend to prioritize the candidates with the highest utilities. using simulations, we demonstrate the effectiveness of these heuristics in situations where agents only have access to partial information.", "categories": "cs.gt cs.ai cs.ma econ.gn q-fin.ec", "created": "2019-05-28", "updated": "2019-05-30", "authors": ["jaelle scheuerman", "jason l. harman", "nicholas mattei", "k. brent venable"], "url": "https://arxiv.org/abs/1905.12104"}, {"title": "centered and non-centered variance inflation factor", "id": "1905.12293", "abstract": "this paper analyzes the diagnostic of near multicollinearity in a multiple linear regression from auxiliary centered regressions (with intercept) and non-centered (without intercept). from these auxiliary regression, the centered and non-centered variance inflation factors are calculated, respectively. it is also presented an expression that relate both of them.", "categories": "econ.em stat.me", "created": "2019-05-29", "updated": "", "authors": ["rom\u00e1n salmer\u00f3n g\u00f3mez", "catalina garc\u00eda garc\u00eda y jos\u00e9 garc\u00eda p\u00e9rez"], "url": "https://arxiv.org/abs/1905.12293"}, {"title": "deep generalized method of moments for instrumental variable analysis", "id": "1905.12495", "abstract": "instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. the application of standard methods such as 2sls, gmm, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and/or the treatment is high-dimensional. in this paper, we propose the deepgmm algorithm to overcome this. our algorithm is based on a new variational reformulation of gmm with optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. we further develop practical techniques for optimization and model selection that make it particularly successful in practice. our algorithm is also computationally tractable and can handle large-scale datasets. numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break.", "categories": "stat.ml cs.lg econ.em", "created": "2019-05-29", "updated": "2020-04-18", "authors": ["andrew bennett", "nathan kallus", "tobias schnabel"], "url": "https://arxiv.org/abs/1905.12495"}, {"title": "on the many-to-one strongly stable fractional matching set", "id": "1905.12500", "abstract": "for a many-to-one matching market where firms have strict and $\\boldsymbol{q}$-responsive preferences, we give a characterization of the set of strongly stable fractional matchings as the union of the convex hull of all connected sets of stable matchings. also, we prove that a strongly stable fractional matching is represented as a convex combination of stable matchings that are ordered in the common preferences of all firms.", "categories": "econ.th", "created": "2019-05-29", "updated": "2020-05-22", "authors": ["pablo a. neme", "jorge oviedo"], "url": "https://arxiv.org/abs/1905.12500"}, {"title": "detectability, duality, and surplus extraction", "id": "1905.12788", "abstract": "we study the problem of surplus extraction in the general environment of mcafee and reny (1992), and provide two alternative proofs of their main theorem. the first is an analogue of the classic argument of cr{\\' e}mer and mclean (1985, 1988), using geometric features of the set of agents' beliefs to construct a menu of contracts extracting the desired surplus. this argument, which requires a finite state space, also leads to a counterexample showing that full extraction is not possible without further significant conditions on agents' beliefs or surplus, even if the designer offers an infinite menu of contracts. the second argument uses duality and applies for an infinite state space, thus yielding the general result of mcafee and reny (1992). by providing a connection to duality, this argument suggests methods for studying surplus extraction in other models in which agents or the designer might have objectives other than risk-neutral value maximization.", "categories": "econ.th", "created": "2019-05-29", "updated": "", "authors": ["giuseppe lopomo", "luca rigotti", "chris shannon"], "url": "https://arxiv.org/abs/1905.12788"}, {"title": "heterogeneity in demand and optimal price conditioning for local rail   transport", "id": "1905.12859", "abstract": "this paper describes the results of research project on optimal pricing for llc \"perm local rail company\". in this study we propose a regression tree based approach for estimation of demand function for local rail tickets considering high degree of demand heterogeneity by various trip directions and the goals of travel. employing detailed data on ticket sales for 5 years we estimate the parameters of demand function and reveal the significant variation in price elasticity of demand. while in average the demand is elastic by price, near a quarter of trips is characterized by weakly elastic demand. lower elasticity of demand is correlated with lower degree of competition with other transport and inflexible frequency of travel.", "categories": "econ.em cs.lg", "created": "2019-05-30", "updated": "", "authors": ["evgeniy m. ozhegov", "alina ozhegova"], "url": "https://arxiv.org/abs/1905.12859"}, {"title": "the income fluctuation problem and the evolution of wealth", "id": "1905.13045", "abstract": "we analyze the household savings problem in a general setting where returns on assets, non-financial income and impatience are all state dependent and fluctuate over time. all three processes can be serially correlated and mutually dependent. rewards can be bounded or unbounded and wealth can be arbitrarily large. extending classic results from an earlier literature, we determine conditions under which (a) solutions exist, are unique and are globally computable, (b) the resulting wealth dynamics are stationary, ergodic and geometrically mixing, and (c) the wealth distribution has a pareto tail. we show how these results can be used to extend recent studies of the wealth distribution. our conditions have natural economic interpretations in terms of asymptotic growth rates for discounting and return on savings.", "categories": "econ.th econ.em", "created": "2019-05-29", "updated": "2020-02-28", "authors": ["qingyin ma", "john stachurski", "alexis akira toda"], "url": "https://arxiv.org/abs/1905.13045"}, {"title": "nonparametric sample splitting", "id": "1905.13140", "abstract": "this paper develops a threshold regression model, where the threshold is determined by an unknown relation between two variables. the threshold function is estimated fully nonparametrically. the observations are allowed to be cross-sectionally dependent and the model can be applied to determine an unknown spatial border for sample splitting over a random field. the uniform rate of convergence and the nonstandard limiting distribution of the nonparametric threshold estimator are derived. the root-n consistency and the asymptotic normality of the regression slope parameter estimator are also obtained. empirical relevance is illustrated by estimating an economic border induced by the housing price difference between queens and brooklyn in new york city, where the economic border deviates substantially from the administrative one.", "categories": "econ.em", "created": "2019-05-30", "updated": "", "authors": ["yoonseok lee", "yulong wang"], "url": "https://arxiv.org/abs/1905.13140"}, {"title": "labor market outcomes and early schooling: evidence from school entry   policies using exact date of birth", "id": "1905.13281", "abstract": "we use a rich, census-like brazilian dataset containing information on spatial mobility, schooling, and income in which we can link children to parents to assess the impact of early education on several labor market outcomes. brazilian public primary schools admit children up to one year younger than the national minimum age to enter school if their birthday is before an arbitrary threshold, causing an exogenous variation in schooling at adulthood. using a regression discontinuity design, we estimate one additional year of schooling increases labor income in 25.8% - almost twice as large as estimated using mincerian models. around this cutoff there is also a gap of 9.6% on the probability of holding a college degree in adulthood, with which we estimate the college premium and find a 201% increase in labor income. we test the robustness of our estimates using placebo variables, alternative model specifcations and mccrary density tests.", "categories": "econ.gn q-fin.ec", "created": "2019-05-30", "updated": "", "authors": ["pedro cavalcante oliveira", "daniel duque"], "url": "https://arxiv.org/abs/1905.13281"}, {"title": "characterizing shadow price via lagrangian multiplier for nonsmooth   problem", "id": "1905.13622", "abstract": "in this paper, a relation between shadow price and the lagrangian multiplier for nonsmooth problem is explored. it is shown that the lagrangian multiplier is the upper bound of shadow price for convex optimization and a class of lipschtzian optimizations. this work can be used in shadow pricing for nonsmooth situation. the several nonsmooth functions involved in this class of lipschtzian optimizations is listed. finally, an application to electricity pricing is discussed.", "categories": "econ.th", "created": "2019-05-31", "updated": "", "authors": ["yan gao"], "url": "https://arxiv.org/abs/1905.13622"}, {"title": "resolving new keynesian anomalies with wealth in the utility function", "id": "1905.13645", "abstract": "at the zero lower bound, the new keynesian model predicts that output and inflation collapse to implausibly low levels, and that government spending and forward guidance have implausibly large effects. to resolve these anomalies, we introduce wealth into the utility function; the justification is that wealth is a marker of social status, and people value status. since people partly save to accrue social status, the euler equation is modified. as a result, when the marginal utility of wealth is sufficiently large, the dynamical system representing the zero-lower-bound equilibrium transforms from a saddle to a source---which resolves all the anomalies.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-05-31", "updated": "2019-12-11", "authors": ["pascal michaillat", "emmanuel saez"], "url": "https://arxiv.org/abs/1905.13645"}, {"title": "on policy evaluation with aggregate time-series shocks", "id": "1905.13660", "abstract": "we propose a general strategy for estimating treatment effects, in contexts where the only source of exogenous variation is a sequence of aggregate time-series shocks. we start by arguing that commonly used estimation procedures tend to ignore crucial time-series aspects of the data. next, we develop a graphical tool and a formal test to illustrate the issues of the design using data from prominent studies in development economics and macroeconomic. motivated by these studies, we construct a new iv estimator, which is based on the time-series model for the aggregate shock. we analyze the statistical properties of our estimator in a practically relevant case, where both cross-sectional and time-series dimensions are of similar size. finally, to provide a causal interpretation for our estimator, we analyze a new causal model that allows taking into account both rich unobserved heterogeneity in potential outcomes and unobserved aggregate shocks.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-05-31", "updated": "2020-05-29", "authors": ["dmitry arkhangelsky", "vasily korovkin"], "url": "https://arxiv.org/abs/1905.13660"}, {"title": "counterfactual analysis under partial identification using locally   robust refinement", "id": "1906.00003", "abstract": "structural models that admit multiple reduced forms, such as game-theoretic models with multiple equilibria, pose challenges in practice, especially when parameters are set-identified and the identified set is large. in such cases, researchers often choose to focus on a particular subset of equilibria for counterfactual analysis, but this choice can be hard to justify. this paper shows that some parameter values can be more ``desirable'' than others for counterfactual analysis, even if they are empirically equivalent given the data. in particular, within the identified set, some counterfactual predictions can exhibit more robustness than others, against local perturbations of the reduced forms (e.g. the equilibrium selection rule). we provide a representation of this subset which can be used to simplify the implementation. we illustrate our message using moment inequality models, and provide an empirical application based on a model with top-coded data.", "categories": "econ.em stat.me", "created": "2019-05-31", "updated": "2020-04-06", "authors": ["nathan canen", "kyungchul song"], "url": "https://arxiv.org/abs/1906.00003"}, {"title": "nprobust: nonparametric kernel-based estimation and robust   bias-corrected inference", "id": "1906.00198", "abstract": "nonparametric kernel density and local polynomial regression estimators are very popular in statistics, economics, and many other disciplines. they are routinely employed in applied work, either as part of the main empirical analysis or as a preliminary ingredient entering some other estimation or inference procedure. this article describes the main methodological and numerical features of the software package nprobust, which offers an array of estimation and inference procedures for nonparametric kernel-based density and local polynomial regression methods, implemented in both the r and stata statistical platforms. the package includes not only classical bandwidth selection, estimation, and inference methods (wand and jones, 1995; fan and gijbels, 1996), but also other recent developments in the statistics and econometrics literatures such as robust bias-corrected inference and coverage error optimal bandwidth selection (calonico, cattaneo and farrell, 2018, 2019). furthermore, this article also proposes a simple way of estimating optimal bandwidths in practice that always delivers the optimal mean square error convergence rate regardless of the specific evaluation point, that is, no matter whether it is implemented at a boundary or interior point. numerical performance is illustrated using an empirical application and simulated data, where a detailed numerical comparison with other r packages is given.", "categories": "stat.co econ.em stat.me", "created": "2019-06-01", "updated": "", "authors": ["sebastian calonico", "matias d. cattaneo", "max h. farrell"], "url": "https://arxiv.org/abs/1906.00198"}, {"title": "lspartition: partitioning-based least squares regression", "id": "1906.00202", "abstract": "nonparametric partitioning-based least squares regression is an important tool in empirical work. common examples include regressions based on splines, wavelets, and piecewise polynomials. this article discusses the main methodological and numerical features of the r software package lspartition, which implements modern estimation and inference results for partitioning-based least squares (series) regression estimation. this article discusses the main methodological and numerical features of the r software package lspartition, which implements results for partitioning-based least squares (series) regression estimation and inference from cattaneo and farrell (2013) and cattaneo, farrell, and feng (2019). these results cover the multivariate regression function as well as its derivatives. first, the package provides data-driven methods to choose the number of partition knots optimally, according to integrated mean squared error, yielding optimal point estimation. second, robust bias correction is implemented to combine this point estimator with valid inference. third, the package provides estimates and inference for the unknown function both pointwise and uniformly in the conditioning variables. in particular, valid confidence bands are provided. finally, an extension to two-sample analysis is developed, which can be used in treatment-control comparisons and related problems", "categories": "stat.co econ.em stat.me", "created": "2019-06-01", "updated": "2019-08-08", "authors": ["matias d. cattaneo", "max h. farrell", "yingjie feng"], "url": "https://arxiv.org/abs/1906.00202"}, {"title": "kernel instrumental variable regression", "id": "1906.00232", "abstract": "instrumental variable (iv) regression is a strategy for learning causal relationships in observational data. if measurements of input x and output y are confounded, the causal relationship can nonetheless be identified if an instrumental variable z is available that influences x directly, but is conditionally independent of y given x and the unmeasured confounder. the classic two-stage least squares algorithm (2sls) simplifies the estimation problem by modeling all relationships as linear functions. we propose kernel instrumental variable regression (kiv), a nonparametric generalization of 2sls, modeling relations among x, y, and z as nonlinear functions in reproducing kernel hilbert spaces (rkhss). we prove the consistency of kiv under mild assumptions, and derive conditions under which convergence occurs at the minimax optimal rate for unconfounded, single-stage rkhs regression. in doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. in experiments, kiv outperforms state of the art alternatives for nonparametric iv regression.", "categories": "cs.lg econ.em math.fa math.st stat.ml stat.th", "created": "2019-06-01", "updated": "2020-07-15", "authors": ["rahul singh", "maneesh sahani", "arthur gretton"], "url": "https://arxiv.org/abs/1906.00232"}, {"title": "conventions and coalitions in repeated games", "id": "1906.00280", "abstract": "we develop a theory of repeated interaction for coalitional behavior. we consider stage games where both individuals and coalitions may deviate. however, coalition members cannot commit to long-run behavior, and anticipate that today's actions influence tomorrow's behavior. we evaluate the degree to which history-dependence can deter coalitional deviations. if monitoring is perfect, every feasible and strictly individually rational payoff can be supported by history-dependent conventions. by contrast, if players can make secret side-payments to each other, every coalition achieves a coalitional minmax value, potentially reducing the set of supportable payoffs to the core of the stage game.", "categories": "econ.th", "created": "2019-06-01", "updated": "2020-01-31", "authors": ["s. nageeb ali", "ce liu"], "url": "https://arxiv.org/abs/1906.00280"}, {"title": "at what level should one cluster standard errors in paired experiments,   and in stratified experiments with small strata?", "id": "1906.00288", "abstract": "in paired experiments, units are matched into pairs, and one unit of each pair is randomly assigned to treatment. to estimate the treatment effect, researchers often regress their outcome on a treatment indicator and pair fixed effects, clustering standard errors at the unit-of-randomization level. we show that the variance estimator in this regression may be severely downward biased: under constant treatment effect, its expectation equals 1/2 of the true variance. instead, we show that researchers should cluster their standard errors at the pair level. using simulations, we show that those results extend to stratified experiments with few units per strata.", "categories": "econ.em", "created": "2019-06-01", "updated": "2020-09-15", "authors": ["cl\u00e9ment de chaisemartin", "jaime ramirez-cuellar"], "url": "https://arxiv.org/abs/1906.00288"}, {"title": "the theory of weak revealed preference", "id": "1906.00296", "abstract": "we offer a rationalization of the weak generalized axiom of revealed preference (wgarp) for both finite and infinite data sets of consumer choice. we call it maximin rationalization, in which each pairwise choice is associated with a \"local\" utility function. we develop its associated weak revealed-preference theory. we show that preference recoverability and welfare analysis \\`a la varian (1982) may not be informative enough, when the weak axiom holds, but when consumers are not utility maximizers. we clarify the reasons for this failure and provide new informative bounds for the consumer's true preferences.", "categories": "econ.th econ.em", "created": "2019-06-01", "updated": "", "authors": ["victor h. aguiar", "per hjertstrand", "roberto serrano"], "url": "https://arxiv.org/abs/1906.00296"}, {"title": "artificial intelligence and big data in entrepreneurship: a new era has   begun", "id": "1906.00553", "abstract": "while the disruptive potential of artificial intelligence (ai) and big data has been receiving growing attention and concern in a variety of research and application fields over the last few years, it has not received much scrutiny in contemporary entrepreneurship research so far. here we present some reflections and a collection of papers on the role of ai and big data for this emerging area in the study and application of entrepreneurship research. while being mindful of the potentially overwhelming nature of the rapid progress in machine intelligence and other big data technologies for contemporary structures in entrepreneurship research, we put an emphasis on the reciprocity of the co-evolving fields of entrepreneurship research and practice. how can ai and big data contribute to a productive transformation of the research field and the real-world phenomena (e.g., 'smart entrepreneurship')? we also discuss, however, ethical issues as well as challenges around a potential contradiction between entrepreneurial uncertainty and rule-driven ai rationality. the editorial gives researchers and practitioners orientation and showcases avenues and examples for concrete research in this field. at the same time, however, it is not unlikely that we will encounter unforeseeable and currently inexplicable developments in the field soon. we call on entrepreneurship scholars, educators, and practitioners to proactively prepare for future scenarios.", "categories": "econ.gn q-fin.ec", "created": "2019-06-02", "updated": "", "authors": ["martin obschonka", "david b. audretsch"], "url": "https://arxiv.org/abs/1906.00553"}, {"title": "the laws of motion of the broker call rate in the united states", "id": "1906.00946", "abstract": "in this paper, which is the third installment of the author's trilogy on margin loan pricing, we analyze $1,367$ monthly observations of the u.s. broker call money rate, which is the interest rate at which stock brokers can borrow to fund their margin loans to retail clients. we describe the basic features and mean-reverting behavior of this series and juxtapose the empirically-derived laws of motion with the author's prior theories of margin loan pricing (garivaltis 2019a-b). this allows us to derive stochastic differential equations that govern the evolution of the margin loan interest rate and the leverage ratios of sophisticated brokerage clients (namely, continuous time kelly gamblers). finally, we apply merton's (1974) arbitrage theory of corporate liability pricing to study theoretical constraints on the risk premia that could be generated in the market for call money. apparently, if there is no arbitrage in the u.s. financial markets, the implication is that the total volume of call loans must constitute north of $70\\%$ of the value of all leveraged portfolios.", "categories": "econ.em econ.gn q-fin.ec q-fin.gn q-fin.pm q-fin.rm", "created": "2019-06-03", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1906.00946"}, {"title": "two resolutions of the margin loan pricing puzzle", "id": "1906.01025", "abstract": "this paper supplies two possible resolutions of fortune's (2000) margin-loan pricing puzzle. fortune (2000) noted that the margin loan interest rates charged by stock brokers are very high in relation to the actual (low) credit risk and the cost of funds. if we live in the black-scholes world, the brokers are presumably making arbitrage profits by shorting dynamically precise amounts of their clients' portfolios. first, we extend fortune's (2000) application of merton's (1974) no-arbitrage approach to allow for brokers that can only revise their hedges finitely many times during the term of the loan. we show that extremely small differences in the revision frequency can easily explain the observed variation in margin loan pricing. in fact, four additional revisions per three-day period serve to explain all of the currently observed heterogeneity. second, we study monopolistic (or oligopolistic) margin loan pricing by brokers whose clients are continuous-time kelly gamblers. the broker solves a general stochastic control problem that yields simple and pleasant formulas for the optimal interest rate and the net interest margin. if the author owned a brokerage, he would charge an interest rate of $(r+\\nu)/2-\\sigma^2/4$, where $r$ is the cost of funds, $\\nu$ is the compound-annual growth rate of the s&p 500 index, and $\\sigma$ is the volatility.", "categories": "econ.gn econ.th q-fin.ec q-fin.gn q-fin.pm q-fin.rm", "created": "2019-06-03", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1906.01025"}, {"title": "stress testing network reconstruction via graphical causal model", "id": "1906.01468", "abstract": "an resilience optimal evaluation of financial portfolios implies having plausible hypotheses about the multiple interconnections between the macroeconomic variables and the risk parameters. in this paper, we propose a graphical model for the reconstruction of the causal structure that links the multiple macroeconomic variables and the assessed risk parameters, it is this structure that we call stress testing network (stn). in this model, the relationships between the macroeconomic variables and the risk parameter define a \"relational graph\" among their time-series, where related time-series are connected by an edge. our proposal is based on the temporal causal models, but unlike, we incorporate specific conditions in the structure which correspond to intrinsic characteristics this type of networks. using the proposed model and given the high-dimensional nature of the problem, we used regularization methods to efficiently detect causality in the time-series and reconstruct the underlying causal structure. in addition, we illustrate the use of model in credit risk data of a portfolio. finally, we discuss its uses and practical benefits in stress testing.", "categories": "stat.ap econ.em math.oc stat.co", "created": "2019-06-03", "updated": "2020-01-30", "authors": ["helder rojas", "david dias"], "url": "https://arxiv.org/abs/1906.01468"}, {"title": "assessing disparate impacts of personalized interventions:   identifiability and bounds", "id": "1906.01552", "abstract": "personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. while the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. we prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. we further provide a sensitivity analysis for this assumption by means of sharp partial-identification bounds under violations of monotonicity of varying strengths. we show how to use our results to audit personalized interventions using partially-identified roc and xroc curves and demonstrate this in a case study of a french job training dataset.", "categories": "stat.ml cs.lg econ.em", "created": "2019-06-04", "updated": "", "authors": ["nathan kallus", "angela zhou"], "url": "https://arxiv.org/abs/1906.01552"}, {"title": "indirect inference for locally stationary models", "id": "1906.01768", "abstract": "we propose the use of indirect inference estimation to conduct inference in complex locally stationary models. we develop a local indirect inference algorithm and establish the asymptotic properties of the proposed estimator. due to the nonparametric nature of locally stationary models, the resulting indirect inference estimator exhibits nonparametric rates of convergence. we validate our methodology with simulation studies in the confines of a locally stationary moving average model and a new locally stationary multiplicative stochastic volatility model. using this indirect inference methodology and the new locally stationary volatility model, we obtain evidence of non-linear, time-varying volatility trends for monthly returns on several fama-french portfolios.", "categories": "econ.em", "created": "2019-06-04", "updated": "2020-05-10", "authors": ["david frazier", "bonsoo koo"], "url": "https://arxiv.org/abs/1906.01768"}, {"title": "bayesian nonparametric graphical models for time-varying parameters var", "id": "1906.02140", "abstract": "over the last decade, big data have poured into econometrics, demanding new statistical methods for analysing high-dimensional data and complex non-linear relationships. a common approach for addressing dimensionality issues relies on the use of static graphical structures for extracting the most significant dependence interrelationships between the variables of interest. recently, bayesian nonparametric techniques have become popular for modelling complex phenomena in a flexible and efficient manner, but only few attempts have been made in econometrics. in this paper, we provide an innovative bayesian nonparametric (bnp) time-varying graphical framework for making inference in high-dimensional time series. we include a bayesian nonparametric dependent prior specification on the matrix of coefficients and the covariance matrix by mean of a time-series dpp as in nieto-barajas et al. (2012). following billio et al. (2019), our hierarchical prior overcomes over-parametrization and over-fitting issues by clustering the vector autoregressive (var) coefficients into groups and by shrinking the coefficients of each group toward a common location. our bnp timevarying var model is based on a spike-and-slab construction coupled with dependent dirichlet process prior (dpp) and allows to: (i) infer time-varying granger causality networks from time series; (ii) flexibly model and cluster non-zero time-varying coefficients; (iii) accommodate for potential non-linearities. in order to assess the performance of the model, we study the merits of our approach by considering a well-known macroeconomic dataset. moreover, we check the robustness of the method by comparing two alternative specifications, with dirac and diffuse spike prior distributions.", "categories": "econ.em stat.me", "created": "2019-06-03", "updated": "", "authors": ["matteo iacopini", "luca rossini"], "url": "https://arxiv.org/abs/1906.02140"}, {"title": "game-theoretic optimal portfolios in continuous time", "id": "1906.02216", "abstract": "we consider a two-person trading game in continuous time whereby each player chooses a constant rebalancing rule $b$ that he must adhere to over $[0,t]$. if $v_t(b)$ denotes the final wealth of the rebalancing rule $b$, then player 1 (the `numerator player') picks $b$ so as to maximize $\\mathbb{e}[v_t(b)/v_t(c)]$, while player 2 (the `denominator player') picks $c$ so as to minimize it. in the unique nash equilibrium, both players use the continuous-time kelly rule $b^*=c^*=\\sigma^{-1}(\\mu-r\\textbf{1})$, where $\\sigma$ is the covariance of instantaneous returns per unit time, $\\mu$ is the drift vector of the stock market, and $\\textbf{1}$ is a vector of ones. thus, even over very short intervals of time $[0,t]$, the desire to perform well relative to other traders leads one to adopt the kelly rule, which is ordinarily derived by maximizing the asymptotic exponential growth rate of wealth. hence, we find agreement with bell and cover's (1988) result in discrete time.", "categories": "q-fin.pm econ.gn econ.th q-fin.ec q-fin.gn q-fin.mf", "created": "2019-06-05", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1906.02216"}, {"title": "mapping the sahelian space", "id": "1906.02223", "abstract": "this chapter examines the geographical meaning of the sahel, its fluid boundaries, and its spatial dynamics. unlike other approaches that define the sahel as a bioclimatic zone or as an ungoverned area, it shows that the sahel is primarily a space of circulation in which uncertainty has historically been overcome by mobility. the first part of the paper discusses how pre-colonial empires relied on a network of markets and cities that facilitated trade and social relationships across the region and beyond. the second part explores changing regional mobility patterns precipitated by colonial powers and the new approach they developed to control networks and flows. the third part discusses the contradiction between the mobile strategies adopted by local herders, farmers and traders in the sahel and the territorial development initiatives of modern states and international donors. particular attention is paid in the last section to how the sahel was progressively redefined through a security lens.", "categories": "econ.gn q-fin.ec", "created": "2019-06-05", "updated": "", "authors": ["olivier walther", "denis retaille"], "url": "https://arxiv.org/abs/1906.02223"}, {"title": "the route to chaos in routing games: when is price of anarchy too   optimistic?", "id": "1906.02486", "abstract": "routing games are amongst the most studied classes of games. their two most well-known properties are that learning dynamics converge to equilibria and that all equilibria are approximately optimal. in this work, we perform a stress test for these classic results by studying the ubiquitous dynamics, multiplicative weights update, in different classes of congestion games, uncovering intricate non-equilibrium phenomena. as the system demand increases, the learning dynamics go through period-doubling bifurcations, leading to instabilities, chaos and large inefficiencies even in the simplest case of non-atomic routing games with two paths of linear cost where the price of anarchy is equal to one.   starting with this simple class, we show that every system has a carrying capacity, above which it becomes unstable. if the equilibrium flow is a symmetric $50-50\\%$ split, the system exhibits one period-doubling bifurcation. a single periodic attractor of period two replaces the attracting fixed point. although the price of anarchy is equal to one, in the large population limit the time-average social cost for all but a zero measure set of initial conditions converges to its worst possible value. for asymmetric equilibrium flows, increasing the demand eventually forces the system into li-yorke chaos with positive topological entropy and periodic orbits of all possible periods. remarkably, in all non-equilibrating regimes, the time-average flows on the paths converge exactly to the equilibrium flows, a property akin to no-regret learning in zero-sum games. these results are robust. we extend them to routing games with arbitrarily many strategies, polynomial cost functions, non-atomic as well as atomic routing games and heteregenous users. our results are also applicable to any sequence of shrinking learning rates, e.g., $1/\\sqrt{t}$, by allowing for a dynamically increasing population size.", "categories": "cs.gt econ.gn math.ds nlin.cd physics.soc-ph q-fin.ec", "created": "2019-06-06", "updated": "2019-11-24", "authors": ["thiparat chotibut", "fryderyk falniowski", "micha\u0142 misiurewicz", "georgios piliouras"], "url": "https://arxiv.org/abs/1906.02486"}, {"title": "counterfactual inference for consumer choice across many product   categories", "id": "1906.02635", "abstract": "this paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in parallel. the consumer's utility is additive in the different categories. her preferences about product attributes as well as her price sensitivity vary across products and are in general correlated across products. we build on techniques from the machine learning literature on probabilistic models of matrix factorization, extending the methods to account for time-varying product attributes and products going out of stock. we evaluate the performance of the model using held-out data from weeks with price changes or out of stock products. we show that our model improves over traditional modeling approaches that consider each category in isolation. one source of the improvement is the ability of the model to accurately estimate heterogeneity in preferences (by pooling information across categories); another source of improvement is its ability to estimate the preferences of consumers who have rarely or never made a purchase in a given category in the training data. using held-out data, we show that our model can accurately distinguish which consumers are most price sensitive to a given product. we consider counterfactuals such as personally targeted price discounts, showing that using a richer model such as the one we propose substantially increases the benefits of personalization in discounts.", "categories": "cs.lg econ.em stat.ml", "created": "2019-06-06", "updated": "", "authors": ["rob donnelly", "francisco r. ruiz", "david blei", "susan athey"], "url": "https://arxiv.org/abs/1906.02635"}, {"title": "the interplay between migrants and natives as a determinant of migrants'   assimilation: a coevolutionary approach", "id": "1906.02657", "abstract": "we study the migrants' assimilation, which we conceptualize as forming human capital productive on the labor market of a developed host country, and we link the observed frequent lack of assimilation with the relative deprivation that the migrants start to feel when they move in social space towards the natives. in turn, we presume that the native population is heterogenous and consists of high-skill and low-skill workers. the presence of assimilated migrants might shape the comparison group of the natives, influencing the relative deprivation of the low-skill workers and, in consequence, the choice to form human capital and become highly skilled. to analyse this interrelation between assimilation choices of migrants and skill formation of natives, we construct a coevolutionary model of the open-to-migration economy. showing that the economy might end up in a non-assimilation equilibrium, we discuss welfare consequences of an assimilation policy funded from tax levied on the native population. we identify conditions under which such costly policy can bring the migrants to assimilation and at the same time increase the welfare of the natives, even though the incomes of the former take a beating.", "categories": "econ.th", "created": "2019-06-06", "updated": "", "authors": ["jakub bielawski", "marcin jakubek"], "url": "https://arxiv.org/abs/1906.02657"}, {"title": "from blackwell dominance in large samples to renyi divergences and back   again", "id": "1906.02838", "abstract": "we study repeated independent blackwell experiments; standard examples include drawing multiple samples from a population, or performing a measurement in different locations. in the baseline setting of a binary state of nature, we compare experiments in terms of their informativeness in large samples. addressing a question due to blackwell (1951), we show that generically an experiment is more informative than another in large samples if and only if it has higher renyi divergences.   we apply our analysis to the problem of measuring the degree of dissimilarity between distributions by means of divergences. a useful property of renyi divergences is their additivity with respect to product distributions. our characterization of blackwell dominance in large samples implies that every additive divergence that satisfies the data processing inequality is an integral of renyi divergences.", "categories": "math.st econ.th math.pr stat.th", "created": "2019-06-06", "updated": "2020-09-04", "authors": ["xiaosheng mu", "luciano pomatto", "philipp strack", "omer tamuz"], "url": "https://arxiv.org/abs/1906.02838"}, {"title": "a long short-term memory stochastic volatility model", "id": "1906.02884", "abstract": "stochastic volatility (sv) models are widely used in the financial sector while long short-term memory (lstm) models are successfully used in many large-scale industrial applications of deep learning. our article combines these two methods in a non-trivial way and proposes a model, which we call the lstm-sv model, to capture the dynamics of stochastic volatility. the proposed model overcomes the short-term memory problem in conventional sv models, is able to capture non-linear dependence in the latent volatility process, and often has a better out-of-sample forecast performance than sv models. these properties are illustrated through simulation study and applications to three financial time series datasets: the us stock market weekly index sp500, the australian stock weekly index asx200 and the australian-us dollar daily exchange rates. based on our analysis, we argue that there are significant differences in the underlying dynamics between the volatility process of the sp500 and asx200 datasets and that of the exchange rate dataset. for the stock index data, there is strong evidence of long-term memory and non-linear dependence in the volatility process, while this is not the case for the exchange rates. an user-friendly software package together with the examples reported in the paper are available at https://github.com/vbayeslab.", "categories": "econ.em stat.me stat.ml", "created": "2019-06-06", "updated": "2019-09-30", "authors": ["nghia nguyen", "minh-ngoc tran", "david gunawan", "r. kohn"], "url": "https://arxiv.org/abs/1906.02884"}, {"title": "market implementation of multiple-arrival multiple-deadline   differentiated energy services", "id": "1906.02904", "abstract": "an increasing concern in power systems is how to elicit flexibilities in demand, which leads to nontraditional electricity products for accommodating loads of different flexibility levels. we have proposed multiple-arrival multiple-deadline (mamd) differentiated energy services for the flexible loads which require constant power for specified durations. such loads are indifferent to the actual power delivery time as long as the duration requirements are satisfied between the specified arrival times and deadlines. the focus of this paper is the market implementation of such services. in a forward market, we establish the existence of an efficient competitive equilibrium to verify the economic feasibility, which implies that selfish market participants can attain the maximum social welfare in a distributed manner. we also show the strengths of the mamd services by simulation.", "categories": "cs.sy econ.gn q-fin.ec", "created": "2019-06-07", "updated": "2020-03-12", "authors": ["yanfang mo", "wei chen", "li qiu", "pravin varaiya"], "url": "https://arxiv.org/abs/1906.02904"}, {"title": "battling antibiotic resistance: can machine learning improve   prescribing?", "id": "1906.03044", "abstract": "antibiotic resistance constitutes a major health threat. predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading driver of antibiotic resistance. we train a machine learning algorithm on administrative and microbiological laboratory data from denmark to predict diagnostic test outcomes for urinary tract infections. based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and policy implementation when patient distributions vary over time. the proposed policies delay antibiotic prescriptions for some patients until test results are known and give them instantly to others. we find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. as denmark is one of the most conservative countries in terms of antibiotic use, this result is likely to be a lower bound of what can be achieved elsewhere.", "categories": "econ.gn cs.cy cs.lg q-fin.ec stat.me", "created": "2019-06-05", "updated": "", "authors": ["michael allan ribers", "hannes ullrich"], "url": "https://arxiv.org/abs/1906.03044"}, {"title": "addictive auctions: using lucky-draw and gambling addiction to increase   participation during auctioning", "id": "1906.03237", "abstract": "auction theories are believed to provide a better selling opportunity for the resources to be allocated. various organizations have taken measures to increase trust among participants towards their auction system, but trust alone cannot ensure a high level of participation. we propose a new type of auction system which takes advantage of lucky draw and gambling addictions to increase the engagement level of candidates in an auction. our system makes use of security features present in existing auction systems for ensuring fairness and maintaining trust among participants.", "categories": "econ.th", "created": "2019-06-07", "updated": "", "authors": ["ravin kumar"], "url": "https://arxiv.org/abs/1906.03237"}, {"title": "on the equilibrium uniqueness in cournot competition with demand   uncertainty", "id": "1906.03558", "abstract": "we revisit the linear cournot model with uncertain demand that is studied in lagerl\\\"of (2006)* and provide sufficient conditions for equilibrium uniqueness that complement the existing results. we show that if the distribution of the demand intercept has the decreasing mean residual demand (dmrd) or the increasing generalized failure rate (igfr) property, then uniqueness of equilibrium is guaranteed. the dmrd condition implies log-concavity of the expected profits per unit of output without additional assumptions on the existence or the shape of the density of the demand intercept and, hence, answers in the affirmative the conjecture of lagerl\\\"of (2006) that such conditions may not be necessary. *johan lagerl\\\"of, equilibrium uniqueness in a cournot model with demand uncertainty. the b.e. journal in theoretical economics, vol. 6: iss 1. (topics), article 19:1--6, 2006.", "categories": "econ.th", "created": "2019-06-08", "updated": "2020-03-21", "authors": ["stefanos leonardos", "costis melolidakis"], "url": "https://arxiv.org/abs/1906.03558"}, {"title": "efficient bayesian estimation for garch-type models via sequential monte   carlo", "id": "1906.03828", "abstract": "the advantages of sequential monte carlo (smc) are exploited to develop parameter estimation and model selection methods for garch (generalized autoregressive conditional heteroskedasticity) style models. it provides an alternative method for quantifying estimation uncertainty relative to classical inference. even with long time series, it is demonstrated that the posterior distribution of model parameters are non-normal, highlighting the need for a bayesian approach and an efficient posterior sampling method. efficient approaches for both constructing the sequence of distributions in smc, and leave-one-out cross-validation, for long time series data are also proposed. finally, an unbiased estimator of the likelihood is developed for the bad environment-good environment model, a complex garch-type model, which permits exact bayesian inference not previously available in the literature.", "categories": "stat.ap econ.em stat.co", "created": "2019-06-10", "updated": "2020-03-05", "authors": ["dan li", "adam clements", "christopher drovandi"], "url": "https://arxiv.org/abs/1906.03828"}, {"title": "learned sectors: a fundamentals-driven sector reclassification project", "id": "1906.03935", "abstract": "market sectors play a key role in the efficient flow of capital through the modern global economy. we analyze existing sectorization heuristics, and observe that the most popular - the gics (which informs the s&p 500), and the naics (published by the u.s. government) - are not entirely quantitatively driven, but rather appear to be highly subjective and rooted in dogma. building on inferences from analysis of the capital structure irrelevance principle and the modigliani-miller theoretic universe conditions, we postulate that corporation fundamentals - particularly those components specific to the modigliani-miller universe conditions - would be optimal descriptors of the true economic domain of operation of a company. we generate a set of potential candidate learned sector universes by varying the linkage method of a hierarchical clustering algorithm, and the number of resulting sectors derived from the model (ranging from 5 to 19), resulting in a total of 60 candidate learned sector universes. we then introduce reindexer, a backtest-driven sector universe evaluation research tool, to rank the candidate sector universes produced by our learned sector classification heuristic. this rank was utilized to identify the risk-adjusted return optimal learned sector universe as being the universe generated under clink (i.e. complete linkage), with 17 sectors. the optimal learned sector universe was tested against the benchmark gics classification universe with reindexer, outperforming on both absolute portfolio value, and risk-adjusted return over the backtest period. we conclude that our fundamentals-driven learned sector classification heuristic provides a superior risk-diversification profile than the status quo classification heuristic.", "categories": "q-fin.gn econ.em", "created": "2019-05-30", "updated": "", "authors": ["rukmal weerawarana", "yiyi zhu", "yuzhen he"], "url": "https://arxiv.org/abs/1906.03935"}, {"title": "automation and occupational mobility: a data-driven network model", "id": "1906.04086", "abstract": "the potential impact of automation on the labor market is a topic that has generated significant interest and concern amongst scholars, policymakers, and the broader public. a number of studies have estimated occupation-specific risk profiles by examining the automatability of associated skills and tasks. however, relatively little work has sought to take a more holistic view on the process of labor reallocation and how employment prospects are impacted as displaced workers transition into new jobs. in this paper, we develop a new data-driven model to analyze how workers move through an empirically derived occupational mobility network in response to automation scenarios which increase labor demand for some occupations and decrease it for others. at the macro level, our model reproduces a key stylized fact in the labor market known as the beveridge curve and provides new insights for explaining the curve's counter-clockwise cyclicality. at the micro level, our model provides occupation-specific estimates of changes in short and long-term unemployment corresponding to a given automation shock. we find that the network structure plays an important role in determining unemployment levels, with occupations in particular areas of the network having very few job transition opportunities. such insights could be fruitfully applied to help design more efficient and effective policies aimed at helping workers adapt to the changing nature of the labor market.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-06-10", "updated": "2020-02-03", "authors": ["r. maria del rio-chanona", "penny mealy", "mariano beguerisse-d\u00edaz", "francois lafond", "j. doyne farmer"], "url": "https://arxiv.org/abs/1906.04086"}, {"title": "the regression discontinuity design", "id": "1906.04242", "abstract": "this handbook chapter gives an introduction to the sharp regression discontinuity design, covering identification, estimation, inference, and falsification methods.", "categories": "econ.em stat.ap stat.me", "created": "2019-06-10", "updated": "2020-06-01", "authors": ["matias d. cattaneo", "rocio titiunik", "gonzalo vazquez-bare"], "url": "https://arxiv.org/abs/1906.04242"}, {"title": "bayesian estimation of economic simulation models using neural networks", "id": "1906.04522", "abstract": "recent advances in computing power and the potential to make more realistic assumptions due to increased flexibility have led to the increased prevalence of simulation models in economics. while models of this class, and particularly agent-based models, are able to replicate a number of empirically-observed stylised facts not easily recovered by more traditional alternatives, such models remain notoriously difficult to estimate due to their lack of tractable likelihood functions. while the estimation literature continues to grow, existing attempts have approached the problem primarily from a frequentist perspective, with the bayesian estimation literature remaining comparatively less developed. for this reason, we introduce a bayesian estimation protocol that makes use of deep neural networks to construct an approximation to the likelihood, which we then benchmark against a prominent alternative from the existing literature. overall, we find that our proposed methodology consistently results in more accurate estimates in a variety of settings, including the estimation of financial heterogeneous agent models and the identification of changes in dynamics occurring in models incorporating structural breaks.", "categories": "econ.gn q-fin.cp q-fin.ec", "created": "2019-06-11", "updated": "", "authors": ["donovan platt"], "url": "https://arxiv.org/abs/1906.04522"}, {"title": "regional economic convergence and spatial quantile regression", "id": "1906.04613", "abstract": "the presence of \\b{eta}-convergence in european regions is an important issue to be analyzed. in this paper, we adopt a quantile regression approach in analyzing economic convergence. while previous work has performed quantile regression at the national level, we focus on 187 european nuts2 regions for the period 1981-2009 and use spatial quantile regression to account for spatial dependence.", "categories": "stat.ap econ.em", "created": "2019-06-11", "updated": "", "authors": ["alfredo cartone", "geoffrey jd hewings", "paolo postiglione"], "url": "https://arxiv.org/abs/1906.04613"}, {"title": "ergodicity-breaking reveals time optimal decision making in humans", "id": "1906.04652", "abstract": "ergodicity describes an equivalence between the expectation value and the time average of observables. applied to human behaviour, ergodic theories of decision-making reveal how individuals should tolerate risk in different environments. to optimise wealth over time, agents should adapt their utility function according to the dynamical setting they face. linear utility is optimal for additive dynamics, whereas logarithmic utility is optimal for multiplicative dynamics. whether humans approximate time optimal behavior across different dynamics is unknown. here we compare the effects of additive versus multiplicative gamble dynamics on risky choice. we show that utility functions are modulated by gamble dynamics in ways not explained by prevailing economic theory. instead, as predicted by time optimality, risk aversion increases under multiplicative dynamics, distributing close to the values that maximise the time average growth of wealth. we suggest that our findings motivate a need for explicitly grounding theories of decision-making on ergodic considerations.", "categories": "econ.gn q-fin.ec", "created": "2019-06-11", "updated": "2020-09-25", "authors": ["david meder", "finn rabe", "tobias morville", "kristoffer h. madsen", "magnus t. koudahl", "ray j. dolan", "hartwig r. siebner", "oliver j. hulme"], "url": "https://arxiv.org/abs/1906.04652"}, {"title": "propublica's compas data revisited", "id": "1906.04711", "abstract": "i examine the compas recidivism risk score and criminal history data collected by propublica in 2016 that fueled intense debate and research in the nascent field of 'algorithmic fairness'. propublica's compas data is used in an increasing number of studies to test various definitions of algorithmic fairness. this paper takes a closer look at the actual datasets put together by propublica. in particular, the sub-datasets built to study the likelihood of recidivism within two years of a defendant's original compas survey screening date. i take a new yet simple approach to visualize these data, by analyzing the distribution of defendants across compas screening dates. i find that propublica made an important data processing error when it created these datasets, failing to implement a two-year sample cutoff rule for recidivists in such datasets (whereas it implemented a two-year sample cutoff rule for non-recidivists). when i implement a simple two-year compas screen date cutoff rule for recidivists, i estimate that in the two-year general recidivism dataset propublica kept over 40% more recidivists than it should have. this fundamental problem in dataset construction affects some statistics more than others. it obviously has a substantial impact on the recidivism rate; artificially inflating it. for the two-year general recidivism dataset created by propublica, the two-year recidivism rate is 45.1%, whereas, with the simple compas screen date cutoff correction i implement, it is 36.2%. thus, the two-year recidivism rate in propublica's dataset is inflated by over 24%. this also affects the positive and negative predictive values. on the other hand, this data processing error has little impact on some of the other key statistical measures, which are less susceptible to changes in the relative share of recidivists, such as the false positive and false negative rates, and the overall accuracy.", "categories": "econ.gn cs.cy cs.lg q-fin.ec stat.ap", "created": "2019-06-11", "updated": "2019-07-08", "authors": ["matias barenstein"], "url": "https://arxiv.org/abs/1906.04711"}, {"title": "generalized beta prime distribution: stochastic model of economic   exchange and properties of inequality indices", "id": "1906.04822", "abstract": "we argue that a stochastic model of economic exchange, whose steady-state distribution is a generalized beta prime (also known as gb2), and some unique properties of the latter, are the reason for gb2's success in describing wealth/income distributions. we use housing sale prices as a proxy to wealth/income distribution to numerically illustrate this point. we also explore parametric limits of the distribution to do so analytically. we discuss parametric properties of the inequality indices -- gini, hoover, theil t and theil l -- vis-a-vis those of gb2 and introduce a new inequality index, which serves a similar purpose. we argue that hoover and theil l are more appropriate measures for distributions with power-law dependencies, especially fat tails, such as gb2.", "categories": "econ.em econ.th q-fin.mf q-fin.st", "created": "2019-06-11", "updated": "", "authors": ["m. dashti moghaddam", "jeffrey mills", "r. a. serota"], "url": "https://arxiv.org/abs/1906.04822"}, {"title": "nonparametric identification and estimation with independent, discrete   instruments", "id": "1906.05231", "abstract": "in a nonparametric instrumental regression model, we strengthen the conventional moment independence assumption towards full statistical independence between instrument and error term. this allows us to prove identification results and develop estimators for a structural function of interest when the instrument is discrete, and in particular binary. when the regressor of interest is also discrete with more mass points than the instrument, we state straightforward conditions under which the structural function is partially identified, and give modified assumptions which imply point identification. these stronger assumptions are shown to hold outside of a small set of conditional moments of the error term. estimators for the identified set are given when the structural function is either partially or point identified. when the regressor is continuously distributed, we prove that if the instrument induces a sufficiently rich variation in the joint distribution of the regressor and error term then point identification of the structural function is still possible. this approach is relatively tractable, and under some standard conditions we demonstrate that our point identifying assumption holds on a topologically generic set of density functions for the joint distribution of regressor, error, and instrument. our method also applies to a well-known nonparametric quantile regression framework, and we are able to state analogous point identification results in that context.", "categories": "econ.em", "created": "2019-06-12", "updated": "", "authors": ["isaac loh"], "url": "https://arxiv.org/abs/1906.05231"}, {"title": "growing green: the role of path dependency and structural jumps in the   green economy expansion", "id": "1906.05269", "abstract": "existing research argues that countries increase their production basket by adding products which require similar capabilities to those they already produce, a process referred to as path dependency. green economic growth is a global movement that seeks to achieve economic expansion while at the same time mitigating environmental risks. we postulate that countries engaging in green economic growth are motivated to invest strategically to develop new capabilities that will help them transition to a green economy. as a result, they could potentially increase their production baskets not only by a path dependent process but also by the non path dependent process we term, high investment structural jumps. the main objective of this research is to determine whether countries increase their green production basket mainly by a process of path dependency, or alternatively, by a process of structural jumps. we analyze data from 65 countries and over a period from years 2007 to 2017. we focus on china as our main case study. the results of this research show that countries not only increase their green production baskets based on their available capabilities, following path dependency, but also expand to products that path dependency does not predict by investing in innovating and developing new environmental related technologies.", "categories": "econ.gn q-fin.ec", "created": "2019-06-12", "updated": "2020-04-25", "authors": ["seyyedmilad talebzadehhosseini", "steven r. scheinert", "ivan garibay"], "url": "https://arxiv.org/abs/1906.05269"}, {"title": "sparse approximate factor estimation for high-dimensional covariance   matrices", "id": "1906.05545", "abstract": "we propose a novel estimation approach for the covariance matrix based on the $l_1$-regularized approximate factor model. our sparse approximate factor (saf) covariance estimator allows for the existence of weak factors and hence relaxes the pervasiveness assumption generally adopted for the standard approximate factor model. we prove consistency of the covariance matrix estimator under the frobenius norm as well as the consistency of the factor loadings and the factors.   our monte carlo simulations reveal that the saf covariance estimator has superior properties in finite samples for low and high dimensions and different designs of the covariance matrix. moreover, in an out-of-sample portfolio forecasting application the estimator uniformly outperforms alternative portfolio strategies based on alternative covariance estimation approaches and modeling strategies including the $1/n$-strategy.", "categories": "econ.em math.st q-fin.pm stat.me stat.th", "created": "2019-06-13", "updated": "", "authors": ["maurizio daniele", "winfried pohlmeier", "aygul zagidullina"], "url": "https://arxiv.org/abs/1906.05545"}, {"title": "posterior average effects", "id": "1906.06360", "abstract": "economists are often interested in estimating averages with respect to distributions of unobservables. examples are moments of individual fixed-effects, average partial effects in discrete choice models, and counterfactual simulations in structural models. for such quantities, we propose and study posterior average effects (pae), where the average is computed \\emph{conditional} on the sample, in the spirit of empirical bayes and shrinkage methods. while the usefulness of shrinkage for prediction is well-understood, a justification of posterior conditioning to estimate population averages is currently lacking. we show that pae have minimum worst-case bias under local misspecification of the parametric distribution of unobservables. this provides a rationale for reporting these estimators in applications. we introduce a measure of informativeness of the posterior conditioning, which quantifies the bias of pae relative to parametric model-based estimators, and we study other robustness properties of pae for estimation and prediction. as illustrations, we report pae estimates of distributions of neighborhood effects in the us, and of permanent and transitory components in a model of income dynamics.", "categories": "econ.em stat.me", "created": "2019-06-14", "updated": "2020-10-07", "authors": ["st\u00e9phane bonhomme", "martin weidner"], "url": "https://arxiv.org/abs/1906.06360"}, {"title": "lpdensity: local polynomial density estimation and inference", "id": "1906.06529", "abstract": "density estimation and inference methods are widely used in empirical work. when the underlying distribution has compact support, conventional kernel-based density estimators are no longer consistent near or at the boundary because of their well-known boundary bias. alternative smoothing methods are available to handle boundary points in density estimation, but they all require additional tuning parameter choices or other typically ad hoc modifications depending on the evaluation point and/or approach considered. this article discusses the r and stata package lpdensity implementing a novel local polynomial density estimator proposed and studied in cattaneo, jansson, and ma (2020a,b), which is boundary adaptive and involves only one tuning parameter. the methods implemented also cover local polynomial estimation of the cumulative distribution function and density derivatives. in addition to point estimation and graphical procedures, the package offers consistent variance estimators, mean squared error optimal bandwidth selection, robust bias-corrected inference, and confidence bands construction, among other features. a comparison with other density estimation packages available in r using a monte carlo experiment is provided.", "categories": "stat.co econ.em stat.ap", "created": "2019-06-15", "updated": "2020-08-06", "authors": ["matias d. cattaneo", "michael jansson", "xinwei ma"], "url": "https://arxiv.org/abs/1906.06529"}, {"title": "on the properties of the synthetic control estimator with many periods   and many controls", "id": "1906.06665", "abstract": "we consider the asymptotic properties of the synthetic control (sc) estimator when both the number of pre-treatment periods and control units are large. if potential outcomes follow a linear factor model, we provide conditions under which the factor loadings of the sc unit converge in probability to the factor loadings of the treated unit. this happens when there are weights diluted among an increasing number of control units such that a weighted average of the factor loadings of the control units asymptotically reconstructs the factor loadings of the treated unit. in this case, the sc estimator is asymptotically unbiased even when treatment assignment is correlated with time-varying unobservables. this result can be valid even when the number of control units is larger than the number of pre-treatment periods.", "categories": "econ.em", "created": "2019-06-16", "updated": "2020-05-25", "authors": ["bruno ferman"], "url": "https://arxiv.org/abs/1906.06665"}, {"title": "detecting p-hacking", "id": "1906.06711", "abstract": "we analyze theoretically the problem of testing for p-hacking based on distributions of p-values across multiple studies. we provide general results for when such distributions have testable restrictions under the null of no p-hacking. we find novel additional testable restrictions for p-values based on t-tests. analytical characterizations of the distributions of p-values under the null of no p-hacking and the alternative where there is p-hacking allow us to both analyze the power of existing tests and provide new more powerful statistical tests for p-hacking. results are extended to practical situations where there is publication bias and when reported p-values are rounded. we also show that tests for p-hacking based on distributions of t-statistics can be problematic and may not control size. our proposed tests are shown to have good properties in monte carlo studies and are applied to two datasets of p-values.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-06-16", "updated": "2020-06-11", "authors": ["graham elliott", "nikolay kudrin", "kaspar wuthrich"], "url": "https://arxiv.org/abs/1906.06711"}, {"title": "shape matters: evidence from machine learning on body shape-income   relationship", "id": "1906.06747", "abstract": "we study the association between physical appearance and family income using a novel data which has 3-dimensional body scans to mitigate the issue of reporting errors and measurement errors observed in most previous studies. we apply machine learning to obtain intrinsic features consisting of human body and take into account a possible issue of endogenous body shapes. the estimation results show that there is a significant relationship between physical appearance and family income and the associations are different across the gender. this supports the hypothesis on the physical attractiveness premium and its heterogeneity across the gender.", "categories": "econ.em", "created": "2019-06-16", "updated": "", "authors": ["suyong song", "stephen s. baek"], "url": "https://arxiv.org/abs/1906.06747"}, {"title": "a bewley-huggett model with many consumption goods", "id": "1906.06810", "abstract": "we study a pure-exchange incomplete markets model with heterogeneous agents. in each period, the agents choose how much to save and which bundle of goods to consume while their endowments are fluctuating. we focus on a competitive stationary equilibrium (cse) in which the wealth distribution is invariant, the agents maximize their expected discounted utility, and both the prices of goods and the interest rate are market-clearing. our main contribution is to extend some general equilibrium results to an incomplete markets setting. under mild conditions on the agents' preferences, we show that the aggregate demand for goods depends only on their relative prices and we prove the existence of a cse. when the agents' preferences can be represented by a ces (constant elasticity of substitution) utility function with an elasticity of substitution that is higher than or equal to one, we prove that the cse is unique. under the same preferences, we show that a higher inequality of endowments does not change the equilibrium prices of goods, and decreases the equilibrium interest rate.", "categories": "econ.th", "created": "2019-06-16", "updated": "", "authors": ["bar light"], "url": "https://arxiv.org/abs/1906.06810"}, {"title": "detecting correlations and triangular arbitrage opportunities in the   forex by means of multifractal detrended cross-correlations analysis", "id": "1906.07491", "abstract": "multifractal detrended cross-correlation methodology is described and applied to foreign exchange (forex) market time series. fluctuations of high frequency exchange rates of eight major world currencies over 2010-2018 period are used to study cross-correlations. the study is motivated by fundamental questions in complex systems' response to significant environmental changes and by potential applications in investment strategies, including detecting triangular arbitrage opportunities. dominant multiscale cross-correlations between the exchange rates are found to typically occur at smaller fluctuation levels. however hierarchical organization of ties expressed in terms of dendrograms, with a novel application of the multiscale cross-correlation coefficient, are more pronounced at large fluctuations. the cross-correlations are quantified to be stronger on average between those exchange rate pairs that are bound within triangular relations. some pairs from outside triangular relations are however identified to be exceptionally strongly correlated as compared to the average strength of triangular correlations.this in particular applies to those exchange rates that involve australian and new zealand dollars and reflects their economic relations. significant events with impact on the forex are shown to induce triangular arbitrage opportunities which at the same time reduce cross--correlations on the smallest time scales and act destructively on the multiscale organization of correlations. in 2010--2018 such instances took place in connection with the swiss national bank intervention and the weakening of british pound sterling accompanying the initiation of brexit procedure. the methodology could be applicable to temporal and multiscale pattern detection in any time series.", "categories": "q-fin.st cs.ce econ.gn physics.data-an q-fin.ec", "created": "2019-06-18", "updated": "2019-10-31", "authors": ["robert g\u0119barowski", "pawe\u0142 o\u015bwi\u0119cimka", "marcin w\u0105torek", "stanis\u0142aw dro\u017cd\u017c"], "url": "https://arxiv.org/abs/1906.07491"}, {"title": "nonparametric estimation in a regression model with additive and   multiplicative noise", "id": "1906.07695", "abstract": "in this paper, we consider an unknown functional estimation problem in a general nonparametric regression model with the feature of having both multiplicative and additive noise.we propose two new wavelet estimators in this general context. we prove that they achieve fast convergence rates under the mean integrated square error over besov spaces. the obtained rates have the particularity of being established under weak conditions on the model. a numerical study in a context comparable to stochastic frontier estimation (with the difference that the boundary is not necessarily a production function) supports the theory.", "categories": "math.st econ.em stat.me stat.th", "created": "2019-06-18", "updated": "2020-06-20", "authors": ["christophe chesneau", "salima el kolei", "junke kou", "fabien navarro"], "url": "https://arxiv.org/abs/1906.07695"}, {"title": "signatures of crypto-currency market decoupling from the forex", "id": "1906.07834", "abstract": "based on the high-frequency recordings from kraken, a cryptocurrency exchange and professional trading platform that aims to bring bitcoin and other cryptocurrencies into the mainstream, the multiscale cross-correlations involving the bitcoin (btc), ethereum (eth), euro (eur) and us dollar (usd) are studied over the period between july 1, 2016 and december 31, 2018. it is shown that the multiscaling characteristics of the exchange rate fluctuations related to the cryptocurrency market approach those of the forex. this, in particular, applies to the btc/eth exchange rate, whose hurst exponent by the end of 2018 started approaching the value of 0.5, which is characteristic of the mature world markets. furthermore, the btc/eth direct exchange rate has already developed multifractality, which manifests itself via broad singularity spectra. a particularly significant result is that the measures applied for detecting cross-correlations between the dynamics of the btc/eth and eur/usd exchange rates do not show any noticeable relationships. this may be taken as an indication that the cryptocurrency market has begun decoupling itself from the forex.", "categories": "q-fin.st econ.em", "created": "2019-06-18", "updated": "2019-07-18", "authors": ["stanis\u0142aw dro\u017cd\u017c", "ludovico minati", "pawe\u0142 o\u015bwi\u0119cimka", "marek stanuszek", "marcin w\u0105torek"], "url": "https://arxiv.org/abs/1906.07834"}, {"title": "forecasting the us gdp components in the short run", "id": "1906.07992", "abstract": "the aim of this paper is to estimate short-term forecasts of the us gdp components by expenditure approach sooner than they are officially released by the national institutions of statistics. for this reason, nowcasts along with 1- and 2-quarter forecasts are estimated by using available monthly information, officially released with a considerably smaller delay. the high-dimensionality problem of the monthly dataset used is solved by assuming sparse structures for the choice of leading indicators, capable of adequately explaining the dynamics of the gdp components. variable selection and the estimation of the forecasts is performed by using the lasso method, together with some of its popular modifications. additionally, a modification of the lasso is proposed, combining the methods of lasso and principal components, in order to further improve the forecasting performance. forecast accuracy of the models is evaluated by conducting pseudo-real-time forecasting exercises for four components of the gdp over the sample of 2005-2015, and compared with the benchmark arma models. the main results suggest that lasso is able to outperform arma models when forecasting the gdp components and to identify leading explanatory variables. the proposed modification of the lasso in some cases show further improvement in forecast accuracy.", "categories": "econ.em", "created": "2019-06-19", "updated": "", "authors": ["saulius jokubaitis", "dmitrij celov"], "url": "https://arxiv.org/abs/1906.07992"}, {"title": "from local to global: external validity in a fertility natural   experiment", "id": "1906.08096", "abstract": "we study issues related to external validity for treatment effects using over 100 replications of the angrist and evans (1998) natural experiment on the effects of sibling sex composition on fertility and labor supply. the replications are based on census data from around the world going back to 1960. we decompose sources of error in predicting treatment effects in external contexts in terms of macro and micro sources of variation. in our empirical setting, we find that macro covariates dominate over micro covariates for reducing errors in predicting treatments, an issue that past studies of external validity have been unable to evaluate. we develop methods for two applications to evidence-based decision-making, including determining where to locate an experiment and whether policy-makers should commission new experiments or rely on an existing evidence base for making a policy decision.", "categories": "econ.em stat.ap stat.me", "created": "2019-06-19", "updated": "", "authors": ["rajeev dehejia", "cristian pop-eleches", "cyrus samii"], "url": "https://arxiv.org/abs/1906.08096"}, {"title": "predicting patent citations to measure economic impact of scholarly   research", "id": "1906.08244", "abstract": "a crucial goal of funding research and development has always been to advance economic development. on this basis, a consider-able body of research undertaken with the purpose of determining what exactly constitutes economic impact and how to accurately measure that impact has been published. numerous indicators have been used to measure economic impact, although no single indicator has been widely adapted. based on patent data collected from altmetric we predict patent citations through various social media features using several classification models. patents citing a research paper implies the potential it has for direct application inits field. these predictions can be utilized by researchers in deter-mining the practical applications for their work when applying for patents.", "categories": "cs.dl cs.lg econ.gn q-fin.ec", "created": "2019-06-07", "updated": "", "authors": ["abdul rahman shaikh", "hamed alhoori"], "url": "https://arxiv.org/abs/1906.08244"}, {"title": "the age-period-cohort-interaction model for describing and investigating   inter-cohort deviations and intra-cohort life-course dynamics", "id": "1906.08357", "abstract": "social scientists have frequently sought to understand the distinct effects of age, period, and cohort, but disaggregation of the three dimensions is difficult because cohort = period - age. we argue that this technical difficulty reflects a disconnection between how cohort effect is conceptualized and how it is modeled in the traditional age-period-cohort framework. we propose a new method, called the age-period-cohort-interaction (apc-i) model, that is qualitatively different from previous methods in that it represents ryder's (1965) theoretical account about the conditions under which cohort differentiation may arise. this apc-i model does not require problematic statistical assumptions and the interpretation is straightforward. it quantifies inter-cohort deviations from the age and period main effects and also permits hypothesis testing about intra-cohort life-course dynamics. we demonstrate how this new model can be used to examine age, period, and cohort patterns in women's labor force participation.", "categories": "stat.ap econ.em stat.me", "created": "2019-06-02", "updated": "", "authors": ["liying luo", "james hodges"], "url": "https://arxiv.org/abs/1906.08357"}, {"title": "loan maturity aggregation in interbank lending networks obscures   mesoscale structure and economic functions", "id": "1906.08617", "abstract": "since the 2007-2009 financial crisis, substantial academic effort has been dedicated to improving our understanding of interbank lending networks (ilns). because of data limitations or by choice, the literature largely lacks multiple loan maturities. we employ a complete interbank loan contract dataset to investigate whether maturity details are informative of the network structure. applying the layered stochastic block model of peixoto (2015) and other tools from network science on a time series of bilateral loans with multiple maturity layers in the russian iln, we find that collapsing all such layers consistently obscures mesoscale structure. the optimal maturity granularity lies between completely collapsing and completely separating the maturity layers and depends on the development phase of the interbank market, with a more developed market requiring more layers for optimal description. closer inspection of the inferred maturity bins associated with the optimal maturity granularity reveals specific economic functions, from liquidity intermediation to financing. collapsing a network with multiple underlying maturity layers or extracting one such layer, common in economic research, is therefore not only an incomplete representation of the iln's mesoscale structure, but also conceals existing economic functions. this holds important insights and opportunities for theoretical and empirical studies on interbank market functioning, contagion, stability, and on the desirable level of regulatory data disclosure.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2019-06-12", "updated": "", "authors": ["marnix van soom", "milan van den heuvel", "jan ryckebusch", "koen schoors"], "url": "https://arxiv.org/abs/1906.08617"}, {"title": "a network approach to cartel detection in public auction markets", "id": "1906.08667", "abstract": "competing firms can increase profits by setting prices collectively, imposing significant costs on consumers. such groups of firms are known as cartels and because this behavior is illegal, their operations are secretive and difficult to detect. cartels feel a significant internal obstacle: members feel short-run incentives to cheat. here we present a network-based framework to detect potential cartels in bidding markets based on the idea that the chance a group of firms can overcome this obstacle and sustain cooperation depends on the patterns of its interactions. we create a network of firms based on their co-bidding behavior, detect interacting groups, and measure their cohesion and exclusivity, two group-level features of their collective behavior. applied to a market for school milk, our method detects a known cartel and calculates that it has high cohesion and exclusivity. in a comprehensive set of nearly 150,000 public contracts awarded by the republic of georgia from 2011 to 2016, detected groups with high cohesion and exclusivity are significantly more likely to display traditional markers of cartel behavior. we replicate this relationship between group topology and the emergence of cooperation in a simulation model. our method presents a scalable, unsupervised method to find groups of firms in bidding markets ideally positioned to form lasting cartels.", "categories": "physics.soc-ph cs.si econ.gn q-fin.ec", "created": "2019-06-20", "updated": "", "authors": ["johannes wachs", "j\u00e1nos kert\u00e9sz"], "url": "https://arxiv.org/abs/1906.08667"}, {"title": "on the probability of a causal inference is robust for internal validity", "id": "1906.08726", "abstract": "the internal validity of observational study is often subject to debate. in this study, we define the counterfactuals as the unobserved sample and intend to quantify its relationship with the null hypothesis statistical testing (nhst). we propose the probability of a causal inference is robust for internal validity, i.e., the piv, as a robustness index of causal inference. formally, the piv is the probability of rejecting the null hypothesis again based on both the observed sample and the counterfactuals, provided the same null hypothesis has already been rejected based on the observed sample. under either frequentist or bayesian framework, one can bound the piv of an inference based on his bounded belief about the counterfactuals, which is often needed when the unconfoundedness assumption is dubious. the piv is equivalent to statistical power when the nhst is thought to be based on both the observed sample and the counterfactuals. we summarize the process of evaluating internal validity with the piv into an eight-step procedure and illustrate it with an empirical example (i.e., hong and raudenbush (2005)).", "categories": "stat.ap econ.em stat.me stat.ot", "created": "2019-06-20", "updated": "", "authors": ["tenglong li", "kenneth a. frank"], "url": "https://arxiv.org/abs/1906.08726"}, {"title": "costmap: an open-source software package for developing cost surfaces", "id": "1906.08872", "abstract": "cost surfaces are a quantitative means of assigning social, environmental, and engineering costs that impact movement across landscapes. cost surfaces are a crucial aspect of route optimization and least cost path (lcp) calculations and are used in a wide range of disciplines including computer science, landscape ecology, and energy infrastructure modeling. linear features present a key weakness to traditional routing calculations along costs surfaces because they cannot identify whether moving from a cell to its adjacent neighbors constitutes crossing a linear barrier (increased cost) or following a corridor (reduced cost). following and avoiding linear features can drastically change predicted routes. in this paper, we introduce an approach to address this \"adjacency\" issue using a search kernel that identifies these critical barriers and corridors. we have built this approach into a new java-based open-source software package called costmap (cost surface multi-layer aggregation program), which calculates cost surfaces and cost networks using the search kernel. costmap not only includes the new adjacency capability, it is also a versatile multi-platform package that allows users to input multiple gis data layers and to set weights and rules for developing a weighted-cost network. we compare costmap performance with traditional cost surface approaches and show significant performance gains, both following corridors and avoiding barriers, using examples in a movement ecology framework and pipeline routing for carbon capture, and storage (ccs). we also demonstrate that the new software can straightforwardly calculate cost surfaces on a national scale.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-06-17", "updated": "", "authors": ["brendan hoover", "richard s. middleton", "sean yaw"], "url": "https://arxiv.org/abs/1906.08872"}, {"title": "suboptimal provision of privacy and statistical accuracy when they are   public goods", "id": "1906.09353", "abstract": "with vast databases at their disposal, private tech companies can compete with public statistical agencies to provide population statistics. however, private companies face different incentives to provide high-quality statistics and to protect the privacy of the people whose data are used. when both privacy protection and statistical accuracy are public goods, private providers tend to produce at least one suboptimally, but it is not clear which. we model a firm that publishes statistics under a guarantee of differential privacy. we prove that provision by the private firm results in inefficiently low data quality in this framework.", "categories": "econ.th cs.cr cs.db", "created": "2019-06-21", "updated": "", "authors": ["john m. abowd", "ian m. schmutte", "william sexton", "lars vilhuber"], "url": "https://arxiv.org/abs/1906.09353"}, {"title": "single-crossing implementation", "id": "1906.09671", "abstract": "an election over a finite set of candidates is called single-crossing if, as we sweep through the list of voters from left to right, the relative order of every pair of candidates changes at most once. such elections have many attractive properties: e.g., their majority relation is transitive and they admit efficient algorithms for problems that are np-hard in general. if a given election is not single-crossing, it is important to understand what are the obstacles that prevent it from having this property. in this paper, we propose a mapping between elections and graphs that provides us with a convenient encoding of such obstacles. this mapping enables us to use the toolbox of graph theory in order to analyze the complexity of detecting nearly single-crossing elections, i.e., elections that can be made single-crossing by a small number of modifications.", "categories": "cs.gt econ.th", "created": "2019-06-23", "updated": "", "authors": ["nathann cohenn", "edith elkind", "foram lakhani"], "url": "https://arxiv.org/abs/1906.09671"}, {"title": "gift contagion in online groups: evidence from wechat red packets", "id": "1906.09698", "abstract": "our study seeks to identify the social contagion of in-group gifts: if gifts trigger their recipients to send more gifts subsequently, the actual impact of a gift on group dynamics would be greatly amplified. causal identification of contagion is always challenging in observational data; to identify gift contagion, we leverage a natural experiment using a large sample of 36 million online red packets sent within 174,131 chat groups on wechat, one of the largest social network services worldwide. our natural experiment is enabled by wechat's random gift amount algorithm, with which the amount that a user receives does not depend on her own attributes. we find that, on average, receiving one more dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours. moreover, this effect is much stronger for \"luckiest draw\" recipients or those who receive the largest share from a red packet, suggesting a group norm according to which the luckiest draw recipients should send the first subsequent red packets. additionally, we find that gift contagion is affected by in-group friendship network properties, such as the number of in-group friends and the local clustering coefficient.", "categories": "econ.gn cs.hc cs.si q-fin.ec", "created": "2019-06-23", "updated": "2020-09-06", "authors": ["yuan yuan", "tracy liu", "chenhao tan", "qian chen", "alex pentland", "jie tang"], "url": "https://arxiv.org/abs/1906.09698"}, {"title": "most important fundamental rule of poker strategy", "id": "1906.09895", "abstract": "poker is a large complex game of imperfect information, which has been singled out as a major ai challenge problem. recently there has been a series of breakthroughs culminating in agents that have successfully defeated the strongest human players in two-player no-limit texas hold 'em. the strongest agents are based on algorithms for approximating nash equilibrium strategies, which are stored in massive binary files and unintelligible to humans. a recent line of research has explored approaches for extrapolating knowledge from strong game-theoretic strategies that can be understood by humans. this would be useful when humans are the ultimate decision maker and allow humans to make better decisions from massive algorithmically-generated strategies. using techniques from machine learning we have uncovered a new simple, fundamental rule of poker strategy that leads to a significant improvement in performance over the best prior rule and can also easily be applied by human players.", "categories": "cs.ai cs.lg econ.th", "created": "2019-06-08", "updated": "2020-02-20", "authors": ["sam ganzfried", "max chiswick"], "url": "https://arxiv.org/abs/1906.09895"}, {"title": "semi-parametric realized nonlinear conditional autoregressive expectile   and expected shortfall", "id": "1906.09961", "abstract": "a joint conditional autoregressive expectile and expected shortfall framework is proposed. the framework is extended through incorporating a measurement equation which models the contemporaneous dependence between the realized measures and the latent conditional expectile. nonlinear threshold specification is further incorporated into the proposed framework. a bayesian markov chain monte carlo method is adapted for estimation, whose properties are assessed and compared with maximum likelihood via a simulation study. one-day-ahead var and es forecasting studies, with seven market indices, provide empirical support to the proposed models.", "categories": "q-fin.rm econ.em", "created": "2019-06-20", "updated": "", "authors": ["chao wang", "richard gerlach"], "url": "https://arxiv.org/abs/1906.09961"}, {"title": "a new solution to market definition: an approach based on   multi-dimensional substitutability statistics", "id": "1906.10030", "abstract": "market definition is an important component in the premerger investigation, but the models used in the market definition have not developed much in the past three decades since the critical loss analysis (cla) was proposed in 1989. the cla helps the hypothetical monopolist test to determine whether the hypothetical monopolist is going to profit from the small but significant and non-transitory increase in price (ssnip). however, the cla has long been criticized by academic scholars for its tendency to conclude a narrow market. although the cla was adopted by the 2010 horizontal merger guidelines (the 2010 guidelines), the criticisms are likely still valid. in this dissertation, we discussed the mathematical deduction of cla, the data used, and the ssnip defined by the agencies. based on our research, we concluded that the narrow market conclusion was due to the incorrect implementation of the cla; not the model itself. on the other hand, there are other unresolvable problems in the cla and the hypothetical monopolist test. the ssnip test and the cla are bright resolutions for market definition problem during their time, but we have more advanced tools to solve the task nowadays. in this dissertation, we propose a model which is based directly on the multi-dimensional substitutability between the products and is capable of maximizing the substitutability of product features within each group. since the 2010 guidelines does not exclude the use of models other than the ones mentioned by the guidelines, our method can hopefully supplement the current models to show a better picture of the substitutive relations and provide a more stable definition of the market.", "categories": "econ.gn q-fin.ec", "created": "2019-06-24", "updated": "", "authors": ["yan yang"], "url": "https://arxiv.org/abs/1906.10030"}, {"title": "long run feedback in the broker call money market", "id": "1906.10084", "abstract": "i unravel the basic long run dynamics of the broker call money market, which is the pile of cash that funds margin loans to retail clients (read: continuous time kelly gamblers). call money is assumed to supply itself perfectly inelastically, and to continuously reinvest all principal and interest. i show that the relative size of the money market (that is, relative to the kelly bankroll) is a martingale that nonetheless converges in probability to zero. the margin loan interest rate is a submartingale that converges in mean square to the choke price $r_\\infty:=\\nu-\\sigma^2/2$, where $\\nu$ is the asymptotic compound growth rate of the stock market and $\\sigma$ is its annual volatility. in this environment, the gambler no longer beats the market asymptotically a.s. by an exponential factor (as he would under perfectly elastic supply). rather, he beats the market asymptotically with very high probability (think 98%) by a factor (say 1.87, or 87% more final wealth) whose mean cannot exceed what the leverage ratio was at the start of the model (say, $2:1$). although the ratio of the gambler's wealth to that of an equivalent buy-and-hold investor is a submartingale (always expected to increase), his realized compound growth rate converges in mean square to $\\nu$. this happens because the equilibrium leverage ratio converges to $1:1$ in lockstep with the gradual rise of margin loan interest rates.", "categories": "econ.gn econ.th q-fin.cp q-fin.ec q-fin.gn q-fin.pm", "created": "2019-06-24", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1906.10084"}, {"title": "policy targeting under network interference", "id": "1906.10258", "abstract": "this paper discusses the problem of estimating treatment allocation rules under network interference. i propose a method with several attractive features for applications: (i) it does not rely on the correct specification of a particular structural model; (ii) it exploits heterogeneity in treatment effects for targeting individuals; (iii) it accommodates arbitrary constraints on the policy function and capacity constraints on the number of treated units, and (iv) it can also be implemented when network information is not accessible to policy-makers. i establish a strong set of guarantees on the utilitarian regret, i.e., the difference between the average social welfare attained by the estimated policy function and the maximum attainable welfare. i provide a mixed-integer linear program formulation, which can be solved using off-the-shelf algorithms. i discuss the empirical performance in simulations and illustrate the advantages of the method for targeting information on social networks.", "categories": "econ.em cs.si stat.me stat.ml", "created": "2019-06-24", "updated": "2020-06-11", "authors": ["davide viviano"], "url": "https://arxiv.org/abs/1906.10258"}, {"title": "informed principal problems in bilateral trading", "id": "1906.10311", "abstract": "we study the informed principal problems in a bilateral trade environment where both seller and buyer have private information about the types affecting their interdependent valuations. the seller has full bargaining power to offer a trading mechanism. we refine equilibria by the intuitive criterion. to eliminate each unintuitive equilibrium, we design an indirect mechanism that elicits both parties' common knowledge of the buyer's off-path belief after the mechanism itself is offered. the mechanism uses the elicited belief to decide what allocation is fully implemented. we characterize both parties' interim payoff vectors in intuitive equilibria. the result implies the uniqueness of the seller's payoff vector.", "categories": "econ.th", "created": "2019-06-24", "updated": "2020-10-05", "authors": ["takeshi nishimura"], "url": "https://arxiv.org/abs/1906.10311"}, {"title": "to infinity and beyond: scaling economic theories via logical   compactness", "id": "1906.10333", "abstract": "many economic-theoretic models incorporate finiteness assumptions that, while introduced for simplicity, play a real role in the analysis. such assumptions introduce a conceptual problem, as results that rely on finiteness are often implicitly nonrobust; for example, they may rely on edge effects or artificial boundary conditions. here, we present a unified method that enables us to remove finiteness assumptions, such as those on datasets, market sizes, and time horizons. we then apply our approach to a variety of revealed preference, matching, and exchange economy settings.   the key to our approach is logical compactness, a core result from propositional logic. building on logical compactness, in a revealed-preference setting, we reprove reny's infinite-data version of afriat's theorem and (newly) prove an infinite-data version of mcfadden and richter's characterization of rationalizable stochastic datasets. in a matching setting, we reprove large-market existence results implied by fleiner's analysis, and prove both the strategy-proofness of the man-optimal stable mechanism in infinite markets, and an infinite-market version of nguyen and vohra's existence result for near-feasible stable matchings with couples. in a trading-network setting, we prove that the hatfield et al. result on existence of walrasian equilibria extends to infinite markets. finally, we prove that pereyra's existence result for dynamic two-sided matching markets extends to a doubly-infinite time horizon.", "categories": "cs.gt econ.th", "created": "2019-06-25", "updated": "2020-03-19", "authors": ["yannai a. gonczarowski", "scott duke kominers", "ran i. shorrer"], "url": "https://arxiv.org/abs/1906.10333"}, {"title": "forecasting the remittances of the overseas filipino workers in the   philippines", "id": "1906.10422", "abstract": "this study aims to find a box-jenkins time series model for the monthly ofw's remittance in the philippines. forecasts of ofw's remittance for the years 2018 and 2019 will be generated using the appropriate time series model. the data were retrieved from the official website of bangko sentral ng pilipinas. there are 108 observations, 96 of which were used in model building and the remaining 12 observations were used in forecast evaluation. acf and pacf were used to examine the stationarity of the series. augmented dickey fuller test was used to confirm the stationarity of the series. the data was found to have a seasonal component, thus, seasonality has been considered in the final model which is sarima (2,1,0)x(0,0,2)_12. there are no significant spikes in the acf and pacf of residuals of the final model and the l-jung box q* test confirms further that the residuals of the model are uncorrelated. also, based on the result of the shapiro-wilk test for the forecast errors, the forecast errors can be considered a gaussian white noise. considering the results of diagnostic checking and forecast evaluation, sarima (2,1,0)x(0,0,2)_12 is an appropriate model for the series. all necessary computations were done using the r statistical software.", "categories": "stat.ap econ.em", "created": "2019-06-25", "updated": "", "authors": ["merry christ e. manayaga", "roel f. ceballos"], "url": "https://arxiv.org/abs/1906.10422"}, {"title": "understanding the explosive trend in eu ets prices -- fundamentals or   speculation?", "id": "1906.10572", "abstract": "in 2018, allowance prices in the eu emission trading scheme (eu ets) experienced a run-up from persistently low levels in previous years. regulators attribute this to a comprehensive reform in the same year, and are confident the new price level reflects an anticipated tighter supply of allowances. we ask if this is indeed the case, or if it is an overreaction of the market driven by speculation. we combine several econometric methods - time-varying coefficient regression, formal bubble detection as well as time stamping and crash odds prediction - to juxtapose the regulators' claim versus the concurrent explanation. we find evidence of a long period of explosive behaviour in allowance prices, starting in march 2018 when the reform was adopted. our results suggest that the reform triggered market participants into speculation, and question regulators' confidence in its long-term outcome. this has implications for both the further development of the eu ets, and the long lasting debate about taxes versus emission trading schemes.", "categories": "econ.em", "created": "2019-06-25", "updated": "2020-03-20", "authors": ["marina friedrich", "s\u00e9bastien fries", "michael pahle", "ottmar edenhofer"], "url": "https://arxiv.org/abs/1906.10572"}, {"title": "on capital allocation under information constraints", "id": "1906.10624", "abstract": "attempts to allocate capital across a selection of different investments are often hampered by the fact that investors' decisions are made under limited information (no historical return data) and during an extremely limited timeframe. nevertheless, in some cases, rational investors with a certain level of experience are able to ordinally rank investment alternatives through relative assessments of the probabilities that investments will be successful. however, to apply traditional portfolio optimization models, analysts must use historical (or simulated/expected) return data as the basis for their calculations. this paper develops an alternative portfolio optimization framework that is able to handle this kind of information (given by an ordinal ranking of investment alternatives) and to calculate an optimal capital allocation based on a cobb-douglas function, which we call the sorted weighted portfolio (swp). considering risk-neutral investors, we show that the results of this portfolio optimization model usually outperform the output generated by the (intuitive) equally weighted portfolio (ewp) of different investment alternatives, which is the result of optimization when one is unable to incorporate additional data (the ordinal ranking of the alternatives). to further extend this work, we show that our model can also address risk-averse investors to capture correlation effects.", "categories": "econ.gn q-fin.ec", "created": "2019-06-14", "updated": "2020-04-21", "authors": ["christoph j. b\u00f6rner", "ingo hoffmann", "fabian poetter", "tim schmitz"], "url": "https://arxiv.org/abs/1906.10624"}, {"title": "the syntax of the accounting language: a first step", "id": "1906.10865", "abstract": "we review and interpret two basic propositions published by ellerman (2014). the propositions address the algebraic structure of t accounts and double entry bookkeeping (deb). the paper builds on this previous contribution with the view of reconciling the two, apparently dichotomous, perspectives of accounting measurement: the one that focuses preferably on the stock of wealth and to the one that focuses preferably on the flow of income. the paper claims that t-accounts and deb have an underlying algebraic structure suitable for approaching measurement from either or both perspectives. accountants preferences for stocks or flows can be framed in ways which are mutually consistent. the paper is a first step in addressing this consistency issue. it avoids the difficult mathematics of abstract algebra by applying the concept of syntax to accounting numbers such that the accounting procedure qualifies as a formal language with which accountants convey meaning.", "categories": "econ.gn q-fin.ec", "created": "2019-06-26", "updated": "", "authors": ["frederico botafogo"], "url": "https://arxiv.org/abs/1906.10865"}, {"title": "estimation of the size of informal employment based on administrative   records with non-ignorable selection mechanism", "id": "1906.10957", "abstract": "in this study we used company level administrative data from the national labour inspectorate and the polish social insurance institution in order to estimate the prevalence of informal employment in poland. since the selection mechanism is non-ignorable we employed a generalization of heckman's sample selection model assuming non-gaussian correlation of errors and clustering by incorporation of random effects. we found that 5.7% (4.6%, 7.1%; 95% ci) of registered enterprises in poland, to some extent, take advantage of the informal labour force. our study exemplifies a new approach to measuring informal employment, which can be implemented in other countries. it also contributes to the existing literature by providing, to the best of our knowledge, the first estimates of informal employment at the level of companies based solely on administrative data.", "categories": "stat.ap econ.em", "created": "2019-06-26", "updated": "", "authors": ["maciej ber\u0119sewicz", "dagmara nikulin"], "url": "https://arxiv.org/abs/1906.10957"}, {"title": "proxy expenditure weights for consumer price index: audit sampling   inference for big data statistics", "id": "1906.11208", "abstract": "purchase data from retail chains provide proxy measures of private household expenditure on items that are the most troublesome to collect in the traditional expenditure survey. due to the sheer amount of proxy data, the bias due to coverage and selection errors completely dominates the variance. we develop tests for bias based on audit sampling, which makes use of available survey data that cannot be linked to the proxy data source at the individual level. however, audit sampling fails to yield a meaningful mean squared error estimate, because the sampling variance is too large compared to the bias of the big data estimate. we propose a novel accuracy measure that is applicable in such situations. this can provide a necessary part of the statistical argument for the uptake of big data source, in replacement of traditional survey sampling. an application to disaggregated food price index is used to demonstrate the proposed approach.", "categories": "econ.em stat.me", "created": "2019-06-15", "updated": "", "authors": ["li-chun zhang"], "url": "https://arxiv.org/abs/1906.11208"}, {"title": "the hamiltonian approach to the problem of derivation of production   functions in economic growth theory", "id": "1906.11224", "abstract": "we introduce a general hamiltonian framework that appears to be a natural setting for the derivation of various production functions in economic growth theory, starting with the celebrated cobb-douglas function. employing our method, we investigate some existing models and propose a new one as special cases of the general $n$-dimensional lotka-volterra system of eco-dynamics.", "categories": "econ.th math.sg", "created": "2019-06-26", "updated": "", "authors": ["roman g. smirnov", "kunpeng wang"], "url": "https://arxiv.org/abs/1906.11224"}, {"title": "empirical process results for exchangeable arrays", "id": "1906.11293", "abstract": "exchangeable arrays are natural tools to model common forms of dependence between units of a sample. jointly exchangeable arrays are well suited to dyadic data, where observed random variables are indexed by two units from the same population. examples include trade flows between countries or relationships in a network. separately exchangeable arrays are well suited to multiway clustering, where units sharing the same cluster (e.g. geographical areas or sectors of activity when considering individual wages) may be dependent in an unrestricted way. we prove uniform laws of large numbers and central limit theorems for such exchangeable arrays. we obtain these results under the same moment restrictions and conditions on the class of functions as those typically assumed with i.i.d. data. we also show the convergence of bootstrap processes adapted to such arrays.", "categories": "math.st econ.em stat.th", "created": "2019-06-24", "updated": "2020-05-25", "authors": ["laurent davezies", "xavier d'haultfoeuille", "yannick guyonvarch"], "url": "https://arxiv.org/abs/1906.11293"}, {"title": "dynamically stable matching", "id": "1906.11391", "abstract": "i introduce a stability notion, dynamic stability, for two-sided dynamic matching markets where (i) matching opportunities arrive over time, (ii) matching is one-to-one, and (iii) matching is irreversible. the definition addresses two conceptual issues. first, since not all agents are available to match at the same time, one must establish which agents are allowed to form blocking pairs. second, dynamic matching markets exhibit a form of externality that is not present in static markets: an agent's payoff from remaining unmatched cannot be defined independently of what other contemporaneous agents' outcomes are. dynamically stable matchings always exist. dynamic stability is a necessary condition to ensure timely participation in the economy by ensuring that agents do not strategically delay the time at which they are available to match.", "categories": "econ.th", "created": "2019-06-26", "updated": "2020-03-04", "authors": ["laura doval"], "url": "https://arxiv.org/abs/1906.11391"}, {"title": "broken detailed balance and non-equilibrium dynamics in noisy social   learning models", "id": "1906.11481", "abstract": "we propose new degroot-type social learning models with feedback in a continuous time, to investigate the effect of a noisy information source on consensus formation in a social network. unlike the standard degroot framework, noisy information models destroy consensus formation. on the other hand, the noisy opinion dynamics converge to the equilibrium distribution that encapsulates correlations among agents' opinions. interestingly, such an equilibrium distribution is also a non-equilibrium steady state (ness) with a non-zero probabilistic current loop. thus, noisy information source leads to a ness at long times that encodes persistent correlated opinion dynamics of learning agents. our model provides a simple realization of ness in the context of social learning. other phenomena such as synchronization of opinions when agents are subject to a common noise are also studied.", "categories": "physics.soc-ph cs.si econ.th", "created": "2019-06-27", "updated": "2020-05-13", "authors": ["tushar vaidya", "thiparat chotibut", "georgios piliouras"], "url": "https://arxiv.org/abs/1906.11481"}, {"title": "are cryptocurrency traders pioneers or just risk-seekers? evidence from   brokerage accounts", "id": "1906.11968", "abstract": "are cryptocurrency traders driven by a desire to invest in a new asset class to diversify their portfolio or are they merely seeking to increase their levels of risk? to answer this question, we use individual-level brokerage data and study their behavior in stock trading around the time they engage in their first cryptocurrency trade. we find that when engaging in cryptocurrency trading investors simultaneously increase their risk-seeking behavior in stock trading as they increase their trading intensity and use of leverage. the increase in risk-seeking in stocks is particularly pronounced when volatility in cryptocurrency returns is low, suggesting that their overall behavior is driven by excitement-seeking.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2019-06-27", "updated": "", "authors": ["matthias pelster", "bastian breitmayer", "tim hasso"], "url": "https://arxiv.org/abs/1906.11968"}, {"title": "modeling univariate and multivariate stochastic volatility in r with   stochvol and factorstochvol", "id": "1906.12123", "abstract": "stochastic volatility (sv) models are nonlinear state-space models that enjoy increasing popularity for fitting and predicting heteroskedastic time series. however, due to the large number of latent quantities, their efficient estimation is non-trivial and software that allows to easily fit sv models to data is rare. we aim to alleviate this issue by presenting novel implementations of four sv models delivered in two r packages. several unique features are included and documented. as opposed to previous versions, stochvol is now capable of handling linear mean models, heavy-tailed sv, and sv with leverage. moreover, we newly introduce factorstochvol which caters for multivariate sv. both packages offer a user-friendly interface through the conventional r generics and a range of tailor-made methods. computational efficiency is achieved via interfacing r to c++ and doing the heavy work in the latter. in the paper at hand, we provide a detailed discussion on bayesian sv estimation and showcase the use of the new software through various examples.", "categories": "stat.co econ.em q-fin.cp q-fin.st", "created": "2019-06-28", "updated": "", "authors": ["darjus hosszejni", "gregor kastner"], "url": "https://arxiv.org/abs/1906.12123"}, {"title": "dealing with stochastic volatility in time series using the r package   stochvol", "id": "1906.12134", "abstract": "the r package stochvol provides a fully bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. it utilizes markov chain monte carlo (mcmc) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. the package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other mcmc samplers. the main focus of this paper is to show the functionality of stochvol. in addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data.", "categories": "stat.co econ.em q-fin.cp q-fin.st", "created": "2019-06-28", "updated": "", "authors": ["gregor kastner"], "url": "https://arxiv.org/abs/1906.12134"}, {"title": "katugampola generalized conformal derivative approach to inada   conditions and solow-swan economic growth model", "id": "1907.00130", "abstract": "this article shows a new focus of mathematic analysis for the solow-swan economic growth model, using the generalized conformal derivative katugampola (kgcd). for this, under the same solow-swan model assumptions, the inada conditions are extended, which, for the new model shown here, depending on the order of the kgcd. this order plays an important role in the speed of convergence of the closed solutions obtained with this derivative for capital (k) and for per-capita production (y) in the cases without migration and with negative migration. our approach to the model with the kgcd adds a new parameter to the solow-swan model, the order of the kgcd and not a new state variable. in addition, we propose several possible economic interpretations for that parameter.", "categories": "econ.th math.ds", "created": "2019-06-28", "updated": "", "authors": ["g. fern\u00e1ndez-anaya", "l. a. quezada-t\u00e9llez", "b. nu\u00f1ez-zavala", "d. brun-battistini"], "url": "https://arxiv.org/abs/1907.00130"}, {"title": "p-hacking in clinical trials and how incentives shape the distribution   of results across phases", "id": "1907.00185", "abstract": "clinical research should conform to high standards of ethical and scientific integrity, given that human lives are at stake. however, economic incentives can generate conflicts of interest for investigators, who may be inclined to withhold unfavorable results or even tamper with data in order to achieve desired outcomes. to shed light on the integrity of clinical trial results, this paper systematically analyzes the distribution of p-values of primary outcomes for phase ii and phase iii drug trials reported to the clinicaltrials.gov registry. first, we detect no bunching of results just above the classical 5% threshold for statistical significance. second, a density discontinuity test reveals an upward jump at the 5% threshold for phase iii results by small industry sponsors. third, we document a larger fraction of significant results in phase iii compared to phase ii. linking trials across phases, we find that early favorable results increase the likelihood of continuing into the next phase. once we take into account this selective continuation, we can explain almost completely the excess of significant results in phase iii for trials conducted by large industry sponsors. for small industry sponsors, instead, part of the excess remains unexplained.", "categories": "econ.gn q-fin.ec", "created": "2019-06-29", "updated": "2020-03-19", "authors": ["j\u00e9r\u00f4me adda", "christian decker", "marco ottaviani"], "url": "https://arxiv.org/abs/1907.00185"}, {"title": "relaxing the exclusion restriction in shift-share instrumental variable   estimation", "id": "1907.00222", "abstract": "many economic studies use shift-share instruments to estimate causal effects of a treatment. often, this type of instrument implies that all shares fulfill an exclusion restriction, making the identifying assumption very strict. in this paper, i propose the use of machine learning methods that allow to substantially relax the exclusion restriction, by selecting invalid shares. i apply the methods to simulated data and the estimation of the effects of trade exposure and immigration on wages. after removing shares selected as endogenous, the positive effects of immigration decrease while the effects of chinese import exposure are not affected.", "categories": "econ.em", "created": "2019-06-29", "updated": "2019-12-04", "authors": ["nicolas apfel"], "url": "https://arxiv.org/abs/1907.00222"}, {"title": "bounding causes of effects with mediators", "id": "1907.00399", "abstract": "suppose x and y are binary exposure and outcome variables, and we have full knowledge of the distribution of y, given application of x. from this we know the average causal effect of x on y. we are now interested in assessing, for a case that was exposed and exhibited a positive outcome, whether it was the exposure that caused the outcome. the relevant \"probability of causation\", pc, typically is not identified by the distribution of y given x, but bounds can be placed on it, and these bounds can be improved if we have further information about the causal process. here we consider cases where we know the probabilistic structure for a sequence of complete mediators between x and y. we derive a general formula for calculating bounds on pc for any pattern of data on the mediators (including the case with no data). we show that the largest and smallest upper and lower bounds that can result from any complete mediation process can be obtained in processes with at most two steps. we also consider homogeneous processes with many mediators. pc can sometimes be identified as 0 with negative data, but it cannot be identified at 1 even with positive data on an infinite set of mediators. the results have implications for learning about causation from knowledge of general processes and of data on cases.", "categories": "math.st econ.em stat.th", "created": "2019-06-30", "updated": "", "authors": ["philip dawid", "macartan humphreys", "monica musio"], "url": "https://arxiv.org/abs/1907.00399"}, {"title": "permutation inference with a finite number of heterogeneous clusters", "id": "1907.01049", "abstract": "i introduce a simple permutation procedure to test conventional (non-sharp) hypotheses about the effect of a binary treatment in the presence of a finite number of large, heterogeneous clusters when the treatment effect is identified by comparisons across clusters. the procedure asymptotically controls size by applying a level-adjusted permutation test to a suitable statistic. the adjustments needed for most empirically relevant situations are tabulated in the paper. the adjusted permutation test is easy to implement in practice and performs well at conventional levels of significance with at least four treated clusters and a similar number of control clusters. it is particularly robust to situations where some clusters are much more variable than others. examples and an empirical application are provided.", "categories": "econ.em stat.me", "created": "2019-07-01", "updated": "", "authors": ["andreas hagemann"], "url": "https://arxiv.org/abs/1907.01049"}, {"title": "simulation smoothing for nowcasting with large mixed-frequency vars", "id": "1907.01075", "abstract": "there is currently an increasing interest in large vector autoregressive (var) models. vars are popular tools for macroeconomic forecasting and use of larger models has been demonstrated to often improve the forecasting ability compared to more traditional small-scale models. mixed-frequency vars deal with data sampled at different frequencies while remaining within the realms of vars. estimation of mixed-frequency vars makes use of simulation smoothing, but using the standard procedure these models quickly become prohibitive in nowcasting situations as the size of the model grows. we propose two algorithms that alleviate the computational efficiency of the simulation smoothing algorithm. our preferred choice is an adaptive algorithm, which augments the state vector as necessary to sample also monthly variables that are missing at the end of the sample. for large vars, we find considerable improvements in speed using our adaptive algorithm. the algorithm therefore provides a crucial building block for bringing the mixed-frequency vars to the high-dimensional regime.", "categories": "econ.em", "created": "2019-07-01", "updated": "", "authors": ["sebastian ankargren", "paulina jon\u00e9us"], "url": "https://arxiv.org/abs/1907.01075"}, {"title": "solving the reswitching paradox in the sraffian theory of capital", "id": "1907.01189", "abstract": "the possibility of re-switching of techniques in piero sraffa's intersectoral model, namely the returning capital-intensive techniques with monotonic changes in the profit rate, is traditionally considered as a paradox putting at stake the viability of the neoclassical theory of production. it is argued here that this phenomenon can be rationalized within the neoclassical paradigm. sectoral interdependencies can give rise to non-monotonic effects of progressive variations in income distribution on relative prices. the re-switching of techniques is, therefore, the result of cost-minimizing technical choices facing returning ranks of relative input prices in full consistency with the neoclassical perspective.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-07-02", "updated": "2019-11-27", "authors": ["carlo milana"], "url": "https://arxiv.org/abs/1907.01189"}, {"title": "large volatility matrix prediction with high-frequency data", "id": "1907.01196", "abstract": "we provide a novel method for large volatility matrix prediction with high-frequency data by applying eigen-decomposition to daily realized volatility matrix estimators and capturing eigenvalue dynamics with arma models. given a sequence of daily volatility matrix estimators, we compute the aggregated eigenvectors and obtain the corresponding eigenvalues. eigenvalues in the same relative magnitude form a time series and the arma models are further employed to model the dynamics within each eigenvalue time series to produce a predictor. we predict future large volatility matrix based on the predicted eigenvalues and the aggregated eigenvectors, and demonstrate the advantages of the proposed method in volatility prediction and portfolio allocation problems.", "categories": "stat.ap econ.em", "created": "2019-07-02", "updated": "2019-09-25", "authors": ["xinyu song"], "url": "https://arxiv.org/abs/1907.01196"}, {"title": "a model of presidential debates", "id": "1907.01362", "abstract": "presidential debates are viewed as providing an important public good by revealing information on candidates to voters. we consider an endogenous model of presidential debates in which an incumbent and a challenger (who is privately informed about her own quality) publicly announce whether they are willing to participate in a public debate, taking into account that a voter's choice of candidate depends on her beliefs regarding the candidates' qualities and on the state of nature.it is found that in equilibrium a debate occurs or does not occur independently of the challenger's quality and therefore the candidates' announcements are uninformative. this is because opting-out is perceived to be worse than losing a debate and therefore the challenger never refuses to participate.", "categories": "econ.gn q-fin.ec", "created": "2019-07-02", "updated": "2020-02-26", "authors": ["doron klunover", "john morgan"], "url": "https://arxiv.org/abs/1907.01362"}, {"title": "algorithms for competitive division of chores", "id": "1907.01766", "abstract": "we study the problem of allocating divisible bads (chores) among multiple agents with additive utilities, when money transfers are not allowed. the competitive rule is known to be the best mechanism for goods with additive utilities and was recently extended to chores by bogomolnaia et al (2017). for both goods and chores, the rule produces pareto optimal and envy-free allocations. in the case of goods, the outcome of the competitive rule can be easily computed. competitive allocations solve the eisenberg-gale convex program; hence the outcome is unique and can be approximately found by standard gradient methods. an exact algorithm that runs in polynomial time in the number of agents and goods was given by orlin.   in the case of chores, the competitive rule does not solve any convex optimization problem; instead, competitive allocations correspond to local minima, local maxima, and saddle points of the nash social welfare on the pareto frontier of the set of feasible utilities. the rule becomes multivalued and none of the standard methods can be applied to compute its outcome.   in this paper, we show that all the outcomes of the competitive rule for chores can be computed in strongly polynomial time if either the number of agents or the number of chores is fixed. the approach is based on a combination of three ideas: all consumption graphs of pareto optimal allocations can be listed in polynomial time; for a given consumption graph, a candidate for a competitive allocation can be constructed via explicit formula; and a given allocation can be checked for being competitive using a maximum flow computation as in devanur et al (2002).   our algorithm immediately gives an approximately-fair allocation of indivisible chores by the rounding technique of barman and krishnamurthy (2018).", "categories": "cs.gt cs.ds econ.th", "created": "2019-07-03", "updated": "", "authors": ["simina br\u00e2nzei", "fedor sandomirskiy"], "url": "https://arxiv.org/abs/1907.01766"}, {"title": "multiplicity of time scales in climate, matter, life, and economy", "id": "1907.01902", "abstract": "this topic review communicates working experiences regarding interaction of a multiplicity of processes. our experiences come from climate change modelling, materials science, cell physiology and public health, and macroeconomic modelling. we look at the astonishing advances of recent years in broad-band temporal frequency sampling, multiscale modelling and fast large-scale numerical simulation of complex systems, but also the continuing uncertainty of many science-based results.   we describe and analyse properties that depend on the time scale of the measurement; structural instability; tipping points; thresholds; hysteresis; feedback mechanisms with runaways or stabilizations or delays. we point to grave disorientation in statistical sampling, the interpretation of observations and the design of control when neglecting the presence or emergence of multiple characteristic times.   we explain what these working experiences can demonstrate for environmental research.", "categories": "econ.gn nlin.ps q-fin.ec", "created": "2019-07-01", "updated": "2020-02-05", "authors": ["bernhelm booss-bavnbek", "rasmus kristoffer pedersen", "ulf r\u00f8rb\u00e6k pedersen"], "url": "https://arxiv.org/abs/1907.01902"}, {"title": "an econometric perspective on algorithmic subsampling", "id": "1907.01954", "abstract": "datasets that are terabytes in size are increasingly common, but computer bottlenecks often frustrate a complete analysis of the data. while more data are better than less, diminishing returns suggest that we may not need terabytes of data to estimate a parameter or test a hypothesis. but which rows of data should we analyze, and might an arbitrary subset of rows preserve the features of the original data? this paper reviews a line of work that is grounded in theoretical computer science and numerical linear algebra, and which finds that an algorithmically desirable sketch, which is a randomly chosen subset of the data, must preserve the eigenstructure of the data, a property known as a subspace embedding. building on this work, we study how prediction and inference can be affected by data sketching within a linear regression setup. we show that the sketching error is small compared to the sample size effect which a researcher can control. as a sketch size that is algorithmically optimal may not be suitable for prediction and inference, we use statistical arguments to provide 'inference conscious' guides to the sketch size. when appropriately implemented, an estimator that pools over different sketches can be nearly as efficient as the infeasible one using the full sample.", "categories": "econ.em stat.co", "created": "2019-07-03", "updated": "2020-04-30", "authors": ["sokbae lee", "serena ng"], "url": "https://arxiv.org/abs/1907.01954"}, {"title": "machine learning and behavioral economics for personalized choice   architecture", "id": "1907.02100", "abstract": "behavioral economics changed the way we think about market participants and revolutionized policy-making by introducing the concept of choice architecture. however, even though effective on the level of a population, interventions from behavioral economics, nudges, are often characterized by weak generalisation as they struggle on the level of individuals. recent developments in data science, artificial intelligence (ai) and machine learning (ml) have shown ability to alleviate some of the problems of weak generalisation by providing tools and methods that result in models with stronger predictive power. this paper aims to describe how ml and ai can work with behavioral economics to support and augment decision-making and inform policy decisions by designing personalized interventions, assuming that enough personalized traits and psychological variables can be sampled.", "categories": "econ.gn cs.lg q-fin.ec", "created": "2019-07-03", "updated": "", "authors": ["emir hrnjic", "nikodem tomczak"], "url": "https://arxiv.org/abs/1907.02100"}, {"title": "the informativeness of estimation moments", "id": "1907.02101", "abstract": "this paper introduces measures for how each moment contributes to the precision of parameter estimates in gmm settings. for example, one of the measures asks what would happen to the variance of the parameter estimates if a particular moment was dropped from the estimation. the measures are all easy to compute. we illustrate the usefulness of the measures through two simple examples as well as an application to a model of joint retirement planning of couples. we estimate the model using the uk-bhps, and we find evidence of complementarities in leisure. our sensitivity measures illustrate that the estimate of the complementarity is primarily informed by the distribution of differences in planned retirement dates. the estimated econometric model can be interpreted as a bivariate ordered choice model that allows for simultaneity. this makes the model potentially useful in other applications.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-07-03", "updated": "2020-01-07", "authors": ["bo honore", "thomas jorgensen", "aureo de paula"], "url": "https://arxiv.org/abs/1907.02101"}, {"title": "emergent inequality and endogenous dynamics in a simple behavioral   macroeconomic model", "id": "1907.02155", "abstract": "standard macroeconomic models assume that households are rational in the sense that they are perfect utility maximizers, and explain economic dynamics in terms of shocks that drive the economy away from the stead-state. here we build on a standard macroeconomic model in which a single rational representative household makes a savings decision of how much to consume or invest. in our model households are myopic boundedly rational heterogeneous agents embedded in a social network. from time to time each household updates its savings rate by copying the savings rate of its neighbor with the highest consumption. if the updating time is short, the economy is stuck in a poverty trap, but for longer updating times economic output approaches its optimal value, and we observe a critical transition to an economy with irregular endogenous oscillations in economic output, resembling a business cycle. in this regime households divide into two groups: poor households with low savings rates and rich households with high savings rates. thus inequality and economic dynamics both occur spontaneously as a consequence of imperfect household decision making. our work here supports an alternative program of research that substitutes utility maximization for behaviorally grounded decision making.", "categories": "econ.gn q-fin.ec", "created": "2019-07-03", "updated": "", "authors": ["yuki m. asano", "jakob j. kolb", "jobst heitzig", "j. doyne farmer"], "url": "https://arxiv.org/abs/1907.02155"}, {"title": "heterogeneous regression models for clusters of spatial dependent data", "id": "1907.02212", "abstract": "in economic development, there are often regions that share similar economic characteristics, and economic models on such regions tend to have similar covariate effects. in this paper, we propose a bayesian clustered regression for spatially dependent data in order to detect clusters in the covariate effects. our proposed method is based on the dirichlet process which provides a probabilistic framework for simultaneous inference of the number of clusters and the clustering configurations. the usage of our method is illustrated both in simulation studies and an application to a housing cost dataset of georgia.", "categories": "econ.em stat.co", "created": "2019-07-04", "updated": "2020-04-08", "authors": ["zhihua ma", "yishu xue", "guanyu hu"], "url": "https://arxiv.org/abs/1907.02212"}, {"title": "optimal transport on large networks, a practitioner's guide", "id": "1907.02320", "abstract": "this article presents a set of tools for the modeling of a spatial allocation problem in a large geographic market and gives examples of applications. in our settings, the market is described by a network that maps the cost of travel between each pair of adjacent locations. two types of agents are located at the nodes of this network. the buyers choose the most competitive sellers depending on their prices and the cost to reach them. their utility is assumed additive in both these quantities. each seller, taking as given other sellers prices, sets her own price to have a demand equal to the one we observed. we give a linear programming formulation for the equilibrium conditions. after formally introducing our model we apply it on two examples: prices offered by petrol stations and quality of services provided by maternity wards. these examples illustrate the applicability of our model to aggregate demand, rank prices and estimate cost structure over the network. we insist on the possibility of applications to large scale data sets using modern linear programming solvers such as gurobi. in addition to this paper we released a r toolbox to implement our results and an online tutorial (http://optimalnetwork.github.io)", "categories": "econ.gn cs.ds econ.em q-fin.ec", "created": "2019-07-04", "updated": "2019-08-22", "authors": ["arthur charpentier", "alfred galichon", "lucas vernet"], "url": "https://arxiv.org/abs/1907.02320"}, {"title": "heterogeneous choice sets and preferences", "id": "1907.02337", "abstract": "we propose a robust method of discrete choice analysis when agents' choice sets are unobserved. our core model assumes nothing about agents' choice sets apart from their minimum size. importantly, it leaves unrestricted the dependence, conditional on observables, between agents' choice sets and their preferences. we first establish that the model is partially identified and characterize its sharp identification region. we also show how the model can be used to assess the welfare cost of limited choice sets. we then apply our theoretical findings to learn about households' risk preferences and choice sets from data on their deductible choices in auto collision insurance. we find that the data can be explained by expected utility theory with relatively low levels of risk aversion and heterogeneous choice sets. we also find that a mixed logit model, as well as some familiar models of choice set formation, are rejected in our data.", "categories": "econ.em", "created": "2019-07-04", "updated": "", "authors": ["levon barseghyan", "maura coughlin", "francesca molinari", "joshua c. teitelbaum"], "url": "https://arxiv.org/abs/1907.02337"}, {"title": "random forest estimation of the ordered choice model", "id": "1907.02436", "abstract": "in econometrics so-called ordered choice models are popular when interest is in the estimation of the probabilities of particular values of categorical outcome variables with an inherent ordering, conditional on covariates. in this paper we develop a new machine learning estimator based on the random forest algorithm for such models without imposing any distributional assumptions. the proposed ordered forest estimator provides a flexible estimation method of the conditional choice probabilities that can naturally deal with nonlinearities in the data, while taking the ordering information explicitly into account. in addition to common machine learning estimators, it enables the estimation of marginal effects as well as conducting inference thereof and thus providing the same output as classical econometric estimators based on ordered logit or probit models. an extensive simulation study examines the finite sample properties of the ordered forest and reveals its good predictive performance, particularly in settings with multicollinearity among the predictors and nonlinear functional forms. an empirical application further illustrates the estimation of the marginal effects and their standard errors and demonstrates the advantages of the flexible estimation compared to a parametric benchmark model. a software implementation of the ordered forest is provided in gauss as well as in the r-package orf available on cran.", "categories": "econ.em", "created": "2019-07-04", "updated": "2020-01-04", "authors": ["michael lechner", "gabriel okasa"], "url": "https://arxiv.org/abs/1907.02436"}, {"title": "artificial intelligence alter egos: who benefits from robo-investing?", "id": "1907.03370", "abstract": "artificial intelligence, or ai, enhancements are increasingly shaping our daily lives. financial decision-making is no exception to this. we introduce the notion of ai alter egos, which are shadow robo-investors, and use a unique data set covering brokerage accounts for a large cross-section of investors over a sample from january 2003 to march 2012, which includes the 2008 financial crisis, to assess the benefits of robo-investing. we have detailed investor characteristics and records of all trades. our data set consists of investors typically targeted for robo-advising. we explore robo-investing strategies commonly used in the industry, including some involving advanced machine learning methods. the man versus machine comparison allows us to shed light on potential benefits the emerging robo-advising industry may provide to certain segments of the population, such as low income and/or high risk averse investors.", "categories": "q-fin.pm econ.em q-fin.st", "created": "2019-07-07", "updated": "", "authors": ["catherine d'hondt", "rudy de winne", "eric ghysels", "steve raymond"], "url": "https://arxiv.org/abs/1907.03370"}, {"title": "a mathematical analysis of an election system proposed by gottlob frege", "id": "1907.03643", "abstract": "in 1998 a long-lost proposal for an election law by gottlob frege (1848--1925) was rediscovered in the th\\\"uringer universit\\\"ats- und landesbibliothek in jena, germany. the method that frege proposed for the election of representatives of a constituency features a remarkable concern for the representation of minorities. its core idea is that votes cast for unelected candidates are carried over to the next election, while elected candidates incur a cost of winning. we prove that this sensitivity to past elections guarantees a proportional representation of political opinions in the long run. we find that through a slight modification of frege's original method even stronger proportionality guarantees can be achieved. this modified version of frege's method also provides a novel solution to the apportionment problem, which is distinct from all of the best-known apportionment methods, while still possessing noteworthy proportionality properties.", "categories": "cs.gt econ.th", "created": "2019-07-08", "updated": "2020-09-04", "authors": ["paul harrenstein", "marie-louise lackner", "martin lackner"], "url": "https://arxiv.org/abs/1907.03643"}, {"title": "competing models", "id": "1907.03809", "abstract": "different agents compete to predict a variable of interest. all agents are bayesian, but may have `misspecified models' of the world, i.e., they consider different subsets of explanatory variables to make their prediction. after observing a common dataset, who has the highest confidence in her predictive ability? we characterize it and show that it crucially depends on the sample size. with small samples, it is an agent using a small-dimensional model, in the sense of using a smaller number of variables relative to the true data generating process. with large samples, it is typically an agent with a large-dimensional model, possibly including irrelevant variables, but never excluding relevant ones. applications include auctions of assets where bidders observe the same information but hold different priors.", "categories": "econ.th cs.lg econ.em", "created": "2019-07-08", "updated": "2020-06-18", "authors": ["jose luis montiel olea", "pietro ortoleva", "mallesh m pai", "andrea prat"], "url": "https://arxiv.org/abs/1907.03809"}, {"title": "adaptive inference for a semiparametric generalized autoregressive   conditional heteroskedasticity model", "id": "1907.04147", "abstract": "this paper considers a semiparametric generalized autoregressive conditional heteroskedasticity (s-garch) model. for this model, we first estimate the time-varying long run component for unconditional variance by the kernel estimator, and then estimate the non-time-varying parameters in garch-type short run component by the quasi maximum likelihood estimator (qmle). we show that the qmle is asymptotically normal with the parametric convergence rate. next, we construct a lagrange multiplier test for linear parameter constraint and a portmanteau test for model checking, and obtain their asymptotic null distributions. our entire statistical inference procedure works for the non-stationary data with two important features: first, our qmle and two tests are adaptive to the unknown form of the long run component; second, our qmle and two tests share the same efficiency and testing power as those in variance targeting method when the s-garch model is stationary.", "categories": "stat.me econ.em", "created": "2019-07-09", "updated": "2020-10-02", "authors": ["feiyu jiang", "dong li", "ke zhu"], "url": "https://arxiv.org/abs/1907.04147"}, {"title": "ordinal imitative dynamics", "id": "1907.04272", "abstract": "this paper introduces an evolutionary dynamics based on imitate the better realization (ibr) rule. under this rule, agents in a population game imitate the strategy of a randomly chosen opponent whenever the opponent`s realized payoff is higher than their own. such behavior generates an ordinal mean dynamics which is polynomial in strategy utilization frequencies. we demonstrate that while the dynamics does not possess nash stationarity or payoff monotonicity, under it pure strategies iteratively strictly dominated by pure strategies are eliminated and strict equilibria are locally stable. we investigate the relationship between the dynamics based on the ibr rule and the replicator dynamics. in trivial cases, the two dynamics are topologically equivalent. in rock-paper-scissors games we conjecture that both dynamics exhibit the same types of behavior, but the partitions of the game set do not coincide. in other cases, the ibr dynamics exhibits behaviors that are impossible under the replicator dynamics.", "categories": "econ.th", "created": "2019-07-09", "updated": "", "authors": ["george loginov"], "url": "https://arxiv.org/abs/1907.04272"}, {"title": "relationships between different macroeconomic variables using vecm", "id": "1907.04447", "abstract": "through this paper, an attempt has been made to quantify the underlying relationships between the leading macroeconomic indicators. more clearly, an effort has been made in this paper to assess the cointegrating relationships and examine the error correction behavior revealed by macroeconomic variables using econometric techniques that were initially developed by engle and granger (1987), and further explored by various succeeding papers, with the latest being tu and yi (2017). gross domestic product, discount rate, consumer price index and population of u.s are representatives of the economy that have been used in this study to analyze the relationships between economic indicators and understand how an adverse change in one of these variables might have ramifications on the others. this is performed to corroborate and guide the belief that a policy maker with specified intentions cannot ignore the spillover effects caused by implementation of a certain policy.", "categories": "econ.gn q-fin.ec", "created": "2019-07-09", "updated": "", "authors": ["saannidhya rawat"], "url": "https://arxiv.org/abs/1907.04447"}, {"title": "discrete choice and welfare analysis with unobserved choice sets", "id": "1907.04853", "abstract": "we propose a framework for doing sharp nonparametric welfare analysis in discrete choice models with unobserved variation in choice sets. we recover jointly the distribution of choice sets and the distribution of preferences. to achieve this we use panel data on choices and assume nestedness of the latent choice sets. nestedness means that choice sets of different decision makers are ordered by inclusion. it is satisfied, for instance, when the choice set variation is the result of either a search process or unobserved feasibility. using variation of the uncovered choice sets we show how to do ordinal (nonparametric) welfare comparisons. when one is willing to make additional assumptions about preferences, we show how to nonparametrically identify the ranking over average utilities in the standard multinomial choice setting.", "categories": "econ.em econ.th", "created": "2019-07-09", "updated": "2019-07-14", "authors": ["victor h. aguiar", "nail kashaev"], "url": "https://arxiv.org/abs/1907.04853"}, {"title": "proportional dynamics in exchange economies", "id": "1907.05037", "abstract": "we study the proportional response dynamic in exchange economies, where each player starts with some amount of money and a good. every day, the players bring one unit of their good and submit bids on goods they like, each good gets allocated in proportion to the bid amounts, and each seller collects the bids received. then every player updates the bids proportionally to the contribution of each good in their utility. this dynamic models a process of learning how to bid and has been studied in a series of papers on fisher and production markets, but not in exchange economies. our main results are as follows:   - for linear utilities, the dynamic converges to market equilibrium utilities and allocations, while the bids and prices may cycle. we give a combinatorial characterization of limit cycles for prices and bids.   - we introduce a lazy version of the dynamic, where players may save money for later, and show this converges in everything: utilities, allocations, and prices.   - for ces utilities in the substitute range $[0,1)$, the dynamic converges for all parameters.   this answers an open question about exchange economies with linear utilities, where tatonnement does not converge to market equilibria, and no natural process leading to equilibria was known. we also note that proportional response is a process where the players exchange goods throughout time (in out-of-equilibrium states), while tatonnement only explains how exchange happens in the limit.", "categories": "cs.gt econ.th", "created": "2019-07-11", "updated": "2019-09-25", "authors": ["simina br\u00e2nzei", "nikhil r. devanur", "yuval rabani"], "url": "https://arxiv.org/abs/1907.05037"}, {"title": "a global economic policy uncertainty index from principal component   analysis", "id": "1907.05049", "abstract": "this paper constructs a global economic policy uncertainty index through the principal component analysis of the economic policy uncertainty indices for twenty primary economies around the world. we find that the pca-based global economic policy uncertainty index is a good proxy for the economic policy uncertainty on a global scale, which is quite consistent with the gdp-weighted global economic policy uncertainty index. the pca-based economic policy uncertainty index is found to be positively related with the volatility and correlation of the global financial market, which indicates that the stocks are more volatile and correlated when the global economic policy uncertainty is higher. the pca-based global economic policy uncertainty index performs slightly better because the relationship between the pca-based uncertainty and market volatility and correlation is more significant.", "categories": "econ.gn q-fin.ec", "created": "2019-07-11", "updated": "2019-07-31", "authors": ["peng-fei dai", "xiong xiong", "wei-xing zhou"], "url": "https://arxiv.org/abs/1907.05049"}, {"title": "adaptive pricing in insurance: generalized linear models and gaussian   process regression approaches", "id": "1907.05381", "abstract": "we study the application of dynamic pricing to insurance. we view this as an online revenue management problem where the insurance company looks to set prices to optimize the long-run revenue from selling a new insurance product. we develop two pricing models: an adaptive generalized linear model (glm) and an adaptive gaussian process (gp) regression model. both balance between exploration, where we choose prices in order to learn the distribution of demands & claims for the insurance product, and exploitation, where we myopically choose the best price from the information gathered so far. the performance of the pricing policies is measured in terms of regret: the expected revenue loss caused by not using the optimal price. as is commonplace in insurance, we model demand and claims by glms. in our adaptive glm design, we use the maximum quasi-likelihood estimation (mqle) to estimate the unknown parameters. we show that, if prices are chosen with suitably decreasing variability, the mqle parameters eventually exist and converge to the correct values, which in turn implies that the sequence of chosen prices will also converge to the optimal price. in the adaptive gp regression model, we sample demand and claims from gaussian processes and then choose selling prices by the upper confidence bound rule. we also analyze these glm and gp pricing algorithms with delayed claims. although similar results exist in other domains, this is among the first works to consider dynamic pricing problems in the field of insurance. we also believe this is the first work to consider gaussian process regression in the context of insurance pricing. these initial findings suggest that online machine learning algorithms could be a fruitful area of future investigation and application in insurance.", "categories": "econ.em math.oc q-fin.mf q-fin.rm stat.ml", "created": "2019-07-02", "updated": "", "authors": ["yuqing zhang", "neil walton"], "url": "https://arxiv.org/abs/1907.05381"}, {"title": "singularities and catastrophes in economics: historical perspectives and   future directions", "id": "1907.05582", "abstract": "economic theory is a mathematically rich field in which there are opportunities for the formal analysis of singularities and catastrophes. this article looks at the historical context of singularities through the work of two eminent frenchmen around the late 1960s and 1970s. ren\\'e thom (1923-2002) was an acclaimed mathematician having received the fields medal in 1958, whereas g\\'erard debreu (1921-2004) would receive the nobel prize in economics in 1983. both were highly influential within their fields and given the fundamental nature of their work, the potential for cross-fertilisation would seem to be quite promising. this was not to be the case: debreu knew of thom's work and cited it in the analysis of his own work, but despite this and other applied mathematicians taking catastrophe theory to economics, the theory never achieved a lasting following and relatively few results were published. this article reviews debreu's analysis of the so called ${\\it regular}$ and ${\\it crtitical}$ economies in order to draw some insights into the economic perspective of singularities before moving to how singularities arise naturally in the nash equilibria of game theory. finally a modern treatment of stochastic game theory is covered through recent work on the quantal response equilibrium. in this view the nash equilibrium is to the quantal response equilibrium what deterministic catastrophe theory is to stochastic catastrophe theory, with some caveats regarding when this analogy breaks down discussed at the end.", "categories": "econ.gn q-fin.ec", "created": "2019-07-12", "updated": "", "authors": ["michael s. harr\u00e9", "adam harris", "scott mccallum"], "url": "https://arxiv.org/abs/1907.05582"}, {"title": "on the residues vectors of a rational class of complex functions.   application to autoregressive processes", "id": "1907.05949", "abstract": "complex functions have multiple uses in various fields of study, so analyze their characteristics it is of extensive interest to other sciences. this work begins with a particular class of rational functions of a complex variable; over this is deduced two elementals properties concerning the residues and is proposed one results which establishes one lower bound for the p-norm of the residues vector. applications to the autoregressive processes are presented and the exemplifications are indicated in historical data of electric generation and econometric series.", "categories": "econ.em", "created": "2019-07-12", "updated": "", "authors": ["guillermo daniel scheidereiter", "omar roberto faure"], "url": "https://arxiv.org/abs/1907.05949"}, {"title": "a simulation of the insurance industry: the problem of risk model   homogeneity", "id": "1907.05954", "abstract": "we develop an agent-based simulation of the catastrophe insurance and reinsurance industry and use it to study the problem of risk model homogeneity. the model simulates the balance sheets of insurance firms, who collect premiums from clients in return for ensuring them against intermittent, heavy-tailed risks. firms manage their capital and pay dividends to their investors, and use either reinsurance contracts or cat bonds to hedge their tail risk. the model generates plausible time series of profits and losses and recovers stylized facts, such as the insurance cycle and the emergence of asymmetric, long tailed firm size distributions. we use the model to investigate the problem of risk model homogeneity. under solvency ii, insurance companies are required to use only certified risk models. this has led to a situation in which only a few firms provide risk models, creating a systemic fragility to the errors in these models. we demonstrate that using too few models increases the risk of nonpayment and default while lowering profits for the industry as a whole. the presence of the reinsurance industry ameliorates the problem but does not remove it. our results suggest that it would be valuable for regulators to incentivize model diversity. the framework we develop here provides a first step toward a simulation model of the insurance industry for testing policies and strategies for better capital management.", "categories": "econ.gn q-fin.ec q-fin.rm", "created": "2019-07-12", "updated": "2019-11-20", "authors": ["torsten heinrich", "juan sabuco", "j. doyne farmer"], "url": "https://arxiv.org/abs/1907.05954"}, {"title": "online rental housing market representation and the digital reproduction   of urban inequality", "id": "1907.06118", "abstract": "as the rental housing market moves online, the internet offers divergent possible futures: either the promise of more-equal access to information for previously marginalized homeseekers, or a reproduction of longstanding information inequalities. biases in online listings' representativeness could impact different communities' access to housing search information, reinforcing traditional information segregation patterns through a digital divide. they could also circumscribe housing practitioners' and researchers' ability to draw broad market insights from listings to understand rental supply and affordability. this study examines millions of craigslist rental listings across the us and finds that they spatially concentrate and over-represent whiter, wealthier, and better-educated communities. other significant demographic differences exist in age, language, college enrollment, rent, poverty rate, and household size. most cities' online housing markets are digitally segregated by race and class, and we discuss various implications for residential mobility, community legibility, gentrification, housing voucher utilization, and automated monitoring and analytics in the smart cities paradigm. while craigslist contains valuable crowdsourced data to better understand affordability and available rental supply in real-time, it does not evenly represent all market segments. the internet promises information democratization, and online listings can reduce housing search costs and increase choice sets. however, technology access/preferences and information channel segregation can concentrate such information-broadcasting benefits in already-advantaged communities, reproducing traditional inequalities and reinforcing residential sorting and segregation dynamics. technology platforms like craigslist construct new institutions with the power to shape spatial economies.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-07-13", "updated": "2019-08-27", "authors": ["geoff boeing"], "url": "https://arxiv.org/abs/1907.06118"}, {"title": "necessary and sufficient condition for equilibrium of the hotelling   model", "id": "1907.06200", "abstract": "we study a model of vendors competing to sell a homogeneous product to customers spread evenly along a linear city. this model is based on hotelling's celebrated paper in 1929. our aim in this paper is to present a necessary and sufficient condition for the equilibrium. this yields a representation for the equilibrium. to achieve this, we first formulate the model mathematically. next, we prove that the condition holds if and only if vendors are equilibrium.", "categories": "econ.th cs.it math.it", "created": "2019-07-14", "updated": "", "authors": ["satoshi hayashi", "naoki tsuge"], "url": "https://arxiv.org/abs/1907.06200"}, {"title": "on the evolution of u.s. temperature dynamics", "id": "1907.06303", "abstract": "climate change is a multidimensional shift. while much research has documented rising mean temperature levels, we also examine range-based measures of daily temperature volatility. specifically, using data for select u.s. cities over the past half-century, we compare the evolving time series dynamics of the average temperature level, avg, and the diurnal temperature range, dtr (the difference between the daily maximum and minimum temperatures at a given location). we characterize trend and seasonality in these two series using linear models with time-varying coefficients. these straightforward yet flexible approximations provide evidence of evolving dtr seasonality, stable avg seasonality, and conditionally gaussian but heteroskedastic innovations for both dtr and avg.", "categories": "stat.ap econ.em", "created": "2019-07-14", "updated": "", "authors": ["francis x. diebold", "glenn d. rudebusch"], "url": "https://arxiv.org/abs/1907.06303"}, {"title": "simple adaptive size-exact testing for full-vector and subvector   inference in moment inequality models", "id": "1907.06317", "abstract": "we propose a simple test for moment inequalities that has exact size in normal models with known variance and has uniformly asymptotically exact size more generally. the test compares the quasi-likelihood ratio statistic to a chi-squared critical value, where the degree of freedom is the rank of the inequalities that are active in finite samples. the test requires no simulation and thus is computationally fast and especially suitable for constructing confidence sets for parameters by test inversion. it uses no tuning parameter for moment selection and yet still adapts to the slackness of the moment inequalities. furthermore, we show how the test can be easily adapted for inference on subvectors for the common empirical setting of conditional moment inequalities with nuisance parameters entering linearly.", "categories": "econ.em math.st stat.th", "created": "2019-07-14", "updated": "2020-08-18", "authors": ["gregory cox", "xiaoxia shi"], "url": "https://arxiv.org/abs/1907.06317"}, {"title": "confidentiality and linked data", "id": "1907.06465", "abstract": "data providers such as government statistical agencies perform a balancing act: maximising information published to inform decision-making and research, while simultaneously protecting privacy. the emergence of identified administrative datasets with the potential for sharing (and thus linking) offers huge potential benefits but significant additional risks. this article introduces the principles and methods of linking data across different sources and points in time, focusing on potential areas of risk. we then consider confidentiality risk, focusing in particular on the \"intruder\" problem central to the area, and looking at both risks from data producer outputs and from the release of micro-data for further analysis. finally, we briefly consider potential solutions to micro-data release, both the statistical solutions considered in other contributed articles and non-statistical solutions.", "categories": "cs.cr cs.ir econ.gn q-fin.ec", "created": "2019-07-15", "updated": "", "authors": ["felix ritchie", "jim smith"], "url": "https://arxiv.org/abs/1907.06465"}, {"title": "audits as evidence: experiments, ensembles, and enforcement", "id": "1907.06622", "abstract": "we develop tools for utilizing correspondence experiments to detect illegal discrimination by individual employers. employers violate us employment law if their propensity to contact applicants depends on protected characteristics such as race or sex. we establish identification of higher moments of the causal effects of protected characteristics on callback rates as a function of the number of fictitious applications sent to each job ad. these moments are used to bound the fraction of jobs that illegally discriminate. applying our results to three experimental datasets, we find evidence of significant employer heterogeneity in discriminatory behavior, with the standard deviation of gaps in job-specific callback probabilities across protected groups averaging roughly twice the mean gap. in a recent experiment manipulating racially distinctive names, we estimate that at least 85% of jobs that contact both of two white applications and neither of two black applications are engaged in illegal discrimination. to assess the tradeoff between type i and ii errors presented by these patterns, we consider the performance of a series of decision rules for investigating suspicious callback behavior under a simple two-type model that rationalizes the experimental data. though, in our preferred specification, only 17% of employers are estimated to discriminate on the basis of race, we find that an experiment sending 10 applications to each job would enable accurate detection of 7-10% of discriminators while falsely accusing fewer than 0.2% of non-discriminators. a minimax decision rule acknowledging partial identification of the joint distribution of callback rates yields higher error rates but more investigations than our baseline two-type model. our results suggest illegal labor market discrimination can be reliably monitored with relatively small modifications to existing audit designs.", "categories": "econ.em stat.ap stat.me stat.ml", "created": "2019-07-15", "updated": "2019-07-18", "authors": ["patrick kline", "christopher walters"], "url": "https://arxiv.org/abs/1907.06622"}, {"title": "unforeseen evidence", "id": "1907.07019", "abstract": "i propose a normative updating rule, extended bayesianism, for the incorporation of probabilistic information arising from the process of becoming more aware. extended bayesianism generalizes standard bayesian updating to allow the posterior to reside on richer probability space than the prior. i then provide an observable criterion on prior and posterior beliefs such that they were consistent with extended bayesianism.", "categories": "econ.th cs.ai", "created": "2019-07-16", "updated": "2019-07-17", "authors": ["evan piermont"], "url": "https://arxiv.org/abs/1907.07019"}, {"title": "information processing constraints in travel behaviour modelling: a   generative learning approach", "id": "1907.07036", "abstract": "travel decisions tend to exhibit sensitivity to uncertainty and information processing constraints. these behavioural conditions can be characterized by a generative learning process. we propose a data-driven generative model version of rational inattention theory to emulate these behavioural representations. we outline the methodology of the generative model and the associated learning process as well as provide an intuitive explanation of how this process captures the value of prior information in the choice utility specification. we demonstrate the effects of information heterogeneity on a travel choice, analyze the econometric interpretation, and explore the properties of our generative model. our findings indicate a strong correlation with rational inattention behaviour theory, which suggest that individuals may ignore certain exogenous variables and rely on prior information for evaluating decisions under uncertainty. finally, the principles demonstrated in this study can be formulated as a generalized entropy and utility based multinomial logit model.", "categories": "econ.em stat.ml", "created": "2019-07-16", "updated": "2019-07-25", "authors": ["melvin wong", "bilal farooq"], "url": "https://arxiv.org/abs/1907.07036"}, {"title": "shrinkage in the time-varying parameter model framework using the r   package shrinktvp", "id": "1907.07065", "abstract": "time-varying parameter (tvp) models are widely used in time series analysis to flexibly deal with processes which gradually change over time. however, the risk of overfitting in tvp models is well known. this issue can be dealt with using appropriate global-local shrinkage priors, which pull time-varying parameters towards static ones. in this paper, we introduce the r package shrinktvp (knaus, bitto-nemling, cadonna, and fr\\\"uhwirth-schnatter 2019), which provides a fully bayesian implementation of shrinkage priors for tvp models, taking advantage of recent developments in the literature, in particular that of bitto and fr\\\"uhwirth-schnatter (2019). the package shrinktvp allows for posterior simulation of the parameters through an efficient markov chain monte carlo (mcmc) scheme. moreover, summary and visualization methods, as well as the possibility of assessing predictive performance through log predictive density scores (lpdss), are provided. the computationally intensive tasks have been implemented in c++ and interfaced with r. the paper includes a brief overview of the models and shrinkage priors implemented in the package. furthermore, core functionalities are illustrated, both with simulated and real data.", "categories": "econ.em stat.co", "created": "2019-07-16", "updated": "2019-10-07", "authors": ["angela bitto-nemling", "annalisa cadonna", "sylvia fr\u00fchwirth-schnatter", "peter knaus"], "url": "https://arxiv.org/abs/1907.07065"}, {"title": "nature of thermodynamics equation of state towards economics equation of   state", "id": "1907.07108", "abstract": "this work critics on nature of thermodynamics coordinates and on roles of the variables in the equation of state (eos). coordinate variables in the eos are analyzed so that central concepts are noticed and are used to lay a foundation in building of a new eos or in testing eos status of a newly constructed empirical equation. with these concepts, we classify eos into two classes. we find that the eos of market with unitary price demand and linear price-dependent supply function proposed by \\cite{gumjmarket}, is not an eos because it has only one degree of freedom.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-07-10", "updated": "", "authors": ["burin gumjudpai", "yuthana sethapramote"], "url": "https://arxiv.org/abs/1907.07108"}, {"title": "pathways to good healthcare services and patient satisfaction: an   evolutionary game theoretical approach", "id": "1907.07132", "abstract": "spending by the uk's national health service (nhs) on independent healthcare treatment has been increased in recent years and is predicted to sustain its upward trend with the forecast of population growth. some have viewed this increase as an attempt not to expand the patients' choices but to privatize public healthcare. this debate poses a social dilemma whether the nhs should stop cooperating with private providers. this paper contributes to healthcare economic modelling by investigating the evolution of cooperation among three proposed populations: public healthcare providers, private healthcare providers and patients. the patient population is included as a main player in the decision-making process by expanding patient's choices of treatment. we develop a generic basic model that measures the cost of healthcare provision based on given parameters, such as nhs and private healthcare providers' cost of investments in both sectors, cost of treatments and gained benefits. a patient's costly punishment is introduced as a mechanism to enhance cooperation among the three populations. our findings show that cooperation can be improved with the introduction of punishment (patient's punishment) against defecting providers. although punishment increases cooperation, it is very costly considering the small improvement in cooperation in comparison to the basic model.", "categories": "physics.soc-ph cs.gt cs.ma econ.th math.ds", "created": "2019-07-06", "updated": "", "authors": ["zainab alalawi", "the anh han", "yifeng zeng", "aiman elragig"], "url": "https://arxiv.org/abs/1907.07132"}, {"title": "consistency and matching without replacement", "id": "1907.07288", "abstract": "the paper demonstrates that the matching estimator is not generally consistent for the average treatment effect of the treated when the matching is done without replacement using propensity scores. to achieve consistency, practitioners must either assume that no unit exists with a propensity score greater than one-half or assume that there is no confounding among such units. illustrations suggest that the result applies also to matching using other metrics as long as it is done without replacement.", "categories": "econ.em math.st stat.th", "created": "2019-07-16", "updated": "", "authors": ["fredrik s\u00e4vje"], "url": "https://arxiv.org/abs/1907.07288"}, {"title": "existence and uniqueness of solutions to the stochastic bellman equation   with unbounded shock", "id": "1907.07343", "abstract": "in this paper we develop a general framework to analyze stochastic dynamic problems with unbounded utility functions and correlated and unbounded shocks. we obtain new results of the existence and uniqueness of solutions to the bellman equation through a general fixed point theorem that generalizes known results for banach contractions and local contractions. we study an endogenous growth model as well as the lucas asset pricing model in an exchange economy, significantly expanding their range of applicability.", "categories": "econ.th", "created": "2019-07-17", "updated": "", "authors": ["juan pablo rinc\u00f3n-zapatero"], "url": "https://arxiv.org/abs/1907.07343"}, {"title": "testing for quantile sample selection", "id": "1907.07412", "abstract": "this paper provides tests for detecting sample selection in nonparametric conditional quantile functions. the first test is an omitted predictor test with the propensity score as the omitted variable. as with any omnibus test, in the case of rejection we cannot distinguish between rejection due to genuine selection or to misspecification. thus, we suggest a second test to provide supporting evidence whether the cause for rejection at the first stage was solely due to selection or not. using only individuals with propensity score close to one, this second test relies on an `identification at infinity' argument, but accommodates cases of irregular identification. importantly, neither of the two tests requires parametric assumptions on the selection equation nor a continuous exclusion restriction. data-driven bandwidth procedures are proposed, and monte carlo evidence suggests a good finite sample performance in particular of the first test. finally, we also derive an extension of the first test to nonparametric conditional mean functions, and apply our procedure to test for selection in log hourly wages using uk family expenditure survey data as \\citet{ab2017}.", "categories": "econ.em", "created": "2019-07-17", "updated": "2020-09-23", "authors": ["valentina corradi", "daniel gutknecht"], "url": "https://arxiv.org/abs/1907.07412"}, {"title": "confidence collapse in a multi-household, self-reflexive dsge model", "id": "1907.07425", "abstract": "we investigate a multi-household dsge model in which past aggregate consumption impacts the confidence, and therefore consumption propensity, of individual households. we find that such a minimal setup is extremely rich, and leads to a variety of realistic output dynamics: high output with no crises; high output with increased volatility and deep, short lived recessions; alternation of high and low output states where relatively mild drop in economic conditions can lead to a temporary confidence collapse and steep decline in economic activity. the crisis probability depends exponentially on the parameters of the model, which means that markets cannot efficiently price the associated risk premium. we conclude by stressing that within our framework, {\\it narratives} become an important monetary policy tool, that can help steering the economy back on track.", "categories": "q-fin.gn econ.th physics.soc-ph", "created": "2019-07-17", "updated": "", "authors": ["federico guglielmo morelli", "michael benzaquen", "marco tarzia", "jean-philippe bouchaud"], "url": "https://arxiv.org/abs/1907.07425"}, {"title": "the cyclicality of loan loss provisions under three different accounting   models: the united kingdom, spain, and brazil", "id": "1907.07491", "abstract": "a controversy involving loan loss provisions in banks concerns their relationship with the business cycle. while international accounting standards for recognizing provisions (incurred loss model) would presumably be pro-cyclical, accentuating the effects of the current economic cycle, an alternative model, the expected loss model, has countercyclical characteristics, acting as a buffer against economic imbalances caused by expansionary or contractionary phases in the economy. in brazil, a mixed accounting model exists, whose behavior is not known to be pro-cyclical or countercyclical. the aim of this research is to analyze the behavior of these accounting models in relation to the business cycle, using an econometric model consisting of financial and macroeconomic variables. the study allowed us to identify the impact of credit risk behavior, earnings management, capital management, gross domestic product (gdp) behavior, and the behavior of the unemployment rate on provisions in countries that use different accounting models. data from commercial banks in the united kingdom (incurred loss), in spain (expected loss), and in brazil (mixed model) were used, covering the period from 2001 to 2012. despite the accounting models of the three countries being formed by very different rules regarding possible effects on the business cycles, the results revealed a pro-cyclical behavior of provisions in each country, indicating that when gdp grows, provisions tend to fall and vice versa. the results also revealed other factors influencing the behavior of loan loss provisions, such as earning management.", "categories": "econ.gn q-fin.ec", "created": "2019-07-17", "updated": "", "authors": ["a. m. b. araujo", "p. r. b. lustosa"], "url": "https://arxiv.org/abs/1907.07491"}, {"title": "testing for unobserved heterogeneity via k-means clustering", "id": "1907.07582", "abstract": "clustering methods such as k-means have found widespread use in a variety of applications. this paper proposes a formal testing procedure to determine whether a null hypothesis of a single cluster, indicating homogeneity of the data, can be rejected in favor of multiple clusters. the test is simple to implement, valid under relatively mild conditions (including non-normality, and heterogeneity of the data in aspects beyond those in the clustering analysis), and applicable in a range of contexts (including clustering when the time series dimension is small, or clustering on parameters other than the mean). we verify that the test has good size control in finite samples, and we illustrate the test in applications to clustering vehicle manufacturers and u.s. mutual funds.", "categories": "econ.em", "created": "2019-07-17", "updated": "", "authors": ["andrew j. patton", "brian m. weller"], "url": "https://arxiv.org/abs/1907.07582"}, {"title": "contract design with costly convex self-control", "id": "1907.07628", "abstract": "in this note, we consider the pricing problem of a profit-maximizing monopolist who faces naive consumers with convex self-control preferences.", "categories": "econ.th", "created": "2019-07-17", "updated": "", "authors": ["yusufcan masatlioglu", "daisuke nakajima", "emre ozdenoren"], "url": "https://arxiv.org/abs/1907.07628"}, {"title": "design and evaluation of product aesthetics: a human-machine hybrid   approach", "id": "1907.07786", "abstract": "aesthetics are critically important to market acceptance in many product categories. in the automotive industry in particular, an improved aesthetic design can boost sales by 30% or more. firms invest heavily in designing and testing new product aesthetics. a single automotive \"theme clinic\" costs between \\$100,000 and \\$1,000,000, and hundreds are conducted annually. we use machine learning to augment human judgment when designing and testing new product aesthetics. the model combines a probabilistic variational autoencoder (vae) and adversarial components from generative adversarial networks (gan), along with modeling assumptions that address managerial requirements for firm adoption. we train our model with data from an automotive partner-7,000 images evaluated by targeted consumers and 180,000 high-quality unrated images. our model predicts well the appeal of new aesthetic designs-38% improvement relative to a baseline and substantial improvement over both conventional machine learning models and pretrained deep learning models. new automotive designs are generated in a controllable manner for the design team to consider, which we also empirically verify are appealing to consumers. these results, combining human and machine inputs for practical managerial usage, suggest that machine learning offers significant opportunity to augment aesthetic design.", "categories": "cs.lg cs.cv econ.em stat.ml", "created": "2019-07-17", "updated": "", "authors": ["alex burnap", "john r. hauser", "artem timoshenko"], "url": "https://arxiv.org/abs/1907.07786"}, {"title": "behavioural macroeconomic policy: new perspectives on time inconsistency", "id": "1907.07858", "abstract": "this paper brings together divergent approaches to time inconsistency from macroeconomic policy and behavioural economics. behavioural discount functions from behavioural microeconomics are embedded into a game-theoretic analysis of temptation versus enforcement to construct an encompassing model, nesting combinations of time consistent and time inconsistent preferences. the analysis presented in this paper shows that, with hyperbolic/quasihyperbolic discounting, the enforceable range of inflation targets is narrowed. this suggests limits to the effectiveness of monetary targets, under certain conditions. the paper concludes with a discussion of monetary policy implications, explored specifically in the light of current macroeconomic policy debates.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-07-17", "updated": "", "authors": ["michelle baddeley"], "url": "https://arxiv.org/abs/1907.07858"}, {"title": "powershare mechanics", "id": "1907.07975", "abstract": "this paper proposes the governance framework of a gamified social network for charity crowdfunding fueled by public computing. it introduces optimal scarce resource allocation model, technological configuration of the fire consensus protocol, and multi-layer incentivization structure that maximizes value creation within the network.", "categories": "econ.gn cs.dc q-fin.ec", "created": "2019-07-18", "updated": "", "authors": ["beka dalakishvili", "ana mikatadze"], "url": "https://arxiv.org/abs/1907.07975"}, {"title": "a vine-copula extension for the har model", "id": "1907.08522", "abstract": "the heterogeneous autoregressive (har) model is revised by modeling the joint distribution of the four partial-volatility terms therein involved. namely, today's, yesterday's, last week's and last month's volatility components. the joint distribution relies on a (c-) vine copula construction, allowing to conveniently extract volatility forecasts based on the conditional expectation of today's volatility given its past terms. the proposed empirical application involves more than seven years of high-frequency transaction prices for ten stocks and evaluates the in-sample, out-of-sample and one-step-ahead forecast performance of our model for daily realized-kernel measures. the model proposed in this paper is shown to outperform the har counterpart under different models for marginal distributions, copula construction methods, and forecasting settings.", "categories": "econ.em stat.ap", "created": "2019-07-19", "updated": "", "authors": ["martin magris"], "url": "https://arxiv.org/abs/1907.08522"}, {"title": "modeling morality", "id": "1907.08659", "abstract": "unlike any other field, the science of morality has drawn attention from an extraordinarily diverse set of disciplines. an interdisciplinary research program has formed in which economists, biologists, neuroscientists, psychologists, and even philosophers have been eager to provide answers to puzzling questions raised by the existence of human morality. models and simulations, for a variety of reasons, have played various important roles in this endeavor. their use, however, has sometimes been deemed as useless, trivial and inadequate. the role of models in the science of morality has been vastly underappreciated. this omission shall be remedied here, offering a much more positive picture on the contributions modelers made to our understanding of morality.", "categories": "q-bio.pe econ.th", "created": "2019-07-19", "updated": "", "authors": ["walter veit"], "url": "https://arxiv.org/abs/1907.08659"}, {"title": "on the simulation of the hawkes process via lambert-w functions", "id": "1907.09162", "abstract": "several methods have been developed for the simulation of the hawkes process. the oldest approach is the inverse sampling transform (its) suggested in \\citep{ozaki1979maximum}, but rapidly abandoned in favor of more efficient alternatives. this manuscript shows that the its approach can be conveniently discussed in terms of lambert-w functions. an optimized and efficient implementation suggests that this approach is computationally more performing than more recent alternatives available for the simulation of the hawkes process.", "categories": "econ.em math.pr stat.co stat.me", "created": "2019-07-22", "updated": "", "authors": ["martin magris"], "url": "https://arxiv.org/abs/1907.09162"}, {"title": "x-model: further development and possible modifications", "id": "1907.09206", "abstract": "despite its critical importance, the famous x-model elaborated by ziel and steinert (2016) has neither bin been widely studied nor further developed. and yet, the possibilities to improve the model are as numerous as the fields it can be applied to. the present paper takes advantage of a technique proposed by coulon et al. (2014) to enhance the x-model. instead of using the wholesale supply and demand curves as inputs for the model, we rely on the transformed versions of these curves with a perfectly inelastic demand. as a result, computational requirements of our x-model reduce and its forecasting power increases substantially. moreover, our x-model becomes more robust towards outliers present in the initial auction curves data.", "categories": "econ.em", "created": "2019-07-22", "updated": "", "authors": ["sergei kulakov"], "url": "https://arxiv.org/abs/1907.09206"}, {"title": "competing to persuade a rationally inattentive agent", "id": "1907.09255", "abstract": "firms strategically disclose product information in order to attract consumers, but recipients often find it costly to process all of it, especially when products have complex features. we study a model of competitive information disclosure by two senders, in which the receiver may garble each sender's experiment, subject to a cost increasing in the informativeness of the garbling. for a large class of parameters, it is an equilibrium for the senders to provide the receiver's first best level of information - i.e. as much as she would learn if she herself controlled information provision. information on one sender substitutes for information on the other, which nullifies the profitability of a unilateral provision of less information. thus, we provide a novel channel through which competition with attention costs encourages information disclosure.", "categories": "econ.th", "created": "2019-07-22", "updated": "2020-02-03", "authors": ["vasudha jain", "mark whitmeyer"], "url": "https://arxiv.org/abs/1907.09255"}, {"title": "semi-parametric hierarchical bayes estimates of new yorkers' willingness   to pay for features of shared automated vehicle services", "id": "1907.09639", "abstract": "in this paper, we contrast parametric and semi-parametric representations of unobserved heterogeneity in hierarchical bayesian multinomial logit models and leverage these methods to infer distributions of willingness to pay for features of shared automated vehicle (sav) services. specifically, we compare the multivariate normal (mvn), finite mixture of normals (f-mon) and dirichlet process mixture of normals (dp-mon) mixing distributions. the latter promises to be particularly flexible in respect to the shapes it can assume and unlike other semi-parametric approaches does not require that its complexity is fixed prior to estimation. however, its properties relative to simpler mixing distributions are not well understood. in this paper, we evaluate the performance of the mvn, f-mon and dp-mon mixing distributions using simulated data and real data sourced from a stated choice study on preferences for sav services in new york city. our analysis shows that the dp-mon mixing distribution provides superior fit to the data and performs at least as well as the competing methods at out-of-sample prediction. the dp-mon mixing distribution also offers substantive behavioural insights into the adoption of savs. we find that preferences for in-vehicle travel time by sav with ride-splitting are strongly polarised. whereas one third of the sample is willing to pay between 10 and 80 usd/h to avoid sharing a vehicle with strangers, the remainder of the sample is either indifferent to ride-splitting or even desires it. moreover, we estimate that new technologies such as vehicle automation and electrification are relatively unimportant to travellers. this suggests that travellers may primarily derive indirect, rather than immediate benefits from these new technologies through increases in operational efficiency and lower operating costs.", "categories": "econ.gn q-fin.ec", "created": "2019-07-22", "updated": "", "authors": ["rico krueger", "taha h. rashidi", "akshay vij"], "url": "https://arxiv.org/abs/1907.09639"}, {"title": "a note on universal bilinear portfolios", "id": "1907.09704", "abstract": "this note provides a neat and enjoyable expansion and application of the magnificent ordentlich-cover theory of \"universal portfolios.\" i generalize cover's benchmark of the best constant-rebalanced portfolio (or 1-linear trading strategy) in hindsight by considering the best bilinear trading strategy determined in hindsight for the realized sequence of asset prices. a bilinear trading strategy is a mini two-period active strategy whose final capital growth factor is linear separately in each period's gross return vector for the asset market. i apply cover's ingenious (1991) performance-weighted averaging technique to construct a universal bilinear portfolio that is guaranteed (uniformly for all possible market behavior) to compound its money at the same asymptotic rate as the best bilinear trading strategy in hindsight. thus, the universal bilinear portfolio asymptotically dominates the original (1-linear) universal portfolio in the same technical sense that cover's universal portfolios asymptotically dominate all constant-rebalanced portfolios and all buy-and-hold strategies. in fact, like so many russian dolls, one can get carried away and use these ideas to construct an endless hierarchy of ever more dominant $h$-linear universal portfolios.", "categories": "q-fin.mf econ.th q-fin.cp q-fin.gn q-fin.pm", "created": "2019-07-23", "updated": "", "authors": ["alex garivaltis"], "url": "https://arxiv.org/abs/1907.09704"}, {"title": "prosumage of solar electricity: tariff design, capacity investments, and   power system effects", "id": "1907.09855", "abstract": "we analyze how tariff design incentivizes households to invest in residential photovoltaic and battery systems, and explore selected power sector effects. to this end, we apply an open-source power system model featuring prosumage agents to german 2030 scenarios. results show that lower feed-in tariffs substantially reduce investments in photovoltaics, yet optimal battery sizing and self-generation are relatively robust. with increasing fixed parts of retail tariffs, optimal battery capacities and self-generation are smaller, and households contribute more to non-energy power sector costs. when choosing tariff designs, policy makers should not aim to (dis-)incentivize prosumage as such, but balance effects on renewable capacity expansion and system cost contribution.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2019-07-23", "updated": "", "authors": ["claudia g\u00fcnther", "wolf-peter schill", "alexander zerrahn"], "url": "https://arxiv.org/abs/1907.09855"}, {"title": "rebuttal of \"on nonparametric identification of treatment effects in   duration models\"", "id": "1907.09886", "abstract": "in their iza discussion paper 10247, johansson and lee claim that the main result (proposition 3) in abbring and van den berg (2003b) does not hold. we show that their claim is incorrect. at a certain point within their line of reasoning, they make a rather basic error while transforming one random variable into another random variable, and this leads them to draw incorrect conclusions. as a result, their paper can be discarded.", "categories": "econ.em", "created": "2019-07-20", "updated": "", "authors": ["jaap h. abbring", "gerard j. van den berg"], "url": "https://arxiv.org/abs/1907.09886"}, {"title": "yield uncertainty and strategic formation of supply chain networks", "id": "1907.09943", "abstract": "how does supply uncertainty affect the structure of supply chain networks? to answer this question we consider a setting where retailers and suppliers must establish a costly relationship with each other prior to engaging in trade. suppliers, with uncertain yield, announce wholesale prices, while retailers must decide which suppliers to link to based on their wholesale prices. subsequently, retailers compete with each other in cournot fashion to sell the acquired supply to consumers. we find that in equilibrium retailers concentrate their links among too few suppliers, i.e., there is insufficient diversification of the supply base. we find that either reduction of supply variance or increase of mean supply, increases a supplier's profit. however, these two ways of improving service have qualitatively different effects on welfare: improvement of the expected supply by a supplier makes everyone better off, whereas improvement of supply variance lowers consumer surplus.", "categories": "cs.gt econ.th math.oc q-fin.tr", "created": "2019-07-20", "updated": "", "authors": ["victor amelkin", "rakesh vohra"], "url": "https://arxiv.org/abs/1907.09943"}, {"title": "sustainable business models: a review", "id": "1907.10052", "abstract": "the concept of the sustainable business model describes the rationale of how an organization creates, delivers, and captures value, in economic, social, cultural, or other contexts, in a sustainable way. the process of sustainable business model construction forms an innovative part of a business strategy. different industries and businesses have utilized sustainable business models concept to satisfy their economic, environmental, and social goals simultaneously. however, the success, popularity, and progress of sustainable business models in different application domains are not clear. to explore this issue, this research provides a comprehensive review of sustainable business models literature in various application areas. notable sustainable business models are identified and further classified in fourteen unique categories, and in every category, the progress -- either failure or success -- has been reviewed, and the research gaps are discussed. taxonomy of the applications includes innovation, management and marketing, entrepreneurship, energy, fashion, healthcare, agri-food, supply chain management, circular economy, developing countries, engineering, construction and real estate, mobility and transportation, and hospitality. the key contribution of this study is that it provides an insight into the state of the art of sustainable business models in the various application areas and future research directions. this paper concludes that popularity and the success rate of sustainable business models in all application domains have been increased along with the increasing use of advanced technologies.", "categories": "econ.gn q-fin.ec", "created": "2019-07-23", "updated": "", "authors": ["saeed nosratabadi", "amir mosavi", "shahaboddin shamshirband", "edmundas kazimieras zavadskas", "andry rakotonirainy", "kwok wing chau"], "url": "https://arxiv.org/abs/1907.10052"}, {"title": "optimal auctions for networked markets with externalities", "id": "1907.10080", "abstract": "motivated by the problem of market power in electricity markets, we introduced in previous works a mechanism for simplified markets of two agents with linear cost. in standard procurement auctions, the market power resulting from the quadratic transmission losses allows the producers to bid above their true values, which are their production cost. the mechanism proposed in the previous paper optimally reduces the producers' margin to the society's benefit. in this paper, we extend those results to a more general market made of a finite number of agents with piecewise linear cost functions, which makes the problem more difficult, but simultaneously more realistic. we show that the methodology works for a large class of externalities. we also provide an algorithm to solve the principal allocation problem. our contribution provides a benchmark to assess the sub-optimality of the mechanisms used in practice.", "categories": "econ.th cs.gt math.oc", "created": "2019-07-23", "updated": "", "authors": ["benjamin heymann", "alejandro jofr\u00e9"], "url": "https://arxiv.org/abs/1907.10080"}, {"title": "arrow's theorem through a fixpoint argument", "id": "1907.10381", "abstract": "we present a proof of arrow's theorem from social choice theory that uses a fixpoint argument. specifically, we use banach's result on the existence of a fixpoint of a contractive map defined on a complete metric space. conceptually, our approach shows that dictatorships can be seen as fixpoints of a certain process.", "categories": "econ.th cs.lo", "created": "2019-07-21", "updated": "", "authors": ["frank m. v. feys", "helle hvid hansen"], "url": "https://arxiv.org/abs/1907.10381"}, {"title": "the method of enestr\\\"om and phragm\\'en for parliamentary elections by   means of approval voting", "id": "1907.10590", "abstract": "we study a method for proportional representation that was proposed at the turn from the nineteenth to the twentieth century by gustav enestr\\\"om and edvard phragm\\'en. like phragm\\'en's better-known iterative minimax method, it is assumed that the voters express themselves by means of approval voting. in contrast to the iterative minimax method, however, here one starts by fixing a quota, i.e. the number of votes that give the right to a seat. as a matter of fact, the method of enestr\\\"om and phragm\\'en can be seen as an extension of the method of largest remainders from closed lists to open lists, or also as an adaptation of the single transferable vote to approval rather than preferential voting. the properties of this method are studied and compared with those of other methods of the same kind.", "categories": "econ.th", "created": "2019-07-23", "updated": "", "authors": ["rosa camps", "xavier mora", "laia saumell"], "url": "https://arxiv.org/abs/1907.10590"}, {"title": "liquid speed: on-demand fast trading at distributed exchanges", "id": "1907.10720", "abstract": "exchanges acquire excess processing capacity to accommodate trading activity surges associated with zero-sum high-frequency trader (hft) \"duels.\" the idle capacity's opportunity cost is an externality of low-latency trading. we build a model of decentralized exchanges (dex) with flexible capacity. on dex, hfts acquire speed in real-time from peer-to-peer networks. the price of speed surges during activity bursts, as hfts simultaneously race to market. relative to centralized exchanges, hfts acquire more speed on dex, but for shorter timespans. low-latency \"sprints\" speed up price discovery without harming liquidity. overall, speed rents decrease and fewer resources are locked-in to support zero-sum hft trades.", "categories": "q-fin.tr econ.th q-fin.gn", "created": "2019-07-24", "updated": "", "authors": ["michael brolley", "marius zoican"], "url": "https://arxiv.org/abs/1907.10720"}, {"title": "market and long term accounting operational performance", "id": "1907.11719", "abstract": "following the value relevance literature, this study verifies whether the marketplace differentiates companies of high, medium, and low long-term operational performance, measured by accounting information on profitability, sales variation and indebtedness. the data comprises the corporate financial statements disclosed during the period from 1996 to 2009 and stock prices of companies listed on the sao paulo stock exchange and commodities and futures exchange - bm&fbovespa. the final sample is composed of 142 non-financial companies. five year mobile windows were used, which resulted in ten five-year periods. after checking each company indices, the accounting variables were unified in an index performance summary to synthesize the final performance for each five-year period, which allowed segregation in operational performance levels. multiple regressions were performed using panel data techniques, fixed effects model and dummies variables, and then hypothesis tests were made. regarding the explanatory power of each individual variable, the results show that not all behaviors are according to the research hypothesis and that the brazilian stock market differentiates companies of high and low long-term operational performance. this distinction is not fully perceived between companies of high and medium operational performance.", "categories": "econ.gn q-fin.ec", "created": "2019-07-26", "updated": "", "authors": ["m. s. s. rosa", "p. r. b. lustosa"], "url": "https://arxiv.org/abs/1907.11719"}, {"title": "electronic markets with multiple submodular buyers", "id": "1907.11915", "abstract": "we discuss the problem of setting prices in an electronic market that has more than one buyer. we assume that there are self-interested sellers each selling a distinct item that has an associated cost. each buyer has a submodular valuation for purchasing any subset of items. the goal of the sellers is to set a price for their item such that their profit from possibly selling their item to the buyers is maximized. our most comprehensive results concern a multi copy setting where each seller has m copies of their item and there are m buyers. in this setting, we give a necessary and sufficient condition for the existence of market clearing pure nash equilibrium. we also show that not all equilibria are market clearing even when this condition is satisfied contrary to what was shown in the case of a single buyer in [2]. finally, we investigate the pricing problem for multiple buyers in the limited supply setting when each seller only has a single copy of their item.", "categories": "cs.gt econ.th", "created": "2019-07-27", "updated": "2019-09-23", "authors": ["allan borodin", "akash rakheja"], "url": "https://arxiv.org/abs/1907.11915"}, {"title": "testing for time-varying properties under misspecified conditional mean   and variance", "id": "1907.12107", "abstract": "this study examines statistical performance of tests for time-varying properties under misspecified conditional mean and variance. when we test for time-varying properties of the conditional mean in the case in which data have no time-varying mean but have time-varying variance, asymptotic tests have size distortions. this is improved by the use of a bootstrap method. similarly, when we test for time-varying properties of the conditional variance in the case in which data have time-varying mean but no time-varying variance, asymptotic tests have large size distortions. this is not improved even by the use of bootstrap methods. we show that tests for time-varying properties of the conditional mean by the bootstrap are robust regardless of the time-varying variance model, whereas tests for time-varying properties of the conditional variance do not perform well in the presence of misspecified time-varying mean.", "categories": "econ.em", "created": "2019-07-28", "updated": "2019-08-31", "authors": ["daiki maki", "yasushi ota"], "url": "https://arxiv.org/abs/1907.12107"}, {"title": "cities and space: common power laws and spatial fractal structures", "id": "1907.12289", "abstract": "city size distributions are known to be well approximated by power laws across a wide range of countries. but such distributions are also meaningful at other spatial scales, such as within certain regions of a country. using data from china, france, germany, india, japan, and the us, we first document that large cities are significantly more spaced out than would be expected by chance alone. we next construct spatial hierarchies for countries by first partitioning geographic space using a given number of their largest cities as cell centers, and then continuing this partitioning procedure within each cell recursively. we find that city size distributions in different parts of these spatial hierarchies exhibit power laws that are again far more similar than would be expected by chance alone -- suggesting the existence of a spatial fractal structure.", "categories": "econ.gn q-fin.ec", "created": "2019-07-29", "updated": "", "authors": ["tomoya mori", "tony e. smith", "wen-tai hsu"], "url": "https://arxiv.org/abs/1907.12289"}, {"title": "killer technologies: the destructive creation in the technical change", "id": "1907.12406", "abstract": "killer technology is a radical innovation, based on new products and/or processes, that with high technical and/or economic performance destroys the usage value of established techniques previously sold and used. killer technology is a new concept in economics of innovation that may be useful for bringing a new perspective to explain and generalize the behavior and characteristics of innovations that generate a destructive creation for sustaining technical change. to explore the behavior of killer technologies, a simple model is proposed to analyze and predict how killer technologies destroy and substitute established technologies. empirical evidence of this theoretical framework is based on historical data on the evolution of some example technologies. theoretical framework and empirical evidence hint at general properties of the behavior of killer technologies to explain corporate, industrial, economic and social change and to support best practices for technology management of firms and innovation policy of nations. overall, then, the proposed theoretical framework can lay a foundation for the development of more sophisticated concepts to explain the behavior of vital technologies that generate technological and industrial change in society.", "categories": "econ.gn q-fin.ec", "created": "2019-07-29", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1907.12406"}, {"title": "empirical strategy-proofness", "id": "1907.12408", "abstract": "we study the plausibility of sub-optimal nash equilibria of the direct revelation mechanism associated with a strategy-proof social choice function. by using the recently introduced empirical equilibrium analysis (velez and brown, 2019, arxiv:1804.07986) we determine that this behavior is plausible only when the social choice function violates a non-bossiness condition and information is not interior. analysis of the accumulated experimental and empirical evidence on these games supports our findings.", "categories": "econ.th cs.gt", "created": "2019-07-29", "updated": "2020-07-06", "authors": ["rodrigo a. velez", "alexander l. brown"], "url": "https://arxiv.org/abs/1907.12408"}, {"title": "closed form solutions of lucas uzawa model with externalities via   partial hamiltonian approach. some clarifications", "id": "1907.12623", "abstract": "the main aim of this paper is to give some clarifications to the recent paper published in computational and applied mathematics by naz and chaudhry.", "categories": "econ.th", "created": "2019-07-29", "updated": "", "authors": ["constantin chilarescu"], "url": "https://arxiv.org/abs/1907.12623"}, {"title": "a production function with variable elasticity of factor substitution", "id": "1907.12624", "abstract": "the main aim of this paper is to prove the existence of a new production function with variable elasticity of factor substitution. this production function is a more general form which includes the cobb-douglas production function and the ces production function as particular cases. the econometric estimates presented in the paper confirm some other results and reinforces the conclusion that the sigma is well-below the cobb-douglas value of one.", "categories": "econ.th", "created": "2019-07-29", "updated": "", "authors": ["constantin chilarescu"], "url": "https://arxiv.org/abs/1907.12624"}, {"title": "on the solutions of the lucas-uzawa model", "id": "1907.12658", "abstract": "in a recent paper, naz and chaudry provided two solutions for the model of lucas-uzawa, via the partial hamiltonian approach. the first one of these solutions coincides exactly with that determined by chilarescu. for the second one, they claim that this is a new solution, fundamentally different than that obtained by chilarescu. we will prove in this paper, using the existence and uniqueness theorem of nonlinear differential equations, that this is not at all true.", "categories": "econ.th", "created": "2019-07-29", "updated": "", "authors": ["constantin chilarescu"], "url": "https://arxiv.org/abs/1907.12658"}, {"title": "robust tests for arch in the presence of the misspecified conditional   mean: a comparison of nonparametric approches", "id": "1907.12752", "abstract": "this study compares statistical properties of arch tests that are robust to the presence of the misspecified conditional mean. the approaches employed in this study are based on two nonparametric regressions for the conditional mean. first is the arch test using nadayara-watson kernel regression. second is the arch test using the polynomial approximation regression. the two approaches do not require specification of the conditional mean and can adapt to various nonlinear models, which are unknown a priori. accordingly, they are robust to misspecified conditional mean models. simulation results show that arch tests based on the polynomial approximation regression approach have better statistical properties than arch tests using nadayara-watson kernel regression approach for various nonlinear models.", "categories": "econ.em", "created": "2019-07-30", "updated": "2019-09-01", "authors": ["daiki maki", "yasushi ota"], "url": "https://arxiv.org/abs/1907.12752"}, {"title": "a comparison of first-difference and forward orthogonal deviations gmm", "id": "1907.12880", "abstract": "this paper provides a necessary and sufficient instruments condition assuring two-step generalized method of moments (gmm) based on the forward orthogonal deviations transformation is numerically equivalent to two-step gmm based on the first-difference transformation. the condition also tells us when system gmm, based on differencing, can be computed using forward orthogonal deviations. additionally, it tells us when forward orthogonal deviations and differencing do not lead to the same gmm estimator. when estimators based on these two transformations differ, monte carlo simulations indicate that estimators based on forward orthogonal deviations have better finite sample properties than estimators based on differencing.", "categories": "econ.em", "created": "2019-07-30", "updated": "", "authors": ["robert f. phillips"], "url": "https://arxiv.org/abs/1907.12880"}, {"title": "open shop scheduling games", "id": "1907.12909", "abstract": "this paper takes a game theoretical approach to open shop scheduling problems with unit execution times to minimize the sum of completion times. by supposing an initial schedule and associating each job (consisting in a number of operations) to a different player, we can construct a cooperative tu-game associated with any open shop scheduling problem. we assign to each coalition the maximal cost savings it can obtain through admissible rearrangements of jobs' operations. by providing a core allocation, we show that the associated games are balanced. finally, we relax the definition of admissible rearrangements for a coalition to study to what extend balancedness still holds.", "categories": "cs.gt econ.th", "created": "2019-07-30", "updated": "", "authors": ["ata atay", "pedro calleja", "sergio soteras"], "url": "https://arxiv.org/abs/1907.12909"}, {"title": "predicting credit default probabilities using machine learning   techniques in the face of unequal class distributions", "id": "1907.12996", "abstract": "this study conducts a benchmarking study, comparing 23 different statistical and machine learning methods in a credit scoring application. in order to do so, the models' performance is evaluated over four different data sets in combination with five data sampling strategies to tackle existing class imbalances in the data. six different performance measures are used to cover different aspects of predictive performance. the results indicate a strong superiority of ensemble methods and show that simple sampling strategies deliver better results than more sophisticated ones.", "categories": "econ.em cs.lg", "created": "2019-07-30", "updated": "", "authors": ["anna stelzer"], "url": "https://arxiv.org/abs/1907.12996"}, {"title": "detecting identification failure in moment condition models", "id": "1907.13093", "abstract": "this paper develops an approach to detect identification failures in a large class of moment condition models. this is achieved by introducing a quasi-jacobian matrix which is asymptotically singular under higher-order local identification as well as weak/set identification; in these settings, standard asymptotics are not valid. under (semi)-strong identification, where standard asymptotics are valid, this matrix is asymptotically equivalent to the usual jacobian matrix. after re-scaling, it is thus asymptotically non-singular. together, these results imply that the eigenvalues of the quasi-jacobian can detect potential local and global identification failures. furthermore, the quasi-jacobian is informative about the span of the identification failure. this information permits two-step identification robust subvector inference without any a priori knowledge of the underlying identification structure. monte-carlo simulations and empirical applications illustrate the results.", "categories": "econ.em math.st stat.me stat.th", "created": "2019-07-30", "updated": "2019-08-28", "authors": ["jean-jacques forneron"], "url": "https://arxiv.org/abs/1907.13093"}, {"title": "career choice as an extended spatial evolutionary public goods game", "id": "1907.13296", "abstract": "we propose an extended spatial evolutionary public goods game (sepgg) model to study the dynamics of individual career choice and the corresponding social output. based on the social value orientation theory, we categorized two classes of work, namely the public work if it serves public interests, and the private work if it serves personal interests. under the context of sepgg, choosing public work is to cooperate and choosing private work is to defect. we then investigate the effects of employee productivity, human capital and external subsidies on individual career choices of the two work types, as well as the overall social welfare. from simulation results, we found that when employee productivity of public work is low, people are more willing to enter the private sector. although this will make both the effort level and human capital of individuals doing private work higher than those engaging in public work, the total outcome of the private sector is still lower than that of the public sector provided a low level of public subsidies. when the employee productivity is higher for public work, a certain amount of subsidy can greatly improve system output. on the contrary, when the employee productivity of public work is low, provisions of subsidy to the public sector can result in a decline in social output.", "categories": "physics.soc-ph cs.gt econ.gn q-fin.ec", "created": "2019-07-30", "updated": "", "authors": ["yuan cheng", "yanbo xue", "meng chang"], "url": "https://arxiv.org/abs/1907.13296"}, {"title": "kernel density estimation for undirected dyadic data", "id": "1907.13630", "abstract": "we study nonparametric estimation of density functions for undirected dyadic random variables (i.e., random variables defined for all n\\overset{def}{\\equiv}\\tbinom{n}{2} unordered pairs of agents/nodes in a weighted network of order n). these random variables satisfy a local dependence property: any random variables in the network that share one or two indices may be dependent, while those sharing no indices in common are independent. in this setting, we show that density functions may be estimated by an application of the kernel estimation method of rosenblatt (1956) and parzen (1962). we suggest an estimate of their asymptotic variances inspired by a combination of (i) newey's (1994) method of variance estimation for kernel estimators in the \"monadic\" setting and (ii) a variance estimator for the (estimated) density of a simple network first suggested by holland and leinhardt (1976). more unusual are the rates of convergence and asymptotic (normal) distributions of our dyadic density estimates. specifically, we show that they converge at the same rate as the (unconditional) dyadic sample mean: the square root of the number, n, of nodes. this differs from the results for nonparametric estimation of densities and regression functions for monadic data, which generally have a slower rate of convergence than their corresponding sample mean.", "categories": "math.st econ.em stat.th", "created": "2019-07-31", "updated": "", "authors": ["bryan s. graham", "fengshi niu", "james l. powell"], "url": "https://arxiv.org/abs/1907.13630"}, {"title": "dynamic information design with diminishing sensitivity over news", "id": "1908.00084", "abstract": "a bayesian agent experiences gain-loss utility each period over changes in belief about future consumption (\"news utility\"), with diminishing sensitivity over the magnitude of news. we show the agent's preference between an information structure that delivers news gradually and another that resolves all uncertainty at once depends on his consumption ranking of different states. one-shot resolution is better than gradual bad news, but it is not optimal among all information structures (under common functional forms). in a dynamic cheap-talk framework where a benevolent sender communicates the state over multiple periods, the babbling equilibrium is essentially unique without loss aversion. more loss-averse agents may enjoy higher news utility in equilibrium, contrary to the commitment case. we characterize the family of gradual good news equilibria that exist with high enough loss aversion, and find the sender conveys progressively larger pieces of good news. we discuss applications to media competition and game shows.", "categories": "econ.th", "created": "2019-07-31", "updated": "2020-09-06", "authors": ["jetlir duraj", "kevin he"], "url": "https://arxiv.org/abs/1908.00084"}, {"title": "testing for externalities in network formation using simulation", "id": "1908.00099", "abstract": "we discuss a simplified version of the testing problem considered by pelican and graham (2019): testing for interdependencies in preferences over links among n (possibly heterogeneous) agents in a network. we describe an exact test which conditions on a sufficient statistic for the nuisance parameter characterizing any agent-level heterogeneity. employing an algorithm due to blitzstein and diaconis (2011), we show how to simulate the null distribution of the test statistic in order to estimate critical values and/or p-values. we illustrate our methods using the nyakatoke risk-sharing network. we find that the transitivity of the nyakatoke network far exceeds what can be explained by degree heterogeneity across households alone.", "categories": "stat.me econ.em", "created": "2019-07-31", "updated": "", "authors": ["bryan s. graham", "andrin pelican"], "url": "https://arxiv.org/abs/1908.00099"}, {"title": "hiring in the substance use disorder treatment related sector during the   first five years of medicaid expansion", "id": "1908.00216", "abstract": "effective treatment strategies exist for substance use disorder (sud), however severe hurdles remain in ensuring adequacy of the sud treatment (sudt) workforce as well as improving sudt affordability, access and stigma. although evidence shows recent increases in sud medication access from expanding medicaid availability under the affordable care act, it is yet unknown whether these policies also led to a growth in the changes in the nature of hiring in sudt related workforce, partly due to poor data availability. our study uses novel data to shed light on recent trends in a fast-evolving and policy-relevant labor market, and contributes to understanding the current sudt related workforce and the effect of medicaid expansion on hiring attempts in this sector. we examine attempts over 2010-2018 at hiring in the sudt and related behavioral health sector as background for estimating the causal effect of the 2014-and-beyond state medicaid expansion on these outcomes through \"difference-in-difference\" econometric models. we use burning glass technologies (bgt) data covering virtually all u.s. job postings by employers. nationally, we find little growth in the sector's hiring attempts in 2010-2018 relative to the rest of the economy or to health care as a whole. however, this masks diverging trends in subsectors, which saw reduction in hospital based hiring attempts, increases towards outpatient facilities, and changes in occupational hiring demand shifting from medical personnel towards counselors and social workers. although medicaid expansion did not lead to any statistically significant or meaningful change in overall hiring attempts, there was a shift in the hiring landscape.", "categories": "econ.gn q-fin.ec", "created": "2019-08-01", "updated": "", "authors": ["olga scrivner", "thuy nguyen", "kosali simon", "esm\u00e9 middaugh", "bledi taska", "katy b\u00f6rner"], "url": "https://arxiv.org/abs/1908.00216"}, {"title": "the interest rate for saving as a possibilistic risk", "id": "1908.00445", "abstract": "in the paper there is studied an optimal saving model in which the interest-rate risk for saving is a fuzzy number. the total utility of consumption is defined by using a concept of possibilistic expected utility. a notion of possibilistic precautionary saving is introduced as a measure of the variation of optimal saving level when moving from a sure saving model to a possibilistic risk model. a first result establishes a necessary and sufficient condition that the presence of a possibilistic interest-rate risk generates an extra-saving. this result can be seen as a possibilistic version of a rothschilld and stiglitz theorem on a probabilistic model of saving. a second result of the paper studies the variation of the optimal saving level when moving from a probabilistic model (the interest-rate risk is a random variable) to a possibilistic model (the interest-rate risk is a fuzzy number).", "categories": "econ.th", "created": "2019-07-22", "updated": "", "authors": ["irina georgescu", "jani kinnunen"], "url": "https://arxiv.org/abs/1908.00445"}, {"title": "heterogeneous endogenous effects in networks", "id": "1908.00663", "abstract": "this paper proposes a new method to identify leaders and followers in a network. prior works use spatial autoregression models (sars) which implicitly assume that each individual in the network has the same peer effects on others. mechanically, they conclude the key player in the network to be the one with the highest centrality. however, when some individuals are more influential than others, centrality may fail to be a good measure. i develop a model that allows for individual-specific endogenous effects and propose a two-stage lasso procedure to identify influential individuals in a network. under an assumption of sparsity: only a subset of individuals (which can increase with sample size n) is influential, i show that my 2slss estimator for individual-specific endogenous effects is consistent and achieves asymptotic normality. i also develop robust inference including uniformly valid confidence intervals. these results also carry through to scenarios where the influential individuals are not sparse. i extend the analysis to allow for multiple types of connections (multiple networks), and i show how to use the sparse group lasso to detect which of the multiple connection types is more influential. simulation evidence shows that my estimator has good finite sample performance. i further apply my method to the data in banerjee et al. (2013) and my proposed procedure is able to identify leaders and effective networks.", "categories": "econ.em stat.ap stat.me", "created": "2019-08-01", "updated": "", "authors": ["sida peng"], "url": "https://arxiv.org/abs/1908.00663"}, {"title": "derivation of non-classical stochastic price dynamics equations", "id": "1908.01103", "abstract": "we analyze the relative price change of assets starting from basic supply/demand considerations subject to arbitrary motivations. the resulting stochastic differential equation has coefficients that are functions of supply and demand. we derive these rigorously. the variance in the relative price change is then also dependent on the supply and demand, and is closely connected to the expected return. an important consequence for risk assessment and options pricing is the implication that variance is highest when the magnitude of price change is greatest, and lowest near market extrema. this occurs even if supply and demand are not dependent on price trend. the stochastic equation differs from the standard equation in mathematical finance in which the expected return and variance are decoupled. the methodology has implications for the basic framework for risk assessment, suggesting that volatility should be measured in the context of regimes of price change. the model we propose shows how investors are often misled by the apparent calm of markets near a market peak. risk assessment methods utilizing volatility can be improved using this formulation.", "categories": "econ.th math.pr", "created": "2019-08-02", "updated": "2020-08-24", "authors": ["carey caginalp", "gunduz caginalp"], "url": "https://arxiv.org/abs/1908.01103"}, {"title": "the use of binary choice forests to model and estimate discrete choices", "id": "1908.01109", "abstract": "we show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. this suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. this is confirmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. the framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets.", "categories": "cs.lg econ.em stat.ml", "created": "2019-08-02", "updated": "2019-11-14", "authors": ["ningyuan chen", "guillermo gallego", "zhuodong tang"], "url": "https://arxiv.org/abs/1908.01109"}, {"title": "multiplayer bandit learning, from competition to cooperation", "id": "1908.01135", "abstract": "the stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. we study the effects of competition and cooperation on this tradeoff. suppose there are $k$ arms and two players, alice and bob. in every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. alice's utility is $\\gamma_a + \\lambda \\gamma_b$ (and similarly for bob), where $\\gamma_a$ is alice's total reward and $\\lambda \\in [-1, 1]$ is a cooperation parameter. at $\\lambda = -1$ the players are competing in a zero-sum game, at $\\lambda = 1$, they are fully cooperating, and at $\\lambda = 0$, they are neutral: each player's utility is their own reward. the model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.   with discount factor $\\beta$, the gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $\\mu$, and a predictable arm, with success probability $p$. the value of $p$ where the player is indifferent between the arms is the gittins index $g = g(\\mu,\\beta) > m$, where $m$ is the mean of the risky arm.   we show that competing players explore less than a single player: there is $p^* \\in (m, g)$ so that for all $p > p^*$, the players stay at the predictable arm. however, the players are not myopic: they still explore for some $p > m$. on the other hand, cooperating players explore more than a single player. we also show that neutral players learn from each other, receiving strictly higher total rewards than they would playing alone, for all $ p\\in (p^*, g)$, where $p^*$ is the threshold from the competing case.   finally, we show that competing and neutral players eventually settle on the same arm in every nash equilibrium, while this can fail for cooperating players.", "categories": "cs.gt cs.lg econ.th", "created": "2019-08-03", "updated": "2019-10-11", "authors": ["simina br\u00e2nzei", "yuval peres"], "url": "https://arxiv.org/abs/1908.01135"}, {"title": "linkages and systemic risk in the european insurance sector: some new   evidence based on dynamic spanning trees", "id": "1908.01142", "abstract": "this paper is part of the research on the interlinkages between insurers and their contribution to systemic risk on the insurance market. its main purpose is to present the results of the analysis of linkage dynamics and systemic risk in the european insurance sector which are obtained using correlation networks. these networks are based on dynamic dependence structures modelled using a copula. then, we determine minimum spanning trees (mst). finally, the linkage dynamics is described by means of selected topological network measures.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2019-08-03", "updated": "2019-08-21", "authors": ["anna denkowska", "stanis\u0142aw wanat"], "url": "https://arxiv.org/abs/1908.01142"}, {"title": "creation of knowledge through exchanges of knowledge: evidence from   japanese patent data", "id": "1908.01256", "abstract": "this study shows evidence for collaborative knowledge creation among individual researchers through direct exchanges of their mutual differentiated knowledge. using patent application data from japan, the collaborative output is evaluated according to the quality and novelty of the developed patents, which are measured in terms of forward citations and the order of application within their primary technological category, respectively. knowledge exchange is shown to raise collaborative productivity more through the extensive margin (i.e., the number of patents developed) in the quality dimension, whereas it does so more through the intensive margin in the novelty dimension (i.e., novelty of each patent).", "categories": "econ.gn q-fin.ec", "created": "2019-08-03", "updated": "2020-08-28", "authors": ["tomoya mori", "shosei sakaguchi"], "url": "https://arxiv.org/abs/1908.01256"}, {"title": "estimating unobserved individual heterogeneity using pairwise   comparisons", "id": "1908.01272", "abstract": "we propose a new method for studying environments with unobserved individual heterogeneity. based on model-implied pairwise inequalities, the method classifies individuals in the sample into groups defined by discrete unobserved heterogeneity with unknown support. we establish conditions under which the groups are identified and consistently estimated through our method. we show that the method performs well in finite samples through monte carlo simulation. we then apply the method to estimate a model of lowest-price procurement auctions with unobserved bidder heterogeneity, using data from the california highway procurement market.", "categories": "econ.em stat.ap stat.me", "created": "2019-08-04", "updated": "2020-04-03", "authors": ["elena krasnokutskaya", "kyungchul song", "xun tang"], "url": "https://arxiv.org/abs/1908.01272"}, {"title": "uncertainty in the hot hand fallacy: detecting streaky alternatives to   random bernoulli sequences", "id": "1908.01406", "abstract": "we study a class of permutation tests of the randomness of a collection of bernoulli sequences and their application to analyses of the human tendency to perceive streaks of consecutive successes as overly representative of positive dependence - the hot hand fallacy. in particular, we study permutation tests of the null hypothesis of randomness (i.e., that trials are i.i.d.) based on test statistics that compare the proportion of successes that directly follow k consecutive successes with either the overall proportion of successes or the proportion of successes that directly follow k consecutive failures. we characterize the asymptotic distributions of these test statistics and their permutation distributions under randomness, under a set of general stationary processes, and under a class of markov chain alternatives, which allow us to derive their local asymptotic power. the results are applied to evaluate the empirical support for the hot hand fallacy provided by four controlled basketball shooting experiments. we establish that substantially larger data sets are required to derive an informative measurement of the deviation from randomness in basketball shooting. in one experiment for which we were able to obtain data, multiple testing procedures reveal that one shooter exhibits a shooting pattern significantly inconsistent with randomness - supplying strong evidence that basketball shooting is not random for all shooters all of the time. however, we find that the evidence against randomness in this experiment is limited to this shooter. our results provide a mathematical and statistical foundation for the design and validation of experiments that directly compare deviations from randomness with human beliefs about deviations from randomness, and thereby constitute a direst test of the hot hand fallacy.", "categories": "econ.em stat.ap", "created": "2019-08-04", "updated": "2020-09-08", "authors": ["david m. ritzwoller", "joseph p. romano"], "url": "https://arxiv.org/abs/1908.01406"}, {"title": "efficient fair division with minimal sharing", "id": "1908.01669", "abstract": "a collection of objects, some of which are good and some are bad, is to be divided fairly among agents with different tastes, modeled by additive utility-functions. if the objects cannot be shared, so that each of them must be entirely allocated to a single agent, then a fair division may not exist. what is the smallest number of objects that must be shared between two or more agents in order to attain a fair and efficient division? we focus on pareto-optimal, envy-free and/or proportional allocations. we show that, for a generic instance of the problem -- all instances except of a zero-measure set of degenerate problems -- a fair pareto-optimal division with the smallest possible number of shared objects can be found in polynomial time, assuming that the number of agents is fixed. the problem becomes computationally hard for degenerate instances, where agents' valuations are aligned for many objects.", "categories": "cs.gt econ.th", "created": "2019-08-05", "updated": "2020-09-13", "authors": ["fedor sandomirskiy", "erel segal-halevi"], "url": "https://arxiv.org/abs/1908.01669"}, {"title": "the time importance for prospect theory", "id": "1908.01709", "abstract": "a theory usually comprises assumptions and deduced predictions from them. in this paper, empirical evidences corroborate with assumptions about time for a decision making facing known probabilities and outcomes.", "categories": "econ.gn q-fin.ec", "created": "2019-08-05", "updated": "", "authors": ["jos\u00e9 cl\u00e1udio do nascimento"], "url": "https://arxiv.org/abs/1908.01709"}, {"title": "discovery of bias and strategic behavior in crowdsourced performance   assessment", "id": "1908.01718", "abstract": "with the industry trend of shifting from a traditional hierarchical approach to flatter management structure, crowdsourced performance assessment gained mainstream popularity. one fundamental challenge of crowdsourced performance assessment is the risks that personal interest can introduce distortions of facts, especially when the system is used to determine merit pay or promotion. in this paper, we developed a method to identify bias and strategic behavior in crowdsourced performance assessment, using a rich dataset collected from a professional service firm in china. we find a pattern of \"discriminatory generosity\" on the part of peer evaluation, where raters downgrade their peer coworkers who have passed objective promotion requirements while overrating their peer coworkers who have not yet passed. this introduces two types of biases: the first aimed against more competent competitors, and the other favoring less eligible peers which can serve as a mask of the first bias. this paper also aims to bring angles of fairness-aware data mining to talent and management computing. historical decision records, such as performance ratings, often contain subjective judgment which is prone to bias and strategic behavior. for practitioners of predictive talent analytics, it is important to investigate potential bias and strategic behavior underlying historical decision records.", "categories": "cs.lg econ.em stat.ml", "created": "2019-08-05", "updated": "2019-10-12", "authors": ["yifei huang", "matt shum", "xi wu", "jason zezhong xiao"], "url": "https://arxiv.org/abs/1908.01718"}, {"title": "evaluating pest management strategies: a robust method and its   application to strawberry disease management", "id": "1908.01808", "abstract": "farmers use pesticides to reduce yield losses. the efficacies of pesticide treatments are often evaluated by analyzing the average treatment effects and risks. the stochastic efficiency with respect to a function is often employed in such evaluations through ranking the certainty equivalents of each treatment. the main challenge of using this method is gathering an adequate number of observations to produce results with statistical power. however, in many cases, only a limited number of trials are replicated in field experiments, leaving an inadequate number of observations. in addition, this method focuses only on the farmer's profit without incorporating the impact of disease pressure on yield and profit. the objective of our study is to propose a methodology to address the issue of an insufficient number of observations using simulations and take into account the effect of disease pressure on yield through a quantile regression model. we apply this method to the case of strawberry disease management in florida.", "categories": "econ.gn q-fin.ec", "created": "2019-08-05", "updated": "", "authors": ["ariel soto-caro", "feng wu", "zhengfei guan"], "url": "https://arxiv.org/abs/1908.01808"}, {"title": "analysing global fixed income markets with tensors", "id": "1908.02101", "abstract": "global fixed income returns span across multiple maturities and economies, that is, they naturally reside on multi-dimensional data structures referred to as tensors. in contrast to standard \"flat-view\" multivariate models that are agnostic to data structure and only describe linear pairwise relationships, we introduce a tensor-valued approach to model the global risks shared by multiple interest rate curves. in this way, the estimated risk factors can be analytically decomposed into maturity-domain and country-domain constituents, which allows the investor to devise rigorous and tractable global portfolio management and hedging strategies tailored to each risk domain. an empirical analysis confirms the existence of global risk factors shared by eight developed economies, and demonstrates their ability to compactly describe the global macroeconomic environment.", "categories": "q-fin.pm econ.em eess.sp q-fin.st stat.ap", "created": "2019-08-06", "updated": "2019-12-04", "authors": ["bruno scalzo dees"], "url": "https://arxiv.org/abs/1908.02101"}, {"title": "semiparametric wavelet-based jpeg iv estimator for endogenously   truncated data", "id": "1908.02166", "abstract": "a new and an enriched jpeg algorithm is provided for identifying redundancies in a sequence of irregular noisy data points which also accommodates a reference-free criterion function. our main contribution is by formulating analytically (instead of approximating) the inverse of the transpose of jpegwavelet transform without involving matrices which are computationally cumbersome. the algorithm is suitable for the widely-spread situations where the original data distribution is unobservable such as in cases where there is deficient representation of the entire population in the training data (in machine learning) and thus the covariate shift assumption is violated. the proposed estimator corrects for both biases, the one generated by endogenous truncation and the one generated by endogenous covariates. results from utilizing 2,000,000 different distribution functions verify the applicability and high accuracy of our procedure to cases in which the disturbances are neither jointly nor marginally normally distributed.", "categories": "stat.me cs.cv cs.lg econ.em stat.ml", "created": "2019-08-06", "updated": "", "authors": ["nir billfeld", "moshe kim"], "url": "https://arxiv.org/abs/1908.02166"}, {"title": "estimation of conditional average treatment effects with   high-dimensional data", "id": "1908.02399", "abstract": "given the unconfoundedness assumption, we propose new nonparametric estimators for the reduced dimensional conditional average treatment effect (cate) function. in the first stage, the nuisance functions necessary for identifying cate are estimated by machine learning methods, allowing the number of covariates to be comparable to or larger than the sample size. the second stage consists of a low-dimensional local linear regression, reducing cate to a function of the covariate(s) of interest. we consider two variants of the estimator depending on whether the nuisance functions are estimated over the full sample or over a hold-out sample. building on belloni at al. (2017) and chernozhukov et al. (2018), we derive functional limit theory for the estimators and provide an easy-to-implement procedure for uniform inference based on the multiplier bootstrap. the empirical application revisits the effect of maternal smoking on a baby's birth weight as a function of the mother's age.", "categories": "econ.em", "created": "2019-08-06", "updated": "2020-08-04", "authors": ["qingliang fan", "yu-chin hsu", "robert p. lieli", "yichong zhang"], "url": "https://arxiv.org/abs/1908.02399"}, {"title": "review of the plan for integrating big data analytics program for the   electronic marketing system and customer relationship management: a case   study xyz institution", "id": "1908.02430", "abstract": "this research aims to explore business processes and what the factors have major influence on electronic marketing and crm systems? which data needs to be analyzed and integrated in the system, and how to do that? how effective of integration the electronic marketing and crm with big data enabled to support marketing and customer relation operations. research based on case studies at xyz organization: international language education service in surabaya. research is studying secondary data which is supported by qualitative research methods. using purposive sampling technique with observation and interviewing several respondents who need the system integration. the documentation of interview is coded to keep confidentiality of the informant. method of extending participation, triangulation of data sources, discussions and the adequacy of the theory are uses to validate data. miles and huberman models is uses to do analysis the data interview. results of the research are expected to become a holistic approach to fully integrate the big data analytics program with electronic marketing and crm systems.", "categories": "econ.gn q-fin.ec", "created": "2019-08-06", "updated": "", "authors": ["idha sudianto"], "url": "https://arxiv.org/abs/1908.02430"}, {"title": "efficient estimation by fully modified gls with an application to the   environmental kuznets curve", "id": "1908.02552", "abstract": "this paper develops the asymptotic theory of a fully modified generalized least squares estimator for multivariate cointegrating polynomial regressions. such regressions allow for deterministic trends, stochastic trends and integer powers of stochastic trends to enter the cointegrating relations. our fully modified estimator incorporates: (1) the direct estimation of the inverse autocovariance matrix of the multidimensional errors, and (2) second order bias corrections. the resulting estimator has the intuitive interpretation of applying a weighted least squares objective function to filtered data series. moreover, the required second order bias corrections are convenient byproducts of our approach and lead to standard asymptotic inference. we also study several multivariate kpss-type of tests for the null of cointegration. a comprehensive simulation study shows good performance of the fm-gls estimator and the related tests. as a practical illustration, we reinvestigate the environmental kuznets curve (ekc) hypothesis for six early industrialized countries as in wagner et al. (2020).", "categories": "econ.em stat.me", "created": "2019-08-07", "updated": "2020-08-07", "authors": ["yicong lin", "hanno reuvers"], "url": "https://arxiv.org/abs/1908.02552"}, {"title": "noncooperative dynamics in election interference", "id": "1908.02793", "abstract": "foreign power interference in domestic elections is an existential threat to societies. manifested through myriad methods from war to words, such interference is a timely example of strategic interaction between economic and political agents. we model this interaction between rational game players as a continuous-time differential game, constructing an analytical model of this competition with a variety of payoff structures. all-or-nothing attitudes by only one player regarding the outcome of the game lead to an arms race in which both countries spend increasing amounts on interference and counter-interference operations. we then confront our model with data pertaining to the russian interference in the 2016 united states presidential election contest. we introduce and estimate a bayesian structural time series model of election polls and social media posts by russian twitter troll accounts. our analytical model, while purposefully abstract and simple, adequately captures many temporal characteristics of the election and social media activity. we close with a discussion of our model's shortcomings and suggestions for future research.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-08-07", "updated": "2020-01-09", "authors": ["david rushing dewhurst", "christopher m. danforth", "peter sheridan dodds"], "url": "https://arxiv.org/abs/1908.02793"}, {"title": "bootstrapping a stable computation token", "id": "1908.02946", "abstract": "we outline a token model for truebit, a retrofitting, blockchain enhancement which enables secure, community-based computation. the model addresses the challenge of stable task pricing, as raised in the truebit whitepaper, without appealing to external oracles, exchanges, or hierarchical nodes. the system's sustainable economics and fair market pricing derive from a mintable token format which leverages existing tokens for liquidity. finally, we introduce a governance layer whose lifecycles culminates with permanent dissolution into utility tokens, thereby tending the network towards autonomous decentralization.", "categories": "cs.cr cs.gt econ.th", "created": "2019-08-08", "updated": "", "authors": ["jason teutsch", "sami m\u00e4kel\u00e4", "surya bakshi"], "url": "https://arxiv.org/abs/1908.02946"}, {"title": "obvious manipulations in cake-cutting", "id": "1908.02988", "abstract": "in cake-cutting, strategy-proofness is a very costly requirement in terms of fairness: for n=2 it implies a dictatorial allocation, whereas for n > 2 it requires that one agent receives no cake. we show that a weaker version of this property recently suggested by troyan and morril, called non-obvious manipulability, is compatible with the strong fairness property of proportionality, which guarantees that each agent receives 1/n of the cake. both properties are satisfied by the leftmost leaves mechanism, an adaptation of the dubins - spanier moving knife procedure. most other classical proportional mechanisms in literature are obviously manipulable, including the original moving knife mechanism. non-obvious manipulability explains why leftmost leaves is manipulated less often in practice than other proportional mechanisms.", "categories": "cs.gt econ.th", "created": "2019-08-08", "updated": "2019-10-14", "authors": ["josue ortega", "erel segal-halevi"], "url": "https://arxiv.org/abs/1908.02988"}, {"title": "analysis of networks via the sparse $\\beta$-model", "id": "1908.03152", "abstract": "data in the form of networks are increasingly available in a variety of areas, yet statistical models allowing for parameter estimates with desirable statistical properties for sparse networks remain scarce. to address this, we propose the sparse $\\beta$-model (s$\\beta$m), a new network model that interpolates the celebrated erd\\h{o}s-r\\'enyi model and the $\\beta$-model that assigns one different parameter to each node. by a novel reparameterization of the $\\beta$-model to distinguish global and local parameters, our s$\\beta$m can drastically reduce the dimensionality of the $\\beta$-model by requiring some of the local parameters to be zero. we derive the asymptotic distribution of the maximum likelihood estimator of the s$\\beta$m when the support of the parameter vector is known. when the support is unknown, we formulate a penalized likelihood approach with the $\\ell_0$-penalty. remarkably, we show via a monotonicity lemma that the seemingly combinatorial computational problem due to the $\\ell_0$-penalty can be overcome by assigning nonzero parameters to those nodes with the largest degrees. we further show that a $\\beta$-min condition guarantees our method to identify the true model and provide excess risk bounds for the estimated parameters. the estimation procedure enjoys good finite sample properties as shown by simulation studies. the usefulness of the s$\\beta$m is further illustrated via the analysis of a microfinance take up example.", "categories": "math.st econ.em stat.me stat.th", "created": "2019-08-08", "updated": "2019-08-13", "authors": ["mingli chen", "kengo kato", "chenlei leng"], "url": "https://arxiv.org/abs/1908.03152"}, {"title": "ordinal tax to sustain a digital economy", "id": "1908.03287", "abstract": "recently, the french senate approved a law that imposes a 3% tax on revenue generated from digital services by companies above a certain size. while there is a lot of political debate about economic consequences of this action, it is actually interesting to reverse the question: we consider the long-term implications of an economy with no such digital tax. more generally, we can think of digital services as a special case of products with low or zero cost of transportation. with basic economic models we show that a market with no transportation costs is prone to monopolization as minuscule, random differences in quality are rewarded disproportionally. we then propose a distance-based tax to counter-balance the tendencies of random centralisation. unlike a tax that scales with physical (cardinal) distance, a ranked (ordinal) distance tax leverages the benefits of digitalization while maintaining a stable economy.", "categories": "econ.gn q-fin.ec", "created": "2019-08-06", "updated": "", "authors": ["nate dwyer", "sandro claudio lera", "alex sandy pentland"], "url": "https://arxiv.org/abs/1908.03287"}, {"title": "privacy-aware distributed mobility choice modelling over blockchain", "id": "1908.03446", "abstract": "a generalized distributed tool for mobility choice modelling is presented, where participants do not share personal raw data, while all computations are done locally. participants use blockchain based smart mobility data-market (bsmd), where all transactions are secure and private. nodes in blockchain can transact information with other participants as long as both parties agree to the transaction rules issued by the owner of the data. a case study is presented where a mode choice model is distributed and estimated over bsmd. as an example, the parameter estimation problem is solved on a distributed version of simulated annealing. it is demonstrated that the estimated model parameters are consistent and reproducible.", "categories": "cs.cr econ.em", "created": "2019-08-09", "updated": "2019-08-12", "authors": ["david lopez", "bilal farooq"], "url": "https://arxiv.org/abs/1908.03446"}, {"title": "temporally discounted differential privacy for evolving datasets on an   infinite horizon", "id": "1908.03995", "abstract": "we define discounted differential privacy, as an alternative to (conventional) differential privacy, to investigate privacy of evolving datasets, containing time series over an unbounded horizon. we use privacy loss as a measure of the amount of information leaked by the reports at a certain fixed time. we observe that privacy losses are weighted equally across time in the definition of differential privacy, and therefore the magnitude of privacy-preserving additive noise must grow without bound to ensure differential privacy over an infinite horizon. motivated by the discounted utility theory within the economics literature, we use exponential and hyperbolic discounting of privacy losses across time to relax the definition of differential privacy under continual observations. this implies that privacy losses in distant past are less important than the current ones to an individual. we use discounted differential privacy to investigate privacy of evolving datasets using additive laplace noise and show that the magnitude of the additive noise can remain bounded under discounted differential privacy. we illustrate the quality of privacy-preserving mechanisms satisfying discounted differential privacy on smart-meter measurement time-series of real households, made publicly available by ausgrid (an australian electricity distribution company).", "categories": "cs.cr cs.sy econ.th eess.sp eess.sy", "created": "2019-08-12", "updated": "2020-01-27", "authors": ["farhad farokhi"], "url": "https://arxiv.org/abs/1908.03995"}, {"title": "retrofitting a two-way peg between blockchains", "id": "1908.03999", "abstract": "in december 2015, a bounty emerged to establish both reliable communication and secure transfer of value between the dogecoin and ethereum blockchains. this prized \"dogethereum bridge\" would allow parties to \"lock\" a doge coin on dogecoin and in exchange receive a newly minted wow token in ethereum. any subsequent owner of the wow token could burn it and, in exchange, earn the right to \"unlock\" a doge on dogecoin.   we describe an efficient, trustless, and retrofitting dogethereum construction which requires no fork but rather employs economic collateral to achieve a \"lock\" operation in dogecoin. the protocol relies on bulletproofs, truebit, and parametrized tokens to efficiently and trustlessly relay events from the \"true\" dogecoin blockchain into ethereum. the present construction not only enables cross-platform exchange but also allows ethereum smart contracts to trustlessly access dogecoin. a similar technique adds ethereum-based smart contracts to bitcoin and bitcoin data to ethereum smart contracts.", "categories": "cs.cr cs.lo econ.th", "created": "2019-08-12", "updated": "", "authors": ["jason teutsch", "michael straka", "dan boneh"], "url": "https://arxiv.org/abs/1908.03999"}, {"title": "maximum approximated likelihood estimation", "id": "1908.04110", "abstract": "empirical economic research frequently applies maximum likelihood estimation in cases where the likelihood function is analytically intractable. most of the theoretical literature focuses on maximum simulated likelihood (msl) estimators, while empirical and simulation analyzes often find that alternative approximation methods such as quasi-monte carlo simulation, gaussian quadrature, and integration on sparse grids behave considerably better numerically. this paper generalizes the theoretical results widely known for msl estimators to a general set of maximum approximated likelihood (mal) estimators. we provide general conditions for both the model and the approximation approach to ensure consistency and asymptotic normality. we also show specific examples and finite-sample simulation results.", "categories": "econ.em math.st stat.th", "created": "2019-08-12", "updated": "", "authors": ["michael griebel", "florian heiss", "jens oettershagen", "constantin weiser"], "url": "https://arxiv.org/abs/1908.04110"}, {"title": "interactive coin offerings", "id": "1908.04295", "abstract": "ethereum has emerged as a dynamic platform for exchanging cryptocurrency tokens. while token crowdsales cannot simultaneously guarantee buyers both certainty of valuation and certainty of participation, we show that if each token buyer specifies a desired purchase quantity at each valuation then everyone can successfully participate. our implementation introduces smart contract techniques which recruit outside participants in order to circumvent computational complexity barriers.", "categories": "econ.th cs.cr cs.gt", "created": "2019-08-12", "updated": "", "authors": ["jason teutsch", "vitalik buterin", "christopher brown"], "url": "https://arxiv.org/abs/1908.04295"}, {"title": "fairness and efficiency for probabilistic allocations with participation   constraints", "id": "1908.04336", "abstract": "we propose a notion of fairness for allocation problems in which different agents may have different reservation utilities, stemming from different outside options, or property rights. fairness is usually understood as the absence of envy, but this can be incompatible with reservation utilities. it is possible that alice's envy of bob's assignment cannot be remedied without violating bob's participation constraint. instead, we seek to rule out {\\em justified envy}, defined as envy for which a remedy would not violate any agent's participation constraint. we show that fairness, meaning the absence of justified envy, can be achieved together with efficiency and individual rationality. we introduce a competitive equilibrium approach with price-dependent incomes obtaining the desired properties.", "categories": "econ.th cs.gt", "created": "2019-08-12", "updated": "2020-05-09", "authors": ["federico echenique", "antonio miralles", "jun zhang"], "url": "https://arxiv.org/abs/1908.04336"}, {"title": "wasserstein index generation model: automatic generation of time-series   index with application to economic policy uncertainty", "id": "1908.04369", "abstract": "i propose a novel method, the wasserstein index generation model (wig), to generate a public sentiment index automatically. to test the model`s effectiveness, an application to generate economic policy uncertainty (epu) index is showcased.", "categories": "econ.gn cs.cl q-fin.ec", "created": "2019-08-12", "updated": "2019-11-25", "authors": ["fangzhou xie"], "url": "https://arxiv.org/abs/1908.04369"}, {"title": "zero black-derman-toy interest rate model", "id": "1908.04401", "abstract": "we propose a modification of the classical black-derman-toy (bdt) interest rate tree model, which includes the possibility of a jump with small probability at each step to a practically zero interest rate. the corresponding bdt algorithms are consequently modified to calibrate the tree containing the zero interest rate scenarios. this modification is motivated by the recent 2008-2009 crisis in the united states and it quantifies the risk of a future crises in bond prices and derivatives. the proposed model is useful to price derivatives. this exercise also provides a tool to calibrate the probability of this event. a comparison of option prices and implied volatilities on us treasury bonds computed with both the proposed and the classical tree model is provided, in six different scenarios along the different periods comprising the years 2002-2017.", "categories": "econ.em q-fin.cp", "created": "2019-08-12", "updated": "2020-07-10", "authors": ["grzegorz krzy\u017canowski", "ernesto mordecki", "andr\u00e9s sosa"], "url": "https://arxiv.org/abs/1908.04401"}, {"title": "forecast encompassing tests for the expected shortfall", "id": "1908.04569", "abstract": "we introduce new forecast encompassing tests for the risk measure expected shortfall (es). the es currently receives much attention through its introduction into the basel iii accords, which stipulate its use as the primary market risk measure for the international banking regulation. we utilize joint loss functions for the pair es and value at risk to set up three es encompassing test variants. the tests are built on misspecification robust asymptotic theory and we investigate the finite sample properties of the tests in an extensive simulation study. we use the encompassing tests to illustrate the potential of forecast combination methods for different financial assets.", "categories": "q-fin.rm econ.em math.st stat.th", "created": "2019-08-13", "updated": "2020-08-28", "authors": ["timo dimitriadis", "julie schnaitmann"], "url": "https://arxiv.org/abs/1908.04569"}, {"title": "a scalable verification solution for blockchains", "id": "1908.04756", "abstract": "bitcoin and ethereum, whose miners arguably collectively comprise the most powerful computational resource in the history of mankind, offer no more power for processing and verifying transactions than a typical smart phone. the system described herein bypasses this bottleneck and brings scalable computation to ethereum. our new system consists of a financial incentive layer atop a dispute resolution layer where the latter takes form of a versatile \"verification game.\" in addition to secure outsourced computation, immediate applications include decentralized mining pools whose operator is an ethereum smart contract, a cryptocurrency with scalable transaction throughput, and a trustless means for transferring currency between disjoint cryptocurrency systems.", "categories": "cs.cr econ.th", "created": "2019-08-12", "updated": "", "authors": ["jason teutsch", "christian reitwie\u00dfner"], "url": "https://arxiv.org/abs/1908.04756"}, {"title": "forecasting u.s. textile comparative advantage using autoregressive   integrated moving average models and time series outlier analysis", "id": "1908.04852", "abstract": "to establish an updated understanding of the u.s. textile and apparel (tap) industrys competitive position within the global textile environment, trade data from un-comtrade (1996-2016) was used to calculate the normalized revealed comparative advantage (nrca) index for 169 tap categories at the four-digit harmonized schedule (hs) code level. univariate time series using autoregressive integrated moving average (arima) models forecast short-term future performance of revealed categories with export advantage. accompanying outlier analysis examined permanent level shifts that might convey important information about policy changes, influential drivers and random events.", "categories": "econ.gn q-fin.ec", "created": "2019-08-13", "updated": "", "authors": ["zahra saki", "lori rothenberg", "marguerite moor", "ivan kandilov", "a. blanton godfrey"], "url": "https://arxiv.org/abs/1908.04852"}, {"title": "third person enforcement in a prisoner's dilemma game", "id": "1908.04971", "abstract": "we theoretically study the effect of a third person enforcement on a one-shot prisoner's dilemma game played by two persons, with whom the third person plays repeated prisoner's dilemma games. we find that the possibility of the third person's future punishment causes them to cooperate in the one-shot game.", "categories": "econ.th", "created": "2019-08-14", "updated": "", "authors": ["tatsuhiro shichijo"], "url": "https://arxiv.org/abs/1908.04971"}, {"title": "on rank estimators in increasing dimensions", "id": "1908.05255", "abstract": "the family of rank estimators, including han's maximum rank correlation (han, 1987) as a notable example, has been widely exploited in studying regression problems. for these estimators, although the linear index is introduced for alleviating the impact of dimensionality, the effect of large dimension on inference is rarely studied. this paper fills this gap via studying the statistical properties of a larger family of m-estimators, whose objective functions are formulated as u-processes and may be discontinuous in increasing dimension set-up where the number of parameters, $p_{n}$, in the model is allowed to increase with the sample size, $n$. first, we find that often in estimation, as $p_{n}/n\\rightarrow 0$, $(p_{n}/n)^{1/2}$ rate of convergence is obtainable. second, we establish bahadur-type bounds and study the validity of normal approximation, which we find often requires a much stronger scaling requirement than $p_{n}^{2}/n\\rightarrow 0.$ third, we state conditions under which the numerical derivative estimator of asymptotic covariance matrix is consistent, and show that the step size in implementing the covariance estimator has to be adjusted with respect to $p_{n}$. all theoretical results are further backed up by simulation studies.", "categories": "math.st econ.em stat.th", "created": "2019-08-14", "updated": "", "authors": ["yanqin fan", "fang han", "wei li", "xiao-hua zhou"], "url": "https://arxiv.org/abs/1908.05255"}, {"title": "why finnish polytechnics reject top applicants", "id": "1908.05443", "abstract": "i use a panel of higher education clearinghouse data to study the centralized assignment of applicants to finnish polytechnics. i show that on a yearly basis, large numbers of top applicants unnecessarily remain unassigned to any program. there are programs which rejected applicants would find acceptable, but the assignment mechanism both discourages applicants from applying, and stops programs from admitting those who do. a mechanism which would admit each year's most eligible applicants has the potential to substantially reduce re-applications, thereby shortening the long queues into finnish higher education.", "categories": "econ.gn q-fin.ec", "created": "2019-08-15", "updated": "", "authors": ["kristian koerselman"], "url": "https://arxiv.org/abs/1908.05443"}, {"title": "nonparametric identification of first-price auction with unobserved   competition: a density discontinuity framework", "id": "1908.05476", "abstract": "we consider nonparametric identification of independent private value first-price auction models, in which the analyst only observes winning bids. our benchmark model assumes an exogenous number of bidders $n$. we show that, if the bidders observe $n$, the resulting discontinuities in the winning bid density can be used to identify the distribution of $n$. the private value distribution can be identified in a second step. a second class of models considers endogenously-determined $n$, due to a reserve price or an entry cost. if bidders observe $n$, these models are also identifiable using winning bid discontinuities. if bidders cannot observe $n$, however, identification is not possible unless the analyst observes an instrument which affects the reserve price or entry cost. lastly, we derive some testable restrictions for whether bidders observe the number of competitors and whether endogenous participation is due to a reserve price or entry cost. an application to usfs timber auction data illustrates the usefulness of our theoretical results for competition analysis, showing that nearly one bid out of three can be non competitive. it also suggests that the risk aversion bias caused by a mismeasured competition can be large.", "categories": "econ.em", "created": "2019-08-15", "updated": "", "authors": ["emmanuel guerre", "yao luo"], "url": "https://arxiv.org/abs/1908.05476"}, {"title": "automation impacts on china's polarized job market", "id": "1908.05518", "abstract": "when facing threats from automation, a worker residing in a large chinese city might not be as lucky as a worker in a large u.s. city, depending on the type of large city in which one resides. empirical studies found that large u.s. cities exhibit resilience to automation impacts because of the increased occupational and skill specialization. however, in this study, we observe polarized responses in large chinese cities to automation impacts. the polarization might be attributed to the elaborate master planning of the central government, through which cities are assigned with different industrial goals to achieve globally optimal economic success and, thus, a fast-growing economy. by dividing chinese cities into two groups based on their administrative levels and premium resources allocated by the central government, we find that chinese cities follow two distinct industrial development trajectories, one trajectory owning government support leads to a diversified industrial structure and, thus, a diversified job market, and the other leads to specialty cities and, thus, a specialized job market. by revisiting the automation impacts on a polarized job market, we observe a simpson's paradox through which a larger city of a diversified job market results in greater resilience, whereas larger cities of specialized job markets are more susceptible. these findings inform policy makers to deploy appropriate policies to mitigate the polarized automation impacts.", "categories": "econ.gn q-fin.ec", "created": "2019-08-15", "updated": "", "authors": ["haohui 'caron' chen", "xun li", "morgan frank", "xiaozhen qin", "weipan xu", "manuel cebrian", "iyad rahwan"], "url": "https://arxiv.org/abs/1908.05518"}, {"title": "the inverted u-shaped effect of urban hotspots spatial compactness on   urban economic growth", "id": "1908.05530", "abstract": "the compact city, as a sustainable concept, is intended to augment the efficiency of urban function. however, previous studies have concentrated more on morphology than on structure. the present study focuses on urban structural elements, i.e., urban hotspots consisting of high-density and high-intensity socioeconomic zones, and explores the economic performance associated with their spatial structure. we use nighttime luminosity (ntl) data and the loubar method to identify and extract the hotspot and ultimately draw two conclusions. first, with population increasing, the hotspot number scales sublinearly with an exponent of approximately 0.50~0.55, regardless of the location in china, the eu or the us, while the intersect values are totally different, which is mainly due to different economic developmental level. secondly, we demonstrate that the compactness of hotspots imposes an inverted u-shaped influence on economic growth, which implies that an optimal compactness coefficient does exist. these findings are helpful for urban planning.", "categories": "econ.gn q-fin.ec", "created": "2019-08-15", "updated": "", "authors": ["weipan xu", "haohui'caron' chen", "enrique frias-martinez", "manuel cebrian", "xun li"], "url": "https://arxiv.org/abs/1908.05530"}, {"title": "probabilistic verification in mechanism design", "id": "1908.05556", "abstract": "we introduce a model of probabilistic verification in a mechanism design setting. the principal verifies the agent's claims with statistical tests. the agent's probability of passing each test depends on his type. in our framework, the revelation principle holds. we characterize whether each type has an associated test that best screens out all the other types. in that case, the testing technology can be represented in a tractable reduced form. in a quasilinear environment, we solve for the revenue-maximizing mechanism by introducing a new expression for the virtual value that encodes the effect of testing.", "categories": "econ.th cs.gt", "created": "2019-08-15", "updated": "", "authors": ["ian ball", "deniz kattwinkel"], "url": "https://arxiv.org/abs/1908.05556"}, {"title": "injectivity and the law of demand", "id": "1908.05714", "abstract": "establishing that a demand mapping is injective is core first step for a variety of methodologies. when a version of the law of demand holds, global injectivity can be checked by seeing whether the demand mapping is constant over any line segments. when we add the assumption of differentiability, we obtain necessary and sufficient conditions for injectivity that generalize classical \\cite{gale1965jacobian} conditions for quasi-definite jacobians.", "categories": "econ.em", "created": "2019-08-15", "updated": "", "authors": ["roy allen"], "url": "https://arxiv.org/abs/1908.05714"}, {"title": "isotonic regression discontinuity designs", "id": "1908.05752", "abstract": "in isotonic regression discontinuity designs, the average outcome and the treatment assignment probability are monotone in the running variable. we introduce novel nonparametric estimators for sharp and fuzzy designs based on the isotonic regression. the large sample distributions of introduced estimators are driven by scaled brownian motions originating from zero and moving in opposite directions. since these distributions are not pivotal, we also introduce a novel trimmed wild bootstrap procedure, which does not require additional nonparametric smoothing, typically needed in such settings, and show its consistency. we find in monte carlo experiments that shape restrictions can improve dramatically the finite-sample performance of unrestricted estimators. to illustrate the empirical applicability of our approach, we estimate the incumbency effect in the u.s. house elections following an influential study of (lee, 2008).", "categories": "math.st econ.em stat.ap stat.me stat.th", "created": "2019-08-15", "updated": "2020-03-11", "authors": ["andrii babii", "rohit kumar"], "url": "https://arxiv.org/abs/1908.05752"}, {"title": "a model of a randomized experiment with an application to the prowess   clinical trial", "id": "1908.05810", "abstract": "i develop a model of a randomized experiment with a binary intervention and a binary outcome. potential outcomes in the intervention and control groups give rise to four types of participants. fixing ideas such that the outcome is mortality, some participants would live regardless, others would be saved, others would be killed, and others would die regardless. these potential outcome types are not observable. however, i use the model to develop estimators of the number of participants of each type. the model relies on the randomization within the experiment and on deductive reasoning. i apply the model to an important clinical trial, the prowess trial, and i perform a monte carlo simulation calibrated to estimates from the trial. the reduced form from the trial shows a reduction in mortality, which provided a rationale for fda approval. however, i find that the intervention killed two participants for every three it saved.", "categories": "stat.me econ.em", "created": "2019-08-15", "updated": "2020-07-22", "authors": ["amanda kowalski"], "url": "https://arxiv.org/abs/1908.05810"}, {"title": "counting defiers", "id": "1908.05811", "abstract": "the late monotonicity assumption of imbens and angrist (1994) precludes \"defiers,\" individuals whose treatment always runs counter to the instrument, in the terminology of balke and pearl (1993) and angrist et al. (1996). i allow for defiers in a model with a binary instrument and a binary treatment. the model is explicit about the randomization process that gives rise to the instrument. i use the model to develop estimators of the counts of defiers, always takers, compliers, and never takers. i propose separate versions of the estimators for contexts in which the parameter of the randomization process is unspecified, which i intend for use with natural experiments with virtual random assignment. i present an empirical application that revisits angrist and evans (1998), which examines the impact of virtual random assignment of the sex of the first two children on subsequent fertility. i find that subsequent fertility is much more responsive to the sex mix of the first two children when defiers are allowed.", "categories": "stat.me econ.em", "created": "2019-08-15", "updated": "2020-07-22", "authors": ["amanda kowalski"], "url": "https://arxiv.org/abs/1908.05811"}, {"title": "testing the drift-diffusion model", "id": "1908.05824", "abstract": "the drift diffusion model (ddm) is a model of sequential sampling with diffusion (brownian) signals, where the decision maker accumulates evidence until the process hits a stopping boundary, and then stops and chooses the alternative that corresponds to that boundary. this model has been widely used in psychology, neuroeconomics, and neuroscience to explain the observed patterns of choice and response times in a range of binary choice decision problems. this paper provides a statistical test for ddm's with general boundaries. we first prove a characterization theorem: we find a condition on choice probabilities that is satisfied if and only if the choice probabilities are generated by some ddm. moreover, we show that the drift and the boundary are uniquely identified. we then use our condition to nonparametrically estimate the drift and the boundary and construct a test statistic.", "categories": "econ.em econ.th", "created": "2019-08-15", "updated": "", "authors": ["drew fudenberg", "whitney k. newey", "philipp strack", "tomasz strzalecki"], "url": "https://arxiv.org/abs/1908.05824"}, {"title": "forward-selected panel data approach for program evaluation", "id": "1908.05894", "abstract": "policy evaluation is central to economic data analysis, but economists mostly work with observational data in view of limited opportunities to carry out controlled experiments. in the potential outcome framework, the panel data approach (hsiao, ching and wan, 2012) constructs the counterfactual by exploiting the correlation between cross-sectional units in panel data. the choice of cross-sectional control units, a key step in their implementation, is nevertheless unresolved in data-rich environment when many possible controls are at the researcher's disposal. we propose the forward selection method to choose control units, and establish validity of post-selection inference. our asymptotic framework allows the number of possible controls to grow much faster than the time dimension. the easy-to-implement algorithms and their theoretical guarantee extend the panel data approach to big data settings. monte carlo simulations are conducted to demonstrate the finite sample performance of the proposed method. two empirical examples illustrate the usefulness of our procedure when many controls are available in real-world applications.", "categories": "econ.em", "created": "2019-08-16", "updated": "", "authors": ["zhentao shi", "jingyi huang"], "url": "https://arxiv.org/abs/1908.05894"}, {"title": "a model of discrete choice based on reinforcement learning under   short-term memory", "id": "1908.06133", "abstract": "a family of models of individual discrete choice are constructed by means of statistical averaging of choices made by a subject in a reinforcement learning process, where the subject has short, k-term memory span. the choice probabilities in these models combine in a non-trivial, non-linear way the initial learning bias and the experience gained through learning. the properties of such models are discussed and, in particular, it is shown that probabilities deviate from luce's choice axiom, even if the initial bias adheres to it. moreover, we shown that the latter property is recovered as the memory span becomes large.   two applications in utility theory are considered. in the first, we use the discrete choice model to generate binary preference relation on simple lotteries. we show that the preferences violate transitivity and independence axioms of expected utility theory. furthermore, we establish the dependence of the preferences on frames, with risk aversion for gains, and risk seeking for losses. based on these findings we propose next a parametric model of choice based on the probability maximization principle, as a model for deviations from expected utility principle. to illustrate the approach we apply it to the classical problem of demand for insurance.", "categories": "econ.em", "created": "2019-08-16", "updated": "", "authors": ["misha perepelitsa"], "url": "https://arxiv.org/abs/1908.06133"}, {"title": "measuring international uncertainty using global vector autoregressions   with drifting parameters", "id": "1908.06325", "abstract": "this paper investigates the time-varying impacts of international macroeconomic uncertainty shocks. we use a global vector autoregressive specification with drifting coefficients and factor stochastic volatility in the errors to model six economies jointly. the measure of uncertainty is constructed endogenously by estimating a scalar driving the innovation variances of the latent factors, which is also included in the mean of the process. to achieve regularization, we use bayesian techniques for estimation, and introduce a set of hierarchical global-local priors. the adopted priors center the model on a constant parameter specification with homoscedastic errors, but allow for time-variation if suggested by likelihood information. moreover, we assume coefficients across economies to be similar, but provide sufficient flexibility via the hierarchical prior for country-specific idiosyncrasies. the results point towards pronounced real and financial effects of uncertainty shocks in all countries, with differences across economies and over time.", "categories": "econ.em stat.ap", "created": "2019-08-17", "updated": "2019-12-17", "authors": ["michael pfarrhofer"], "url": "https://arxiv.org/abs/1908.06325"}, {"title": "the family of alpha,[a,b] stochastic orders: risk vs. expected value", "id": "1908.06398", "abstract": "in this paper we provide a novel family of stochastic orders, which we call the $\\alpha,[a,b]$-convex decreasing and $\\alpha,[a,b]$-concave increasing stochastic orders, that generalizes second order stochastic dominance. these stochastic orders allow us to compare two lotteries, where one lottery has a higher expected value and is also riskier than the other lottery.   the main motivation for introducing the $\\alpha,[a,b]$-convex decreasing stochastic orders is that they allow us to derive novel comparative statics results for important applications in economics that could not be derived using previous stochastic orders. in particular, our comparative statics results are useful when an increase in the lottery's riskiness increases the agent's optimal action, but an increase in the lottery's expected value decreases the agent's optimal action. for this kind of situation, we provide a tool to determine which of these two forces dominates -- riskiness or expected value. we apply our results in consumption-savings problems, self-protection problems, and in a bayesian game.", "categories": "math.pr econ.th", "created": "2019-08-18", "updated": "2020-02-16", "authors": ["bar light", "andres perlroth"], "url": "https://arxiv.org/abs/1908.06398"}, {"title": "spectral inference for large stochastic blockmodels with nodal   covariates", "id": "1908.06438", "abstract": "in many applications of network analysis, it is important to distinguish between observed and unobserved factors affecting network structure. to this end, we develop spectral estimators for both unobserved blocks and the effect of covariates in stochastic blockmodels. our main strategy is to reformulate the stochastic blockmodel estimation problem as recovery of latent positions in a generalized random dot product graph. on the theoretical side, we establish asymptotic normality of our estimators for the subsequent purpose of performing inference. on the applied side, we show that computing our estimator is much faster than standard variational expectation--maximization algorithms and scales well for large networks. the results in this paper provide a foundation to estimate the effect of observed covariates as well as unobserved latent community structure on the probability of link formation in networks.", "categories": "stat.me econ.em stat.co stat.ml", "created": "2019-08-18", "updated": "", "authors": ["angelo mele", "lingxin hao", "joshua cape", "carey e. priebe"], "url": "https://arxiv.org/abs/1908.06438"}, {"title": "positional voting and doubly stochastic matrices", "id": "1908.06506", "abstract": "we provide elementary proofs of several results concerning the possible outcomes arising from a fixed profile within the class of positional voting systems. our arguments enable a simple and explicit construction of paradoxical profiles, and we also demonstrate how to choose weights that realize desirable results from a given profile. the analysis ultimately boils down to thinking about positional voting systems in terms of doubly stochastic matrices.", "categories": "math.co econ.th math.ho", "created": "2019-08-18", "updated": "2020-08-14", "authors": ["jacqueline anderson", "brian camara", "john pike"], "url": "https://arxiv.org/abs/1908.06506"}, {"title": "enhancing the demand for labour survey by including skills from online   job advertisements using model-assisted calibration", "id": "1908.06731", "abstract": "in the article we describe an enhancement to the demand for labour (dl) survey conducted by statistics poland, which involves the inclusion of skills obtained from online job advertisements. the main goal is to provide estimates of the demand for skills (competences), which is missing in the dl survey. to achieve this, we apply a data integration approach combining traditional calibration with the lasso-assisted approach to correct representation error in the online data. faced with the lack of access to unit-level data from the dl survey, we use estimated population totals and propose a~bootstrap approach that accounts for the uncertainty of totals reported by statistics poland. we show that the calibration estimator assisted with lasso outperforms traditional calibration in terms of standard errors and reduces representation bias in skills observed in online job ads. our empirical results show that online data significantly overestimate interpersonal, managerial and self-organization skills while underestimating technical and physical skills. this is mainly due to the under-representation of occupations categorised as craft and related trades workers and plant and machine operators and assemblers.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-08-08", "updated": "", "authors": ["maciej ber\u0119sewicz", "greta bia\u0142kowska", "krzysztof marcinkowski", "magdalena ma\u015blak", "piotr opiela", "robert pater", "katarzyna zadroga"], "url": "https://arxiv.org/abs/1908.06731"}, {"title": "robonomics: the study of robot-human peer-to-peer financial transactions   and agreements", "id": "1908.07393", "abstract": "the concept of a blockchain has given way to the development of cryptocurrencies, enabled smart contracts, and unlocked a plethora of other disruptive technologies. but, beyond its use case in cryptocurrencies, and in network coordination and automation, blockchain technology may have serious sociotechnical implications in the future co-existence of robots and humans. motivated by the recent explosion of interest around blockchains, and our extensive work on open-source blockchain technology and its integration into robotics - this paper provides insights in ways in which blockchains and other decentralized technologies can impact our interactions with robot agents and the social integration of robots into human society.", "categories": "cs.cy cs.ro econ.gn q-fin.ec", "created": "2019-08-18", "updated": "", "authors": ["irvin steve cardenas", "jong-hoon kim"], "url": "https://arxiv.org/abs/1908.07393"}, {"title": "optimal search segmentation mechanisms for online platform markets", "id": "1908.07489", "abstract": "online platforms, such as airbnb, hotels.com, amazon, uber and lyft, can control and optimize many aspects of product search to improve the efficiency of marketplaces. here we focus on a common model, called the discriminatory control model, where the platform chooses to display a subset of sellers who sell products at prices determined by the market and a buyer is interested in buying a single product from one of the sellers. under the commonly-used model for single product selection by a buyer, called the multinomial logit model, and the bertrand game model for competition among sellers, we show the following result: to maximize social welfare, the optimal strategy for the platform is to display all products; however, to maximize revenue, the optimal strategy is to only display a subset of the products whose qualities are above a certain threshold. we extend our results to cournot competition model, and show that the optimal search segmentation mechanisms for both social welfare maximization and revenue maximization also have simple threshold structures. the threshold in each case depends on the quality of all products, the platform's objective and seller's competition model, and can be computed in linear time in the number of products.", "categories": "cs.gt econ.th", "created": "2019-08-20", "updated": "", "authors": ["zhenzhe zheng", "r. srikant"], "url": "https://arxiv.org/abs/1908.07489"}, {"title": "new developments in revealed preference theory: decisions under risk,   uncertainty, and intertemporal choice", "id": "1908.07561", "abstract": "this survey reviews recent developments in revealed preference theory. it discusses the testable implications of theories of choice that are germane to specific economic environments. the focus is on expected utility in risky environments; subjected expected utility and maxmin expected utility in the presence of uncertainty; and exponentially discounted utility for intertemporal choice. the testable implications of these theories for data on choice from classical linear budget sets are described, and shown to follow a common thread. the theories all imply an inverse relation between prices and quantities, with different qualifications depending on the functional forms in the theory under consideration.", "categories": "econ.th cs.gt econ.em", "created": "2019-08-20", "updated": "2019-12-03", "authors": ["federico echenique"], "url": "https://arxiv.org/abs/1908.07561"}, {"title": "realistic versus rational secret sharing", "id": "1908.07581", "abstract": "the study of rational secret sharing initiated by halpern and teague regards the reconstruction of the secret in secret sharing as a game. it was shown that participants (parties) may refuse to reveal their shares and so the reconstruction may fail. moreover, a refusal to reveal the share may be a dominant strategy of a party.   in this paper we consider secret sharing as a sub-action or subgame of a larger action/game where the secret opens a possibility of consumption of a certain common good. we claim that utilities of participants will be dependent on the nature of this common good. in particular, halpern and teague scenario corresponds to a rivalrous and excludable common good. we consider the case when this common good is non-rivalrous and non-excludable and find many natural nash equilibria. we list several applications of secret sharing to demonstrate our claim and give corresponding scenarios. in such circumstances the secret sharing scheme facilitates a power sharing agreement in the society. we also state that non-reconstruction may be beneficial for this society and give several examples.", "categories": "cs.cr econ.th", "created": "2019-08-20", "updated": "", "authors": ["yvo desmedt", "arkadii slinko"], "url": "https://arxiv.org/abs/1908.07581"}, {"title": "analyzing commodity futures using factor state-space models with wishart   stochastic volatility", "id": "1908.07798", "abstract": "we propose a factor state-space approach with stochastic volatility to model and forecast the term structure of future contracts on commodities. our approach builds upon the dynamic 3-factor nelson-siegel model and its 4-factor svensson extension and assumes for the latent level, slope and curvature factors a gaussian vector autoregression with a multivariate wishart stochastic volatility process. exploiting the conjugacy of the wishart and the gaussian distribution, we develop a computationally fast and easy to implement mcmc algorithm for the bayesian posterior analysis. an empirical application to daily prices for contracts on crude oil with stipulated delivery dates ranging from one to 24 months ahead show that the estimated 4-factor svensson model with two curvature factors provides a good parsimonious representation of the serial correlation in the individual prices and their volatility. it also shows that this model has a good out-of-sample forecast performance.", "categories": "stat.co econ.em stat.me", "created": "2019-08-21", "updated": "", "authors": ["tore selland kleppe", "roman liesenfeld", "guilherme valle moura", "atle oglend"], "url": "https://arxiv.org/abs/1908.07798"}, {"title": "a doubly corrected robust variance estimator for linear gmm", "id": "1908.07821", "abstract": "we propose a new finite sample corrected variance estimator for the linear generalized method of moments (gmm) including the one-step, two-step, and iterated estimators. our formula additionally corrects for the over-identification bias in variance estimation on top of the commonly used finite sample correction of windmeijer (2005) which corrects for the bias from estimating the efficient weight matrix, so is doubly corrected. an important feature of the proposed double correction is that it automatically provides robustness to misspecification of the moment condition. in contrast, the conventional variance estimator and the windmeijer correction are inconsistent under misspecification. that is, the proposed double correction formula provides a convenient way to obtain improved inference under correct specification and robustness against misspecification at the same time.", "categories": "econ.em", "created": "2019-08-21", "updated": "2020-05-29", "authors": ["jungbin hwang", "byunghoon kang", "seojeong lee"], "url": "https://arxiv.org/abs/1908.07821"}, {"title": "a complex net of intertwined complements: measuring interdimensional   dependence among the poor", "id": "1908.07870", "abstract": "the choice of appropriate measures of deprivation, identification and aggregation of poverty has been a challenge for many years. the works of sen, atkinson and others have been the cornerstone for most of the literature on poverty measuring. recent contributions have focused in what we now know as multidimensional poverty measuring. current aggregation and identification measures for multidimensional poverty make the implicit assumption that dimensions are independent of each other, thus ignoring the natural dependence between them. in this article a variant of the usual method of deprivation measuring is presented. it allows the existence of the forementioned connections by drawing from geometric and networking notions. this new methodology relies on previous identification and aggregation methods, but with small modifications to prevent arbitrary manipulations. it is also proved that this measure still complies with the axiomatic framework of its predecessor. moreover, the general form of latter can be considered a particular case of this new measure, although this identification is not unique.", "categories": "econ.gn q-fin.ec", "created": "2019-08-21", "updated": "", "authors": ["felipe del canto m"], "url": "https://arxiv.org/abs/1908.07870"}, {"title": "decision-facilitating information in hidden-action setups: an   agent-based approach", "id": "1908.07998", "abstract": "the hidden-action model captures a fundamental problem of principal-agent theory and provides an optimal sharing rule when only the outcome but not the effort can be observed. however, the hidden-action model builds on various explicit and also implicit assumptions about the information of the contracting parties. this paper relaxes key assumptions regarding the availability of information included in the hidden-action model in order to study whether and, if so, how fast the optimal sharing rule is achieved and how this is affected by the various types of information employed in the principal-agent relation. our analysis particularly focuses on information about the environment and about feasible actions for the agent. we follow an approach to transfer closed-form mathematical models into agent-based computational models and show that the extent of information about feasible options to carry out a task only has an impact on performance if decision makers are well informed about the environment, and that the decision whether to perform exploration or exploitation when searching for new feasible options only affects performance in specific situations. having good information about the environment, on the contrary, appears to be crucial in almost all situations.", "categories": "econ.gn econ.th q-fin.ec", "created": "2019-08-21", "updated": "2020-04-13", "authors": ["stephan leitner", "friederike wall"], "url": "https://arxiv.org/abs/1908.07998"}, {"title": "forecasting e-scooter competition with direct and access trips by mode   and distance in new york city", "id": "1908.08127", "abstract": "given the lack of demand forecasting models for e-scooter sharing systems, we address this research gap using data from portland, or, and new york city. a log-log regression model is estimated for e-scooter trips based on user age, income, labor force participation, and health insurance coverage, with an adjusted r squared value of 0.663. when applied to the manhattan market, the model predicts 66k daily e-scooter trips, which would translate to 67 million usd in annual revenue (based on average 12-minute trips and historical fare pricing models). we propose a novel nonlinear, multifactor model to break down the number of daily trips by the alternate modes of transportation that they would likely substitute. the final model parameters reveal a relationship with taxi trips as well as access/egress trips with public transit in manhattan. our model estimates that e-scooters would replace at most 1% of taxi trips; the model can explain $800,000 of the annual revenue from this competition. the distance structure of revenue from access/egress trips is found to differ significantly from that of substituted taxi trips.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-08-21", "updated": "", "authors": ["mina lee", "joseph y. j. chow", "gyugeun yoon", "brian yueshuai he"], "url": "https://arxiv.org/abs/1908.08127"}, {"title": "outgroup homogeneity bias causes ingroup favoritism", "id": "1908.08203", "abstract": "ingroup favoritism, the tendency to favor ingroup over outgroup, is often explained as a product of intergroup conflict, or correlations between group tags and behavior. such accounts assume that group membership is meaningful, whereas human data show that ingroup favoritism occurs even when it confers no advantage and groups are transparently arbitrary. another possibility is that ingroup favoritism arises due to perceptual biases like outgroup homogeneity, the tendency for humans to have greater difficulty distinguishing outgroup members than ingroup ones. we present a prisoner's dilemma model, where individuals use bayesian inference to learn how likely others are to cooperate, and then act rationally to maximize expected utility. we show that, when such individuals exhibit outgroup homogeneity bias, ingroup favoritism between arbitrary groups arises through direct reciprocity. however, this outcome may be mitigated by: (1) raising the benefits of cooperation, (2) increasing population diversity, and (3) imposing a more restrictive social structure.", "categories": "econ.th q-bio.pe", "created": "2019-08-22", "updated": "", "authors": ["marcel montrey", "thomas r. shultz"], "url": "https://arxiv.org/abs/1908.08203"}, {"title": "equilibrium in production chains with multiple upstream partners", "id": "1908.08208", "abstract": "in this paper, we extend and improve the production chain model introduced by kikuchi et al. (2018). utilizing the theory of monotone concave operators, we prove the existence, uniqueness, and global stability of equilibrium price, hence improving their results on production networks with multiple upstream partners. we propose an algorithm for computing the equilibrium price function that is more than ten times faster than successive evaluations of the operator. the model is then generalized to a stochastic setting that offers richer implications for the distribution of firms in a production network.", "categories": "econ.th", "created": "2019-08-22", "updated": "", "authors": ["meng yu", "junnan zhang"], "url": "https://arxiv.org/abs/1908.08208"}, {"title": "implementing result-based agri-environmental payments by means of   modelling", "id": "1908.08219", "abstract": "from a theoretical point of view, result-based agri-environmental payments are clearly preferable to action-based payments. however, they suffer from two major practical disadvantages: costs of measuring the results and payment uncertainty for the participating farmers. in this paper, we propose an alternative design to overcome these two disadvantages by means of modelling (instead of measuring) the results. we describe the concept of model-informed result-based agri-environmental payments (mirbap), including a hypothetical example of payments for the protection and enhancement of soil functions. we offer a comprehensive discussion of the relative advantages and disadvantages of mirbap, showing that it not only unites most of the advantages of result-based and action-based schemes, but also adds two new advantages: the potential to address trade-offs among multiple policy objectives and management for long-term environmental effects. we argue that mirbap would be a valuable addition to the agri-environmental policy toolbox and a reflection of recent advancements in agri-environmental modelling.", "categories": "econ.gn q-fin.ec", "created": "2019-08-22", "updated": "", "authors": ["bartosz bartkowski", "nils droste", "mareike lie\u00df", "william sidemo-holm", "ulrich weller", "mark v. brady"], "url": "https://arxiv.org/abs/1908.08219"}, {"title": "the many shapley values for model explanation", "id": "1908.08474", "abstract": "the shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. the use of the shapley value is justified by citing [16] showing that it is the \\emph{unique} method that satisfies certain good properties (\\emph{axioms}).   there are, however, a multiplicity of ways in which the shapley value is operationalized in the attribution problem. these differ in how they reference the model, the training data, and the explanation context. these give very different results, rendering the uniqueness result meaningless. furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model.   in this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the shapley value for attribution, and propose a technique called baseline shapley (bshap) that is backed by a proper uniqueness result. we also contrast bshap with integrated gradients, another extension of shapley value to the continuous setting.", "categories": "cs.ai cs.lg econ.th", "created": "2019-08-22", "updated": "2020-02-07", "authors": ["mukund sundararajan", "amir najmi"], "url": "https://arxiv.org/abs/1908.08474"}, {"title": "online inference for advertising auctions", "id": "1908.08600", "abstract": "advertisers that engage in real-time bidding (rtb) to display their ads commonly have two goals: learning their optimal bidding policy and estimating the expected effect of exposing users to their ads. typical strategies to accomplish one of these goals tend to ignore the other, creating an apparent tension between the two. this paper exploits the economic structure of the bid optimization problem faced by advertisers to show that these two objectives can actually be perfectly aligned. by framing the advertiser's problem as a multi-armed bandit (mab) problem, we propose a modified thompson sampling (ts) algorithm that concurrently learns the optimal bidding policy and estimates the expected effect of displaying the ad while minimizing economic losses from potential sub-optimal bidding. simulations show that not only the proposed method successfully accomplishes the advertiser's goals, but also does so at a much lower cost than more conventional experimentation policies aimed at performing causal inference.", "categories": "cs.lg cs.gt econ.em stat.ml", "created": "2019-08-22", "updated": "", "authors": ["caio waisman", "harikesh s. nair", "carlos carrion", "nan xu"], "url": "https://arxiv.org/abs/1908.08600"}, {"title": "a simple model suggesting economically rational sample-size choice   drives irreproducibility", "id": "1908.08702", "abstract": "several systematic studies have suggested that a large fraction of published research is not reproducible. one probable reason for low reproducibility is insufficient sample size, resulting in low power and low positive predictive value. it has been suggested that insufficient sample-size choice is driven by a combination of scientific competition and 'positive publication bias'. here we formalize this intuition in a simple model, in which scientists choose economically rational sample sizes, balancing the cost of experimentation with income from publication. specifically, assuming that a scientist's income derives only from 'positive' findings (positive publication bias) and that individual samples cost a fixed amount, allows to leverage basic statistical formulas into an economic optimality prediction. we find that if effects have i) low base probability, ii) small effect size or iii) low grant income per publication, then the rational (economically optimal) sample size is small. furthermore, for plausible distributions of these parameters we find a robust emergence of a bimodal distribution of obtained statistical power and low overall reproducibility rates, both matching empirical findings. finally, we explore conditional equivalence testing as a means to align economic incentives with adequate sample sizes. overall, the model describes a simple mechanism explaining both the prevalence and the persistence of small sample sizes, and is well suited for empirical validation. it proposes economic rationality, or economic pressures, as a principal driver of irreproducibility and suggests strategies to change this.", "categories": "econ.gn cs.sy eess.sy q-fin.ec stat.me", "created": "2019-08-23", "updated": "2020-02-13", "authors": ["oliver braganza"], "url": "https://arxiv.org/abs/1908.08702"}, {"title": "nonparametric estimation of causal heterogeneity under high-dimensional   confounding", "id": "1908.08779", "abstract": "this paper considers the practically important case of nonparametrically estimating heterogeneous average treatment effects that vary with a limited number of discrete and continuous covariates in a selection-on-observables framework where the number of possible confounders is very large. we propose a two-step estimator for which the first step is estimated by machine learning. we show that this estimator has desirable statistical properties like consistency, asymptotic normality and rate double robustness. in particular, we derive the coupled convergence conditions between the nonparametric and the machine learning steps. we also show that estimating population average treatment effects by averaging the estimated heterogeneous effects is semi-parametrically efficient. the new estimator is an empirical example of the effects of mothers' smoking during pregnancy on the resulting birth weight.", "categories": "econ.em", "created": "2019-08-23", "updated": "", "authors": ["michael zimmert", "michael lechner"], "url": "https://arxiv.org/abs/1908.08779"}, {"title": "government expenditure on research plans and their diversity", "id": "1908.08786", "abstract": "in this study, we consider research and development investment by the government. our study is motivated by the bias in the budget allocation owing to the competitive funding system. in our model, each researcher presents research plans and expenses, and the government selects a research plan in two periods---before and after the government knows its favorite plan---and spends funds on the adopted program in each period. we demonstrate that, in a subgame perfect equilibrium, the government adopts equally as many active plans as possible. in an equilibrium, the selected plans are distributed proportionally. thus, the investment in research projects is symmetric and unbiased. our results imply that equally widespread expenditure across all research fields is better than the selection of and concentration in some specific fields.", "categories": "econ.gn q-fin.ec", "created": "2019-08-03", "updated": "", "authors": ["ryosuke ishii", "kuninori nakagawa"], "url": "https://arxiv.org/abs/1908.08786"}, {"title": "revealed preferences for matching with contracts", "id": "1908.08823", "abstract": "many-to-many matching with contracts is studied in the framework of revealed preferences. all preferences are described by choice functions that satisfy natural conditions. under a no-externality assumption individual preferences can be aggregated into a single choice function expressing a collective preference. in this framework, a two-sided matching problem may be described as an agreement problem between two parties: the two parties must find a stable agreement, i.e., a set of contracts from which no party will want to take away any contract and to which the two parties cannot agree to add any contract. on such stable agreements each party's preference relation is a partial order and the two parties have inverse preferences. an algorithm is presented that generalizes algorithms previously proposed in less general situations. this algorithm provides a stable agreement that is preferred to all stable agreements by one of the parties and therefore less preferred than all stable agreements by the other party. the number of steps of the algorithm is linear in the size of the set of contracts, i.e., polynomial in the size of the problem. the algorithm provides a proof that stable agreements form a lattice under the two inverse preference relations. under additional assumptions on the role of money in preferences, agreement problems can describe general two-sided markets in which goods are exchanged for money. stable agreements provide a solution concept, including prices, that is more general than competitive equilibria. they satisfy an almost one price law for identical items.", "categories": "cs.gt econ.th", "created": "2019-08-23", "updated": "2020-03-04", "authors": ["daniel lehmann"], "url": "https://arxiv.org/abs/1908.08823"}, {"title": "dyadic regression", "id": "1908.09029", "abstract": "dyadic data, where outcomes reflecting pairwise interaction among sampled units are of primary interest, arise frequently in social science research. regression analyses with such data feature prominently in many research literatures (e.g., gravity models of trade). the dependence structure associated with dyadic data raises special estimation and, especially, inference issues. this chapter reviews currently available methods for (parametric) dyadic regression analysis and presents guidelines for empirical researchers.", "categories": "econ.em stat.ap", "created": "2019-08-23", "updated": "", "authors": ["bryan s. graham"], "url": "https://arxiv.org/abs/1908.09029"}, {"title": "constraint qualifications in partial identification", "id": "1908.09103", "abstract": "the literature on stochastic programming typically restricts attention to problems that fulfill constraint qualifications. the literature on estimation and inference under partial identification frequently restricts the geometry of identified sets with diverse high-level assumptions. these superficially appear to be different approaches to closely related problems. we extensively analyze their relation. among other things, we show that for partial identification through pure moment inequalities, numerous assumptions from the literature essentially coincide with the mangasarian-fromowitz constraint qualification. this clarifies the relation between well-known contributions, including within econometrics, and elucidates stringency, as well as ease of verification, of some high-level assumptions in seminal papers.", "categories": "econ.em", "created": "2019-08-24", "updated": "2020-06-18", "authors": ["hiroaki kaido", "francesca molinari", "j\u00f6rg stoye"], "url": "https://arxiv.org/abs/1908.09103"}, {"title": "inference on weighted average value function in high-dimensional state   space", "id": "1908.09173", "abstract": "this paper gives a consistent, asymptotically normal estimator of the expected value function when the state space is high-dimensional and the first-stage nuisance functions are estimated by modern machine learning tools. first, we show that value function is orthogonal to the conditional choice probability, therefore, this nuisance function needs to be estimated only at $n^{-1/4}$ rate. second, we give a correction term for the transition density of the state variable. the resulting orthogonal moment is robust to misspecification of the transition density and does not require this nuisance function to be consistently estimated. third, we generalize this result by considering the weighted expected value. in this case, the orthogonal moment is doubly robust in the transition density and additional second-stage nuisance functions entering the correction term. we complete the asymptotic theory by providing bounds on second-order asymptotic terms.", "categories": "stat.ml cs.lg econ.em", "created": "2019-08-24", "updated": "", "authors": ["victor chernozhukov", "whitney newey", "vira semenova"], "url": "https://arxiv.org/abs/1908.09173"}, {"title": "the ridge path estimator for linear instrumental variables", "id": "1908.09237", "abstract": "this paper presents the asymptotic behavior of a linear instrumental variables (iv) estimator that uses a ridge regression penalty. the regularization tuning parameter is selected empirically by splitting the observed data into training and test samples. conditional on the tuning parameter, the training sample creates a path from the iv estimator to a prior. the optimal tuning parameter is the value along this path that minimizes the iv objective function for the test sample.   the empirically selected regularization tuning parameter becomes an estimated parameter that jointly converges with the parameters of interest. the asymptotic distribution of the tuning parameter is a nonstandard mixture distribution. monte carlo simulations show the asymptotic distribution captures the characteristics of the sampling distributions and when this ridge estimator performs better than two-stage least squares.", "categories": "econ.em stat.ml", "created": "2019-08-24", "updated": "", "authors": ["nandana sengupta", "fallaw sowell"], "url": "https://arxiv.org/abs/1908.09237"}, {"title": "revenue sharing in the internet: a moral hazard approach and a   net-neutrality perspective", "id": "1908.09580", "abstract": "revenue sharing contracts between content providers (cps) and internet service providers (isps) can act as leverage for enhancing the infrastructure of the internet. isps can be incentivized to make investments in network infrastructure that improve quality of service (qos) for users if attractive contracts are negotiated between them and cps. the idea here is that part of the net profit gained by cps are given to isps to invest in the network. the moral hazard economic framework is used to model such an interaction, in which a principal determines a contract, and an agent reacts by adapting her effort. in our setting, several competitive cps interact through one common isp. two cases are studied: (i) the isp differentiates between the cps and makes a (potentially) different investment to improve the qos of each cp, and (ii) the isp does not differentiate between cps and makes a common investment for both. the last scenario can be viewed as \\emph{network neutral behavior} on the part of the isp. we analyse the optimal contracts and show that the cp that can better monetize its demand always prefers the non-neutral regime. interestingly, isp revenue, as well as social utility, are also found to be higher under the non-neutral regime.", "categories": "econ.gn cs.gt q-fin.ec", "created": "2019-08-26", "updated": "", "authors": ["fehmina malik", "manjesh k. ~hanawal", "yezekael hayel", "jayakrishnan nair"], "url": "https://arxiv.org/abs/1908.09580"}, {"title": "spatial pattern and city size distribution", "id": "1908.09706", "abstract": "many large cities are found at locations with certain first nature advantages. yet, those exogenous locational features may not be the most potent forces governing the spatial pattern of cities. in particular, population size, spacing and industrial composition of cities exhibit simple, persistent and monotonic relationships. theories of economic agglomeration suggest that this regularity is a consequence of interactions between endogenous agglomeration and dispersion forces. this paper reviews the extant formal models that explain the spatial pattern together with the size distribution of cities, and discusses the remaining research questions to be answered in this literature. to obtain results about explicit spatial patterns of cities, a model needs to depart from the most popular two-region and systems-of-cities frameworks in urban and regional economics in which there is no variation in interregional distance. this is one of the major reasons that only few formal models have been proposed in this literature. to draw implications as much as possible from the extant theories, this review involves extensive discussions on the behavior of the many-region extension of these models. the mechanisms that link the spatial pattern of cities and the diversity in city sizes are also discussed in detail.", "categories": "econ.gn q-fin.ec", "created": "2019-08-26", "updated": "", "authors": ["tomoya mori"], "url": "https://arxiv.org/abs/1908.09706"}, {"title": "optimal life-cycle consumption and investment decisions under   age-dependent risk preferences", "id": "1908.09976", "abstract": "in this article we solve the problem of maximizing the expected utility of future consumption and terminal wealth to determine the optimal pension or life-cycle fund strategy for a cohort of pension fund investors. the setup is strongly related to a dc pension plan where additionally (individual) consumption is taken into account. the consumption rate is subject to a time-varying minimum level and terminal wealth is subject to a terminal floor. moreover, the preference between consumption and terminal wealth as well as the intertemporal coefficient of risk aversion are time-varying and therefore depend on the age of the considered pension cohort. the optimal consumption and investment policies are calculated in the case of a black-scholes financial market framework and hyperbolic absolute risk aversion (hara) utility functions. we generalize ye (2008) (2008 american control conference, 356-362) by adding an age-dependent coefficient of risk aversion and extend steffensen (2011) (journal of economic dynamics and control, 35(5), 659-667), hentschel (2016) (doctoral dissertation, ulm university) and aase (2017) (stochastics, 89(1), 115-141) by considering consumption in combination with terminal wealth and allowing for consumption and terminal wealth floors via an application of hara utility functions. a case study on fitting several models to realistic, time-dependent life-cycle consumption and relative investment profiles shows that only our extended model with time-varying preference parameters provides sufficient flexibility for an adequate fit. this is of particular interest to life-cycle products for (private) pension investments or pension insurance in general.", "categories": "q-fin.mf econ.th math.oc q-fin.pm", "created": "2019-08-26", "updated": "", "authors": ["andreas lichtenstern", "pavel v. shevchenko", "rudi zagst"], "url": "https://arxiv.org/abs/1908.09976"}, {"title": "future competitive bioenergy technologies in the german heat sector:   findings from an economic optimization approach", "id": "1908.10065", "abstract": "meeting the defined greenhouse gas (ghg) reduction targets in germany is only possible by switching to renewable technologies in the energy sector. a major share of that reduction needs to be covered by the heat sector, which accounts for ~35% of the energy based emissions in germany. biomass is the renewable key player in the heterogeneous heat sector today. its properties such as weather independency, simple storage and flexible utilization open up a wide field of applications for biomass. however, in a future heat sector fulfilling ghg reduction targets and energy sectors being increasingly connected: which bioenergy technology concepts are competitive options against other renewable heating systems? in this paper, the cost optimal allocation of the limited german biomass potential is investigated under longterm scenarios using a mathematical optimization approach. the model results show that bioenergy can be a competitive option in the future. especially the use of biomass from residues can be highly competitive in hybrid combined heat and power (chp) pellet combustion plants in the private household sector. however, towards 2050, wood based biomass use in high temperature industry applications is found to be the most cost efficient way to reduce heat based emissions by 95% in 2050.", "categories": "econ.gn q-fin.ec", "created": "2019-08-27", "updated": "2019-08-28", "authors": ["matthias jordan", "volker lenz", "markus millinger", "katja oehmichen", "daniela thr\u00e4n"], "url": "https://arxiv.org/abs/1908.10065"}, {"title": "interaction of a hydrogen refueling station network for heavy-duty   vehicles and the power system in germany for 2050", "id": "1908.10119", "abstract": "a potential solution to reduce greenhouse gas (ghg) emissions in the transport sector is to use alternatively fueled vehicles (afv). heavy-duty vehicles (hdv) emit a large share of ghg emissions in the transport sector and are therefore the subject of growing attention from global regulators. fuel cell and green hydrogen technologies are a promising option to decarbonize hdvs, as their fast refueling and long vehicle ranges are in line with current logistic operation concepts. moreover, the application of green hydrogen in transport could enable more effective integration of renewable energies (re) across different energy sectors. this paper explores the interplay between hdv hydrogen refueling stations (hrs) that produce hydrogen locally and the power system by combining an infrastructure location planning model and an energy system optimization model that takes grid expansion options into account. two scenarios - one sizing refueling stations in symbiosis with the power system and one sizing them independently of it - are assessed regarding their impacts on the total annual energy system costs, regional re integration and the levelized cost of hydrogen (lcoh). the impacts are calculated based on locational marginal pricing for 2050. depending on the integration scenario, we find average lcoh of between 5.66 euro/kg and 6.20 euro/kg, for which nodal electricity prices are the main determining factor as well as a strong difference in lcoh between north and south germany. from a system perspective, investing in hdv-hrs in symbiosis with the power system rather than independently promises cost savings of around one billion-euros per annum. we therefore conclude that the co-optimization of multiple energy sectors is important for investment planning and has the potential to exploit synergies.", "categories": "econ.gn q-fin.ec", "created": "2019-08-27", "updated": "", "authors": ["philipp kluschke", "fabian neumann"], "url": "https://arxiv.org/abs/1908.10119"}, {"title": "improving information from manipulable data", "id": "1908.10330", "abstract": "data-based decisionmaking must account for the manipulation of data by agents who are aware of how decisions are being made and want to affect their allocations. we study a framework in which, due to such manipulation, data becomes less informative when decisions depend more strongly on data. we formalize why and how a decisionmaker should commit to underutilizing data. doing so attenuates information loss and thereby improves allocation accuracy.", "categories": "econ.th", "created": "2019-08-27", "updated": "2020-04-04", "authors": ["alex frankel", "navin kartik"], "url": "https://arxiv.org/abs/1908.10330"}, {"title": "theory of weak identification in semiparametric models", "id": "1908.10478", "abstract": "we provide general formulation of weak identification in semiparametric models and an efficiency concept. weak identification occurs when a parameter is weakly regular, i.e., when it is locally homogeneous of degree zero. when this happens, consistent or equivariant estimation is shown to be impossible. we then show that there exists an underlying regular parameter that fully characterizes the weakly regular parameter. while this parameter is not unique, concepts of sufficiency and minimality help pin down a desirable one. if estimation of minimal sufficient underlying parameters is inefficient, it introduces noise in the corresponding estimation of weakly regular parameters, whence we can improve the estimators by local asymptotic rao-blackwellization. we call an estimator weakly efficient if it does not admit such improvement. new weakly efficient estimators are presented in linear iv and nonlinear regression models. simulation of a linear iv model demonstrates how 2sls and optimal iv estimators are improved.", "categories": "econ.em math.st stat.th", "created": "2019-08-27", "updated": "2020-08-03", "authors": ["tetsuya kaji"], "url": "https://arxiv.org/abs/1908.10478"}, {"title": "coase meets bellman: dynamic programming and production chains", "id": "1908.10557", "abstract": "we show that competitive equilibria in a range of useful production chain models can be recovered as the solutions to a class of dynamic programming problems. bringing dynamic programming to bear on the equilibrium structure of production chains adds analytical power and opens new avenues for computation. in addition, the dynamic programming problem that we use to explore production chains is of interest in its own right, since it provides new optimality results for intertemporal choice in an empirically relevant setting.", "categories": "econ.gn q-fin.ec", "created": "2019-08-28", "updated": "", "authors": ["tomoo kikuchi", "kazuo nishimura", "john stachurski", "junnan zhang"], "url": "https://arxiv.org/abs/1908.10557"}, {"title": "infinitely stochastic micro forecasting", "id": "1908.10636", "abstract": "forecasting costs is now a front burner in empirical economics. we propose an unconventional tool for stochastic prediction of future expenses based on the individual (micro) developments of recorded events. consider a firm, enterprise, institution, or state, which possesses knowledge about particular historical events. for each event, there is a series of several related subevents: payments or losses spread over time, which all leads to an infinitely stochastic process at the end. nevertheless, the issue is that some already occurred events do not have to be necessarily reported. the aim lies in forecasting future subevent flows coming from already reported, occurred but not reported, and yet not occurred events. our methodology is illustrated on quantitative risk assessment, however, it can be applied to other areas such as startups, epidemics, war damages, advertising and commercials, digital payments, or drug prescription as manifested in the paper. as a theoretical contribution, inference for infinitely stochastic processes is developed. in particular, a non-homogeneous poisson process with non-homogeneous poisson processes as marks is used, which includes for instance the cox process as a special case.", "categories": "econ.em stat.ap", "created": "2019-08-28", "updated": "2019-09-19", "authors": ["mat\u00fa\u0161 maciak", "ostap okhrin", "michal pe\u0161ta"], "url": "https://arxiv.org/abs/1908.10636"}, {"title": "a cardinal comparison of experts", "id": "1908.10649", "abstract": "in various situations, decision makers face experts that may provide conflicting advice. this advice may be in the form of probabilistic forecasts over critical future events. we consider a setting where the two forecasters provide their advice repeatedly and ask whether the decision maker can learn to compare and rank the two forecasters based on past performance. we take an axiomatic approach and propose three natural axioms that a comparison test should comply with. we propose a test that complies with our axioms. perhaps, not surprisingly, this test is closely related to the likelihood ratio of the two forecasts over the realized sequence of events. more surprisingly, this test is essentially unique. furthermore, using results on the rate of convergence of supermartingales, we show that whenever the two experts\\textquoteright{} advice are sufficiently distinct, the proposed test will detect the informed expert in any desired degree of precision in some fixed finite time.", "categories": "econ.th", "created": "2019-08-28", "updated": "2020-02-13", "authors": ["itay kavaler", "rann smorodinsky"], "url": "https://arxiv.org/abs/1908.10649"}, {"title": "publish and perish: creative destruction and macroeconomic theory", "id": "1908.10680", "abstract": "a number of macroeconomic theories, very popular in the 1980s, seem to have completely disappeared and been replaced by the dynamic stochastic general equilibrium (dsge) approach. we will argue that this replacement is due to a tacit agreement on a number of assumptions, previously seen as mutually exclusive, and not due to a settlement by 'nature'. as opposed to econometrics and microeconomics and despite massive progress in the access to data and the use of statistical software, macroeconomic theory appears not to be a cumulative science so far. observational equivalence of different models and the problem of identification of parameters of the models persist as will be highlighted by examining two examples: one in growth theory and a second in testing inflation persistence.", "categories": "econ.gn q-fin.ec", "created": "2019-08-19", "updated": "", "authors": ["jean-bernard chatelain", "kirsten ralf"], "url": "https://arxiv.org/abs/1908.10680"}, {"title": "centrality-oriented causality -- a study of eu agricultural subsidies   and digital developement in poland", "id": "1908.11099", "abstract": "results of a convincing causal statistical inference related to socio-economic phenomena are treated as especially desired background for conducting various socio-economic programs or government interventions. unfortunately, quite often real socio-economic issues do not fulfill restrictive assumptions of procedures of causal analysis proposed in the literature. this paper indicates certain empirical challenges and conceptual opportunities related to applications of procedures of data depth concept into a process of causal inference as to socio-economic phenomena. we show, how to apply a statistical functional depths in order to indicate factual and counterfactual distributions commonly used within procedures of causal inference. thus a modification of rubin causality concept is proposed, i.e., a centrality-oriented causality concept. the presented framework is especially useful in a context of conducting causal inference basing on official statistics, i.e., basing on already existing databases. methodological considerations related to extremal depth, modified band depth, fraiman-muniz depth, and multivariate wilcoxon sum rank statistic are illustrated by means of example related to a study of an impact of eu direct agricultural subsidies on a digital development in poland in a period of 2012-2019.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2019-08-29", "updated": "2019-09-16", "authors": ["kosiorowski daniel", "jerzy p. rydlewski"], "url": "https://arxiv.org/abs/1908.11099"}, {"title": "a multi-scale symmetry analysis of uninterrupted trends returns of daily   financial indices", "id": "1908.11204", "abstract": "we present a symmetry analysis of the distribution of variations of different financial indices, by means of a statistical procedure developed by the authors based on a symmetry statistic by einmahl and mckeague. we applied this statistical methodology to financial uninterrupted daily trends returns and to other derived observable. in our opinion, to study distributional symmetry, trends returns offer more advantages than the commonly used daily financial returns; the two most important being: 1) trends returns involve sampling over different time scales and 2) by construction, this variable time series contains practically the same number of non-negative and negative entry values. we also show that these time multi-scale returns display distributional bi-modality. daily financial indices analyzed in this work, are the mexican ipc, the american djia, dax from germany and the japanese market index nikkei, covering a time period from 11-08-1991 to 06-30-2017. we show that, at the time scale resolution and significance considered in this paper, it is almost always feasible to find an interval of possible symmetry points containing one most plausible symmetry point denoted by c. finally, we study the temporal evolution of c showing that this point is seldom zero and responds with sensitivity to extreme market events.", "categories": "q-fin.st econ.em", "created": "2019-08-27", "updated": "", "authors": ["c. m. rodr\u00edguez-mart\u00ednez", "h. f. coronel-brizio", "a. r. hern\u00e1ndez-montoya"], "url": "https://arxiv.org/abs/1908.11204"}, {"title": "stock price forecasting and hypothesis testing using neural networks", "id": "1908.11212", "abstract": "in this work we use recurrent neural networks and multilayer perceptrons to predict nyse, nasdaq and amex stock prices from historical data. we experiment with different architectures and compare data normalization techniques. then, we leverage those findings to question the efficient-market hypothesis through a formal statistical test.", "categories": "q-fin.st cs.lg econ.em stat.ml", "created": "2019-08-27", "updated": "", "authors": ["kerda varaku"], "url": "https://arxiv.org/abs/1908.11212"}, {"title": "culture and the disposition effect", "id": "1908.11492", "abstract": "we study the relationship between national culture and the disposition effect by investigating international differences in the degree of investors' disposition effect. we utilize brokerage data of 387,993 traders from 83 countries and find great variation in the degree of the disposition effect across the world. we find that the cultural dimensions of long-term orientation and indulgence help to explain why certain nationalities are more prone to the disposition effect. we also find support on an international level for the role of age and gender in explaining the disposition effect.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2019-08-29", "updated": "", "authors": ["bastian breitmayer", "tim hasso", "matthias pelster"], "url": "https://arxiv.org/abs/1908.11492"}, {"title": "predicting consumer default: a deep learning approach", "id": "1908.11498", "abstract": "we develop a model to predict consumer default based on deep learning. we show that the model consistently outperforms standard credit scoring models, even though it uses the same data. our model is interpretable and is able to provide a score to a larger class of borrowers relative to standard credit scoring models while accurately tracking variations in systemic risk. we argue that these properties can provide valuable insights for the design of policies targeted at reducing consumer default and alleviating its burden on borrowers and lenders, as well as macroprudential regulation.", "categories": "econ.gn cs.lg q-fin.ec", "created": "2019-08-29", "updated": "2019-10-03", "authors": ["stefania albanesi", "domonkos f. vamossy"], "url": "https://arxiv.org/abs/1908.11498"}, {"title": "the economics of minority language use: theory and empirical evidence   for a language game model", "id": "1908.11604", "abstract": "language and cultural diversity is a fundamental aspect of the present world. we study three modern multilingual societies -- the basque country, ireland and wales -- which are endowed with two, linguistically distant, official languages: $a$, spoken by all individuals, and $b$, spoken by a bilingual minority. in the three cases it is observed a decay in the use of minoritarian $b$, a sign of diversity loss. however, for the \"council of europe\" the key factor to avoid the shift of $b$ is its use in all domains. thus, we investigate the language choices of the bilinguals by means of an evolutionary game theoretic model. we show that the language population dynamics has reached an evolutionary stable equilibrium where a fraction of bilinguals have shifted to speak $a$. thus, this equilibrium captures the decline in the use of $b$. to test the theory we build empirical models that predict the use of $b$ for each proportion of bilinguals. we show that model-based predictions fit very well the observed use of basque, irish, and welsh.", "categories": "econ.em stat.ap", "created": "2019-08-30", "updated": "", "authors": ["stefan sperlich", "jose-ramon uriarte"], "url": "https://arxiv.org/abs/1908.11604"}, {"title": "racial disparities in voting wait times: evidence from smartphone data", "id": "1909.00024", "abstract": "equal access to voting is a core feature of democratic government. using data from millions of smartphone users, we quantify a racial disparity in voting wait times across a nationwide sample of polling places during the 2016 us presidential election. relative to entirely-white neighborhoods, residents of entirely-black neighborhoods waited 29% longer to vote and were 74% more likely to spend more than 30 minutes at their polling place. this disparity holds when comparing predominantly white and black polling places within the same states and counties, and survives numerous robustness and placebo tests. our results document large racial differences in voting wait times and demonstrates that geospatial data can be an effective tool to both measure and monitor these disparities.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-08-30", "updated": "", "authors": ["m. keith chen", "kareem haggag", "devin g. pope", "ryne rohla"], "url": "https://arxiv.org/abs/1909.00024"}, {"title": "rethinking travel behavior modeling representations through embeddings", "id": "1909.00154", "abstract": "this paper introduces the concept of travel behavior embeddings, a method for re-representing discrete variables that are typically used in travel demand modeling, such as mode, trip purpose, education level, family type or occupation. this re-representation process essentially maps those variables into a latent space called the \\emph{embedding space}. the benefit of this is that such spaces allow for richer nuances than the typical transformations used in categorical variables (e.g. dummy encoding, contrasted encoding, principal components analysis). while the usage of latent variable representations is not new per se in travel demand modeling, the idea presented here brings several innovations: it is an entirely data driven algorithm; it is informative and consistent, since the latent space can be visualized and interpreted based on distances between different categories; it preserves interpretability of coefficients, despite being based on neural network principles; and it is transferrable, in that embeddings learned from one dataset can be reused for other ones, as long as travel behavior keeps consistent between the datasets.   the idea is strongly inspired on natural language processing techniques, namely the word2vec algorithm. such algorithm is behind recent developments such as in automatic translation or next word prediction. our method is demonstrated using a model choice model, and shows improvements of up to 60\\% with respect to initial likelihood, and up to 20% with respect to likelihood of the corresponding traditional model (i.e. using dummy variables) in out-of-sample evaluation. we provide a new python package, called pytre (python travel embeddings), that others can straightforwardly use to replicate our results or improve their own models. our experiments are themselves based on an open dataset (swissmetro).", "categories": "econ.em cs.cl cs.lg", "created": "2019-08-31", "updated": "", "authors": ["francisco c. pereira"], "url": "https://arxiv.org/abs/1909.00154"}, {"title": "mapping firms' locations in technological space: a topological analysis   of patent statistics", "id": "1909.00257", "abstract": "we propose a new method to characterize firms' inventive activities via topological data analysis (tda) that represents high-dimensional data in a shape graph. applying this method to 333 major firms' patents in 1976-2005 reveals substantial heterogeneity: some firms remain undifferentiated; others develop unique portfolios. firms with unique trajectories, which we define graph-theoretically as \"flares\" in the mapper graph, perform better. this association is statistically and economically significant, and continues to hold after we control for portfolio size and firm survivorship. comparison with existing techniques suggests the method's usefulness for data visualization and exploratory empirical research more generally.", "categories": "econ.em cs.dm math.at math.co", "created": "2019-08-31", "updated": "2020-09-18", "authors": ["emerson g. escolar", "yasuaki hiraoka", "mitsuru igami", "yasin ozcan"], "url": "https://arxiv.org/abs/1909.00257"}, {"title": "fixed-k inference for conditional extremal quantiles", "id": "1909.00294", "abstract": "we develop a new extreme value theory for repeated cross-sectional and panel data to construct asymptotically valid confidence intervals (cis) for conditional extremal quantiles from a fixed number $k$ of nearest-neighbor tail observations. as a by-product, we also construct cis for extremal quantiles of coefficients in linear random coefficient models. for any fixed $k$, the cis are uniformly valid without parametric assumptions over a set of nonparametric data generating processes associated with various tail indices. simulation studies show that our cis exhibit superior small-sample coverage and length properties than alternative nonparametric methods based on asymptotic normality. applying the proposed method to natality vital statistics, we study factors of extremely low birth weights. we find that signs of major effects are the same as those found in preceding studies based on parametric models, but with different magnitudes.", "categories": "econ.em", "created": "2019-08-31", "updated": "2020-07-19", "authors": ["yuya sasaki", "yulong wang"], "url": "https://arxiv.org/abs/1909.00294"}, {"title": "a two-stage market mechanism for electricity with renewable generation", "id": "1909.00508", "abstract": "we consider a two stage market mechanism for trading electricity including renewable generation as an alternative to the widely used multi-settlement market structure. the two stage market structure allows for recourse decisions by the market operator, which is not possible in today's markets. we allow for different generation cost curves in the forward and the real-time stage. we have considered costs of demand response programs, and black outs but have ignored network structure for the sake of simplicity. our first result is to show existence (by construction) of a sequential competitive equilibrium (sceq) in such a two-stage market. we then argue social welfare properties of such an sceq. we then design a market mechanism that achieves social welfare maximization when the market participants are non-strategic.", "categories": "cs.gt cs.sy econ.gn eess.sy q-fin.ec", "created": "2019-09-01", "updated": "", "authors": ["nathan dahlin", "rahul jain"], "url": "https://arxiv.org/abs/1909.00508"}, {"title": "buy-online-and-pick-up-in-store in omnichannel retailing", "id": "1909.00822", "abstract": "in this paper, we extend the model of gao and su (2016) and consider an omnichannel strategy in which inventory can be replenished when a retailer sells only in physical stores. with \"buy-online-and-pick-up-in-store\" (bops) having been introduced, consumers can choose to buy directly online, buy from a retailer using bops, or go directly to a store to make purchases without using bops. the retailer is able to select the inventory level to maximize the probability of inventory availability at the store. furthermore, the retailer can incur an additional cost to reduce the bops ordering lead time, which results in a lowered hassle cost for consumers who use bops. in conclusion, we found that there are two types of equilibrium: that in which all consumers go directly to the store without using bops and that in which all consumers use bops.", "categories": "econ.gn q-fin.ec", "created": "2019-09-02", "updated": "2019-09-04", "authors": ["yasuyuki kusuda"], "url": "https://arxiv.org/abs/1909.00822"}, {"title": "sortedeffects: sorted causal effects in r", "id": "1909.00836", "abstract": "chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. this method consists of reporting percentiles of the partial effects in addition to the average commonly used to summarize the heterogeneity in the partial effects. they also proposed to use the sorted effects to carry out classification analysis where the observational units are classified as most and least affected if their causal effects are above or below some tail sorted effects. the r package sortedeffects implements the estimation and inference methods therein and provides tools to visualize the results. this vignette serves as an introduction to the package and displays basic functionality of the functions within.", "categories": "econ.em stat.co", "created": "2019-09-02", "updated": "2019-11-06", "authors": ["shuowen chen", "victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "ye luo"], "url": "https://arxiv.org/abs/1909.00836"}, {"title": "rational inattention and perceptual distance", "id": "1909.00888", "abstract": "this paper uses an axiomatic foundation to create a new measure for the cost of learning that allows for multiple perceptual distances in a single choice environment so that some events can be harder to differentiate between than others. the new measure maintains the tractability of shannon's classic measure but produces richer choice predictions and identifies a new form of informational bias significant for welfare and counterfactual analysis.", "categories": "econ.th", "created": "2019-09-02", "updated": "2019-12-26", "authors": ["david walker-jones"], "url": "https://arxiv.org/abs/1909.00888"}, {"title": "co2 mitigation model for china's residential building sector", "id": "1909.01249", "abstract": "this paper aims to investigate the factors that can mitigate carbon-dioxide (co2) intensity and further assess cmrbs in china based on a household scale via decomposition analysis. here we show that: three types of housing economic indicators and the final emission factor significantly contributed to the decrease in co2 intensity in the residential building sector. in addition, the cmrbs from 2001-2016 was 1816.99 mtco2, and the average mitigation intensity during this period was 266.12 kgco2/household/year. furthermore, the energy-conservation and emission-mitigation strategy caused cmrbs to effectively increase and is the key to promoting a more significant emission mitigation in the future. overall, this paper covers the cmrbs assessment gap in china, and the proposed assessment model can be regarded as a reference for other countries and cities for measuring the retrospective co2 mitigation effect in residential buildings.", "categories": "econ.gn q-fin.ec", "created": "2019-09-03", "updated": "", "authors": ["minda ma", "weiguang cai"], "url": "https://arxiv.org/abs/1909.01249"}, {"title": "are bitcoins price predictable? evidence from machine learning   techniques using technical indicators", "id": "1909.01268", "abstract": "the uncertainties in future bitcoin price make it difficult to accurately predict the price of bitcoin. accurately predicting the price for bitcoin is therefore important for decision-making process of investors and market players in the cryptocurrency market. using historical data from 01/01/2012 to 16/08/2019, machine learning techniques (generalized linear model via penalized maximum likelihood, random forest, support vector regression with linear kernel, and stacking ensemble) were used to forecast the price of bitcoin. the prediction models employed key and high dimensional technical indicators as the predictors. the performance of these techniques were evaluated using mean absolute percentage error (mape), root mean square error (rmse), mean absolute error (mae), and coefficient of determination (r-squared). the performance metrics revealed that the stacking ensemble model with two base learner (random forest and generalized linear model via penalized maximum likelihood) and support vector regression with linear kernel as meta-learner was the optimal model for forecasting bitcoin price. the mape, rmse, mae, and r-squared values for the stacking ensemble model were 0.0191%, 15.5331 usd, 124.5508 usd, and 0.9967 respectively. these values show a high degree of reliability in predicting the price of bitcoin using the stacking ensemble model. accurately predicting the future price of bitcoin will yield significant returns for investors and market players in the cryptocurrency market.", "categories": "q-fin.st econ.em stat.ap stat.ml", "created": "2019-09-03", "updated": "", "authors": ["samuel asante gyamerah"], "url": "https://arxiv.org/abs/1909.01268"}, {"title": "bias and consistency in three-way gravity models", "id": "1909.01327", "abstract": "we study the incidental parameter problem in \"three-way\" poisson pseudo-maximum likelihood (\"ppml\") gravity models recently recommended for identifying the effects of trade policies and in other network panel data settings. despite the number and variety of fixed effects this model entails, we confirm it is consistent for small $t$ and we show it is in fact the only estimator among a wide range of pml gravity estimators that is generally consistent in this context when $t$ is small. at the same time, asymptotic confidence intervals in fixed-$t$ panels are not correctly centered at the true point estimates, and cluster-robust variance estimates used to construct standard errors are generally biased as well. we characterize each of these biases analytically and show both numerically and empirically that they are salient even for real-data settings with a large number of countries. we also offer practical remedies that can be used to obtain more reliable inferences of the effects of trade policies and other time-varying gravity variables.", "categories": "econ.em", "created": "2019-09-03", "updated": "2020-01-27", "authors": ["martin weidner", "thomas zylkin"], "url": "https://arxiv.org/abs/1909.01327"}, {"title": "testing nonparametric shape restrictions", "id": "1909.01675", "abstract": "we describe and examine a test for a general class of shape constraints, such as constraints on the signs of derivatives, u-(s-)shape, symmetry, quasi-convexity, log-convexity, $r$-convexity, among others, in a nonparametric framework using partial sums empirical processes. we show that, after a suitable transformation, its asymptotic distribution is a functional of the standard brownian motion, so that critical values are available. however, due to the possible poor approximation of the asymptotic critical values to the finite sample ones, we also describe a valid bootstrap algorithm.", "categories": "stat.me econ.em math.st stat.th", "created": "2019-09-04", "updated": "2020-06-08", "authors": ["tatiana komarova", "javier hidalgo"], "url": "https://arxiv.org/abs/1909.01675"}, {"title": "inference in differences-in-differences: how much should we trust in   independent clusters?", "id": "1909.01782", "abstract": "we analyze the conditions in which ignoring spatial correlation is problematic for inference in differences-in-differences models. we show that the relevance of spatial correlation for inference (when it is ignored) depends on the amount of spatial correlation that remains after we control for the time- and group-invariant unobservables. as a consequence, details such as the time frame used in the estimation, and the choice of the estimator, will be key determinants on the degree of distortions we should expect when spatial correlation is ignored. simulations with real datasets corroborate these conclusions. these findings provide a better understanding on when spatial correlation should be more problematic, and provide important guidelines on how to minimize inference problems due to spatial correlation.", "categories": "econ.em", "created": "2019-09-04", "updated": "2020-09-03", "authors": ["bruno ferman"], "url": "https://arxiv.org/abs/1909.01782"}, {"title": "scoring strategic agents", "id": "1909.01888", "abstract": "i introduce a model of scoring. an intermediary aggregates multiple features of a sender into a score. a receiver sees this score and takes a decision. the receiver wants his decision to match the sender's latent characteristic. but the sender wants the most favorable decision, and she can distort each of her features at a private cost. i characterize the receiver-optimal scoring rule. this rule underweights some features to deter sender distortion, and overweights other features to keep the score unbiased. the receiver prefers this score to seeing the sender's full features because the coarser information mitigates his commitment problem.", "categories": "econ.th", "created": "2019-09-04", "updated": "2019-11-08", "authors": ["ian ball"], "url": "https://arxiv.org/abs/1909.01888"}, {"title": "illiquid financial markets and monetary policy", "id": "1909.01889", "abstract": "this paper analyzes the role of money in asset markets characterized by search frictions. we develop a dynamic framework that brings together a model for illiquid financial assets `a la duffie, garleanu, and pedersen, and a search-theoretic model of monetary exchange `a la lagos and wright. the presence of decentralized financial markets generates an essential role for money, which helps investors re-balance their portfolios. we provide conditions that guarantee the existence of a monetary equilibrium. in this case, asset prices are always above their fundamental value, and this differential represents a liquidity premium. we are able to derive an asset pricing theory that delivers an explicit connection between monetary policy, asset prices, and welfare. we obtain a negative relationship between inflation and equilibrium asset prices. this key result stems from the complementarity between money and assets in our framework.", "categories": "econ.th", "created": "2019-09-04", "updated": "", "authors": ["athanasios geromichalos", "juan m. licari", "jose suarez-lledo"], "url": "https://arxiv.org/abs/1909.01889"}, {"title": "state drug policy effectiveness: comparative policy analysis of drug   overdose mortality", "id": "1909.01936", "abstract": "opioid overdose rates have reached an epidemic level and state-level policy innovations have followed suit in an effort to prevent overdose deaths. state-level drug law is a set of policies that may reinforce or undermine each other, and analysts have a limited set of tools for handling the policy collinearity using statistical methods. this paper uses a machine learning method called hierarchical clustering to empirically generate \"policy bundles\" by grouping states with similar sets of policies in force at a given time together for analysis in a 50-state, 10-year interrupted time series regression with drug overdose deaths as the dependent variable. policy clusters were generated from 138 binomial variables observed by state and year from the prescription drug abuse policy system. clustering reduced the policies to a set of 10 bundles. the approach allows for ranking of the relative effect of different bundles and is a tool to recommend those most likely to succeed. this study shows that a set of policies balancing medication assisted treatment, naloxone access, good samaritan laws, medication assisted treatment, prescription drug monitoring programs and legalization of medical marijuana leads to a reduced number of overdose deaths, but not until its second year in force.", "categories": "stat.ap cs.lg econ.em stat.ml", "created": "2019-09-03", "updated": "2020-10-05", "authors": ["jarrod olson", "po-hsu allen chen", "marissa white", "nicole brennan", "ning gong"], "url": "https://arxiv.org/abs/1909.01936"}, {"title": "using wasserstein generative adversarial networks for the design of   monte carlo simulations", "id": "1909.02210", "abstract": "when researchers develop new econometric methods it is common practice to compare the performance of the new methods to those of existing methods in monte carlo studies. the credibility of such monte carlo studies is often limited because of the freedom the researcher has in choosing the design. in recent years a new class of generative models emerged in the machine learning literature, termed generative adversarial networks (gans) that can be used to systematically generate artificial data that closely mimics real economic datasets, while limiting the degrees of freedom for the researcher and optionally satisfying privacy guarantees with respect to their training data. in addition if an applied researcher is concerned with the performance of a particular statistical method on a specific data set (beyond its theoretical properties in large samples), she may wish to assess the performance, e.g., the coverage rate of confidence intervals or the bias of the estimator, using simulated data which resembles her setting. tol illustrate these methods we apply wasserstein gans (wgans) to compare a number of different estimators for average treatment effects under unconfoundedness in three distinct settings (corresponding to three real data sets) and present a methodology for assessing the robustness of the results. in this example, we find that (i) there is not one estimator that outperforms the others in all three settings, so researchers should tailor their analytic approach to a given setting, and (ii) systematic simulation studies can be helpful for selecting among competing methods in this situation.", "categories": "econ.em stat.me", "created": "2019-09-05", "updated": "2020-07-21", "authors": ["susan athey", "guido imbens", "jonas metzger", "evan munro"], "url": "https://arxiv.org/abs/1909.02210"}, {"title": "an experiment on network density and sequential learning", "id": "1909.02220", "abstract": "we conduct a sequential social-learning experiment where subjects take turns guessing a hidden state based on private signals and the guesses of a subset of their predecessors. a network determines the observable predecessors, and we compare subjects' accuracy on sparse and dense networks. accuracy gains from social learning are twice as large on sparse networks compared to dense networks. models of naive inference where agents ignore correlation between observations predict this comparative static in network density, while the finding is difficult to reconcile with rational-learning models.", "categories": "econ.th cs.si econ.gn q-fin.ec", "created": "2019-09-05", "updated": "2019-10-05", "authors": ["krishna dasaratha", "kevin he"], "url": "https://arxiv.org/abs/1909.02220"}, {"title": "shrinkage estimation of network spillovers with factor structured errors", "id": "1909.02823", "abstract": "this paper explores the estimation of a panel data model with cross-sectional interaction that is flexible both in its approach to specifying the network of connections between cross-sectional units, and in controlling for unobserved heterogeneity. it is assumed that there are different sources of information available on a network, which can be represented in the form of multiple weights matrices. these matrices may reflect observed links, different measures of connectivity, groupings or other network structures, and the number of matrices may be increasing with sample size. a penalised quasi-maximum likelihood estimator is proposed which aims to alleviate the risk of network misspecification by shrinking the coefficients of irrelevant weights matrices to exactly zero. moreover, controlling for unobserved factors in estimation provides a safeguard against the misspecification that might arise from unobserved heterogeneity. the estimator is shown to be consistent and selection consistent as both $n$ and $t$ tend to infinity, and its limiting distribution is characterised. finite sample performance is assessed by means of a monte carlo simulation and the method is applied to study the prevalence of network spillovers in determining growth rates across countries.", "categories": "econ.em", "created": "2019-09-06", "updated": "2020-09-21", "authors": ["ayden higgins", "federico martellosio"], "url": "https://arxiv.org/abs/1909.02823"}, {"title": "modelling cooperation in a dynamic healthcare system", "id": "1909.03070", "abstract": "our research is concerned with studying behavioural changes within a dynamic system, i.e. health care, and their effects on the decision-making process. evolutionary game theory is applied to investigate the most probable strategy(ies) adopted by individuals in a finite population based on the interactions among them with an eye to modelling behaviour using the following metrics: cost of investment, cost of management, cost of treatment, reputation benefit for the provider(s), and the gained health benefit for the patient.", "categories": "cs.gt cs.ma econ.th math.ds", "created": "2019-09-06", "updated": "", "authors": ["zainab alalawi", "yifeng zeng", "the anh han", "aiman elragig"], "url": "https://arxiv.org/abs/1909.03070"}, {"title": "identifying different definitions of future in the assessment of future   economic conditions: application of pu learning and text mining", "id": "1909.03348", "abstract": "the economy watcher survey, which is a market survey published by the japanese government, contains \\emph{assessments of current and future economic conditions} by people from various fields. although this survey provides insights regarding economic policy for policymakers, a clear definition of the word \"future\" in future economic conditions is not provided. hence, the assessments respondents provide in the survey are simply based on their interpretations of the meaning of \"future.\" this motivated us to reveal the different interpretations of the future in their judgments of future economic conditions by applying weakly supervised learning and text mining. in our research, we separate the assessments of future economic conditions into economic conditions of the near and distant future using learning from positive and unlabeled data (pu learning). because the dataset includes data from several periods, we devised new architecture to enable neural networks to conduct pu learning based on the idea of multi-task learning to efficiently learn a classifier. our empirical analysis confirmed that the proposed method could separate the future economic conditions, and we interpreted the classification results to obtain intuitions for policymaking.", "categories": "econ.em", "created": "2019-09-07", "updated": "2020-04-22", "authors": ["masahiro kato"], "url": "https://arxiv.org/abs/1909.03348"}, {"title": "modular structure in labour networks reveals skill basins", "id": "1909.03379", "abstract": "labour networks, where industries are connected based on worker transitions, have been previously deployed to study the evolution of industrial structure ('related diversification') across cities and regions. beyond estimating skill-overlap between industry pairs, such networks characterise the structure of inter-industry labour mobility and knowledge diffusion in an economy. here we investigate the structure of the network of inter-industry worker flows in the irish economy, seeking to identify groups of industries exhibiting high internal mobility and skill-overlap. we argue that these industry clusters represent skill basins in which skilled labour circulate and diffuse knowledge, and delineate the size of the skilled labour force available to an industry.   deploying a multi-scale community detection algorithm, we uncover a hierarchical modular structure composed of clusters of industries at different scales. at one end of the scale, we observe a macro division of the economy into services and manufacturing. at the other end of the scale, we detect a fine-grained partition of industries into tightly knit groupings. in particular, we find workers from finance, computing, and the public sector rarely transition into the extended economy. hence, these industries form isolated clusters which are disconnected from the broader economy, posing a range of risks to both workers and firms. finally, we develop a methodology based on industry growth patterns to reveal the optimal scale at which labour pooling operates in terms of skill-sharing and skill-seeking within industry clusters.", "categories": "econ.gn q-fin.ec", "created": "2019-09-07", "updated": "", "authors": ["neave o'clery", "eoin flaherty", "stephen kinsella"], "url": "https://arxiv.org/abs/1909.03379"}, {"title": "education projects for sustainable development: evidence from ural   federal university", "id": "1909.03429", "abstract": "sustainable development is a worldwide recognized social and political goal, discussed in both academic and political discourse and with much research on the topic related to sustainable development in higher education. since mental models are formed more effectively at school age, we propose a new way of thinking that will help achieve this goal. this paper was written in the context of russia, where the topic of sustainable development in education is poorly developed. the authors used the classical methodology of the case analysis. the analysis and interpretation of the results were conducted in the framework of the institutional theory. presented is the case of ural federal university, which has been working for several years on the creation of a device for the purification of industrial sewer water in the framework of an initiative student group. schoolchildren recently joined the program, and such projects have been called university-to-school projects. successful solutions of inventive tasks contribute to the formation of mental models. this case has been analyzed in terms of institutionalism, and the authors argue for the primacy of mental institutions over normative ones during sustainable society construction. this case study is the first to analyze a partnership between a federal university and local schools regarding sustainable education and proposes a new way of thinking.", "categories": "econ.gn q-fin.ec", "created": "2019-09-08", "updated": "2019-09-21", "authors": ["marina volkova", "jol stoffers", "dmitry kochetkov"], "url": "https://arxiv.org/abs/1909.03429"}, {"title": "multiway cluster robust double/debiased machine learning", "id": "1909.03489", "abstract": "this paper investigates double/debiased machine learning (dml) under multiway clustered sampling environments. we propose a novel multiway cross fitting algorithm and a multiway dml estimator based on this algorithm. we also develop a multiway cluster robust standard error formula. simulations indicate that the proposed procedure has favorable finite sample performance. applying the proposed method to market share data for demand analysis, we obtain larger two-way cluster robust standard errors than non-robust ones.", "categories": "econ.em", "created": "2019-09-08", "updated": "2020-03-04", "authors": ["harold d. chiang", "kengo kato", "yukun ma", "yuya sasaki"], "url": "https://arxiv.org/abs/1909.03489"}, {"title": "an economic topology of the brexit vote", "id": "1909.03490", "abstract": "a quest to understand the decision of the uk to leave the european union, brexit, in the referendum of june 2016 has occupied academics, the media and politicians alike. as the debate about what the future relationship will look like rages, the referendum is given renewed importance as an indicator of the likely success, or otherwise, of any forward plans. topological data analysis offers an ability to faithfully extract maximal information from complex multi-dimensional datasets of the type that have been gathered on brexit voting. within the complexity it is shown that support for leave drew from a far more similar demographic than remain. obtaining votes from this concise set was more straightforward for leave campaigners than was remain's task of mobilising a diverse group to oppose brexit. broad patterns are consistent with extant empirical work, but the strength of tda ball mapper means that evidence is offered to enrich the narrative on immobility, and being ``left-behind'' by eu membership, that could not be found before. a detailed understanding emerges which comments robustly on why britain voted as it did. a start point for the policy development that must follow is given.", "categories": "econ.em", "created": "2019-09-08", "updated": "", "authors": ["pawel dlotko", "simon rudkin", "wanling qiu"], "url": "https://arxiv.org/abs/1909.03490"}, {"title": "a fixed-point policy-iteration-type algorithm for symmetric nonzero-sum   stochastic impulse control games", "id": "1909.03574", "abstract": "nonzero-sum stochastic differential games with impulse controls offer a realistic and far-reaching modelling framework for applications within finance, energy markets, and other areas, but the difficulty in solving such problems has hindered their proliferation. semi-analytical approaches make strong assumptions pertaining to very particular cases. to the author's best knowledge, the only numerical method in the literature is the heuristic one we put forward to solve an underlying system of quasi-variational inequalities. focusing on symmetric games, this paper presents a simpler, more precise and efficient fixed-point policy-iteration-type algorithm which removes the strong dependence on the initial guess and the relaxation scheme of the previous method. a rigorous convergence analysis is undertaken with natural assumptions on the players strategies, which admit graph-theoretic interpretations in the context of weakly chained diagonally dominant matrices. a novel provably convergent single-player impulse control solver is also provided. the main algorithm is used to compute with high precision equilibrium payoffs and nash equilibria of otherwise very challenging problems, and even some which go beyond the scope of the currently available theory.", "categories": "math.na cs.na econ.gn math.oc q-fin.ec", "created": "2019-09-08", "updated": "2020-06-15", "authors": ["diego zabaljauregui"], "url": "https://arxiv.org/abs/1909.03574"}, {"title": "systemic risk clustering of china internet financial based on t-sne   machine learning algorithm", "id": "1909.03808", "abstract": "with the rapid development of internet finance, a large number of studies have shown that internet financial platforms have different financial systemic risk characteristics when they are subject to macroeconomic shocks or fragile internal crisis. from the perspective of regional development of internet finance, this paper uses t-sne machine learning algorithm to obtain data mining of china's internet finance development index involving 31 provinces and 335 cities and regions. the conclusion of the peak and thick tail characteristics, then proposed three classification risks of internet financial systemic risk, providing more regionally targeted recommendations for the systematic risk of internet finance.", "categories": "q-fin.st econ.em", "created": "2019-08-30", "updated": "", "authors": ["mi chuanmin", "xu runjie", "lin qingtong"], "url": "https://arxiv.org/abs/1909.03808"}, {"title": "tree-based synthetic control methods: consequences of moving the us   embassy", "id": "1909.03968", "abstract": "we recast the synthetic controls for evaluating policies as a counterfactual prediction problem and replace its linear regression with a non-parametric model inspired by machine learning. the proposed method enables us to achieve more accurate counterfactual predictions. we apply our method to a highly-debated policy: the movement of the us embassy to jerusalem. in israel and palestine, we find that the average number of weekly conflicts has increased by roughly 103 % over 48 weeks since the movement was announced on december 6, 2017. using conformal inference tests, we justify our model and find the increase to be statistically significant.", "categories": "econ.em", "created": "2019-09-09", "updated": "2020-04-17", "authors": ["nicolaj n\u00f8rgaard m\u00fchlbach"], "url": "https://arxiv.org/abs/1909.03968"}, {"title": "climate policy under spatial heat transport: cooperative and   noncooperative regional outcomes", "id": "1909.04009", "abstract": "we build a novel stochastic dynamic regional integrated assessment model (iam) of the climate and economic system including a number of important climate science elements that are missing in most iams. these elements are spatial heat transport from the equator to the poles, sea level rise, permafrost thaw and tipping points. we study optimal policies under cooperation and noncooperation between two regions (the north and the tropic-south) in the face of risks and recursive utility. we introduce a new general computational algorithm to find feedback nash equilibrium. our results suggest that when the elements of climate science are ignored, important policy variables such as the optimal regional carbon tax and adaptation could be seriously biased. we also find the regional carbon tax is significantly smaller in the feedback nash equilibrium than in the social planner's problem in each region, and the north has higher carbon taxes than the tropic-south.", "categories": "econ.gn q-fin.ec", "created": "2019-09-09", "updated": "", "authors": ["yongyang cai", "william brock", "anastasios xepapadeas", "kenneth judd"], "url": "https://arxiv.org/abs/1909.04009"}, {"title": "taxing dissent: the impact of a social media tax in uganda", "id": "1909.04107", "abstract": "we examine the impact of a new tool for suppressing the expression of dissent---a daily tax on social media use. using a synthetic control framework, we estimate that the tax reduced the number of georeferenced twitter users in uganda by 13 percent. the estimated treatment effects are larger for poorer and less frequent users. despite the overall decline in twitter use, tweets referencing collective action increased by 31 percent and observed protests increased by 47 percent. these results suggest that taxing social media use may not be an effective tool for reducing political dissent.", "categories": "econ.gn q-fin.ec", "created": "2019-09-09", "updated": "", "authors": ["levi boxell", "zachary steinert-threlkeld"], "url": "https://arxiv.org/abs/1909.04107"}, {"title": "dynamics of reallocation within india's income distribution", "id": "1909.04452", "abstract": "we investigate the nature and extent of reallocation occurring within the indian income distribution, with a particular focus on the dynamics of the bottom of the distribution. specifically, we use a stochastic model of geometric brownian motion with a reallocation parameter that was constructed to capture the quantum and direction of composite redistribution implied in the income distribution. it is well known that inequality has been rising in india in the recent past, but the assumption has been that while the rich benefit more than proportionally from economic growth, the poor are also better off than before. findings from our model refute this, as we find that since the early 2000s reallocation has consistently been negative, and that the indian income distribution has entered a regime of perverse redistribution of resources from the poor to the rich. outcomes from the model indicate not only that income shares of the bottom decile (~1%) and bottom percentile (~0.03%) are at historic lows, but also that real incomes of the bottom decile (-2.5%) and percentile (-6%) have declined in the 2000s. we validate these findings using income distribution data and find support for our contention of persistent negative reallocation in the 2000s. we characterize these findings in the context of increasing informalization of the workforce in the formal manufacturing and service sectors, as well as the growing economic insecurity of the agricultural workforce in india. significant structural changes will be required to address this phenomenon.", "categories": "econ.em physics.soc-ph", "created": "2019-09-10", "updated": "2020-10-07", "authors": ["anand sahasranaman", "henrik jeldtoft jensen"], "url": "https://arxiv.org/abs/1909.04452"}, {"title": "is culture related to strong science? an empirical investigation", "id": "1909.04521", "abstract": "national culture is among those societal factors which could influence research and innovation activities. in this study, we investigated the associations of two national culture models with citation impact of nations (measured by the proportion of papers belonging to the 10% and 1% most cited papers in the corresponding fields, pptop 10% and pptop 1%). bivariate statistical analyses showed that of six hofstede's national culture dimensions (hncd), three dimensions of power distance, individualism, and uncertainty avoidance had statistically significant associations with citation impact of nations. the study also revealed that of two inglehart-welzel cultural values (iwcv), the value survival versus self-expression is statistically significantly related to citation impact indicators. we additionally calculated multiple regression analyses controlling for the possible effects of confounding factors including international migrant stock, investments in research and development, number of researchers, international co-authorships, and national self-citations. the results revealed that the statistically significant associations of hncd with citation impact indicators disappeared. but the statistically significant relationship between survivals versus self-expression values and citation impact indicators remained stable even after controlling for the confounding variables. thus, the freedom of expression and trust in society might contribute to better scholarly communication systems, higher level of international collaborations, and further quality research.", "categories": "cs.dl cs.it econ.gn math.it q-fin.ec", "created": "2019-09-09", "updated": "2020-08-24", "authors": ["mahmood khosrowjerdi", "lutz bornmann"], "url": "https://arxiv.org/abs/1909.04521"}, {"title": "virtual historical simulation for estimating the conditional var of   large portfolios", "id": "1909.04661", "abstract": "in order to estimate the conditional risk of a portfolio's return, two strategies can be advocated. a multivariate strategy requires estimating a dynamic model for the vector of risk factors, which is often challenging, when at all possible, for large portfolios. a univariate approach based on a dynamic model for the portfolio's return seems more attractive. however, when the combination of the individual returns is time varying, the portfolio's return series is typically non stationary which may invalidate statistical inference. an alternative approach consists in reconstituting a \"virtual portfolio\", whose returns are built using the current composition of the portfolio and for which a stationary dynamic model can be estimated.   this paper establishes the asymptotic properties of this method, that we call virtual historical simulation. numerical illustrations on simulated and real data are provided.", "categories": "econ.em math.st stat.th", "created": "2019-09-10", "updated": "", "authors": ["christian francq", "jean-michel zakoian"], "url": "https://arxiv.org/abs/1909.04661"}, {"title": "regression to the mean's impact on the synthetic control method: bias   and sensitivity analysis", "id": "1909.04706", "abstract": "to make informed policy recommendations from observational data, we must be able to discern true treatment effects from random noise and effects due to confounding. difference-in-difference techniques which match treated units to control units based on pre-treatment outcomes, such as the synthetic control approach have been presented as principled methods to account for confounding. however, we show that use of synthetic controls or other matching procedures can introduce regression to the mean (rtm) bias into estimates of the average treatment effect on the treated. through simulations, we show rtm bias can lead to inflated type i error rates as well as decreased power in typical policy evaluation settings. further, we provide a novel correction for rtm bias which can reduce bias and attain appropriate type i error rates. this correction can be used to perform a sensitivity analysis which determines how results may be affected by rtm. we use our proposed correction and sensitivity analysis to reanalyze data concerning the effects of california's proposition 99, a large-scale tobacco control program, on statewide smoking rates.", "categories": "stat.me econ.em", "created": "2019-09-10", "updated": "", "authors": ["nicholas illenberger", "dylan s. small", "pamela a. shaw"], "url": "https://arxiv.org/abs/1909.04706"}, {"title": "linear equilibria for dynamic lqg games with asymmetric information and   dependent types", "id": "1909.04834", "abstract": "we consider a non-zero-sum linear quadratic gaussian (lqg) dynamic game with asymmetric information. each player observes privately a noisy version of a (hidden) state of the world $v$, resulting in dependent private observations. we study perfect bayesian equilibria (pbe) for this game with equilibrium strategies that are linear in players' private estimates of $v$. the main difficulty arises from the fact that players need to construct estimates on other players' estimate on $v$, which in turn would imply that an infinite hierarchy of estimates on estimates needs to be constructed, rendering the problem unsolvable. we show that this is not the case: each player's estimate on other players' estimates on $v$ can be summarized into her own estimate on $v$ and some appropriately defined public information. based on this finding we characterize the pbe through a backward/forward algorithm akin to dynamic programming for the standard lqg control problem. unlike the standard lqg problem, however, kalman filter covariance matrices, as well as some other required quantities, are observation-dependent and thus cannot be evaluated off-line through a forward recursion.", "categories": "econ.gn cs.sy eess.sy q-fin.ec", "created": "2019-09-10", "updated": "", "authors": ["nasimeh heydaribeni", "achilleas anastasopoulos"], "url": "https://arxiv.org/abs/1909.04834"}, {"title": "bayesian inference on volatility in the presence of infinite jump   activity and microstructure noise", "id": "1909.04853", "abstract": "volatility estimation based on high-frequency data is key to accurately measure and control the risk of financial assets. a l\\'{e}vy process with infinite jump activity and microstructure noise is considered one of the simplest, yet accurate enough, models for financial data at high-frequency. utilizing this model, we propose a \"purposely misspecified\" posterior of the volatility obtained by ignoring the jump-component of the process. the misspecified posterior is further corrected by a simple estimate of the location shift and re-scaling of the log likelihood. our main result establishes a bernstein-von mises (bvm) theorem, which states that the proposed adjusted posterior is asymptotically gaussian, centered at a consistent estimator, and with variance equal to the inverse of the fisher information. in the absence of microstructure noise, our approach can be extended to inferences of the integrated variance of a general it\\^o semimartingale. simulations are provided to demonstrate the accuracy of the resulting credible intervals, and the frequentist properties of the approximate bayesian inference based on the adjusted posterior.", "categories": "math.st econ.em q-fin.st stat.th", "created": "2019-09-11", "updated": "", "authors": ["qi wang", "jos\u00e9 e. figueroa-l\u00f3pez", "todd kuffner"], "url": "https://arxiv.org/abs/1909.04853"}, {"title": "estimating the volatility of bitcoin using garch models", "id": "1909.04903", "abstract": "in this paper, an application of three garch-type models (sgarch, igarch, and tgarch) with student t-distribution, generalized error distribution (ged), and normal inverse gaussian (nig) distribution are examined. the new development allows for the modeling of volatility clustering effects, the leptokurtic and the skewed distributions in the return series of bitcoin. comparative to the two distributions, the normal inverse gaussian distribution captured adequately the fat tails and skewness in all the garch type models. the tgarch model was the best model as it described the asymmetric occurrence of shocks in the bitcoin market. that is, the response of investors to the same amount of good and bad news are distinct. from the empirical results, it can be concluded that tgarch-nig was the best model to estimate the volatility in the return series of bitcoin. generally, it would be optimal to use the nig distribution in garch type models since time series of most cryptocurrency are leptokurtic.", "categories": "q-fin.st econ.em", "created": "2019-09-11", "updated": "2019-10-04", "authors": ["samuel asante gyamerah"], "url": "https://arxiv.org/abs/1909.04903"}, {"title": "matching estimators with few treated and many control observations", "id": "1909.05093", "abstract": "we analyze the properties of matching estimators when there are few treated, but many control observations. we show that, under standard assumptions, the nearest neighbor matching estimator for the average treatment effect on the treated is asymptotically unbiased in this framework. however, when the number of treated observations is fixed, the estimator is not consistent, and it is generally not asymptotically normal. since standard inference methods are inadequate, we propose alternative inference methods that are asymptotically valid in this framework. we discuss in details the advantages and disadvantages of these methods relative to standard alternatives for inference. we consider the implications of our findings for other types of matching estimators and for synthetic control applications.", "categories": "econ.em", "created": "2019-09-11", "updated": "2020-09-13", "authors": ["bruno ferman"], "url": "https://arxiv.org/abs/1909.05093"}, {"title": "validating weak-form market efficiency in united states stock markets   with trend deterministic price data and machine learning", "id": "1909.05151", "abstract": "the efficient market hypothesis has been a staple of economics research for decades. in particular, weak-form market efficiency -- the notion that past prices cannot predict future performance -- is strongly supported by econometric evidence. in contrast, machine learning algorithms implemented to predict stock price have been touted, to varying degrees, as successful. moreover, some data scientists boast the ability to garner above-market returns using price data alone. this study endeavors to connect existing econometric research on weak-form efficient markets with data science innovations in algorithmic trading. first, a traditional exploration of stationarity in stock index prices over the past decade is conducted with augmented dickey-fuller and variance ratio tests. then, an algorithmic trading platform is implemented with the use of five machine learning algorithms. econometric findings identify potential stationarity, hinting technical evaluation may be possible, though algorithmic trading results find little predictive power in any machine learning model, even when using trend-specific metrics. accounting for transaction costs and risk, no system achieved above-market returns consistently. our findings reinforce the validity of weak-form market efficiency.", "categories": "q-fin.st cs.ce cs.lg econ.em", "created": "2019-09-11", "updated": "", "authors": ["samuel showalter", "jeffrey gropp"], "url": "https://arxiv.org/abs/1909.05151"}, {"title": "de-biased machine learning in instrumental variable models for treatment   effects", "id": "1909.05244", "abstract": "instrumental variable identification is a strategy in causal statistics for estimating the counterfactual effect of treatment $d$ on output $y$ controlling for covariates $\\mathbf{x}$ using observational data. even in the presence of an unmeasured confounder of $(y,d)$, the treatment effect on the subpopulation of compliers can nonetheless be identified if an instrumental variable $z$ is available. we introduce a de-biased machine learning (dml) approach to estimating complier parameters with high-dimensional data. complier parameters include local average treatment effect, average complier characteristics, and complier counterfactual outcome distributions. in our approach, the de-biasing is itself performed by machine learning, a variant called automatic de-biased machine learning (auto-dml). we prove our estimator is consistent, asymptotically normal, and semi-parametrically efficient. in experiments, our estimator outperforms state-of-the-art alternatives, and it does not require ad hoc trimming or censoring of a learned propensity score. we use it to estimate the effect of 401(k) participation on the distribution of net financial assets.", "categories": "stat.ml cs.lg econ.em math.st stat.th", "created": "2019-09-10", "updated": "2020-06-11", "authors": ["rahul singh", "liyang sun"], "url": "https://arxiv.org/abs/1909.05244"}, {"title": "recovering preferences from finite data", "id": "1909.05457", "abstract": "we study preferences recovered from finite choice experiments and provide sufficient conditions for convergence to a unique underlying \"true\" preference. our conditions are weak, and therefore valid in a wide range of economic environments. we develop applications to expected utility theory, choice over consumption bundles, menu choice and intertemporal consumption. our framework unifies the revealed preference tradition with models that allow for errors.", "categories": "econ.th econ.em", "created": "2019-09-12", "updated": "2019-12-10", "authors": ["christopher p. chambers", "federico echenique", "nicolas lambert"], "url": "https://arxiv.org/abs/1909.05457"}, {"title": "quantile regression methods for first-price auctions", "id": "1909.05542", "abstract": "the paper proposes a quantile-regression inference framework for first-price auctions with symmetric risk-neutral bidders under the independent private-value paradigm. it is first shown that a private-value quantile regression generates a quantile regression for the bids. the private-value quantile regression can be easily estimated from the bid quantile regression and its derivative with respect to the quantile level. this also allows to test for various specification or exogeneity null hypothesis using the observed bids in a simple way. a new local polynomial technique is proposed to estimate the latter over the whole quantile level interval. plug-in estimation of functionals is also considered, as needed for the expected revenue or the case of crra risk-averse bidders, which is amenable to our framework. a quantile-regression analysis to usfs timber is found more appropriate than the homogenized-bid methodology and illustrates the contribution of each explanatory variables to the private-value distribution. linear interactive sieve extensions are proposed and studied in the appendices.", "categories": "econ.em", "created": "2019-09-12", "updated": "2020-09-23", "authors": ["nathalie gimenes", "emmanuel guerre"], "url": "https://arxiv.org/abs/1909.05542"}, {"title": "estimation and applications of quantile regression for binary   longitudinal data", "id": "1909.05560", "abstract": "this paper develops a framework for quantile regression in binary longitudinal data settings. a novel markov chain monte carlo (mcmc) method is designed to fit the model and its computational efficiency is demonstrated in a simulation study. the proposed approach is flexible in that it can account for common and individual-specific parameters, as well as multivariate heterogeneity associated with several covariates. the methodology is applied to study female labor force participation and home ownership in the united states. the results offer new insights at the various quantiles, which are of interest to policymakers and researchers alike.", "categories": "econ.em stat.me", "created": "2019-09-12", "updated": "", "authors": ["mohammad arshad rahman", "angela vossmeyer"], "url": "https://arxiv.org/abs/1909.05560"}, {"title": "discrete choice prox-functions on the simplex", "id": "1909.05591", "abstract": "we derive new prox-functions on the simplex from additive random utility models of discrete choice. they are convex conjugates of the corresponding surplus functions. in particular, we explicitly derive the convexity parameter of discrete choice prox-functions associated with generalized extreme value models, and specifically with generalized nested logit models. incorporated into subgradient schemes, discrete choice prox-functions lead to natural probabilistic interpretations of the iteration steps. as illustration we discuss an economic application of discrete choice prox-functions in consumer theory. the dual averaging scheme from convex programming naturally adjusts demand within a consumption cycle.", "categories": "math.oc econ.th", "created": "2019-09-12", "updated": "", "authors": ["david m\u00fcller", "yurii nesterov", "vladimir shikhman"], "url": "https://arxiv.org/abs/1909.05591"}, {"title": "the emergence of innovation complexity at different geographical and   technological scales", "id": "1909.05604", "abstract": "we define a novel quantitative strategy inspired by the ecological notion of nestedness to single out the scale at which innovation complexity emerges from the aggregation of specialized building blocks. our analysis not only suggests that the innovation space can be interpreted as a natural system in which advantageous capabilities are selected by evolutionary pressure, but also that the emerging structure of capabilities is not independent of the scale of observation at which they are observed. expanding on this insight allows us to understand whether the capabilities characterizing the innovation space at a given scale are compatible with a complex evolutionary dynamics or, rather, a set of essentially independent activities allowing to reduce the system at that scale to a set of disjoint non interacting sub-systems. this yields a measure of the innovation complexity of the system, i.e. of the degree of interdependence between the sets of capabilities underlying the system's building blocks.", "categories": "econ.gn nlin.ao physics.soc-ph q-bio.pe q-fin.ec", "created": "2019-09-12", "updated": "", "authors": ["emanuele pugliese", "lorenzo napolitano", "matteo chinazzi", "guido chiarotti"], "url": "https://arxiv.org/abs/1909.05604"}, {"title": "a consistent lm type specification test for semiparametric panel data   models", "id": "1909.05649", "abstract": "this paper develops a consistent series-based specification test for semiparametric panel data models with fixed effects. the test statistic resembles the lagrange multiplier (lm) test statistic in parametric models and is based on a quadratic form in the restricted model residuals. the use of series methods facilitates both estimation of the null model and computation of the test statistic. the asymptotic distribution of the test statistic is standard normal, so that appropriate critical values can easily be computed. the projection property of series estimators allows me to develop a degrees of freedom correction. this correction makes it possible to account for the estimation variance and obtain refined asymptotic results. it also substantially improves the finite sample performance of the test.", "categories": "econ.em", "created": "2019-09-12", "updated": "", "authors": ["ivan korolev"], "url": "https://arxiv.org/abs/1909.05649"}, {"title": "a new concept of technology with systemic-purposeful perpsective:   theory, examples and empirical application", "id": "1909.05689", "abstract": "although definitions of technology exist to explain the patterns of technological innovations, there is no general definition that explain the role of technology for humans and other animal species in environment. the goal of this study is to suggest a new concept of technology with a systemic-purposeful perspective for technology analysis. technology here is a complex system of artifact, made and_or used by living systems, that is composed of more than one entity or sub-system and a relationship that holds between each entity and at least one other entity in the system, selected considering practical, technical and_or economic characteristics to satisfy needs, achieve goals and_or solve problems of users for purposes of adaptation and_or survival in environment. technology t changes current modes of cognition and action to enable makers and_or users to take advantage of important opportunities or to cope with consequential environmental threats. technology, as a complex system, is formed by different elements given by incremental and radical innovations. technological change generates the progress from a system t1 to t2, t3, etc. driven by changes of technological trajectories and technological paradigms. several examples illustrate here these concepts and a simple model with a preliminary empirical analysis shows how to operationalize the suggested definition of technology. overall, then, the role of adaptation (i.e. reproductive advantage) can be explained as a main driver of technology use for adopters to take advantage of important opportunities or to cope with environmental threats. this study begins the process of clarifying and generalizing, as far as possible, the concept of technology with a new perspective that it can lay a foundation for the development of more sophisticated concepts and theories to explain technological and economic change in environment.", "categories": "econ.gn q-fin.ec", "created": "2019-09-11", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1909.05689"}, {"title": "fast algorithms for the quantile regression process", "id": "1909.05782", "abstract": "the widespread use of quantile regression methods depends crucially on the existence of fast algorithms. despite numerous algorithmic improvements, the computation time is still non-negligible because researchers often estimate many quantile regressions and use the bootstrap for inference. we suggest two new fast algorithms for the estimation of a sequence of quantile regressions at many quantile indexes. the first algorithm applies the preprocessing idea of portnoy and koenker (1997) but exploits a previously estimated quantile regression to guess the sign of the residuals. this step allows for a reduction of the effective sample size. the second algorithm starts from a previously estimated quantile regression at a similar quantile index and updates it using a single newton-raphson iteration. the first algorithm is exact, while the second is only asymptotically equivalent to the traditional quantile regression estimator. we also apply the preprocessing idea to the bootstrap by using the sample estimates to guess the sign of the residuals in the bootstrap sample. simulations show that our new algorithms provide very large improvements in computation time without significant (if any) cost in the quality of the estimates. for instance, we divide by 100 the time required to estimate 99 quantile regressions with 20 regressors and 50,000 observations.", "categories": "econ.em stat.co stat.me", "created": "2019-09-12", "updated": "2020-04-06", "authors": ["victor chernozhukov", "iv\u00e1n fern\u00e1ndez-val", "blaise melly"], "url": "https://arxiv.org/abs/1909.05782"}, {"title": "constrained pseudo-market equilibrium", "id": "1909.05986", "abstract": "we propose a market solution to the problem of resource allocation subject to constraints, such as considerations of diversity or geographical distribution. constraints give rise to pecuniary externalities, which are internalized via prices. agents pay to the extent that their purchases affect the value (at equilibrium prices) of the relevant constraints. the result is a constrained-efficient market equilibrium outcome. the outcome is fair whenever the constraints do not single out individual agents, which happens, for example with geographical distribution constraints. in economies with endowments, moreover, we can address participation constraints. our equilibrium outcomes are then constrained efficient and approximately individually rational.", "categories": "econ.th", "created": "2019-09-12", "updated": "2020-06-01", "authors": ["federico echenique", "antonio miralles", "jun zhang"], "url": "https://arxiv.org/abs/1909.05986"}, {"title": "the optimal deterrence of crime: a focus on the time preference of dwi   offenders", "id": "1909.06509", "abstract": "we develop a general model for finding the optimal penal strategy based on the behavioral traits of the offenders. we focus on how the discount rate (level of time discounting) affects criminal propensity on the individual level, and how the aggregation of these effects influences criminal activities on the population level. the effects are aggregated based on the distribution of discount rate among the population. we study this distribution empirically through a survey with 207 participants, and we show that it follows zero-inflated exponential distribution. we quantify the effectiveness of the penal strategy as its net utility for the population, and show how this quantity can be maximized. when we apply the maximization procedure on the offense of impaired driving (dwi), we discover that the effectiveness of dwi deterrence depends critically on the amount of fine and prison condition.", "categories": "econ.gn econ.th q-fin.ec", "created": "2019-09-13", "updated": "2019-09-20", "authors": ["yuqing wang", "yan ru pei"], "url": "https://arxiv.org/abs/1909.06509"}, {"title": "comparing the forecasting of cryptocurrencies by bayesian time-varying   volatility models", "id": "1909.06599", "abstract": "this paper studies the forecasting ability of cryptocurrency time series. this study is about the four most capitalized cryptocurrencies: bitcoin, ethereum, litecoin and ripple. different bayesian models are compared, including models with constant and time-varying volatility, such as stochastic volatility and garch. moreover, some crypto-predictors are included in the analysis, such as s\\&p 500 and nikkei 225. in this paper the results show that stochastic volatility is significantly outperforming the benchmark of var in both point and density forecasting. using a different type of distribution, for the errors of the stochastic volatility the student-t distribution came out to be outperforming the standard normal approach.", "categories": "econ.em econ.gn q-fin.ec q-fin.gn", "created": "2019-09-14", "updated": "", "authors": ["rick bohte", "luca rossini"], "url": "https://arxiv.org/abs/1909.06599"}, {"title": "statistical inference for statistical decisions", "id": "1909.06853", "abstract": "the wald development of statistical decision theory addresses decision making with sample data. wald's concept of a statistical decision function (sdf) embraces all mappings of the form [data -> decision]. an sdf need not perform statistical inference; that is, it need not use data to draw conclusions about the true state of nature. inference-based sdfs have the sequential form [data -> inference -> decision]. this paper motivates inference-based sdfs as practical procedures for decision making that may accomplish some of what wald envisioned. the paper first addresses binary choice problems, where all sdfs may be viewed as hypothesis tests. it next considers as-if optimization, which uses a point estimate of the true state as if the estimate were accurate. it then extends this idea to as-if maximin and minimax-regret decisions, which use point estimates of some features of the true state as if they were accurate. the paper primarily uses finite-sample maximum regret to evaluate the performance of inference-based sdfs. to illustrate abstract ideas, it presents specific findings concerning treatment choice and point prediction with sample data.", "categories": "econ.em stat.me", "created": "2019-09-15", "updated": "", "authors": ["charles f. manski"], "url": "https://arxiv.org/abs/1909.06853"}, {"title": "deep neural networks for choice analysis: architectural design with   alternative-specific utility functions", "id": "1909.07481", "abstract": "whereas deep neural network (dnn) is increasingly applied to choice analysis, it is challenging to reconcile domain-specific behavioral knowledge with generic-purpose dnn, to improve dnn's interpretability and predictive power, and to identify effective regularization methods for specific tasks. this study designs a particular dnn architecture with alternative-specific utility functions (asu-dnn) by using prior behavioral knowledge. unlike a fully connected dnn (f-dnn), which computes the utility value of an alternative k by using the attributes of all the alternatives, asu-dnn computes it by using only k's own attributes. theoretically, asu-dnn can dramatically reduce the estimation error of f-dnn because of its lighter architecture and sparser connectivity. empirically, asu-dnn has 2-3% higher prediction accuracy than f-dnn over the whole hyperparameter space in a private dataset that we collected in singapore and a public dataset in r mlogit package. the alternative-specific connectivity constraint, as a domain-knowledge-based regularization method, is more effective than the most popular generic-purpose explicit and implicit regularization methods and architectural hyperparameters. asu-dnn is also more interpretable because it provides a more regular substitution pattern of travel mode choices than f-dnn does. the comparison between asu-dnn and f-dnn can also aid in testing the behavioral knowledge. our results reveal that individuals are more likely to compute utility by using an alternative's own attributes, supporting the long-standing practice in choice modeling. overall, this study demonstrates that prior behavioral knowledge could be used to guide the architecture design of dnn, to function as an effective domain-knowledge-based regularization method, and to improve both the interpretability and predictive power of dnn in choice analysis.", "categories": "cs.lg cs.ai econ.gn q-fin.ec stat.ml", "created": "2019-09-16", "updated": "", "authors": ["shenhao wang", "jinhua zhao"], "url": "https://arxiv.org/abs/1909.07481"}, {"title": "arrow, hausdorff, and ambiguities in the choice of preferred states in   complex systems", "id": "1909.07771", "abstract": "arrow's `impossibility' theorem asserts that there are no satisfactory methods of aggregating individual preferences into collective preferences in many complex situations. this result has ramifications in economics, politics, i.e., the theory of voting, and the structure of tournaments. by identifying the objects of choice with mathematical sets, and preferences with hausdorff measures of the distances between sets, it is possible to extend arrow's arguments from a sociological to a mathematical setting. one consequence is that notions of reversibility can be expressed in terms of the relative configurations of patterns of sets.", "categories": "econ.th cs.ai", "created": "2019-09-10", "updated": "", "authors": ["t. erber", "m. j. frank"], "url": "https://arxiv.org/abs/1909.07771"}, {"title": "distributional conformal prediction", "id": "1909.07889", "abstract": "we propose a robust method for constructing conditionally valid prediction intervals based on regression models for conditional distributions such as quantile and distribution regression. our approach exploits the probability integral transform and relies on permuting estimated ``ranks'' instead of regression residuals. unlike residuals, these ranks are independent of the covariates, which allows us to establish the conditional validity of the resulting prediction intervals under consistent estimation of the conditional distributions. we also establish theoretical performance guarantees under arbitrary model misspecification. the usefulness of the proposed method is illustrated based on two applications. first, we study the problem of predicting daily returns using realized volatility. second, we consider a synthetic control setting where the goal is to predict a country's counterfactual gdp growth rate based on the contemporaneous gdp growth rates of other countries.", "categories": "econ.em stat.me", "created": "2019-09-17", "updated": "", "authors": ["victor chernozhukov", "kaspar w\u00fcthrich", "yinchu zhu"], "url": "https://arxiv.org/abs/1909.07889"}, {"title": "strategic formation and reliability of supply chain networks", "id": "1909.08021", "abstract": "supply chains are the backbone of the global economy. disruptions to them can be costly. centrally managed supply chains invest in ensuring their resilience. decentralized supply chains, however, must rely upon the self-interest of their individual components to maintain the resilience of the entire chain.   we examine the incentives that independent self-interested agents have in forming a resilient supply chain network in the face of production disruptions and competition. in our model, competing suppliers are subject to yield uncertainty (they deliver less than ordered) and congestion (lead time uncertainty or, \"soft\" supply caps). competing retailers must decide which suppliers to link to based on both price and reliability. in the presence of yield uncertainty only, the resulting supply chain networks are sparse. retailers concentrate their links on a single supplier, counter to the idea that they should mitigate yield uncertainty by diversifying their supply base. this happens because retailers benefit from supply variance. it suggests that competition will amplify output uncertainty. when congestion is included as well, the resulting networks are denser and resemble the bipartite expander graphs that have been proposed in the supply chain literature, thereby, providing the first example of endogenous formation of resilient supply chain networks, without resilience being explicitly encoded in payoffs. finally, we show that a supplier's investments in improved yield can make it worse off. this happens because high production output saturates the market, which, in turn lowers prices and profits for participants.", "categories": "cs.gt cs.sy econ.th eess.sy math.oc q-fin.tr", "created": "2019-09-17", "updated": "2020-01-14", "authors": ["victor amelkin", "rakesh vohra"], "url": "https://arxiv.org/abs/1909.08021"}, {"title": "adjusted qmle for the spatial autoregressive parameter", "id": "1909.08141", "abstract": "one simple, and often very effective, way to attenuate the impact of nuisance parameters on maximum likelihood estimation of a parameter of interest is to recenter the profile score for that parameter. we apply this general principle to the quasi-maximum likelihood estimator (qmle) of the autoregressive parameter $\\lambda$ in a spatial autoregression. the resulting estimator for $\\lambda$ has better finite sample properties compared to the qmle for $\\lambda$, especially in the presence of a large number of covariates. it can also solve the incidental parameter problem that arises, for example, in social interaction models with network fixed effects, or in spatial panel models with individual or time fixed effects. however, spatial autoregressions present specific challenges for this type of adjustment, because recentering the profile score may cause the adjusted estimate to be outside the usual parameter space for $\\lambda$. conditions for this to happen are given, and implications are discussed. for inference, we propose confidence intervals based on a lugannani--rice approximation to the distribution of the adjusted qmle of $\\lambda$. based on our simulations, the coverage properties of these intervals are excellent even in models with a large number of covariates.", "categories": "econ.em", "created": "2019-09-17", "updated": "", "authors": ["federico martellosio", "grant hillier"], "url": "https://arxiv.org/abs/1909.08141"}, {"title": "evaluating effects of tuition fees: lasso for the case of germany", "id": "1909.08299", "abstract": "we study the effect of the introduction of university tuition fees on the enrollment behavior of students in germany. for this, an appropriate lasso-technique is crucial in order to identify the magnitude and significance of the effect due to potentially many relevant controlling factors and only a short time frame where fees existed. we show that a post-double selection strategy combined with stability selection determines a significant negative impact of fees on student enrollment and identifies relevant variables. this is in contrast to previous empirical studies and a plain linear panel regression which cannot detect any effect of tuition fees in this case. in our study, we explicitly deal with data challenges in the response variable in a transparent way and provide respective robust results. moreover, we control for spatial cross-effects capturing the heterogeneity in the introduction scheme of fees across federal states (\"bundesl\\\"ander\"), which can set their own educational policy. we also confirm the validity of our lasso approach in a comprehensive simulation study.", "categories": "stat.ap econ.em stat.ml", "created": "2019-09-18", "updated": "", "authors": ["konstantin g\u00f6rgen", "melanie schienle"], "url": "https://arxiv.org/abs/1909.08299"}, {"title": "nonparametric estimation of the random coefficients model: an elastic   net approach", "id": "1909.08434", "abstract": "this paper investigates and extends the computationally attractive nonparametric random coefficients estimator of fox, kim, ryan, and bajari (2011). we show that their estimator is a special case of the nonnegative lasso, explaining its sparse nature observed in many applications. recognizing this link, we extend the estimator, transforming it to a special case of the nonnegative elastic net. the extension improves the estimator's recovery of the true support and allows for more accurate estimates of the random coefficients' distribution. our estimator is a generalization of the original estimator and therefore, is guaranteed to have a model fit at least as good as the original one. a theoretical analysis of both estimators' properties shows that, under conditions, our generalized estimator approximates the true distribution more accurately. two monte carlo experiments and an application to a travel mode data set illustrate the improved performance of the generalized estimator.", "categories": "econ.em", "created": "2019-09-18", "updated": "2019-09-19", "authors": ["florian heiss", "stephan hetzenecker", "maximilian osterhaus"], "url": "https://arxiv.org/abs/1909.08434"}, {"title": "overconfidence and prejudice", "id": "1909.08497", "abstract": "we explore conclusions a person draws from observing society when he allows for the possibility that individuals' outcomes are affected by group-level discrimination. injecting a single non-classical assumption, that the agent is overconfident about himself, we explain key observed patterns in social beliefs, and make a number of additional predictions. first, the agent believes in discrimination against any group he is in more than an outsider does, capturing widely observed self-centered views of discrimination. second, the more group memberships the agent shares with an individual, the more positively he evaluates the individual. this explains one of the most basic facts about social judgments, in-group bias, as well as \"legitimizing myths\" that justify an arbitrary social hierarchy through the perceived superiority of the privileged group. third, biases are sensitive to how the agent divides society into groups when evaluating outcomes. this provides a reason why some ethnically charged questions should not be asked, as well as a potential channel for why nation-building policies might be effective. fourth, giving the agent more accurate information about himself increases all his biases. fifth, the agent is prone to substitute biases, implying that the introduction of a new outsider group to focus on creates biases against the new group but lowers biases vis a vis other groups. sixth, there is a tendency for the agent to agree more with those in the same groups. as a microfoundation for our model, we provide an explanation for why an overconfident agent might allow for potential discrimination in evaluating outcomes, even when he initially did not conceive of this possibility.", "categories": "econ.th", "created": "2019-09-18", "updated": "", "authors": ["paul heidhues", "botond k\u0151szegi", "philipp strack"], "url": "https://arxiv.org/abs/1909.08497"}, {"title": "informe-pais brasil", "id": "1909.08564", "abstract": "structural socioeconomic analysis of brazil. all basic information about this south american country is gathered in a comprehensive outlook that includes the challenges brazil faces, as well as their causes and posible economic solutions.", "categories": "econ.gn q-fin.ec", "created": "2019-09-18", "updated": "", "authors": ["juan gonzalez-blanco"], "url": "https://arxiv.org/abs/1909.08564"}, {"title": "new policy design for food accessibility to the people in need", "id": "1909.08648", "abstract": "food insecurity is a term used to measure hunger and food deprivation of a large population. as per the 2015 statistics provided by feeding america - one of the largest domestic hunger-relief organizations in the united states, 42.2 million americans live in food insecure households, including 29.1 million adults and 13.1 million children. this constitutes about 13.1% of households that are food insecure. food banks have been developed to improve food security for the needy. we have developed a novel food distribution policy using suitable welfare and poverty indices and functions. in this work, we propose an equitable and fair distribution of donated foods as per the demands and requirements of the people, thus ensuring minimum wastage of food (perishable and non-perishable) with focus towards nutrition. we present results and analysis based on the application of the proposed policy using the information of a local food bank as a case study. the results show that the new policy performs better than the current methods in terms of population being covered and reduction of food wastage obtaining suitable levels of nutrition.", "categories": "econ.gn q-fin.ec", "created": "2019-09-18", "updated": "", "authors": ["rahul srinivas sucharitha", "seokcheon lee"], "url": "https://arxiv.org/abs/1909.08648"}, {"title": "generational political dynamics of retirement pensions systems: an agent   based model", "id": "1909.08706", "abstract": "the increasing difficulties in financing the welfare state and in particular public retirement pensions have been one of the outcomes both of the decrease of fertility and birth rates combined with the increase of life expectancy. the dynamics of retirement pensions are usually studied in economics using overlapping generation models. these models are based on simplifying assumptions like the use of a representative agent to ease the problem of tractability. alternatively, we propose to use agent-based modelling (abm), relaxing the need for those assumptions and enabling the use of interacting and heterogeneous agents assigning special importance to the study of inter-generational relations. we treat pension dynamics both in economics and political perspectives. the model we build, following the odd protocol, will try to understand the dynamics of choice of public versus private retirement pensions resulting from the conflicting preferences of different agents but also from the cooperation between them. the aggregation of these individual preferences is done by voting. we combine a microsimulation approach following the evolution of synthetic populations along time, with the abm approach studying the interactions between the different agent types. our objective is to depict the conditions for the survival of the public pensions system emerging from the relation between egoistic and altruistic individual and collective behaviours.", "categories": "econ.gn q-fin.ec", "created": "2019-09-13", "updated": "", "authors": ["s\u00e9rgio bacelar", "luis antunes"], "url": "https://arxiv.org/abs/1909.08706"}, {"title": "legal architecture and design for gulf cooperation council economic   integration", "id": "1909.08798", "abstract": "the cooperation council for the arab states of the gulf (gcc) is generally regarded as a success story for economic integration in arab countries. the idea of regional integration gained ground by signing the gcc charter. it envisioned a closer economic relationship between member states.although economic integration among gcc member states is an ambitious step in the right direction, there are gaps and challenges ahead. the best way to address the gaps and challenges that exist in formulating integration processes in the gcc is to start with a clear set of rules and put the necessary mechanisms in place. integration attempts must also exhibit a high level of commitment in order to deflect dynamics of disintegration that have all too often frustrated meaningful integration in arab countries. if the gcc can address these issues, it could become an economic powerhouse within arab countries and even asia.", "categories": "econ.gn q-fin.ec", "created": "2019-09-19", "updated": "", "authors": ["bashar h. malkawi"], "url": "https://arxiv.org/abs/1909.08798"}, {"title": "the effect of oil price on united arab emirates goods trade deficit with   the united states", "id": "1909.09057", "abstract": "we seek to investigate the effect of oil price on uae goods trade deficit with the u.s. the current increase in the price of oil and the absence of significant studies in the uae economy are the main motives behind the current study. our paper focuses on a small portion of uae trade, which is 11% of the uae foreign trade, however, it is a significant part since the u.s. is a major trade partner with the uae. the current paper concludes that oil price has a significant positive influence on real imports. at the same time, oil price does not have a significant effect on real exports. as a result, any increase in the price of oil increases goods trade deficit of the uae economy. the policy implication of the current paper is that the revenue of oil sales is not used to encourage uae real exports.", "categories": "econ.gn q-fin.ec", "created": "2019-09-19", "updated": "", "authors": ["osama d. sweidan", "bashar h. malkawi"], "url": "https://arxiv.org/abs/1909.09057"}, {"title": "the design and operation of rules of origin in greater arab free trade   area: challenges of implementation and reform", "id": "1909.09061", "abstract": "rules of origin (roo) are pivotal element of the greater arab free trade area (gafta). roo are basically established to ensure that only eligible products receive preferential tariff treatment. taking into consideration the profound implications of roo for enhancing trade flows and facilitating the success of regional integration, this article sheds light on the way that roo in gafta are designed and implemented. moreover, the article examines the extent to which roo still represents an obstacle to the full implementation of gafta. in addition, the article provides ways to overcome the most important shortcomings of roo text in the agreement and ultimately offering possible solutions to those issues.", "categories": "econ.gn q-fin.ec", "created": "2019-09-19", "updated": "", "authors": ["bashar h. malkawi", "mohammad i. el-shafie"], "url": "https://arxiv.org/abs/1909.09061"}, {"title": "methods, models, and the evolution of moral psychology", "id": "1909.09198", "abstract": "why are we good? why are we bad? questions regarding the evolution of morality have spurred an astoundingly large interdisciplinary literature. some significant subset of this body of work addresses questions regarding our moral psychology: how did humans evolve the psychological properties which underpin our systems of ethics and morality? here i do three things. first, i discuss some methodological issues, and defend particularly effective methods for addressing many research questions in this area. second, i give an in-depth example, describing how an explanation can be given for the evolution of guilt---one of the core moral emotions---using the methods advocated here. last, i lay out which sorts of strategic scenarios generally are the ones that our moral psychology evolved to `solve', and thus which models are the most useful in further exploring this evolution.", "categories": "econ.gn q-fin.ec", "created": "2019-09-09", "updated": "", "authors": ["cailin o'connor"], "url": "https://arxiv.org/abs/1909.09198"}, {"title": "systemic cascades on inhomogeneous random financial networks", "id": "1909.09239", "abstract": "this systemic risk paper introduces inhomogeneous random financial networks (irfns). such models are intended to describe parts, or the entirety, of a highly heterogeneous network of banks and their interconnections, in the global financial system. both the balance sheets and the stylized crisis behaviour of banks are ingredients of the network model. a systemic crisis is pictured as triggered by a shock to banks' balance sheets, which then leads to the propagation of damaging shocks and the potential for amplification of the crisis, ending with the system in a cascade equilibrium. under some conditions the model has ``locally tree-like independence (lti)'', where a general percolation theoretic argument leads to an analytic fixed point equation describing the cascade equilibrium when the number of banks $n$ in the system is taken to infinity. this paper focusses on mathematical properties of the framework in the context of eisenberg-noe solvency cascades generalized to account for fractional bankruptcy charges. new results including a definition and proof of the ``lti property'' of the eisenberg-noe solvency cascade mechanism lead to explicit $n=\\infty$ fixed point equations that arise under very general model specifications. the essential formulas are shown to be implementable via well-defined approximation schemes, but numerical exploration of some of the wide range of potential applications of the method is left for future work.", "categories": "q-fin.gn econ.gn math.pr q-fin.ec", "created": "2019-09-19", "updated": "", "authors": ["t. r. hurd"], "url": "https://arxiv.org/abs/1909.09239"}, {"title": "how to design a derivatives market?", "id": "1909.09257", "abstract": "we consider the problem of designing a derivatives exchange aiming at addressing clients needs in terms of listed options and providing suitable liquidity. we proceed into two steps. first we use a quantization method to select the options that should be displayed by the exchange. then, using a principal-agent approach, we design a make take fees contract between the exchange and the market maker. the role of this contract is to provide incentives to the market maker so that he offers small spreads for the whole range of listed options, hence attracting transactions and meeting the commercial requirements of the exchange.", "categories": "q-fin.tr econ.gn q-fin.ec", "created": "2019-09-19", "updated": "", "authors": ["bastien baldacci", "paul jusselin", "mathieu rosenbaum"], "url": "https://arxiv.org/abs/1909.09257"}, {"title": "discerning solution concepts", "id": "1909.09320", "abstract": "the empirical analysis of discrete complete-information games has relied on behavioral restrictions in the form of solution concepts, such as nash equilibrium. choosing the right solution concept is crucial not just for identification of payoff parameters, but also for the validity and informativeness of counterfactual exercises and policy implications. we say that a solution concept is discernible if it is possible to determine whether it generated the observed data on the players' behavior and covariates. we propose a set of conditions that make it possible to discern solution concepts. in particular, our conditions are sufficient to tell whether the players' choices emerged from nash equilibria. we can also discern between rationalizable behavior, maxmin behavior, and collusive behavior. finally, we identify the correlation structure of unobserved shocks in our model using a novel approach.", "categories": "econ.em econ.th", "created": "2019-09-20", "updated": "", "authors": ["nail kashaev", "bruno salcedo"], "url": "https://arxiv.org/abs/1909.09320"}, {"title": "double-robust identification for causal panel data models", "id": "1909.09412", "abstract": "we study identification and estimation of causal effects of a binary treatment in settings with panel data. we highlight that there are two paths to identification in the presence of unobserved confounders. first, the conventional path based on making assumptions on the relation between the potential outcomes and the unobserved confounders. second, a design-based path where assumptions are made about the relation between the treatment assignment and the confounders. we introduce different sets of assumptions that follow the two paths, and develop double robust approaches to identification where we exploit both approaches, similar in spirit to the double robust approaches to estimation in the program evaluation literature.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-09-20", "updated": "", "authors": ["dmitry arkhangelsky", "guido w. imbens"], "url": "https://arxiv.org/abs/1909.09412"}, {"title": "productivity propagation with networks transformation", "id": "1909.09641", "abstract": "we model sectoral production by cascading binary compounding processes. the sequence of processes is discovered in a self-similar hierarchical structure stylized in the economy-wide networks of production. nested substitution elasticities and hicks-neutral productivity growth are measured such that the general equilibrium feedbacks between all sectoral unit cost functions replicate the transformation of networks observed as a set of two temporally distant input-output coefficient matrices. we examine this system of unit cost functions to determine how idiosyncratic sectoral productivity shocks propagate into aggregate macroeconomic fluctuations in light of potential network transformation. additionally, we study how sectoral productivity increments propagate into the dynamic general equilibrium, thereby allowing network transformation and ultimately producing social benefits.", "categories": "econ.gn q-fin.ec", "created": "2019-09-20", "updated": "2020-04-04", "authors": ["satoshi nakano", "kazuhiko nishimura"], "url": "https://arxiv.org/abs/1909.09641"}, {"title": "desperate times call for desperate measures: government spending   multipliers in hard times", "id": "1909.09824", "abstract": "we investigate state-dependent effects of fiscal multipliers and allow for endogenous sample splitting to determine whether the us economy is in a slack state. when the endogenized slack state is estimated as the period of the unemployment rate higher than about 12 percent, the estimated cumulative multipliers are significantly larger during slack periods than non-slack periods and are above unity. we also examine the possibility of time-varying regimes of slackness and find that our empirical results are robust under a more flexible framework. our estimation results point out the importance of the heterogenous effects of fiscal policy and shed light on the prospect of fiscal policy in response to economic shocks from the current covid-19 pandemic.", "categories": "econ.gn q-fin.ec", "created": "2019-09-21", "updated": "2020-05-28", "authors": ["sokbae lee", "yuan liao", "myung hwan seo", "youngki shin"], "url": "https://arxiv.org/abs/1909.09824"}, {"title": "subspace clustering for panel data with interactive effects", "id": "1909.09928", "abstract": "in this paper, a statistical model for panel data with unobservable grouped factor structures which are correlated with the regressors and the group membership can be unknown. the factor loadings are assumed to be in different subspaces and the subspace clustering for factor loadings are considered. a method called least squares subspace clustering estimate (lssc) is proposed to estimate the model parameters by minimizing the least-square criterion and to perform the subspace clustering simultaneously. the consistency of the proposed subspace clustering is proved and the asymptotic properties of the estimation procedure are studied under certain conditions. a monte carlo simulation study is used to illustrate the advantages of the proposed method. further considerations for the situations that the number of subspaces for factors, the dimension of factors and the dimension of subspaces are unknown are also discussed. for illustrative purposes, the proposed method is applied to study the linkage between income and democracy across countries while subspace patterns of unobserved factors and factor loadings are allowed.", "categories": "econ.em", "created": "2019-09-21", "updated": "", "authors": ["jiangtao duan", "wei gao", "hao qu", "hon keung tony"], "url": "https://arxiv.org/abs/1909.09928"}, {"title": "what do adoption patterns of solar panels observed so far tell about   governments' incentive? insight from diffusion models", "id": "1909.10017", "abstract": "the paper uses diffusion models to understand the main determinants of diffusion of solar photovoltaic panels (spp) worldwide, focusing on the role of public incentives. we applied the generalized bass model (gbm) to adoption data of 26 countries between 1992-2016. the spp market appears as a frail and complicate one, lacking public media support. even the major shocks in adoption curves, following state incentive implemented after 2006, failed to go beyond short-term effects and therefore were unable to provide sustained momentum to the market. this suggests that further barriers to adoption should be removed.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2019-09-22", "updated": "", "authors": ["anita m. bunea", "pietro manfredi", "pompeo della posta", "mariangela guidolin"], "url": "https://arxiv.org/abs/1909.10017"}, {"title": "meaningful causal decompositions in health equity research: definition,   identification, and estimation through a weighting framework", "id": "1909.10060", "abstract": "causal decomposition analyses can help build the evidence base for interventions that address health disparities (inequities). they ask how disparities in outcomes may change under hypothetical intervention. through study design and assumptions, they can rule out alternate explanations such as confounding, selection-bias, and measurement error, thereby identifying potential targets for intervention. unfortunately, the literature on causal decomposition analysis and related methods have largely ignored equity concerns that actual interventionists would respect, limiting their relevance and practical value. this paper addresses these concerns by explicitly considering what covariates the outcome disparity and hypothetical intervention adjust for (so-called allowable covariates) and the equity value judgements these choices convey, drawing from the bioethics, biostatistics, epidemiology, and health services research literatures. from this discussion, we generalize decomposition estimands and formulae to incorporate allowable covariate sets, to reflect equity choices, while still allowing for adjustment of non-allowable covariates needed to satisfy causal assumptions. for these general formulae, we provide weighting-based estimators based on adaptations of ratio-of-mediator-probability and inverse-odds-ratio weighting. we discuss when these estimators reduce to already used estimators under certain equity value judgements, and a novel adaptation under other judgements.", "categories": "stat.me econ.em", "created": "2019-09-22", "updated": "2020-09-15", "authors": ["john w. jackson"], "url": "https://arxiv.org/abs/1909.10060"}, {"title": "inference for linear conditional moment inequalities", "id": "1909.10062", "abstract": "we consider inference based on linear conditional moment inequalities, which arise in a wide variety of economic applications, including many structural models. we show that linear conditional structure greatly simplifies confidence set construction, allowing for computationally tractable projection inference in settings with nuisance parameters. next, we derive least favorable critical values that avoid conservativeness due to projection. finally, we introduce a conditional inference approach which ensures a strong form of insensitivity to slack moments, as well as a hybrid technique which combines the least favorable and conditional methods. our conditional and hybrid approaches are new even in settings without nuisance parameters. we find good performance in simulations based on wollmann (2018), especially for the hybrid approach.", "categories": "econ.em", "created": "2019-09-22", "updated": "", "authors": ["isaiah andrews", "jonathan roth", "ariel pakes"], "url": "https://arxiv.org/abs/1909.10062"}, {"title": "specification testing in nonparametric instrumental quantile regression", "id": "1909.10129", "abstract": "there are many environments in econometrics which require nonseparable modeling of a structural disturbance. in a nonseparable model with endogenous regressors, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable variable. under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. a failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. this paper develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. in addition, test statistics to justify the model simplification are established. finite sample properties are examined in a monte carlo study and an empirical illustration is provided.", "categories": "econ.em", "created": "2019-09-22", "updated": "", "authors": ["christoph breunig"], "url": "https://arxiv.org/abs/1909.10129"}, {"title": "goodness-of-fit tests based on series estimators in nonparametric   instrumental regression", "id": "1909.10133", "abstract": "this paper proposes several tests of restricted specification in nonparametric instrumental regression. based on series estimators, test statistics are established that allow for tests of the general model against a parametric or nonparametric specification as well as a test of exogeneity of the vector of regressors. the tests' asymptotic distributions under correct specification are derived and their consistency against any alternative model is shown. under a sequence of local alternative hypotheses, the asymptotic distributions of the tests is derived. moreover, uniform consistency is established over a class of alternatives whose distance to the null hypothesis shrinks appropriately as the sample size increases. a monte carlo study examines finite sample performance of the test statistics.", "categories": "econ.em", "created": "2019-09-22", "updated": "", "authors": ["christoph breunig"], "url": "https://arxiv.org/abs/1909.10133"}, {"title": "weighted envy-freeness in indivisible item allocation", "id": "1909.10502", "abstract": "we introduce and analyze new envy-based fairness concepts for agents with weights that quantify their entitlements in the allocation of indivisible items. we propose two variants of weighted envy-freeness up to one item (wef1): strong, where the envy can be eliminated by removing an item from the envied agent's bundle, and weak, where the envy can be eliminated either by removing an item as in the strong version or by replicating an item from the envied agent's bundle in the envying agent's bundle. we prove that for additive valuations, an allocation that is both pareto optimal and strongly wef1 always exists; however, an allocation that maximizes the weighted nash social welfare may not be strongly wef1 but always satisfies the weak version of the property. moreover, we establish that a generalization of the round-robin picking sequence algorithm produces in polynomial time a strongly wef1 allocation for an arbitrary number of agents; for two agents, we can efficiently achieve both strong wef1 and pareto optimality by adapting the adjusted winner procedure. our work exhibits several aspects in which weighted fair division is richer and more challenging than its unweighted counterpart.", "categories": "cs.ai cs.gt econ.th", "created": "2019-09-23", "updated": "2020-04-29", "authors": ["mithun chakraborty", "ayumi igarashi", "warut suksompong", "yair zick"], "url": "https://arxiv.org/abs/1909.10502"}, {"title": "structural change analysis of active cryptocurrency market", "id": "1909.10679", "abstract": "structural change analysis of active cryptocurrency market", "categories": "q-fin.st econ.em stat.ap", "created": "2019-09-23", "updated": "", "authors": ["c. y. tan", "y. b. koh", "k. h. ng", "k. h. ng"], "url": "https://arxiv.org/abs/1909.10679"}, {"title": "optimizing execution cost using stochastic control", "id": "1909.10762", "abstract": "we devise an optimal allocation strategy for the execution of a predefined number of stocks in a given time frame using the technique of discrete-time stochastic control theory for a defined market model. this market structure allows an instant execution of the market orders and has been analyzed based on the assumption of discretized geometric movement of the stock prices. we consider two different cost functions where the first function involves just the fiscal cost while the cost function of the second kind incorporates the risks of non-strategic constrained investments along with fiscal costs. precisely, the strategic development of constrained execution of k stocks within a stipulated time frame of t units is established mathematically using a well-defined stochastic behaviour of stock prices and the same is compared with some of the commonly-used execution strategies using the historical stock price data.", "categories": "q-fin.mf econ.th", "created": "2019-09-24", "updated": "", "authors": ["akshay bansal", "diganta mukherjee"], "url": "https://arxiv.org/abs/1909.10762"}, {"title": "time-consistent decisions and rational expectation equilibrium existence   in dsge models", "id": "1909.10915", "abstract": "under some initial conditions, it is shown that time consistency requirements prevent rational expectation equilibrium (ree) existence for dynamic stochastic general equilibrium models induced by consumer heterogeneity, in contrast to static models. however, one can consider ree-prohibiting initial conditions as limits of other initial conditions. the ree existence issue then is overcome by using a limit of economies. this shows that significant care must be taken of when dealing with rational expectation equilibria.", "categories": "econ.th", "created": "2019-09-23", "updated": "2020-05-18", "authors": ["minseong kim"], "url": "https://arxiv.org/abs/1909.10915"}, {"title": "scalable fair division for 'at most one' preferences", "id": "1909.10925", "abstract": "allocating multiple scarce items across a set of individuals is an important practical problem. in the case of divisible goods and additive preferences a convex program can be used to find the solution that maximizes nash welfare (mnw). the mnw solution is equivalent to finding the equilibrium of a market economy (aka. the competitive equilibrium from equal incomes, ceei) and thus has good properties such as pareto optimality, envy-freeness, and incentive compatibility in the large. unfortunately, this equivalence (and nice properties) breaks down for general preference classes. motivated by real world problems such as course allocation and recommender systems we study the case of additive `at most one' (amo) preferences - individuals want at most 1 of each item and lotteries are allowed. we show that in this case the mnw solution is still a convex program and importantly is a ceei solution when the instance gets large but has a `low rank' structure. thus a polynomial time algorithm can be used to scale ceei (which is in general ppad-hard) for amo preferences. we examine whether the properties guaranteed in the limit hold approximately in finite samples using several real datasets.", "categories": "cs.gt cs.ma econ.em", "created": "2019-09-24", "updated": "", "authors": ["christian kroer", "alexander peysakhovich"], "url": "https://arxiv.org/abs/1909.10925"}, {"title": "a peek into the unobservable: hidden states and bayesian inference for   the bitcoin and ether price series", "id": "1909.10957", "abstract": "conventional financial models fail to explain the economic and monetary properties of cryptocurrencies due to the latter's dual nature: their usage as financial assets on the one side and their tight connection to the underlying blockchain structure on the other. in an effort to examine both components via a unified approach, we apply a recently developed non-homogeneous hidden markov (nhhm) model with an extended set of financial and blockchain specific covariates on the bitcoin (btc) and ether (eth) price data. based on the observable series, the nhhm model offers a novel perspective on the underlying microstructure of the cryptocurrency market and provides insight on unobservable parameters such as the behavior of investors, traders and miners. the algorithm identifies two alternating periods (hidden states) of inherently different activity -- fundamental versus uninformed or noise traders -- in the bitcoin ecosystem and unveils differences in both the short/long run dynamics and in the financial characteristics of the two states, such as significant explanatory variables, extreme events and varying series autocorrelation. in a somewhat unexpected result, the bitcoin and ether markets are found to be influenced by markedly distinct indicators despite their perceived correlation. the current approach backs earlier findings that cryptocurrencies are unlike any conventional financial asset and makes a first step towards understanding cryptocurrency markets via a more comprehensive lens.", "categories": "econ.em q-fin.st", "created": "2019-09-24", "updated": "", "authors": ["constandina koki", "stefanos leonardos", "georgios piliouras"], "url": "https://arxiv.org/abs/1909.10957"}, {"title": "the converse envelope theorem", "id": "1909.11219", "abstract": "i prove an envelope theorem with a converse: the envelope formula is equivalent to a first-order condition. like milgrom and segal's (2002) envelope theorem, my result requires no structure on the choice set. i use the converse envelope theorem to extend to abstract outcomes the canonical result in mechanism design that any increasing allocation is implementable, and apply this to selling information.", "categories": "econ.th", "created": "2019-09-24", "updated": "2020-02-14", "authors": ["ludvig sinander"], "url": "https://arxiv.org/abs/1909.11219"}, {"title": "a new approach to fair distribution of welfare", "id": "1909.11346", "abstract": "we consider transferable-utility profit-sharing games that arise from settings in which agents need to jointly choose one of several alternatives, and may use transfers to redistribute the welfare generated by the chosen alternative. one such setting is the shared-rental problem, in which students jointly rent an apartment and need to decide which bedroom to allocate to each student, depending on the student's preferences. many solution concepts have been proposed for such settings, ranging from mechanisms without transfers, such as random priority and the eating mechanism, to mechanisms with transfers, such as envy free solutions, the shapley value, and the kalai-smorodinsky bargaining solution. we seek a solution concept that satisfies three natural properties, concerning efficiency, fairness and decomposition. we observe that every solution concept known (to us) fails to satisfy at least one of the three properties. we present a new solution concept, designed so as to satisfy the three properties. a certain submodularity condition (which holds in interesting special cases such as the shared-rental setting) implies both existence and uniqueness of our solution concept.", "categories": "econ.th cs.gt", "created": "2019-09-25", "updated": "", "authors": ["moshe babaioff", "uriel feige"], "url": "https://arxiv.org/abs/1909.11346"}, {"title": "propaganda, alternative media, and accountability in fragile democracies", "id": "1909.11836", "abstract": "we develop a model of electoral accountability with mainstream and alternative media. in addition to regular high- and low-competence types, the incumbent may be an aspiring autocrat who controls the mainstream media and will subvert democracy if retained in office. a truthful alternative media can help voters identify and remove these subversive types while re-electing competent leaders. a malicious alternative media, in contrast, spreads false accusations about the incumbent and demotivates policy effort. if the alternative media is very likely be malicious and hence is unreliable, voters ignore it and use only the mainstream media to hold regular incumbents accountable, leaving aspiring autocrats to win re-election via propaganda that portrays them as effective policymakers. when the alternative media's reliability is intermediate, voters heed its warnings about subversive incumbents, but the prospect of being falsely accused demotivates effort by regular incumbents and electoral accountability breaks down.", "categories": "econ.gn q-fin.ec", "created": "2019-09-25", "updated": "2020-03-29", "authors": ["anqi li", "davin raiha", "kenneth w. shotts"], "url": "https://arxiv.org/abs/1909.11836"}, {"title": "time-inconsistent stopping, myopic adjustment & equilibrium stability:   with a mean-variance application", "id": "1909.11921", "abstract": "for a discrete time markov chain and in line with strotz' consistent planning we develop a framework for problems of optimal stopping that are time-inconsistent due to the consideration of a non-linear function of an expected reward. we consider pure and mixed stopping strategies and a (subgame perfect nash) equilibrium. we provide different necessary and sufficient equilibrium conditions including a verification theorem. using a fixed point argument we provide equilibrium existence results. we adapt and study the notion of the myopic adjustment process and introduce different kinds of equilibrium stability. we show that neither existence nor uniqueness of equilibria should generally be expected. the developed theory is applied to a mean-variance problem and a variance problem.", "categories": "math.oc econ.th math.pr", "created": "2019-09-26", "updated": "2020-01-22", "authors": ["s\u00f6ren christensen", "kristoffer lindensj\u00f6"], "url": "https://arxiv.org/abs/1909.11921"}, {"title": "inference in nonparametric series estimation with specification searches   for the number of series terms", "id": "1909.12162", "abstract": "nonparametric series regression often involves specification search over the tuning parameter, i.e., evaluating estimates and confidence intervals with a different number of series terms. this paper develops pointwise and uniform inferences for conditional mean functions in nonparametric series estimations that are uniform in the number of series terms. as a result, this paper constructs confidence intervals and confidence bands with possibly data-dependent series terms that have valid asymptotic coverage probabilities. this paper also considers a partially linear model setup and develops inference methods for the parametric part uniform in the number of series terms. the finite sample performance of the proposed methods is investigated in various simulation setups as well as in an illustrative example, i.e., the nonparametric estimation of the wage elasticity of the expected labor supply from blomquist and newey (2002).", "categories": "econ.em", "created": "2019-09-26", "updated": "2020-02-24", "authors": ["byunghoon kang"], "url": "https://arxiv.org/abs/1909.12162"}, {"title": "debiased/double machine learning for instrumental variable quantile   regressions", "id": "1909.12592", "abstract": "the aim of this paper is to investigate estimation and inference on a low-dimensional causal parameter in the presence of high-dimensional controls in an instrumental variable quantile regression. the estimation and inference are based on the neyman-type orthogonal moment conditions, that are relatively insensitive to the estimation of the nuisance parameters. the monte carlo experiments show that the econometric procedure performs well. we also apply the procedure to reinvestigate two empirical studies: the quantile treatment effect of 401(k) participation on accumulated wealth, and the distributional effect of job-training program participation on trainee earnings.", "categories": "econ.em", "created": "2019-09-27", "updated": "", "authors": ["jau-er chen", "jia-jyun tien"], "url": "https://arxiv.org/abs/1909.12592"}, {"title": "decision models for workforce and technology planning in services", "id": "1909.12829", "abstract": "today's service companies operate in a technology-oriented and knowledge-intensive environment while recruiting and training individuals from an increasingly diverse population. one of the resulting challenges is ensuring strategic alignment between their two key resources - technology and workforce - through the resource planning and allocation processes. the traditional hierarchical decision approach to resource planning and allocation considers only technology planning as a strategic-level decision, with workforce recruiting and training planning as a subsequent tactical-level decision. however, two other decision approaches - joint and integrated - elevate workforce planning to the same strategic level as technology planning. thus we investigate the impact of strategically aligning technology and workforce decisions through the comparison of joint and integrated models to each other and to a baseline hierarchical model in terms of the total cost. numerical experiments are conducted to characterize key features of solutions provided by these approaches under conditions typically found in this type of service company. our results show that the integrated model is the lowest cost across all conditions. this is because the integrated approach maintains a small but skilled workforce that can operate new and more advanced technology with higher capacity. however, the cost performance of the joint model is very close to the integrated model under many conditions and is easier to implement computationally and managerially, making it a good choice in many environments. managerial insights derived from this study can serve as a valuable guide for choosing the proper decision approach for technology-oriented and knowledge-intensive service companies.", "categories": "econ.gn q-fin.ec", "created": "2019-09-27", "updated": "", "authors": ["gang li", "joy m. field", "hongxun jiang", "tian he", "youming pang"], "url": "https://arxiv.org/abs/1909.12829"}, {"title": "revenue allocation in formula one: a pairwise comparison approach", "id": "1909.12931", "abstract": "a model is proposed to allocate formula one world championship prize money among the constructors. the methodology is based on pairwise comparison matrices, allows for the use of any weighting method, and makes possible to tune the level of inequality. we introduce an axiom called scale invariance, which requires the ranking of the teams to be independent of the parameter controlling inequality. the eigenvector method is revealed to violate this condition in our dataset, while the row geometric mean method always satisfies it. the revenue allocation is not influenced by the arbitrary valuation given to the race prizes in the official points scoring system of formula one and takes the intensity of pairwise preferences into account, contrary to the standard condorcet method. our suggestion can be used to share revenues among groups when group members are ranked several times.", "categories": "econ.gn cs.ai q-fin.ec", "created": "2019-09-25", "updated": "2020-09-24", "authors": ["d\u00f3ra gr\u00e9ta petr\u00f3czy", "l\u00e1szl\u00f3 csat\u00f3"], "url": "https://arxiv.org/abs/1909.12931"}, {"title": "monotonicity-constrained nonparametric estimation and inference for   first-price auctions", "id": "1909.12974", "abstract": "we propose a new nonparametric estimator for first-price auctions with independent private values that imposes the monotonicity constraint on the estimated inverse bidding strategy. we show that our estimator has a smaller asymptotic variance than that of guerre, perrigne and vuong's (2000) estimator. in addition to establishing pointwise asymptotic normality of our estimator, we provide a bootstrap-based approach to constructing uniform confidence bands for the density function of latent valuations.", "categories": "econ.em stat.me", "created": "2019-09-27", "updated": "", "authors": ["jun ma", "vadim marmer", "artyom shneyerov", "pai xu"], "url": "https://arxiv.org/abs/1909.12974"}, {"title": "equity premium puzzle or faulty economic modelling?", "id": "1909.13019", "abstract": "in this paper, we revisit the equity premium puzzle reported in 1985 by mehra and prescott. we show that the large equity premium that they report can be explained by choosing a more appropriate distribution for the return data. we demonstrate that the high-risk aversion value observed by mehra and prescott may be attributable to the problem of fitting a proper distribution to the historical returns and partly caused by poorly fitting the tail of the return distribution. we describe a new distribution that better fits the return distribution and when used to describe historical returns can explain the large equity risk premium and thereby explains the puzzle.", "categories": "econ.gn q-fin.ec", "created": "2019-09-27", "updated": "2020-01-11", "authors": ["abootaleb shirvani", "stoyan v. stoyanov", "frank j. fabozzi", "svetlozar t. rachev"], "url": "https://arxiv.org/abs/1909.13019"}, {"title": "modelling the health impact of food taxes and subsidies with price   elasticities: the case for additional scaling of food consumption using the   total food expenditure elasticity", "id": "1909.13179", "abstract": "background food taxes and subsidies are one intervention to address poor diets. price elasticity (pe) matrices are commonly used to model the change in food purchasing. usually a pe matrix is generated in one setting then applied to another setting with differing starting consumption and prices of foods. this violates econometric assumptions resulting in likely misestimation of total food consumption. we illustrate rescaling all consumption after applying a pe matrix using a total food expenditure elasticity (tfee, the expenditure elasticity for all food combined given the policy induced change in the total price of food). we use case studies of nz$2 per 100g saturated fat (safa) tax, nz$0.4 per 100g sugar tax, and a 20% fruit and vegetable (f&v) subsidy. methods we estimated changes in food purchasing using a nz pe matrix applied conventionally, then with tfee adjustment. impacts were quantified for total food expenditure and health adjusted life years (halys) for the total nz population alive in 2011 over the rest of their lifetime using a multistate lifetable model. results two nz studies gave tfees of 0.68 and 0.83, with international estimates ranging from 0.46 to 0.90. without tfee adjustment, total food expenditure decreased with the tax policies and increased with the f&v subsidy, implausible directions of shift given economic theory. after tfee adjustment, haly gains reduced by a third to a half for the two taxes and reversed from an apparent health loss to a health gain for the f&v subsidy. with tfee adjustment, haly gains (in 1000s) were 1,805 (95% uncertainty interval 1,337 to 2,340) for the safa tax, 1,671 (1,220 to 2,269) for the sugar tax, and 953 (453 to 1,308) for the f&v subsidy. conclusions if pe matrices are applied in settings beyond where they were derived, additional scaling is likely required. we suggest that the tfee is a useful scalar.", "categories": "econ.gn q-fin.ec", "created": "2019-09-28", "updated": "", "authors": ["tony blakely", "nhung nghiem", "murat genc", "anja mizdrak", "linda cobiac", "cliona ni mhurchu", "boyd swinburn", "peter scarborough", "christine cleghorn"], "url": "https://arxiv.org/abs/1909.13179"}, {"title": "undiscounted bandit games", "id": "1909.13323", "abstract": "we analyze undiscounted continuous-time games of strategic experimentation with two-armed bandits. the risky arm generates payoffs according to a l\\'{e}vy process with an unknown average payoff per unit of time which nature draws from an arbitrary finite set. observing all actions and realized payoffs, plus a free background signal, players use markov strategies with the common posterior belief about the unknown parameter as the state variable. we show that the unique symmetric markov perfect equilibrium can be computed in a simple closed form involving only the payoff of the safe arm, the expected current payoff of the risky arm, and the expected full-information payoff, given the current belief. in particular, the equilibrium does not depend on the precise specification of the payoff-generating processes.", "categories": "econ.th", "created": "2019-09-29", "updated": "2020-08-25", "authors": ["godfrey keller", "sven rady"], "url": "https://arxiv.org/abs/1909.13323"}, {"title": "on incentive compatibility in dynamic mechanism design with exit option   in a markovian environment", "id": "1909.13720", "abstract": "this paper studies dynamic mechanism design in a quasilinear markovian environment and analyzes a direct mechanism model of a principal-agent framework in which the agent is allowed to exit at any period. we consider that the agent's private information, referred to as state, evolves over time. the agent makes decisions of whether to stop or continue and what to report at each period. the principal, on the other hand, chooses decision rules consisting of an allocation rule and a set of payment rules to maximize her ex-ante expected payoff. in order to influence the agent's stopping decision, one of the terminal payment rules is posted-price, i.e., it depends only on the realized stopping time of the agent. we define the incentive compatibility in this dynamic environment in terms of bellman equations, which is then simplified by establishing a one-shot deviation principle. given the optimality of the stopping rule, a sufficient condition for incentive compatibility is obtained by constructing the state-dependent payment rules in terms of a set of functions parameterized by the allocation rule. a necessary condition is derived from envelope theorem, which explicitly formulates the state-dependent payment rules in terms of allocation rules. a class of monotone environment is considered to characterize the optimal stopping by a threshold rule. the posted-price payment rules are then pinned down in terms of the allocation rule and the threshold function up to a constant. the incentive compatibility constraints restrict the design of the posted-price payment rule by a regular condition.", "categories": "math.ds cs.sy econ.th eess.sy", "created": "2019-09-30", "updated": "2019-10-02", "authors": ["tao zhang", "quanyan zhu"], "url": "https://arxiv.org/abs/1909.13720"}, {"title": "macroscopic approximation methods for the analysis of adaptive networked   agent-based models: the example of a two-sector investment model", "id": "1909.13758", "abstract": "in this paper, we propose a statistical aggregation method for agent-based models with heterogeneous agents that interact both locally on a complex adaptive network and globally on a market. the method combines three approaches from statistical physics: (a) moment closure, (b) pair approximation of adaptive network processes, and (c) thermodynamic limit of the resulting stochastic process. as an example of use, we develop a stochastic agent-based model with heterogeneous households that invest in either a fossil-fuel or renewables-based sector while allocating labor on a competitive market. using the adaptive voter model, the model describes agents as social learners that interact on a dynamic network. we apply the approximation methods to derive a set of ordinary differential equations that approximate the macro-dynamics of the model. a comparison of the reduced analytical model with numerical simulations shows that the approximation fits well for a wide range of parameters. the proposed method makes it possible to use analytical tools to better understand the dynamical properties of models with heterogeneous agents on adaptive networks. we showcase this with a bifurcation analysis that identifies parameter ranges with multi-stabilities. the method can thus help to explain emergent phenomena from network interactions and make them mathematically traceable.", "categories": "econ.th nlin.ao", "created": "2019-09-30", "updated": "2020-08-07", "authors": ["jakob j. kolb", "finn m\u00fcller-hansen", "j\u00fcrgen kurths", "jobst heitzig"], "url": "https://arxiv.org/abs/1909.13758"}, {"title": "an econometric analysis of the italian cultural supply", "id": "1910.00073", "abstract": "price indexes in time and space is a most relevant topic in statistical analysis from both the methodological and the application side. in this paper a price index providing a novel and effective solution to price indexes over several periods and among several countries, that is in both a multi-period and a multilateral framework, is devised. the reference basket of the devised index is the union of the intersections of the baskets of all periods/countries in pairs. as such, it provides a broader coverage than usual indexes. index closed-form expressions and updating formulas are provided and properties investigated. last, applications with real and simulated data provide evidence of the performance of the index at stake.", "categories": "econ.em", "created": "2019-09-30", "updated": "2020-05-19", "authors": ["consuelo nava", "maria grazia zoia"], "url": "https://arxiv.org/abs/1910.00073"}, {"title": "parallel algorithm for approximating nash equilibrium in multiplayer   stochastic games with application to naval strategic planning", "id": "1910.00193", "abstract": "many real-world domains contain multiple agents behaving strategically with probabilistic transitions and uncertain (potentially infinite) duration. such settings can be modeled as stochastic games. while algorithms have been developed for solving (i.e., computing a game-theoretic solution concept such as nash equilibrium) two-player zero-sum stochastic games, research on algorithms for non-zero-sum and multiplayer stochastic games is limited. we present a new algorithm for these settings, which constitutes the first parallel algorithm for multiplayer stochastic games. we present experimental results on a 4-player stochastic game motivated by a naval strategic planning scenario, showing that our algorithm is able to quickly compute strategies constituting nash equilibrium up to a very small degree of approximation error.", "categories": "cs.gt cs.ai cs.cr cs.ma econ.th", "created": "2019-10-01", "updated": "2020-03-13", "authors": ["sam ganzfried", "conner laughlin", "charles morefield"], "url": "https://arxiv.org/abs/1910.00193"}, {"title": "usage-based vehicle insurance: driving style factors of accident   probability and severity", "id": "1910.00460", "abstract": "the paper introduces an approach to telematics devices data application in automotive insurance. we conduct a comparative analysis of different types of devices that collect information on vehicle utilization and driving style of its driver, describe advantages and disadvantages of these devices and indicate the most efficient from the insurer point of view. the possible formats of telematics data are described and methods of their processing to a format convenient for modelling are proposed. we also introduce an approach to classify the accidents strength. using all the available information, we estimate accident probability models for different types of accidents and identify an optimal set of factors for each of the models. we assess the quality of resulting models using both in-sample and out-of-sample estimates.", "categories": "stat.ap cs.cy cs.lg econ.em", "created": "2019-10-01", "updated": "2019-10-04", "authors": ["konstantin korishchenko", "ivan stankevich", "nikolay pilnik", "daria petrova"], "url": "https://arxiv.org/abs/1910.00460"}, {"title": "an introduction to flexible methods for policy evaluation", "id": "1910.00641", "abstract": "this chapter covers different approaches to policy evaluation for assessing the causal effect of a treatment or intervention on an outcome of interest. as an introduction to causal inference, the discussion starts with the experimental evaluation of a randomized treatment. it then reviews evaluation methods based on selection on observables (assuming a quasi-random treatment given observed covariates), instrumental variables (inducing a quasi-random shift in the treatment), difference-in-differences and changes-in-changes (exploiting changes in outcomes over time), as well as regression discontinuities and kinks (using changes in the treatment assignment at some threshold of a running variable). the chapter discusses methods particularly suited for data with many observations for a flexible (i.e. semi- or nonparametric) modeling of treatment effects, and/or many (i.e. high dimensional) observed covariates by applying machine learning to select and control for covariates in a data-driven way. this is not only useful for tackling confounding by controlling for instance for factors jointly affecting the treatment and the outcome, but also for learning effect heterogeneities across subgroups defined upon observable covariates and optimally targeting those groups for which the treatment is most effective.", "categories": "econ.em stat.me stat.ml", "created": "2019-10-01", "updated": "", "authors": ["martin huber"], "url": "https://arxiv.org/abs/1910.00641"}, {"title": "informational content of factor structures in simultaneous binary   response models", "id": "1910.01318", "abstract": "we study the informational content of factor structures in discrete triangular systems. factor structures have been employed in a variety of settings in cross sectional and panel data models, and in this paper we formally quantify their identifying power in a bivariate system often employed in the treatment effects literature. our main findings are that imposing a factor structure yields point identification of parameters of interest, such as the coefficient associated with the endogenous regressor in the outcome equation, under weaker assumptions than usually required in these systems. in particular, we show that an exclusion restriction, requiring an explanatory variable in the outcome equation to be excluded from the treatment equation, is no longer necessary for identification. under such settings, we propose a rank estimator for both the factor loading and the causal effect parameter that are root-n consistent and asymptotically normal. the estimator's finite sample properties are evaluated through a simulation study. we also establish identification results in models with more general factor structures, that are characterized by nonparametric functional forms and multiple idiosyncratic shocks.", "categories": "econ.em", "created": "2019-10-03", "updated": "", "authors": ["shakeeb khan", "arnaud maurel", "yichong zhang"], "url": "https://arxiv.org/abs/1910.01318"}, {"title": "microfoundations of discounting", "id": "1910.02137", "abstract": "an important question in economics is how people choose between different payments in the future. the classical normative model predicts that a decision maker discounts a later payment relative to an earlier one by an exponential function of the time between them. descriptive models use non-exponential functions to fit observed behavioral phenomena, such as preference reversal. here we propose a model of discounting, consistent with standard axioms of choice, in which decision makers maximize the growth rate of their wealth. four specifications of the model produce four forms of discounting -- no discounting, exponential, hyperbolic, and a hybrid of exponential and hyperbolic -- two of which predict preference reversal. our model requires no assumption of behavioral bias or payment risk.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-10-04", "updated": "2020-01-08", "authors": ["alexander t. i. adamou", "yonatan berman", "diomides p. mavroyiannis", "ole b. peters"], "url": "https://arxiv.org/abs/1910.02137"}, {"title": "predicting popularity of ev charging infrastructure from gis data", "id": "1910.02498", "abstract": "the availability of charging infrastructure is essential for large-scale adoption of electric vehicles (ev). charging patterns and the utilization of infrastructure have consequences not only for the energy demand, loading local power grids but influence the economic returns, parking policies and further adoption of evs. we develop a data-driven approach that is exploiting predictors compiled from gis data describing the urban context and urban activities near charging infrastructure to explore correlations with a comprehensive set of indicators measuring the performance of charging infrastructure. the best fit was identified for the size of the unique group of visitors (popularity) attracted by the charging infrastructure. consecutively, charging infrastructure is ranked by popularity. the question of whether or not a given charging spot belongs to the top tier is posed as a binary classification problem and predictive performance of logistic regression regularized with an l-1 penalty, random forests and gradient boosted regression trees is evaluated. obtained results indicate that the collected predictors contain information that can be used to predict the popularity of charging infrastructure. the significance of predictors and how they are linked with the popularity are explored as well. the proposed methodology can be used to inform charging infrastructure deployment strategies.", "categories": "stat.ap cs.lg econ.em", "created": "2019-10-06", "updated": "", "authors": ["milan straka", "pasquale de falco", "gabriella ferruzzi", "daniela proto", "gijs van der poel", "shahab khormali", "\u013eubo\u0161 buzna"], "url": "https://arxiv.org/abs/1910.02498"}, {"title": "racial disparities in debt collection", "id": "1910.02570", "abstract": "a distinct set of disadvantages experienced by black americans increases their likelihood of experiencing negative financial shocks, decreases their ability to mitigate the impact of such shocks, and ultimately results in debt collection cases being far more common in black neighborhoods than in non-black neighborhoods. in this paper, we create a novel dataset that links debt collection court cases with information from credit reports to document the disparity in debt collection judgments across black and non-black neighborhoods and to explore potential mechanisms that could be driving this judgment gap. we find that majority black neighborhoods experience approximately 40% more judgments than non-black neighborhoods, even after controlling for differences in median incomes, median credit scores, and default rates. the racial disparity in judgments cannot be explained by differences in debt characteristics across black and non-black neighborhoods, nor can it be explained by differences in attorney representation, the share of contested judgments, or differences in neighborhood lending institutions.", "categories": "econ.gn q-fin.ec", "created": "2019-10-06", "updated": "", "authors": ["jessica lavoice", "domonkos f. vamossy"], "url": "https://arxiv.org/abs/1910.02570"}, {"title": "a 2-dimensional functional central limit theorem for non-stationary   dependent random fields", "id": "1910.02577", "abstract": "we obtain an elementary invariance principle for multi-dimensional brownian sheet where the underlying random fields are not necessarily independent or stationary. possible applications include unit-root tests for spatial as well as panel data models.", "categories": "math.pr econ.em", "created": "2019-10-06", "updated": "", "authors": ["michael c. tseng"], "url": "https://arxiv.org/abs/1910.02577"}, {"title": "boosting high dimensional predictive regressions with time varying   parameters", "id": "1910.03109", "abstract": "high dimensional predictive regressions are useful in wide range of applications. however, the theory is mainly developed assuming that the model is stationary with time invariant parameters. this is at odds with the prevalent evidence for parameter instability in economic time series, but theories for parameter instability are mainly developed for models with a small number of covariates. in this paper, we present two $l_2$ boosting algorithms for estimating high dimensional models in which the coefficients are modeled as functions evolving smoothly over time and the predictors are locally stationary. the first method uses componentwise local constant estimators as base learner, while the second relies on componentwise local linear estimators. we establish consistency of both methods, and address the practical issues of choosing the bandwidth for the base learners and the number of boosting iterations. in an extensive application to macroeconomic forecasting with many potential predictors, we find that the benefits to modeling time variation are substantial and they increase with the forecast horizon. furthermore, the timing of the benefits suggests that the great moderation is associated with substantial instability in the conditional mean of various economic series.", "categories": "econ.em stat.me", "created": "2019-10-07", "updated": "", "authors": ["kashif yousuf", "serena ng"], "url": "https://arxiv.org/abs/1910.03109"}, {"title": "application of machine learning in forecasting international trade   trends", "id": "1910.03112", "abstract": "international trade policies have recently garnered attention for limiting cross-border exchange of essential goods (e.g. steel, aluminum, soybeans, and beef). since trade critically affects employment and wages, predicting future patterns of trade is a high-priority for policy makers around the world. while traditional economic models aim to be reliable predictors, we consider the possibility that machine learning (ml) techniques allow for better predictions to inform policy decisions. open-government data provide the fuel to power the algorithms that can explain and forecast trade flows to inform policies. data collected in this article describe international trade transactions and commonly associated economic factors. machine learning (ml) models deployed include: arima, gboosting, xgboosting, and lightgbm for predicting future trade patterns, and k-means clustering of countries according to economic factors. unlike short-term and subjective (straight-line) projections and medium-term (aggre-gated) projections, ml methods provide a range of data-driven and interpretable projections for individual commodities. models, their results, and policies are introduced and evaluated for prediction quality.", "categories": "econ.em cs.lg", "created": "2019-10-07", "updated": "", "authors": ["feras batarseh", "munisamy gopinath", "ganesh nalluru", "jayson beckman"], "url": "https://arxiv.org/abs/1910.03112"}, {"title": "reversals of signal-posterior monotonicity imply a bias of screening", "id": "1910.03117", "abstract": "this note strengthens the main result of lagziel and lehrer (2019) (ll) \"a bias in screening\" using chambers healy (2011) (ch) \"reversals of signal-posterior monotonicity for any bounded prior\". ll show that the conditional expectation of an unobserved variable of interest, given that a noisy signal of it exceeds a cutoff, may decrease in the cutoff. ch prove that the distribution of a variable conditional on a lower signal may first order stochastically dominate the distribution conditional on a higher signal.   the nonmonotonicity result is also extended to the empirically relevant exponential and pareto distributions, and to a wide range of signals.", "categories": "econ.th", "created": "2019-10-07", "updated": "2019-11-03", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/1910.03117"}, {"title": "noncompliance in randomized control trials without exclusion   restrictions", "id": "1910.03204", "abstract": "for randomized experiments with noncompliance, i propose a method to identify treatment effects without exclusion restrictions. it exploits a baseline survey that is commonly available in randomized experiments. i show the identification of the average treatment effect on the treated (att) and the local average treatment effect (late), assuming that a baseline variable maintains similar rank orders to the control outcome. i apply this strategy to a microcredit experiment with one-sided noncompliance to identify the att. i find that the instrumental variable (iv) estimate of log revenue is 2.2 times larger than my preferred estimate. supplemental materials are available online.", "categories": "econ.gn q-fin.ec", "created": "2019-10-08", "updated": "2020-02-02", "authors": ["masayuki sawada"], "url": "https://arxiv.org/abs/1910.03204"}, {"title": "optimal control of prevention and treatment in a basic   macroeconomic-epidemiological model", "id": "1910.03383", "abstract": "we analyze the optimal control of disease prevention and treatment in a basic sis model. we develop a simple macroeconomic setup in which the social planner determines how to optimally intervene, through income taxation, in order to minimize the social cost, inclusive of infection and economic costs, of the spread of an epidemic disease. the disease lowers economic production and thus income by reducing the size of the labor force employed in productive activities, tightening thus the economy's overall resources constraint. we consider a framework in which the planner uses the collected tax revenue to intervene in either prevention (aimed at reducing the rate of infection) or treatment (aimed at increasing the speed of recovery). both optimal prevention and treatment policies allow the economy to achieve a disease-free equilibrium in the long run but their associated costs are substantially different along the transitional dynamic path. by quantifying the social costs associated with prevention and treatment we determine which policy is most cost-effective under different circumstances, showing that prevention (treatment) is desirable whenever the infectivity rate is low (high).", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-10-08", "updated": "", "authors": ["davide la torre", "tufail malik", "simone marsiglio"], "url": "https://arxiv.org/abs/1910.03383"}, {"title": "estimating and decomposing most productive scale size in parallel dea   networks with shared inputs: a case of china's five-year plans", "id": "1910.03421", "abstract": "attaining the optimal scale size of production systems is an issue frequently found in the priority questions on management agendas of various types of organizations. determining the most productive scale size (mpss) allows the decision makers not only to know the best scale size that their systems can achieve but also to tell the decision makers how to move the inefficient systems onto the mpss region. this paper investigates the mpss concept for production systems consisting of multiple subsystems connected in parallel. first, we propose a relational model where the mpss of the whole system and the internal subsystems are measured in a single dea implementation. then, it is proved that the mpss of the system can be decomposed as the weighted sum of the mpss of the individual subsystems. the main result is that the system is overall mpss if and only if it is mpss in each subsystem. mpss decomposition allows the decision makers to target the non-mpss subsystems so that the necessary improvements can be readily suggested. an application of china's five-year plans (fyps) with shared inputs is used to show the applicability of the proposed model for estimating and decomposing mpss in parallel network dea. industry and agriculture sectors are selected as two parallel subsystems in the fyps. interesting findings have been noticed. using the same amount of resources, the industry sector had a better economic scale than the agriculture sector. furthermore, the last two fyps, 11th and 12th, were the perfect two fyps among the others.", "categories": "econ.gn q-fin.ec", "created": "2019-10-08", "updated": "2019-10-10", "authors": ["saeed assani", "jianlin jiang", "ahmad assani", "feng yang"], "url": "https://arxiv.org/abs/1910.03421"}, {"title": "on the feasibility of parsimonious variable selection for hotelling's   $t^2$-test", "id": "1910.03669", "abstract": "hotelling's $t^2$-test for the mean of a multivariate normal distribution is one of the triumphs of classical multivariate analysis. it is uniformly most powerful among invariant tests, and admissible, proper bayes, and locally and asymptotically minimax among all tests. nonetheless, investigators often prefer non-invariant tests, especially those obtained by selecting only a small subset of variables from which the $t^2$-statistic is to be calculated, because such reduced statistics are more easily interpretable for their specific application. thus it is relevant to ask the extent to which power is lost when variable selection is limited to very small subsets of variables, e.g. of size one (yielding univariate student-$t^2$ tests) or size two (yielding bivariate $t^2$-tests). this study presents some evidence, admittedly fragmentary and incomplete, suggesting that in some cases no power may be lost over a wide range of alternatives.", "categories": "math.st econ.em q-bio.qm q-fin.st stat.th", "created": "2019-10-08", "updated": "", "authors": ["michael d. perlman"], "url": "https://arxiv.org/abs/1910.03669"}, {"title": "political openness and armed conflict: evidence from local councils in   colombia", "id": "1910.03712", "abstract": "in this paper, i empirically investigate how the openness of political institutions to diverse representation can impact conflict-related violence. by exploiting plausibly exogenous variations in the number of councillors in colombian municipalities, i develop two sets of results. first, regression discontinuity estimates show that larger municipal councils have a considerably greater number of political parties with at least one elected representative. i interpret this result as evidence that larger municipal councils are more open to diverse political participation. the estimates also reveal that non-traditional parties are the main beneficiaries of this greater political openness. second, regression discontinuity estimates show that political openness substantially decreases conflict-related violence, namely the killing of civilian non-combatants. by exploiting plausibly exogenous variations in local election results, i show that the lower level of political violence stems from greater participation by parties with close links to armed groups. using data about the types of violence employed by these groups, and representation at higher levels of government, i argue that armed violence has decreased not because of power-sharing arrangements involving armed groups linked to the parties with more political representation, but rather because armed groups with less political power and visibility are deterred from initiating certain types of violence.", "categories": "econ.gn q-fin.ec", "created": "2019-10-08", "updated": "2020-08-11", "authors": ["hector galindo-silva"], "url": "https://arxiv.org/abs/1910.03712"}, {"title": "most productive scale size of china's regional r&d value chain: a mixed   structure network", "id": "1910.03805", "abstract": "this paper offers new mathematical models to measure the most productive scale size (mpss) of production systems with mixed structure networks (mixed of series and parallel). in the first property, we deal with a general multi-stage network which can be transformed, using dummy processes, into a series of parallel networks. in the second property, we consider a direct network combined with series and parallel structure. in this paper, we propose new models to measure the overall mpss of the production systems and their internal processes. mpss decomposition is discussed and examined. as a real-life application, this study measures the efficiency and mpss of research and development (r&d) activities of chinese provinces within an r&d value chain network. in the r&d value chain, profitability and marketability stages are connected in series, where the profitability stage is composed of operation and r&d efforts connected in parallel. the mpss network model provides not only the mpss measurement but also values that indicate the appropriate degree of intermediate measures for the two stages. improvement strategy is given for each region based on the gap between the current and the appropriate level of intermediate measures. our findings show that the marketability efficiency values of chinese r&d regions were low, and no regions are operated under the mpss. as a result, most chinese regions performed inefficiently regarding both profitability and marketability. this finding provides initial evidence that the generally lower profitability and marketability efficiency of chinese regions is a severe problem that may be due to wasted resources on production and r&d.", "categories": "econ.gn math.oc q-fin.ec", "created": "2019-10-09", "updated": "", "authors": ["saeed assani", "jianlin jiang", "ahmad assani", "feng yang"], "url": "https://arxiv.org/abs/1910.03805"}, {"title": "quasi maximum likelihood estimation and inference of large approximate   dynamic factor models via the em algorithm", "id": "1910.03821", "abstract": "this paper studies quasi maximum likelihood estimation of dynamic factor models for large panels of time series. specifically, we consider the case in which the autocorrelation of the factors is explicitly accounted for and therefore the factor model has a state-space form. estimation of the factors and their loadings is implemented by means of the expectation maximization algorithm, jointly with the kalman smoother. we prove that, as both the dimension of the panel $n$ and the sample size $t$ diverge to infinity, the estimated loadings, factors, and common components are $\\min(\\sqrt n,\\sqrt t)$-consistent and asymptotically normal. although the model is estimated under the unrealistic constraint of independent idiosyncratic errors, this mis-specification does not affect consistency. moreover, we give conditions under which the derived asymptotic distribution can still be used for inference even in case of mis-specifications. our results are confirmed by a montecarlo simulation exercise where we compare the performance of our estimators with principal components.", "categories": "math.st econ.em stat.th", "created": "2019-10-09", "updated": "", "authors": ["matteo barigozzi", "matteo luciani"], "url": "https://arxiv.org/abs/1910.03821"}, {"title": "the impacts of the alaska permanent fund dividend on high school status   completion rates", "id": "1910.04083", "abstract": "direct cash transfer programs have shown success as poverty interventions in both the developing and developed world, yet little research exists examining the society-wide outcomes of an unconditional cash transfer program disbursed without means-testing. this paper attempts to determine the impact of direct cash transfers on educational outcomes in a developed society by investigating the impacts of the alaska permanent fund dividend, which was launched in 1982 and continues to be disbursed on an annual basis to every alaskan. a synthetic control model is deployed to examine the path of educational attainment among alaskans between 1977 and 1991 in order to determine if high school status completion rates after the launch of the dividend diverge from the synthetic in a manner suggestive of a treatment effect.", "categories": "econ.gn q-fin.ec", "created": "2019-10-09", "updated": "", "authors": ["mattathias lerner"], "url": "https://arxiv.org/abs/1910.04083"}, {"title": "identification and estimation of svarma models with independent and   non-gaussian inputs", "id": "1910.04087", "abstract": "this paper analyzes identifiability properties of structural vector autoregressive moving average (svarma) models driven by independent and non-gaussian shocks. it is well known, that svarma models driven by gaussian errors are not identified without imposing further identifying restrictions on the parameters. even in reduced form and assuming stability and invertibility, vector autoregressive moving average models are in general not identified without requiring certain parameter matrices to be non-singular. independence and non-gaussianity of the shocks is used to show that they are identified up to permutations and scalings. in this way, typically imposed identifying restrictions are made testable. furthermore, we introduce a maximum-likelihood estimator of the non-gaussian svarma model which is consistent and asymptotically normally distributed.", "categories": "econ.em", "created": "2019-10-09", "updated": "", "authors": ["bernd funovits"], "url": "https://arxiv.org/abs/1910.04087"}, {"title": "identifiability of structural singular vector autoregressive models", "id": "1910.04096", "abstract": "we generalize well-known results on structural identifiability of vector autoregressive models (var) to the case where the innovation covariance matrix has reduced rank. structural singular var models appear, for example, as solutions of rational expectation models where the number of shocks is usually smaller than the number of endogenous variables, and as an essential building block in dynamic factor models. we show that order conditions for identifiability are misleading in the singular case and provide a rank condition for identifiability of the noise parameters. since the yule-walker equations may have multiple solutions, we analyze the effect of restrictions on the system parameters on over- and underidentification in detail and provide easily verifiable conditions.", "categories": "econ.em", "created": "2019-10-09", "updated": "", "authors": ["bernd funovits", "alexander braumann"], "url": "https://arxiv.org/abs/1910.04096"}, {"title": "the disparate equilibria of algorithmic decision making when individuals   invest rationally", "id": "1910.04123", "abstract": "the long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. focusing on settings where each individual desires a positive classification---including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. by characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. we consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. we show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. in contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.", "categories": "cs.gt econ.th", "created": "2019-10-04", "updated": "", "authors": ["lydia t. liu", "ashia wilson", "nika haghtalab", "adam tauman kalai", "christian borgs", "jennifer chayes"], "url": "https://arxiv.org/abs/1910.04123"}, {"title": "taxation and social justice", "id": "1910.04155", "abstract": "the link between taxation and justice is a classic debate issue, while also being very relevant at a time of changing environmental factors and conditions of the social and economic system. technologically speaking, there are three types of taxes: progressive, proportional and regressive. although justice, like freedom, is an element and manifestation of the imagined reality in citizens minds, the state must comply with it. in particular, the tax system has to adapt to the mass imagined reality in order for it to appear fairer and more acceptable.", "categories": "econ.gn q-fin.ec", "created": "2019-10-08", "updated": "", "authors": ["boyan durankev"], "url": "https://arxiv.org/abs/1910.04155"}, {"title": "averaging estimation for instrumental variables quantile regression", "id": "1910.04245", "abstract": "this paper proposes averaging estimation methods to improve the finite-sample efficiency of the instrumental variables quantile regression (ivqr) estimation. first, i apply cheng, liao, shi's (2019) averaging gmm framework to the ivqr model. i propose using the usual quantile regression moments for averaging to take advantage of cases when endogeneity is not too strong. i also propose using two-stage least squares slope moments to take advantage of cases when heterogeneity is not too strong. the empirical optimal weight formula of cheng et al. (2019) helps optimize the bias-variance tradeoff, ensuring uniformly better (asymptotic) risk of the averaging estimator over the standard ivqr estimator under certain conditions. my implementation involves many computational considerations and builds on recent developments in the quantile literature. second, i propose a bootstrap method that directly averages among ivqr, quantile regression, and two-stage least squares estimators. more specifically, i find the optimal weights in the bootstrap world and then apply the bootstrap-optimal weights to the original sample. the bootstrap method is simpler to compute and generally performs better in simulations, but it lacks the formal uniform dominance results of cheng et al. (2019). simulation results demonstrate that in the multiple-regressors/instruments case, both the gmm averaging and bootstrap estimators have uniformly smaller risk than the ivqr estimator across data-generating processes (dgps) with all kinds of combinations of different endogeneity levels and heterogeneity levels. in dgps with a single endogenous regressor and instrument, where averaging estimation is known to have least opportunity for improvement, the proposed averaging estimators outperform the ivqr estimator in some cases but not others.", "categories": "econ.em", "created": "2019-10-09", "updated": "", "authors": ["xin liu"], "url": "https://arxiv.org/abs/1910.04245"}, {"title": "robust monopoly regulation", "id": "1910.04260", "abstract": "we study the regulation of a monopolistic firm using a robust-design approach. we solve for the policy that minimizes the regulator's worst-case regret, where the regret is the difference between his complete-information payoff minus his realized payoff. when the regulator's payoff is consumers' surplus, it is optimal to impose a price cap. the optimal cap balances the benefit from more surplus for consumers and the loss from underproduction. when his payoff is consumers' surplus plus the firm's profit, he offers a piece-rate subsidy in order to mitigate underproduction, but caps the total subsidy so as not to incentivize severe overproduction.", "categories": "econ.th cs.gt", "created": "2019-10-09", "updated": "", "authors": ["yingni guo", "eran shmaya"], "url": "https://arxiv.org/abs/1910.04260"}, {"title": "representing all stable matchings by walking a maximal chain", "id": "1910.04401", "abstract": "the seminal book of gusfield and irving [gi89] provides a compact and algorithmically useful way to represent the collection of stable matches corresponding to a given set of preferences. in this paper, we reinterpret the main results of [gi89], giving a new proof of the characterization which is able to bypass a lot of the \"theory building\" of the original works. we also provide a streamlined and efficient way to compute this representation. our proofs and algorithms emphasize the connection to well-known properties of the deferred acceptance algorithm.", "categories": "cs.gt econ.th", "created": "2019-10-10", "updated": "", "authors": ["linda cai", "clayton thomas"], "url": "https://arxiv.org/abs/1910.04401"}, {"title": "transboundary pollution externalities: think globally, act locally?", "id": "1910.04469", "abstract": "we analyze the implications of transboundary pollution externalities on environmental policymaking in a spatial and finite time horizon setting. we focus on a simple regional optimal pollution control problem in order to compare the global and local solutions in which, respectively, the transboundary externality is and is not taken into account in the determination of the optimal policy by individual local policymakers. we show that the local solution is suboptimal and as such a global approach to environmental problems is effectively needed. our conclusions hold true in different frameworks, including situations in which the spatial domain is either bounded or unbounded, and situations in which macroeconomic-environmental feedback effects are taken into account. we also show that if every local economy implements an environmental policy stringent enough, then the global average level of pollution will fall. if this is the case, over the long run the entire global economy will be able to achieve a completely pollution-free status.", "categories": "econ.th", "created": "2019-10-10", "updated": "", "authors": ["davide la torre", "danilo liuzzi", "simone marsiglio"], "url": "https://arxiv.org/abs/1910.04469"}, {"title": "risk as challenge: a dual system stochastic model for binary choice   behavior", "id": "1910.04487", "abstract": "challenge theory (ct), a new approach to decision under risk departs significantly from expected utility, and is based on firmly psychological, rather than economic, assumptions. the paper demonstrates that a purely cognitive-psychological paradigm for decision under risk can yield excellent predictions, comparable to those attained by more complex economic or psychological models that remain attached to conventional economic constructs and assumptions. the study presents a new model for predicting the popularity of choices made in binary risk problems. a ct-based regression model is tested on data gathered from 126 respondents who indicated their preferences with respect to 44 choice problems. results support ct's central hypothesis, strongly associating between the challenge index (ci) attributable to every binary risk problem, and the observed popularity of the bold prospect in that problem (with r=-0.92 and r=-0.93 for gains and for losses, respectively). the novelty of the ct perspective as a new paradigm is illuminated by its simple, single-index (ci) representation of psychological effects proposed by prospect theory for describing choice behavior (certainty effect, reflection effect, overweighting small probabilities and loss aversion).", "categories": "econ.gn q-fin.ec", "created": "2019-10-10", "updated": "", "authors": ["samuel shye", "ido haber"], "url": "https://arxiv.org/abs/1910.04487"}, {"title": "robust likelihood ratio tests for incomplete economic models", "id": "1910.04610", "abstract": "this study develops a framework for testing hypotheses on structural parameters in incomplete models. such models make set-valued predictions and hence do not generally yield a unique likelihood function. the model structure, however, allows us to construct tests based on the least favorable pairs of likelihoods using the theory of huber and strassen (1973). we develop tests robust to model incompleteness that possess certain optimality properties. we also show that sharp identifying restrictions play a role in constructing such tests in a computationally tractable manner. a framework for analyzing the local asymptotic power of the tests is developed by embedding the least favorable pairs into a model that allows local approximations under the limits of experiments argument. examples of the hypotheses we consider include those on the presence of strategic interaction effects in discrete games of complete information. monte carlo experiments demonstrate the robust performance of the proposed tests.", "categories": "econ.em math.st stat.th", "created": "2019-10-10", "updated": "2019-12-02", "authors": ["hiroaki kaido", "yi zhang"], "url": "https://arxiv.org/abs/1910.04610"}, {"title": "on existence of equilibrium under social coalition structures", "id": "1910.04648", "abstract": "in a strategic form game a strategy profile is an equilibrium if no viable coalition of agents (or players) benefits (in the pareto sense) from jointly changing their strategies. weaker or stronger equilibrium notions can be defined by considering various restrictions on coalition formation. in a nash equilibrium, for instance, the assumption is that viable coalitions are singletons, and in a super strong equilibrium, every coalition is viable. restrictions on coalition formation can be justified by communication limitations, coordination problems or institutional constraints. in this paper, inspired by social structures in various real-life scenarios, we introduce certain restrictions on coalition formation, and on their basis we introduce a number of equilibrium notions. as an application we study our equilibrium notions in resource selection games (rsgs), and we present a complete set of existence and non-existence results for general rsgs and their important special cases.", "categories": "cs.gt econ.th", "created": "2019-10-10", "updated": "", "authors": ["bugra caskurlu", "ozgun ekici", "fatih erdem kizilkaya"], "url": "https://arxiv.org/abs/1910.04648"}, {"title": "predicting auction price of vehicle license plate with deep residual   learning", "id": "1910.04879", "abstract": "due to superstition, license plates with desirable combinations of characters are highly sought after in china, fetching prices that can reach into the millions in government-held auctions. despite the high stakes involved, there has been essentially no attempt to provide price estimates for license plates. we present an end-to-end neural network model that simultaneously predict the auction price, gives the distribution of prices and produces latent feature vectors. while both types of neural network architectures we consider outperform simpler machine learning methods, convolutional networks outperform recurrent networks for comparable training time or model complexity. the resulting model powers our online price estimator and search engine.", "categories": "cs.cv cs.lg econ.gn q-fin.ec", "created": "2019-10-08", "updated": "", "authors": ["vinci chow"], "url": "https://arxiv.org/abs/1910.04879"}, {"title": "latent dirichlet analysis of categorical survey responses", "id": "1910.04883", "abstract": "beliefs are important determinants of an individual's choices and economic outcomes, so understanding how they comove and differ across individuals is of considerable interest. researchers often rely on surveys that report individual beliefs as qualitative data. we propose using a bayesian hierarchical latent class model to analyze the comovements and observed heterogeneity in categorical survey responses. we show that the statistical model corresponds to an economic structural model of information acquisition, which guides interpretation and estimation of the model parameters. an algorithm based on stochastic optimization is proposed to estimate a model for repeated surveys when responses follow a dynamic structure and conjugate priors are not appropriate. guidance on selecting the number of belief types is also provided. two examples are considered. the first shows that there is information in the michigan survey responses beyond the consumer sentiment index that is officially published. the second shows that belief types constructed from survey responses can be used in a subsequent analysis to estimate heterogeneous returns to education.", "categories": "econ.em stat.me", "created": "2019-10-10", "updated": "2020-07-17", "authors": ["evan munro", "serena ng"], "url": "https://arxiv.org/abs/1910.04883"}, {"title": "rational hyperbolic discounting", "id": "1910.05209", "abstract": "how much should you receive in a week to be indifferent to \\$ 100 in six months? note that the indifference requires a rule to ensure the similarity between early and late payments. assuming that rational individuals have low accuracy, then the following rule is valid: if the amounts to be paid are much less than the personal wealth, then the $q$-exponential discounting guarantees indifference in several periods. thus, the discounting can be interpolated between hyperbolic and exponential functions due to the low accuracy to distinguish time averages when the payments have low impact on personal wealth. therefore, there are physical conditions that allow the hyperbolic discounting regardless psycho-behavioral assumption.", "categories": "econ.gn q-fin.ec", "created": "2019-10-11", "updated": "2020-02-28", "authors": ["jos\u00e9 cl\u00e1udio do nascimento"], "url": "https://arxiv.org/abs/1910.05209"}, {"title": "measuring productivity dispersion: a parametric approach using the   l\\'{e}vy alpha-stable distribution", "id": "1910.05219", "abstract": "productivity levels and growth are extremely heterogeneous among firms. a vast literature has developed to explain the origins of productivity shocks, their dispersion, evolution and their relationship to the business cycle. we examine in detail the distribution of labor productivity levels and growth, and observe that they exhibit heavy tails. we propose to model these distributions using the four parameter l\\'{e}vy stable distribution, a natural candidate deriving from the generalised central limit theorem. we show that it is a better fit than several standard alternatives, and is remarkably consistent over time, countries and sectors. in all samples considered, the tail parameter is such that the theoretical variance of the distribution is infinite, so that the sample standard deviation increases with sample size. we find a consistent positive skewness, a markedly different behaviour between the left and right tails, and a positive relationship between productivity and size. the distributional approach allows us to test different measures of dispersion and find that productivity dispersion has slightly decreased over the past decade.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2019-10-11", "updated": "", "authors": ["jangho yang", "torsten heinrich", "julian winkler", "fran\u00e7ois lafond", "pantelis koutroumpis", "j. doyne farmer"], "url": "https://arxiv.org/abs/1910.05219"}, {"title": "a geometric model of opinion polarization", "id": "1910.05274", "abstract": "we introduce a simple, geometric model of opinion polarization. it is a model of political persuasion, as well as marketing and advertising, utilizing social values. it focuses on the interplay between different topics and wide-reaching persuasion efforts in the media. we demonstrate that societal opinion polarization in our model often arises as an unintended byproduct of organizations attempting to sell a product or idea. we discuss some exploratory examples, analyzing how polarization occurs and evolves. we also examine some computational aspects of choosing the most effective means of influencing agents in our model, and the connections of those strategic considerations with polarization.", "categories": "cs.si econ.th", "created": "2019-10-11", "updated": "2020-07-10", "authors": ["jan h\u0105z\u0142a", "yan jin", "elchanan mossel", "govind ramnarayan"], "url": "https://arxiv.org/abs/1910.05274"}, {"title": "networks of monetary flow at native resolution", "id": "1910.05596", "abstract": "people and companies move money with every financial transaction they make. we aim to understand how such activity gives rise to large-scale patterns of monetary flow. in this work, we trace the movement of e-money through the accounts of a mobile money system using the provider's own transaction records. the resulting transaction sequences---balance-respecting trajectories---are data objects that represent observed monetary flows. common sequential motifs correspond to known use-cases of mobile money: digital payments, digital transfers, and money storage. we find that each activity creates a distinct network structure within the system, and we uncover coordinated gaming of the mobile money provider's commission schedule. moreover, we find that e-money passes through the system in anywhere from minutes to months. this pronounced heterogeneity, even within the same use-case, can inform the modeling of turnover in money supply. our methodology relates economic activity at the transaction level to large-scale patterns of monetary flow, broadening the scope of empirical study about the network and temporal structure of the economy.", "categories": "physics.soc-ph cs.cy econ.gn q-fin.ec", "created": "2019-10-12", "updated": "", "authors": ["carolina mattsson"], "url": "https://arxiv.org/abs/1910.05596"}, {"title": "universal basic income: the last bullet in the darkness", "id": "1910.05658", "abstract": "universal basic income (ubi) has recently been gaining traction. arguments exist on both sides in favor of and against it. like any other financial tool, ubi can be useful if used with discretion. this paper seeks to clarify how ubi affects the economy, including how it can be beneficial. the key point is to regulate the rate of ubi based on the inflation rate. this should be done by an independent institution from the executive branch of the government. if implemented correctly, ubi can add a powerful tool to the federal reserve toolkit. ubi can be used to reintroduce inflation to the countries which suffer long-lasting deflationary environment. ubi has the potential to decrease the wealth disparity, decrease the national debt, increase productivity, and increase comparative advantage of the economy. ubi also can substitute the current welfare systems because of its transparency and efficiency. this article focuses more on the united states, but similar ideas can be implemented in other developed nations.", "categories": "econ.gn q-fin.ec", "created": "2019-10-12", "updated": "", "authors": ["mohammad rasoolinejad"], "url": "https://arxiv.org/abs/1910.05658"}, {"title": "principled estimation of regression discontinuity designs", "id": "1910.06381", "abstract": "regression discontinuity designs are frequently used to estimate the causal effect of election outcomes and policy interventions. in these contexts, treatment effects are typically estimated with covariates included to improve efficiency. while including covariates improves precision asymptotically, in practice, treatment effects are estimated with a small number of observations, resulting in considerable fluctuations in treatment effect magnitude and precision depending upon the covariates chosen. this practice thus incentivizes researchers to select covariates which maximize treatment effect statistical significance rather than precision. here, i propose a principled approach for estimating rdds which provides a means of improving precision with covariates while minimizing adverse incentives. this is accomplished by integrating the adaptive lasso, a machine learning method, into rdd estimation using an r package developed for this purpose, adaptiverdd. using simulations, i show that this method significantly improves treatment effect precision, particularly when estimating treatment effects with fewer than 200 observations.", "categories": "stat.ap econ.em", "created": "2019-10-14", "updated": "2020-05-04", "authors": ["l. jason anastasopoulos"], "url": "https://arxiv.org/abs/1910.06381"}, {"title": "precisamos de uma contabilidade ambiental para as \"amaz\\^onias\"   paraense?", "id": "1910.06499", "abstract": "this paper has the following objectives: to understand the concepts of environmental accounting in brazil; make criticisms and propositions anchored in the reality or demand of environmental accounting for amazonia paraense. the methodological strategy was a critical analysis of ferreira's books (2007); ribeiro (2010) and tinoco and kraemer (2011) using their correlation with the scientific production of authors discussing the paraense amazon, besides our experience as researchers of this territory. as a result, we created three sections: one for understanding the current constructs of environmental accounting, one for criticism and one for propositions.", "categories": "econ.gn q-fin.ec", "created": "2019-10-14", "updated": "", "authors": ["ailton castro pinheiro"], "url": "https://arxiv.org/abs/1910.06499"}, {"title": "matrix completion, counterfactuals, and factor analysis of missing data", "id": "1910.06677", "abstract": "this paper suggests an imputation procedure that uses the factors estimated from a \\textsc{tall} block along with the re-rotated loadings estimated from a \\textsc{wide} block to impute missing values in a panel of data. under a strong factor assumption, it is shown that the common component can be consistently estimated but there will be four different convergence rates. re-estimation of the factors from the imputed data matrix can accelerate convergence. a complete characterization of the sampling error is obtained without requiring regularization or imposing the missing at random assumption. under the assumption that potential outcome has a factor structure, we provide a distribution theory for the estimated average and individual treatment effects on the treated.", "categories": "econ.em", "created": "2019-10-15", "updated": "2020-05-07", "authors": ["jushan bai", "serena ng"], "url": "https://arxiv.org/abs/1910.06677"}, {"title": "the cobb-douglas production function revisited", "id": "1910.06739", "abstract": "charles cobb and paul douglas in 1928 used data from the us manufacturing sector for 1899-1922 to introduce what is known today as the cobb-douglas production function that has been widely used in economic theory for decades. we employ the r programming language to fit the formulas for the parameters of the cobb-douglas production function generated by the authors recently via the bi-hamiltonian approach to the same data set utilized by cobb and douglas. we conclude that the formulas for the output elasticities and total factor productivity are compatible with the original 1928 data.", "categories": "econ.gn math.sg q-fin.ec", "created": "2019-10-11", "updated": "2019-10-20", "authors": ["roman g. smirnov", "kunpeng wang"], "url": "https://arxiv.org/abs/1910.06739"}, {"title": "dynamically aggregating diverse information", "id": "1910.07015", "abstract": "an agent has access to multiple information sources, each of which provides information about a different attribute of an unknown state. information is acquired continuously\\textemdash where the agent chooses both which sources to sample from, and also how to allocate attention across them\\textemdash until an endogenously chosen time, at which point a decision is taken. we provide an exact characterization of the optimal information acquisition strategy for settings where the attributes are not too strongly correlated. we then apply this characterization to derive new results regarding: (1) endogenous information acquisition for binary choice, and (2) strategic information provision by competing news sources.", "categories": "econ.th cs.gt", "created": "2019-10-15", "updated": "2020-07-12", "authors": ["annie liang", "xiaosheng mu", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/1910.07015"}, {"title": "games of incomplete information played by statisticians", "id": "1910.07018", "abstract": "players are statistical learners who learn about payoffs from data. they may interpret the same data differently, but have common knowledge of a class of learning procedures. i propose a metric for the analyst's \"confidence\" in a strategic prediction, based on the probability that the prediction is consistent with the realized data. the main results characterize the analyst's confidence in a given prediction as the quantity of data grows large, and provide bounds for small datasets. the approach generates new predictions, e.g. that speculative trade is more likely given high-dimensional data, and that coordination is less likely given noisy data.", "categories": "econ.th cs.gt", "created": "2019-10-15", "updated": "2020-07-09", "authors": ["annie liang"], "url": "https://arxiv.org/abs/1910.07018"}, {"title": "measuring the completeness of theories", "id": "1910.07022", "abstract": "we use machine learning to provide a tractable measure of the amount of predictable variation in the data that a theory captures, which we call its \"completeness.\" we apply this measure to three problems: assigning certain equivalents to lotteries, initial play in games, and human generation of random sequences. we discover considerable variation in the completeness of existing models, which sheds light on whether to focus on developing better models with the same features or instead to look for new features that will improve predictions. we also illustrate how and why completeness varies with the experiments considered, which highlights the role played in choosing which experiments to run.", "categories": "econ.th cs.gt cs.lg", "created": "2019-10-15", "updated": "", "authors": ["drew fudenberg", "jon kleinberg", "annie liang", "sendhil mullainathan"], "url": "https://arxiv.org/abs/1910.07022"}, {"title": "multivariate forecasting evaluation: on sensitive and strictly proper   scoring rules", "id": "1910.07325", "abstract": "in recent years, probabilistic forecasting is an emerging topic, which is why there is a growing need of suitable methods for the evaluation of multivariate predictions. we analyze the sensitivity of the most common scoring rules, especially regarding quality of the forecasted dependency structures. additionally, we propose scoring rules based on the copula, which uniquely describes the dependency structure for every probability distribution with continuous marginal distributions. efficient estimation of the considered scoring rules and evaluation methods such as the diebold-mariano test are discussed. in detailed simulation studies, we compare the performance of the renowned scoring rules and the ones we propose. besides extended synthetic studies based on recently published results we also consider a real data example. we find that the energy score, which is probably the most widely used multivariate scoring rule, performs comparably well in detecting forecast errors, also regarding dependencies. this contradicts other studies. the results also show that a proposed copula score provides very strong distinction between models with correct and incorrect dependency structure. we close with a comprehensive discussion on the proposed methodology.", "categories": "stat.me econ.em stat.ml stat.ot", "created": "2019-10-16", "updated": "", "authors": ["florian ziel", "kevin berk"], "url": "https://arxiv.org/abs/1910.07325"}, {"title": "standard errors for panel data models with unknown clusters", "id": "1910.07406", "abstract": "this paper develops a new standard-error estimator for linear panel data models. the proposed estimator is robust to heteroskedasticity, serial correlation, and cross-sectional correlation of unknown forms. the serial correlation is controlled by the newey-west method. to control for cross-sectional correlations, we propose to use the thresholding method, without assuming the clusters to be known. we establish the consistency of the proposed estimator. monte carlo simulations show the method works well. an empirical application is considered.", "categories": "econ.em", "created": "2019-10-16", "updated": "2020-05-18", "authors": ["jushan bai", "sung hoon choi", "yuan liao"], "url": "https://arxiv.org/abs/1910.07406"}, {"title": "identifying network ties from panel data: theory and an application to   tax competition", "id": "1910.07452", "abstract": "social interactions determine many economic behaviors, but information on social ties does not exist in most publicly available and widely used datasets. we present results on the identification of social networks from observational panel data that contains no information on social ties between agents. in the context of a canonical social interactions model, we provide sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified. while this result is relevant across different estimation strategies, we then describe how high-dimensional estimation techniques can be used to estimate the interactions model based on the adaptive elastic net gmm method. we employ the method to study tax competition across us states. we find the identified social interactions matrix implies tax competition differs markedly from the common assumption of competition between geographically neighboring states, providing further insights for the long-standing debate on the relative roles of factor mobility and yardstick competition in driving tax setting behavior across states. most broadly, our identification and application show the analysis of social interactions can be extended to economic realms where no network data exists.", "categories": "econ.em stat.ap", "created": "2019-10-16", "updated": "2020-04-07", "authors": ["aureo de paula", "imran rasul", "pedro souza"], "url": "https://arxiv.org/abs/1910.07452"}, {"title": "asymptotic theory of $l$-statistics and integrable empirical processes", "id": "1910.07572", "abstract": "this paper develops asymptotic theory of integrals of empirical quantile functions with respect to random weight functions, which is an extension of classical $l$-statistics. they appear when sample trimming or winsorization is applied to asymptotically linear estimators. the key idea is to consider empirical processes in the spaces appropriate for integration. first, we characterize weak convergence of empirical distribution functions and random weight functions in the space of bounded integrable functions. second, we establish the delta method for empirical quantile functions as integrable functions. third, we derive the delta method for $l$-statistics. finally, we prove weak convergence of their bootstrap processes, showing validity of nonparametric bootstrap.", "categories": "math.st econ.em stat.th", "created": "2019-10-16", "updated": "", "authors": ["tetsuya kaji"], "url": "https://arxiv.org/abs/1910.07572"}, {"title": "a projection framework for testing shape restrictions that form convex   cones", "id": "1910.07689", "abstract": "this paper develops a uniformly valid and asymptotically nonconservative test based on projection for a class of shape restrictions. the key insight we exploit is that these restrictions form convex cones, a simple and yet elegant structure that has been barely harnessed in the literature. based on a monotonicity property afforded by such a geometric structure, we construct a bootstrap procedure that, unlike many studies in nonstandard settings, dispenses with estimation of local parameter spaces, and the critical values are obtained in a way as simple as computing the test statistic. moreover, by appealing to strong approximations, our framework accommodates nonparametric regression models as well as distributional/density-related and structural settings. since the test entails a tuning parameter (due to the nonstandard nature of the problem), we propose a data-driven choice and prove its validity. monte carlo simulations confirm that our test works well.", "categories": "econ.em math.st stat.me stat.th", "created": "2019-10-16", "updated": "2020-08-30", "authors": ["zheng fang", "juwon seo"], "url": "https://arxiv.org/abs/1910.07689"}, {"title": "fighting for not-so-religious souls: the role of religious competition   in non-religious conflicts", "id": "1910.07707", "abstract": "many countries embroiled in non-religious civil conflicts have experienced a dramatic increase in religious competition in recent years. this study examines whether increasing competition between religions affects violence in non-religious conflicts. the study focuses on colombia, a deeply catholic country that has suffered one of the world's longest-running internal conflicts and, in the last few decades, has witnessed an intense increase in religious competition between the catholic church and new non-catholic churches. the estimation of a dynamic treatment effect model shows that establishing the first non-catholic church in a municipality substantially increases the probability of an attack by a left-wing guerrilla group. further analysis suggests that the increase in guerrilla attacks is associated with the expectation among guerrilla groups that their membership will decline as a consequence of more intense competition with religious groups for followers.", "categories": "econ.gn q-fin.ec", "created": "2019-10-17", "updated": "2020-08-11", "authors": ["hector galindo-silva", "guy tchuente"], "url": "https://arxiv.org/abs/1910.07707"}, {"title": "econometric models of network formation", "id": "1910.07781", "abstract": "this article provides a selective review on the recent literature on econometric models of network formation. the survey starts with a brief exposition on basic concepts and tools for the statistical description of networks. i then offer a review of dyadic models, focussing on statistical models on pairs of nodes and describe several developments of interest to the econometrics literature. the article also presents a discussion of non-dyadic models where link formation might be influenced by the presence or absence of additional links, which themselves are subject to similar influences. this is related to the statistical literature on conditionally specified models and the econometrics of game theoretical models. i close with a (non-exhaustive) discussion of potential areas for further development.", "categories": "econ.em", "created": "2019-10-17", "updated": "2020-01-11", "authors": ["aureo de paula"], "url": "https://arxiv.org/abs/1910.07781"}, {"title": "currency based on time standard", "id": "1910.07859", "abstract": "the total economic time capacity of a year 525600 minutes is postulated as a time standard for a new monetary minute currency in this evaluation study. consequently, the monetary minute monmin is defined as a 1/525600 part of the total economic time capacity of a year. the value cmonmin of the monetary minute monmin is equal to a 1/525600 part of the gdp, p.c., expressed in a specific state currency c. there is described how the monetary minutes monmin are determined, and how their values cmonmin are calculated based on the gdp and all the population in specific economies. the monetary minutes trace different aggregate productivity, i.e. exploitation of the total time capacity of a year for generating of the gdp in economies of different states.", "categories": "econ.gn q-fin.ec", "created": "2019-10-17", "updated": "", "authors": ["tomas kala"], "url": "https://arxiv.org/abs/1910.07859"}, {"title": "forecasting under long memory and nonstationarity", "id": "1910.08202", "abstract": "long memory in the sense of slowly decaying autocorrelations is a stylized fact in many time series from economics and finance. the fractionally integrated process is the workhorse model for the analysis of these time series. nevertheless, there is mixed evidence in the literature concerning its usefulness for forecasting and how forecasting based on it should be implemented.   employing pseudo-out-of-sample forecasting on inflation and realized volatility time series and simulations we show that methods based on fractional integration clearly are superior to alternative methods not accounting for long memory, including autoregressions and exponential smoothing. our proposal of choosing a fixed fractional integration parameter of $d=0.5$ a priori yields the best results overall, capturing long memory behavior, but overcoming the deficiencies of methods using an estimated parameter.   regarding the implementation of forecasting methods based on fractional integration, we use simulations to compare local and global semiparametric and parametric estimators of the long memory parameter from the whittle family and provide asymptotic theory backed up by simulations to compare different mean estimators. both of these analyses lead to new results, which are also of interest outside the realm of forecasting.", "categories": "econ.em", "created": "2019-10-17", "updated": "", "authors": ["uwe hassler", "marc-oliver pohle"], "url": "https://arxiv.org/abs/1910.08202"}, {"title": "large dimensional latent factor modeling with missing observations and   applications to causal inference", "id": "1910.08273", "abstract": "this paper develops the inferential theory for latent factor models estimated from large dimensional panel data with missing observations. we estimate a latent factor model by applying principal component analysis to an adjusted covariance matrix estimated from partially observed panel data. we derive the asymptotic distribution for the estimated factors, loadings and the imputed values under a general approximate factor model. the key application is to estimate counterfactual outcomes in causal inference from panel data. the unobserved control group is modeled as missing values, which are inferred from the latent factor model. the inferential theory for the imputed values allows us to test for individual treatment effects at any time. we apply our method to portfolio investment strategies and find that around 14% of their average returns are significantly reduced by the academic publication of these strategies.", "categories": "econ.em", "created": "2019-10-18", "updated": "2019-11-22", "authors": ["ruoxuan xiong", "markus pelger"], "url": "https://arxiv.org/abs/1910.08273"}, {"title": "beating the house: identifying inefficiencies in sports betting markets", "id": "1910.08858", "abstract": "inefficient markets allow investors to consistently outperform the market. to demonstrate that inefficiencies exist in sports betting markets, we created a betting algorithm that generates above market returns for the nfl, nba, ncaaf, ncaab, and wnba betting markets. to formulate our betting strategy, we collected and examined a novel dataset of bets, and created a non-parametric win probability model to find positive expected value situations. as the united states supreme court has recently repealed the federal ban on sports betting, research on sports betting markets is increasingly relevant for the growing sports betting industry.", "categories": "econ.gn q-fin.ec q-fin.gn stat.ap", "created": "2019-10-19", "updated": "2019-10-22", "authors": ["sathya ramesh", "ragib mostofa", "marco bornstein", "john dobelman"], "url": "https://arxiv.org/abs/1910.08858"}, {"title": "overcoming free-riding in bandit games", "id": "1910.08953", "abstract": "this paper considers a class of experimentation games with l\\'{e}vy bandits encompassing those of bolton and harris (1999) and keller, rady and cripps (2005). its main result is that efficient (perfect bayesian) equilibria exist whenever players' payoffs have a diffusion component. hence, the trade-offs emphasized in the literature do not rely on the intrinsic nature of bandit models but on the commonly adopted solution concept (mpe). this is not an artifact of continuous time: we prove that such equilibria arise as limits of equilibria in the discrete-time game. furthermore, it suffices to relax the solution concept to strongly symmetric equilibrium.", "categories": "econ.th cs.gt", "created": "2019-10-20", "updated": "", "authors": ["johannes h\u00f6rner", "nicolas klein", "sven rady"], "url": "https://arxiv.org/abs/1910.08953"}, {"title": "feasible generalized least squares for panel data with cross-sectional   and serial correlations", "id": "1910.09004", "abstract": "this paper considers generalized least squares (gls) estimation for linear panel data models. by estimating the large error covariance matrix consistently, the proposed feasible gls (fgls) estimator is more efficient than the ordinary least squares (ols) in the presence of heteroskedasticity, serial, and cross-sectional correlations. to take into account the serial correlations, we employ the banding method. to take into account the cross-sectional correlations, we suggest to use the thresholding method. we establish the limiting distribution of the proposed estimator. a monte carlo study is considered. the proposed method is applied to an empirical application.", "categories": "econ.em", "created": "2019-10-20", "updated": "2020-08-04", "authors": ["jushan bai", "sung hoon choi", "yuan liao"], "url": "https://arxiv.org/abs/1910.09004"}, {"title": "multi-stage compound real options valuation in residential pv-battery   investment", "id": "1910.09132", "abstract": "strategic valuation of efficient and well-timed network investments under uncertain electricity market environment has become increasingly challenging, because there generally exist multiple interacting options in these investments, and failing to systematically consider these options can lead to decisions that undervalue the investment. in our work, a real options valuation (rov) framework is proposed to determine the optimal strategy for executing multiple interacting options within a distribution network investment, to mitigate the risk of financial losses in the presence of future uncertainties. to demonstrate the characteristics of the proposed framework, we determine the optimal strategy to economically justify the investment in residential pv-battery systems for additional grid supply during peak demand periods. the options to defer, and then expand, are considered as multi-stage compound options, since the option to expand is a subsequent option of the former. these options are valued via the least squares monte carlo method, incorporating uncertainty over growing power demand, varying diesel fuel price, and the declining cost of pv-battery technology as random variables. finally, a sensitivity analysis is performed to demonstrate how the proposed framework responds to uncertain events. the proposed framework shows that executing the interacting options at the optimal timing increases the investment value.", "categories": "econ.em q-fin.gn", "created": "2019-10-20", "updated": "", "authors": ["yiju ma", "kevin swandi", "archie chapman", "gregor verbic"], "url": "https://arxiv.org/abs/1910.09132"}, {"title": "pricing mechanism for resource sustainability in competitive online   learning multi-agent systems", "id": "1910.09314", "abstract": "in this paper, we consider the problem of resource congestion control for competing online learning agents. on the basis of non-cooperative game as the model for the interaction between the agents, and the noisy online mirror ascent as the model for rational behavior of the agents, we propose a novel pricing mechanism which gives the agents incentives for sustainable use of the resources. our mechanism is distributed and resource-centric, in the sense that it is done by the resources themselves and not by a centralized instance, and that it is based rather on the congestion state of the resources than the preferences of the agents. in case that the noise is persistent, and for several choices of the intrinsic parameter of the agents, such as their learning rate, and of the mechanism parameters, such as the learning rate of -, the progressivity of the price-setters, and the extrinsic price sensitivity of the agents, we show that the accumulative violation of the resource constraints of the resulted iterates is sub-linear w.r.t. the time horizon. moreover, we provide numerical simulations to support our theoretical findings.", "categories": "cs.lg cs.gt cs.ma cs.sy econ.th eess.sy", "created": "2019-10-21", "updated": "", "authors": ["ezra tampubolon", "holger boche"], "url": "https://arxiv.org/abs/1910.09314"}, {"title": "a path-sampling method to partially identify causal effects in   instrumental variable models", "id": "1910.09502", "abstract": "partial identification approaches are a flexible and robust alternative to standard point-identification approaches in general instrumental variable models. however, this flexibility comes at the cost of a ``curse of cardinality'': the number of restrictions on the identified set grows exponentially with the number of points in the support of the endogenous treatment. this article proposes a novel path-sampling approach to this challenge. it is designed for partially identifying causal effects of interest in the most complex models with continuous endogenous treatments. a stochastic process representation allows to seamlessly incorporate assumptions on individual behavior into the model. some potential applications include dose-response estimation in randomized trials with imperfect compliance, the evaluation of social programs, welfare estimation in demand models, and continuous choice models. as a demonstration, the method provides informative nonparametric bounds on household expenditures under the assumption that expenditure is continuous. the mathematical contribution is an approach to approximately solving infinite dimensional linear programs on path spaces via sampling.", "categories": "econ.em math.st stat.th", "created": "2019-10-21", "updated": "2020-06-29", "authors": ["florian gunsilius"], "url": "https://arxiv.org/abs/1910.09502"}, {"title": "relative net utility and the saint petersburg paradox", "id": "1910.09544", "abstract": "the famous saint petersburg paradox (st. petersburg paradox) shows that the theory of expected value does not capture the real-world economics of decision-making problems. over the years, many economic theories were developed to resolve the paradox and explain gaps in the economic value theory in the evaluation of economic decisions, the subjective utility of the expected outcomes, and risk aversion as observed in the game of the st. petersburg paradox. in this paper, we use the concept of the relative net utility to resolve the st. petersburg paradox. because the net utility concept is able to explain both behavioral economics and the st. petersburg paradox, it is deemed to be a universal approach to handling utility. this paper shows how the information content of the notion of net utility value allows us to capture a broader context of the impact of a decision's possible achievements. it discusses the necessary conditions that the utility function has to conform to avoid the paradox. combining these necessary conditions allows us to define the theorem of indifference in the evaluation of economic decisions and to present the role of the relative net utility and net utility polarity in a value rational decision-making process.", "categories": "econ.gn cs.ai q-fin.ec", "created": "2019-10-21", "updated": "2020-05-18", "authors": ["daniel muller", "tshilidzi marwala"], "url": "https://arxiv.org/abs/1910.09544"}, {"title": "quasi maximum likelihood estimation of non-stationary large approximate   dynamic factor models", "id": "1910.09841", "abstract": "this paper considers estimation of large dynamic factor models with common and idiosyncratic trends by means of the expectation maximization algorithm, implemented jointly with the kalman smoother. we show that, as the cross-sectional dimension $n$ and the sample size $t$ diverge to infinity, the common component for a given unit estimated at a given point in time is $\\min(\\sqrt n,\\sqrt t)$-consistent. the case of local levels and/or local linear trends trends is also considered. by means of a montecarlo simulation exercise, we compare our approach with estimators based on principal component analysis.", "categories": "econ.em", "created": "2019-10-22", "updated": "", "authors": ["matteo barigozzi", "matteo luciani"], "url": "https://arxiv.org/abs/1910.09841"}, {"title": "mesoscale impact of trader psychology on stock markets: a multi-agent ai   approach", "id": "1910.10099", "abstract": "recent advances in the fields of machine learning and neurofinance have yielded new exciting research perspectives in practical inference of behavioural economy in financial markets and microstructure study. we here present the latest results from a recently published stock market simulator built around a multi-agent system architecture, in which each agent is an autonomous investor trading stocks by reinforcement learning (rl) via a centralised double-auction limit order book. the rl framework allows for the implementation of specific behavioural and cognitive traits known to trader psychology, and thus to study the impact of these traits on the whole stock market at the mesoscale. more precisely, we narrowed our agent design to three such psychological biases known to have a direct correspondence with rl theory, namely delay discounting, greed, and fear. we compared ensuing simulated data to real stock market data over the past decade or so, and find that market stability benefits from larger populations of agents prone to delay discounting and most astonishingly, to greed.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.st", "created": "2019-10-10", "updated": "", "authors": ["j. lussange", "s. palminteri", "s. bourgeois-gironde", "b. gutkin"], "url": "https://arxiv.org/abs/1910.10099"}, {"title": "principal component analysis: a generalized gini approach", "id": "1910.10133", "abstract": "a principal component analysis based on the generalized gini correlation index is proposed (gini pca). the gini pca generalizes the standard pca based on the variance. it is shown, in the gaussian case, that the standard pca is equivalent to the gini pca. it is also proven that the dimensionality reduction based on the generalized gini correlation matrix, that relies on city-block distances, is robust to outliers. monte carlo simulations and an application on cars data (with outliers) show the robustness of the gini pca and provide different interpretations of the results compared with the variance pca.", "categories": "stat.me econ.em stat.co", "created": "2019-10-22", "updated": "", "authors": ["n/a charpentier", "n/a arthur", "n/a mussard", "n/a stephane", "tea ouraga"], "url": "https://arxiv.org/abs/1910.10133"}, {"title": "how well can we learn large factor models without assuming strong   factors?", "id": "1910.10382", "abstract": "in this paper, we consider the problem of learning models with a latent factor structure. the focus is to find what is possible and what is impossible if the usual strong factor condition is not imposed. we study the minimax rate and adaptivity issues in two problems: pure factor models and panel regression with interactive fixed effects. for pure factor models, if the number of factors is known, we develop adaptive estimation and inference procedures that attain the minimax rate. however, when the number of factors is not specified a priori, we show that there is a tradeoff between validity and efficiency: any confidence interval that has uniform validity for arbitrary factor strength has to be conservative; in particular its width is bounded away from zero even when the factors are strong. conversely, any data-driven confidence interval that does not require as an input the exact number of factors (including weak ones) and has shrinking width under strong factors does not have uniform coverage and the worst-case coverage probability is at most 1/2. for panel regressions with interactive fixed effects, the tradeoff is much better. we find that the minimax rate for learning the regression coefficient does not depend on the factor strength and propose a simple estimator that achieves this rate. however, when weak factors are allowed, uncertainty in the number of factors can cause a great loss of efficiency although the rate is not affected. in most cases, we find that the strong factor condition (and/or exact knowledge of number of factors) improves efficiency, but this condition needs to be imposed by faith and cannot be verified in data for inference purposes.", "categories": "math.st econ.em stat.th", "created": "2019-10-23", "updated": "2019-11-06", "authors": ["yinchu zhu"], "url": "https://arxiv.org/abs/1910.10382"}, {"title": "nonparametric identification of an interdependent value model with buyer   covariates from first-price auction bids", "id": "1910.10646", "abstract": "this paper introduces a version of the interdependent value model of milgrom and weber (1982), where the signals are given by an index gathering signal shifters observed by the econometrician and private ones specific to each bidders. the model primitives are shown to be nonparametrically identified from first-price auction bids under a testable mild rank condition. identification holds for all possible signal values. this allows to consider a wide range of counterfactuals where this is important, as expected revenue in second-price auction. an estimation procedure is briefly discussed.", "categories": "econ.em", "created": "2019-10-23", "updated": "", "authors": ["nathalie gimenes", "emmanuel guerre"], "url": "https://arxiv.org/abs/1910.10646"}, {"title": "convex relaxation based locational marginal prices", "id": "1910.10673", "abstract": "we propose and analyze semidefinite relaxation based locational marginal prices (rlmps) for real and reactive power in electricity markets. our analysis reveals that when the non-convex economic dispatch problem has zero duality gap, the rlmps exhibit properties similar to locational marginal prices with linearized power flow equations. otherwise, they behave similar to convex hull prices. restricted to radial distribution networks, rlmps reduce to second-order cone relaxation based distribution locational marginal prices. we illustrate our theoretical results on numerical examples.", "categories": "eess.sy cs.sy econ.gn q-fin.ec", "created": "2019-10-23", "updated": "2020-08-30", "authors": ["anna winnicki", "mariola ndrio", "subhonmesh bose"], "url": "https://arxiv.org/abs/1910.10673"}, {"title": "fast and flexible bayesian inference in time-varying parameter   regression models", "id": "1910.10779", "abstract": "in this paper, we write the time-varying parameter regression model involving k explanatory variables and t observations as a constant coefficient regression model with tk explanatory variables. in contrast with much of the existing literature which assumes coefficients to evolve according to a random walk, this specification does not restrict the form that the time-variation in coefficients can take. we develop computationally efficient bayesian econometric methods based on the singular value decomposition of the tk regressors. in artificial data, we find our methods to be accurate and much faster than standard approaches in terms of computation time. in an empirical exercise involving inflation forecasting using a large number of predictors, we find our methods to forecast better than alternative approaches and document different patterns of parameter change than are found with approaches which assume random walk evolution of parameters.", "categories": "econ.em stat.co", "created": "2019-10-23", "updated": "2020-01-12", "authors": ["niko hauzenberger", "florian huber", "gary koop", "luca onorante"], "url": "https://arxiv.org/abs/1910.10779"}, {"title": "necessary and sufficient condition for equilibrium of the hotelling   model on a circle", "id": "1910.11154", "abstract": "we study a model of vendors competing to sell a homogeneous product to customers spread evenly along a circular city. this model is based on hotelling's celebrated paper in 1929. our aim in this paper is to present a necessary and sufficient condition for the equilibrium. this yields a representation for the equilibrium. to achieve this, we first formulate the model mathematically. next, we prove that the condition holds if and only if vendors are equilibrium.", "categories": "econ.th", "created": "2019-10-23", "updated": "2019-10-28", "authors": ["satoshi hayashi", "naoki tsuge"], "url": "https://arxiv.org/abs/1910.11154"}, {"title": "coalition-structured governance improves cooperation to provide public   goods", "id": "1910.11337", "abstract": "while the benefits of common and public goods are shared, they tend to be scarce when contributions are provided voluntarily. failure to cooperate in the provision or preservation of these goods is fundamental to sustainability challenges, ranging from local fisheries to global climate change. in the real world, such cooperative dilemmas occur in multiple interactions with complex strategic interests and frequently without full information. we argue that voluntary cooperation enabled across multiple coalitions (akin to polycentricity) not only facilitates greater generation of non-excludable public goods, but may also allow evolution toward a more cooperative, stable, and inclusive approach to governance. contrary to any previous study, we show that these merits of multi-coalition governance are far more general than the singular examples occurring in the literature, and are robust under diverse conditions of excludability, congestability of the non-excludable public good, and arbitrary shapes of the return-to-contribution function. we first confirm the intuition that a single coalition without enforcement and with players pursuing their self-interest without knowledge of returns to contribution is prone to cooperative failure. next, we demonstrate that the same pessimistic model but with a multi-coalition structure of governance experiences relatively higher cooperation by enabling recognition of marginal gains of cooperation in the game at stake. in the absence of enforcement, public-goods regimes that evolve through a proliferation of voluntary cooperative forums can maintain and increase cooperation more successfully than singular, inclusive regimes.", "categories": "econ.gn nlin.ao q-fin.ec", "created": "2019-10-23", "updated": "", "authors": ["v\u00edtor v. vasconcelos", "phillip m. hannam", "simon a. levin", "jorge m. pacheco"], "url": "https://arxiv.org/abs/1910.11337"}, {"title": "the persuasion duality", "id": "1910.11392", "abstract": "we present a unified duality approach to bayesian persuasion. the optimal dual variable, interpreted as a price function, is shown to be a supergradient of the concave closure of the objective function at the prior belief. under regularity conditions, our general duality result implies known results for the case when the objective function depends only on the expected state. we apply our approach to characterize the optimal signal in the case when the state is two-dimensional.", "categories": "econ.th", "created": "2019-10-24", "updated": "", "authors": ["piotr dworczak", "anton kolotilin"], "url": "https://arxiv.org/abs/1910.11392"}, {"title": "the politics of personalized news aggregation", "id": "1910.11405", "abstract": "we study how personalized news aggregation for rational inattentive voters (nari) affects policy polarization and public opinion. in a two-candidate electoral competition game, an attention-maximizing infomediary aggregates information about candidates' valence into news. voters decide whether to consume news, trading off the expected gain from improved expressive voting against the attention cost. nari generates policy polarization even if candidates are office-motivated. personalized news serves extreme voters with skewed signals and makes them the disciplining entities of policy polarization. analysis of disciplining voters' identities, preferences and beliefs sheds light on the political effects of recent regulatory proposals to tame tech giants.", "categories": "econ.gn econ.th q-fin.ec", "created": "2019-10-24", "updated": "2020-08-16", "authors": ["lin hu", "anqi li", "ilya segal"], "url": "https://arxiv.org/abs/1910.11405"}, {"title": "does car sharing reduce greenhouse gas emissions? life cycle assessment   of the modal shift and lifetime shift rebound effects", "id": "1910.11570", "abstract": "car-sharing platforms provide access to a shared rather than a private fleet of automobiles distributed in the region. participation in such services induces changes in mobility behaviour as well as vehicle ownership patterns that could have positive environmental impacts. this study contributes to the understanding of the total mobility-related greenhouse gas emissions reduction related to business-to-consumer car-sharing participation. a comprehensive model which takes into account distances travelled annually by the major urban transport modes as well as their life-cycle emissions factors is proposed, and the before-and-after analysis is conducted for an average car-sharing member in three geographical cases (netherlands, san francisco, calgary). in addition to non-operational emissions for all the transport modes involved, this approach considers the rebound effects associated with the modal shift effect (substituting driving distances with alternative modes) and the lifetime shift effect for the shared automobiles, phenomena which have been barely analysed in the previous studies. as a result, in contrast to the previous impact assessments in the field, a significantly more modest reduction of the annual total mobility-related life-cycle greenhouse gas emissions caused by car-sharing participation has been estimated, 3-18% for three geographical case studies investigated (versus up to 67% estimated previously). this suggests the significance of the newly considered effects and provides with the practical implications for improved assessments in the future.", "categories": "econ.gn q-fin.ec", "created": "2019-10-25", "updated": "", "authors": ["levon amatuni", "juudit ottelin", "bernhard steubing", "jos\u00e9 mogollon"], "url": "https://arxiv.org/abs/1910.11570"}, {"title": "building social networks under consent: a survey", "id": "1910.11693", "abstract": "this survey explores the literature on game-theoretic models of network formation under the hypothesis of mutual consent in link formation. the introduction of consent in link formation imposes a coordination problem in the network formation process. this survey explores the conclusions from this theory and the various methodologies to avoid the main pitfalls. the main insight originates from myerson's work on mutual consent in link formation and his main conclusion that the empty network (the network without any links) always emerges as a strong nash equilibrium in any game-theoretic model of network formation under mutual consent and positive link formation costs. jackson and wolinsky introduced a cooperative framework to avoid this main pitfall. they devised the notion of a pairwise stable network to arrive at equilibrium networks that are mainly non-trivial. unfortunately, this notion of pairwise stability requires coordinated action by pairs of decision makers in link formation. i survey the possible solutions in a purely non-cooperative framework of network formation under mutual consent by exploring potential refinements of the standard nash equilibrium concept to explain the emergence of non-trivial networks. this includes the notions of unilateral and monadic stability. the first one is founded on advanced rational reasoning of individuals about how others would respond to one's efforts to modify the network. the latter incorporates trusting, boundedly rational behaviour into the network formation process. the survey is concluded with an initial exploration of external correlation devices as an alternative framework to address mutual consent in network formation.", "categories": "econ.th cs.gt physics.soc-ph", "created": "2019-10-25", "updated": "2020-04-11", "authors": ["robert p. gilles"], "url": "https://arxiv.org/abs/1910.11693"}, {"title": "inequality in turkey: looking beyond growth", "id": "1910.11780", "abstract": "this paper investigates the relationships between economic growth, investment in human capital and income equality in turkey. the conclusion drawn based on the data from the oecd and the world bank suggests that economic growth can improve income equality depending on the expenditures undertaken by the government. as opposed to the standard view that economic growth and income inequality are positively related, the findings of this paper suggest that other factors such as education and healthcare spending are also driving factors of income inequality in turkey. the proven positive impact of investment in education and health care on income equality could aid policymakers who aim to achieve fairer income equality and economic growth, in investment decisions.", "categories": "econ.gn q-fin.ec", "created": "2019-10-25", "updated": "", "authors": ["bayram cakir", "ipek ergul"], "url": "https://arxiv.org/abs/1910.11780"}, {"title": "estimating a large covariance matrix in time-varying factor models", "id": "1910.11965", "abstract": "this paper deals with the time-varying high dimensional covariance matrix estimation. we propose two covariance matrix estimators corresponding with a time-varying approximate factor model and a time-varying approximate characteristic-based factor model, respectively. the models allow the factor loadings, factor covariance matrix, and error covariance matrix to change smoothly over time. we study the rate of convergence of each estimator. our simulation and empirical study indicate that time-varying covariance matrix estimators generally perform better than time-invariant covariance matrix estimators. also, if characteristics are available that genuinely explain true loadings, the characteristics can be used to estimate loadings more precisely in finite samples; their helpfulness increases when loadings rapidly change.", "categories": "econ.em", "created": "2019-10-25", "updated": "", "authors": ["jaeheon jung"], "url": "https://arxiv.org/abs/1910.11965"}, {"title": "almost quasi-linear utilities in disguise: positive-representation an   extension of roberts' theorem", "id": "1910.12131", "abstract": "this work deals with the implementation of social choice rules using dominant strategies for unrestricted preferences. the seminal gibbard-satterthwaite theorem shows that only few unappealing social choice rules can be implemented unless we assume some restrictions on the preferences or allow monetary transfers. when monetary transfers are allowed and quasi-linear utilities w.r.t. money are assumed, vickrey-clarke-groves (vcg) mechanisms were shown to implement any affine-maximizer, and by the work of roberts, only affine-maximizers can be implemented whenever the type sets of the agents are rich enough.   in this work, we generalize these results and define a new class of preferences: preferences which are positive-represented by a quasi-linear utility. that is, agents whose preference on a subspace of the outcomes can be modeled using a quasi-linear utility. we show that the characterization of vcg mechanisms as the incentive-compatible mechanisms extends naturally to this domain. our result follows from a simple reduction to the characterization of vcg mechanisms. hence, we see our result more as a fuller more correct version of the vcg characterization.   this work also highlights a common misconception in the community attributing the vcg result to the usage of transferable utility. our result shows that the incentive-compatibility of the vcg mechanisms does not rely on money being a common denominator, but rather on the ability of the designer to fine the agents on a continuous (maybe agent-specific) scale.   we think these two insights, considering the utility as a representation and not as the preference itself (which is common in the economic community) and considering utilities which represent the preference only for the relevant domain, would turn out to fruitful in other domains as well.", "categories": "econ.th cs.gt", "created": "2019-10-26", "updated": "", "authors": ["ilan nehama"], "url": "https://arxiv.org/abs/1910.12131"}, {"title": "dual instrumental variable regression", "id": "1910.12358", "abstract": "we present a novel algorithm for instrumental variable (iv) regression, dualiv, which simplifies traditional two-stage methods via a dual formulation. inspired by problems in stochastic programming, we show that the two-stage procedure for nonlinear iv regression can be reformulated as a convex-concave saddle-point problem. our formulation circumvents the first-stage regression which is a potential bottleneck in real-world applications. based on this new approach, we develop a simple kernel-based algorithm with a closed-form solution. empirical results show that we are competitive to existing, more complicated algorithms for instrumental variable regression.", "categories": "stat.ml cs.lg econ.em", "created": "2019-10-27", "updated": "2020-06-07", "authors": ["krikamol muandet", "arash mehrjou", "si kai lee", "anant raj"], "url": "https://arxiv.org/abs/1910.12358"}, {"title": "using network science to quantify economic disruptions in regional   input-output networks", "id": "1910.12498", "abstract": "input output (io) tables provide a standardised way of looking at monetary flows between all industries in an economy. io tables can be thought of as networks - with the nodes being different industries and the edges being the flows between them. we develop a network-based analysis to consider a multi-regional io network at regional and subregional level within a country. we calculate both traditional matrix-based io measures (e.g. 'multipliers') and new network theory-based measures at this higher spatial resolution. we contrast these methods with the results of a disruption model applied to the same io data in order to demonstrate that betweenness centrality gives a good indication of flow on economic disruption, while eigenvector-type centrality measures give results comparable to traditional io multipliers.we also show the effects of treating io networks at different levels of spatial aggregation.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-10-28", "updated": "", "authors": ["emily p. harvey", "dion r. j. o'neale"], "url": "https://arxiv.org/abs/1910.12498"}, {"title": "testing forecast rationality for measures of central tendency", "id": "1910.12545", "abstract": "rational respondents to economic surveys may report as a point forecast any measure of the central tendency of their (possibly latent) predictive distribution, for example the mean, median, mode, or any convex combination thereof. we propose tests of forecast rationality when the measure of central tendency used by the respondent is unknown. we overcome an identification problem that arises when the measures of central tendency are equal or in a local neighborhood of each other, as is the case for (exactly or nearly) symmetric distributions. as a building block, we also present novel tests for the rationality of mode forecasts. we apply our tests to survey forecasts of individual income, greenbook forecasts of u.s. gdp, and random walk forecasts for exchange rates. we find that the greenbook and random walk forecasts are best rationalized as mean, or near-mean forecasts, while the income survey forecasts are best rationalized as mode forecasts.", "categories": "econ.em econ.gn math.st q-fin.ec stat.th", "created": "2019-10-28", "updated": "2020-09-29", "authors": ["timo dimitriadis", "andrew j. patton", "patrick schmidt"], "url": "https://arxiv.org/abs/1910.12545"}, {"title": "lexicographic choice under variable capacity constraints", "id": "1910.13237", "abstract": "in several matching markets, in order to achieve diversity, agents' priorities are allowed to vary across an institution's available seats, and the institution is let to choose agents in a lexicographic fashion based on a predetermined ordering of the seats, called a (capacity-constrained) lexicographic choice rule. we provide a characterization of lexicographic choice rules and a characterization of deferred acceptance mechanisms that operate based on a lexicographic choice structure under variable capacity constraints. we discuss some implications for the boston school choice system and show that our analysis can be helpful in applications to select among plausible choice rules.", "categories": "econ.th", "created": "2019-10-29", "updated": "", "authors": ["battal dogan", "serhat dogan", "kemal yildiz"], "url": "https://arxiv.org/abs/1910.13237"}, {"title": "analyzing china's consumer price index comparatively with that of united   states", "id": "1910.13301", "abstract": "this paper provides a thorough analysis on the dynamic structures and predictability of china's consumer price index (cpi-cn), with a comparison to those of the united states. despite the differences in the two leading economies, both series can be well modeled by a class of seasonal autoregressive integrated moving average model with covariates (s-arimax). the cpi-cn series possess regular patterns of dynamics with stable annual cycles and strong spring festival effects, with fitting and forecasting errors largely comparable to their us counterparts. finally, for the cpi-cn, the diffusion index (di) approach offers improved predictions than the s-arimax models.", "categories": "econ.em stat.ap", "created": "2019-10-29", "updated": "", "authors": ["zhenzhong wang", "yundong tu", "song xi chen"], "url": "https://arxiv.org/abs/1910.13301"}, {"title": "hipsters and the cool: a game theoretic analysis of social identity,   trends and fads", "id": "1910.13385", "abstract": "cultural trends and popularity cycles can be observed all around us, yet our theories of social influence and identity expression do not explain what perpetuates these complex, often unpredictable social dynamics. we propose a theory of social identity expression based on the opposing, but not mutually exclusive, motives to conform and to be unique among one's neighbors in a social network. we then model the social dynamics that arise from these motives. we find that the dynamics typically enter random walks or stochastic limit cycles rather than converging to a static equilibrium. we also prove that without social network structure or, alternatively, without the uniqueness motive, reasonable adaptive dynamics would necessarily converge to equilibrium. thus, we show that nuanced psychological assumptions (recognizing preferences for uniqueness along with conformity) and realistic social network structure are both necessary for explaining how complex, unpredictable cultural trends emerge.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-10-29", "updated": "", "authors": ["russell golman", "aditi jain", "sonica saraf"], "url": "https://arxiv.org/abs/1910.13385"}, {"title": "multilevel evolutionary developmental optimization (medo): a theoretical   framework for understanding preferences and selection dynamics", "id": "1910.13443", "abstract": "what is motivation and how does it work? where do goals come from and how do they vary within and between species and individuals? why do we prefer some things over others? medo is a theoretical framework for understanding these questions in abstract terms, as well as for generating and evaluating specific hypotheses that seek to explain goal-oriented behavior. medo views preferences as selective pressures influencing the likelihood of particular outcomes. with respect to biological organisms, these patterns must compete and cooperate in shaping system evolution. to the extent that shaping processes are themselves altered by experience, this enables feedback relationships where histories of reward and punishment can impact future motivation. in this way, various biases can undergo either amplification or attenuation, resulting in preferences and behavioral orientations of varying degrees of inter-temporal and inter-situational stability. medo specifically models all shaping dynamics in terms of natural selection operating on multiple levels--genetic, neural, and cultural--and even considers aspects of development to themselves be evolutionary processes. thus, medo reflects a kind of generalized darwinism, in that it assumes that natural selection provides a common principle for understanding the emergence of complexity within all dynamical systems in which replication, variation, and selection occur. however, medo combines this evolutionary perspective with economic decision theory, which describes both the preferences underlying individual choices, as well as the preferences underlying choices made by engineers in designing optimized systems. in this way, medo uses economic decision theory to describe goal-oriented behaviors as well as the interacting evolutionary optimization processes from which they emerge. (please note: this manuscript was written and finalized in 2012.)", "categories": "q-bio.nc econ.gn q-fin.ec", "created": "2019-10-27", "updated": "2019-11-09", "authors": ["adam safron"], "url": "https://arxiv.org/abs/1910.13443"}, {"title": "microscopic derivation of mean field game models", "id": "1910.13534", "abstract": "mean field game theory studies the behavior of a large number of interacting individuals in a game theoretic setting and has received a lot of attention in the past decade (lasry and lions, japanese journal of mathematics, 2007). in this work, we derive mean field game partial differential equation systems from deterministic microscopic agent dynamics. the dynamics are given by a particular class of ordinary differential equations, for which an optimal strategy can be computed (bressan, milan journal of mathematics, 2011). we use the concept of nash equilibria and apply the dynamic programming principle to derive the mean field limit equations and we study the scaling behavior of the system as the number of agents tends to infinity and find several mean field game limits. especially we avoid in our derivation the notion of measure derivatives. novel scales are motivated by an example of an agent-based financial market model.", "categories": "math.oc econ.th q-fin.mf", "created": "2019-10-29", "updated": "", "authors": ["martin frank", "michael herty", "torsten trimborn"], "url": "https://arxiv.org/abs/1910.13534"}, {"title": "persuasion with coarse communication", "id": "1910.13547", "abstract": "persuasion is an exceedingly difficult task. a leading cause of this difficulty is the misalignment of preferences, which is studied extensively by the literature on persuasion games. however, the difficulty of communication also has a first order effect on the outcomes and welfare of agents. motivated by this observation, we study a model of bayesian persuasion in which the communication between the sender and the receiver is constrained. this is done by allowing the cardinality of the signal space to be less than the cardinality of the action space and the state space, which limits the number of action recommendations that the sender can make. existence of a maximum to the sender's problem is proven and its properties are characterized. this generalizes the standard bayesian persuasion framework, in which existence results rely on the assumption of rich signal spaces. we analyze the sender's willingness to pay for an additional signal as a function of the prior belief, which can be interpreted as the value of precise communication. we provide an upper bound for this value which applies to all finite persuasion games. while increased precision is always better for the sender, we show that the receiver might prefer coarse communication. we show this by analyzing a game of advice seeking, where the receiver has the ability to choose the size of the signal space.", "categories": "econ.th", "created": "2019-10-29", "updated": "2020-07-16", "authors": ["yunus c. aybas", "eray turkel"], "url": "https://arxiv.org/abs/1910.13547"}, {"title": "disclosure games with large evidence spaces", "id": "1910.13633", "abstract": "this paper studies disclosure games, allowing for large evidence spaces and general disclosure rules. a sender observes a piece of evidence about an unknown state and tries to influence the posterior belief of a receiver by disclosing evidence with possible omission. we focus on truth-leaning equilibria, where the sender discloses truthfully when doing so is optimal, and the receiver does not discount off-path messages. we show that, given any disclosure rule, all equilibria are payoff equivalent and characterize the unique equilibrium value function of the sender. we also propose a method to construct equilibria for a broad class of games. applying these results, we study left-censored disclosure, where evidence is a sequence of signals, and the sender can truncate evidence from the left. in equilibrium, seemingly sub-optimal messages are disclosed, and the sender's disclosure contains the longest truncation that yields the maximal difference between the number of favorable and unfavorable signals.", "categories": "econ.th", "created": "2019-10-29", "updated": "2020-03-10", "authors": ["shaofei jiang"], "url": "https://arxiv.org/abs/1910.13633"}, {"title": "costly verification in collective decisions", "id": "1910.13979", "abstract": "we study how a principal should optimally choose between implementing a new policy and maintaining the status quo when information relevant for the decision is privately held by agents. agents are strategic in revealing their information; the principal cannot use monetary transfers to elicit this information, but can verify an agent's claim at a cost. we characterize the mechanism that maximizes the expected utility of the principal. this mechanism can be implemented as a cardinal voting rule, in which agents can either cast a baseline vote, indicating only whether they are in favor of the new policy, or they make specific claims about their type. the principal gives more weight to specific claims and verifies a claim whenever it is decisive.", "categories": "econ.th", "created": "2019-10-30", "updated": "2020-02-20", "authors": ["albin erlanson", "andreas kleiner"], "url": "https://arxiv.org/abs/1910.13979"}, {"title": "power laws without gibrat's law", "id": "1910.14023", "abstract": "this paper shows that the power law property of the firm size distribution is a robust prediction of the standard entry-exit model of firm dynamics. only one variation is required: the usual restriction that firm productivity lies below an ad hoc upper bound is dropped. we prove that, after this small modification, the pareto tail of the distribution is predicted under a wide and empirically plausible class of specifications for firm-level productivity growth. we also provide necessary and sufficient conditions under which the entry-exit model exhibits a unique stationary recursive equilibrium in the setting where firm size is unbounded.", "categories": "econ.gn q-fin.ec", "created": "2019-10-29", "updated": "", "authors": ["john stachurski"], "url": "https://arxiv.org/abs/1910.14023"}, {"title": "exploring cities of central and eastern europe within transnational   company networks: the core-periphery effect", "id": "1910.14652", "abstract": "after the fall of the berlin wall, central eastern european cities (ceec) integrated the globalized world, characterized by a core-periphery structure and hierarchical interactions between cities. this article gives evidence of the core-periphery effect on ceec in 2013 in terms of differentiation of their urban functions after 1989. we investigate the position of all ceec in transnational company networks in 2013. we examine the orientations of ownership links between firms, the spatial patterns of these networks and the specialization of firms in ceec involved. the major contribution of this paper consists in giving proof of a core-periphery structure within central eastern europe itself, but also of the diffusion of innovations theory as not only large cities, but also medium-sized and small ones are part of the multinational networks of firms. these findings provide significant insights for the targeting of specific regional policies of the european union.", "categories": "econ.gn q-fin.ec", "created": "2019-10-31", "updated": "", "authors": ["natalia zdanowska"], "url": "https://arxiv.org/abs/1910.14652"}, {"title": "spatial polarisation within foreign trade and transnational firms'   networks. the case of central and eastern europe", "id": "1910.14658", "abstract": "after the fall of the berlin wall, central and eastern europe were subject to strong polarisation processes. this article proposes examines two neglected aspects regarding the transition period: a comparative static assessment of foreign trade since 1967 until 2012 and a city-centred analysis of transnational companies in 2013. results show a growing economic differentiation between the north-west and south-east as well as a division between large metropolises and other cities. these findings may complement the targeting of specific regional strategies such as those conceived within the cohesion policy of the european union.", "categories": "econ.gn q-fin.ec", "created": "2019-10-31", "updated": "", "authors": ["natalia zdanowska"], "url": "https://arxiv.org/abs/1910.14658"}, {"title": "integration into \\'economie-monde and regionalisation of the central   eastern european space since 1989", "id": "1911.00033", "abstract": "the fall of the berlin wall in 1989, modified the relations between cities of the former communist bloc. the european and worldwide reorientation of interactions that followed raises the question of the actual state of historical relationships between central eastern european cities, but also with ex-ussr and ex-yugoslavian ones. do central and eastern european cities reproduce trajectories from the past in a new economic context? this paper will examine their evolution in terms of trade exchanges and air traffic connexions since 1989. they are confronted with transnational firm networks for the recent years. the main contribution is to show a progressive formation of several economic regions in central and eastern europe as a result of integration into braudel's \\'economie-monde.", "categories": "econ.gn q-fin.ec", "created": "2019-10-31", "updated": "", "authors": ["natalia zdanowska"], "url": "https://arxiv.org/abs/1911.00033"}, {"title": "regularized quantile regression with interactive fixed effects", "id": "1911.00166", "abstract": "i consider nuclear norm penalized quantile regression for large $n$ and large $t$ panel data models with interactive fixed effects. the estimator solves a convex minimization problem, not requiring pre-estimation of the (number of the) fixed effects. uniform rates are obtained for both the slope coefficients and the low-rank common component of the interactive fixed effects. the rate of the latter is nearly optimal. to derive the rates, i show new results that establish uniform bounds of norms of certain random matrices of jump processes. these results may have independent interest. finally, i conduct monte carlo simulations to illustrate the estimator's finite sample performance.", "categories": "econ.em", "created": "2019-10-31", "updated": "2020-02-12", "authors": ["junlong feng"], "url": "https://arxiv.org/abs/1911.00166"}, {"title": "dominantly truthful multi-task peer prediction with a constant number of   tasks", "id": "1911.00272", "abstract": "in the setting where participants are asked multiple similar possibly subjective multi-choice questions (e.g. do you like panda express? y/n; do you like chick-fil-a? y/n), a series of peer prediction mechanisms are designed to incentivize honest reports and some of them achieve dominantly truthfulness: truth-telling is a dominant strategy and strictly dominate other \"non-permutation strategy\" with some mild conditions. however, a major issue hinders the practical usage of those mechanisms: they require the participants to perform an infinite number of tasks. when the participants perform a finite number of tasks, these mechanisms only achieve approximated dominant truthfulness. the existence of a dominantly truthful multi-task peer prediction mechanism that only requires a finite number of tasks remains to be an open question that may have a negative result, even with full prior knowledge.   this paper answers this open question by proposing a new mechanism, determinant based mutual information mechanism (dmi-mechanism), that is dominantly truthful when the number of tasks is at least 2c and the number of participants is at least 2. c is the number of choices for each question (c=2 for binary-choice questions). in addition to incentivizing honest reports, dmi-mechanism can also be transferred into an information evaluation rule that identifies high-quality information without verification when there are at least 3 participants. to the best of our knowledge, dmi-mechanism is the first dominantly truthful mechanism that works for a finite number of tasks, not to say a small constant number of tasks.", "categories": "cs.gt econ.th", "created": "2019-11-01", "updated": "", "authors": ["yuqing kong"], "url": "https://arxiv.org/abs/1911.00272"}, {"title": "time discounting under uncertainty", "id": "1911.00370", "abstract": "we study intertemporal decision making under uncertainty. we fully characterize discounted expected utility in a framework \\`a la savage. despite the popularity of this model, no characterization is available in this setting. the concept of stationarity, introduced by koopmans for deterministic discounted utility, plays a central role for both attitudes towards time and towards uncertainty. we show that a strong stationarity axiom characterizes discounted expected utility. when hedging considerations are taken into account, a weaker stationarity axiom generalizes discounted expected utility to choquet discounted expected utility, allowing for non-neutral attitudes towards uncertainty.", "categories": "econ.th", "created": "2019-11-01", "updated": "2020-03-12", "authors": ["lorenzo bastianello", "jos\u00e9 heleno faro"], "url": "https://arxiv.org/abs/1911.00370"}, {"title": "explaining black box decisions by shapley cohort refinement", "id": "1911.00467", "abstract": "we introduce a variable importance measure to quantify the impact of individual input variables to a black box function. our measure is based on the shapley value from cooperative game theory. many measures of variable importance operate by changing some predictor values with others held fixed, potentially creating unlikely or even logically impossible combinations. our cohort shapley measure uses only observed data points. instead of changing the value of a predictor we include or exclude subjects similar to the target subject on that predictor to form a similarity cohort. then we apply shapley value to the cohort averages. we connect variable importance measures from explainable ai to function decompositions from global sensitivity analysis. we introduce a squared cohort shapley value that splits previously studied shapley effects over subjects, consistent with a shapley axiom.", "categories": "cs.lg cs.ai econ.em stat.ml", "created": "2019-11-01", "updated": "2020-10-01", "authors": ["masayoshi mase", "art b. owen", "benjamin seiler"], "url": "https://arxiv.org/abs/1911.00467"}, {"title": "modeling national latent socioeconomic health and examination of policy   effects via causal inference", "id": "1911.00512", "abstract": "this research develops a socioeconomic health index for nations through a model-based approach which incorporates spatial dependence and examines the impact of a policy through a causal modeling framework. as the gross domestic product (gdp) has been regarded as a dated measure and tool for benchmarking a nation's economic performance, there has been a growing consensus for an alternative measure---such as a composite `wellbeing' index---to holistically capture a country's socioeconomic health performance. many conventional ways of constructing wellbeing/health indices involve combining different observable metrics, such as life expectancy and education level, to form an index. however, health is inherently latent with metrics actually being observable indicators of health. in contrast to the gdp or other conventional health indices, our approach provides a holistic quantification of the overall `health' of a nation. we build upon the latent health factor index (lhfi) approach that has been used to assess the unobservable ecological/ecosystem health. this framework integratively models the relationship between metrics, the latent health, and the covariates that drive the notion of health. in this paper, the lhfi structure is integrated with spatial modeling and statistical causal modeling, so as to evaluate the impact of a policy variable (mandatory maternity leave days) on a nation's socioeconomic health, while formally accounting for spatial dependency among the nations. we apply our model to countries around the world using data on various metrics and potential covariates pertaining to different aspects of societal health. the approach is structured in a bayesian hierarchical framework and results are obtained by markov chain monte carlo techniques.", "categories": "stat.ap econ.gn q-fin.ec stat.me", "created": "2019-11-01", "updated": "", "authors": ["f. swen kuh", "grace s. chiu", "anton h. westveld"], "url": "https://arxiv.org/abs/1911.00512"}, {"title": "a two-dimensional propensity score matching method for longitudinal   quasi-experimental studies: a focus on travel behavior and the built   environment", "id": "1911.00667", "abstract": "the lack of longitudinal studies of the relationship between the built environment and travel behavior has been widely discussed in the literature. this paper discusses how standard propensity score matching estimators can be extended to enable such studies by pairing observations across two dimensions: longitudinal and cross-sectional. researchers mimic randomized controlled trials (rcts) and match observations in both dimensions, to find synthetic control groups that are similar to the treatment group and to match subjects synthetically across before-treatment and after-treatment time periods. we call this a two-dimensional propensity score matching (2dpsm). this method demonstrates superior performance for estimating treatment effects based on monte carlo evidence. a near-term opportunity for such matching is identifying the impact of transportation infrastructure on travel behavior.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-11-02", "updated": "2019-11-05", "authors": ["haotian zhong", "wei li", "marlon g. boarnet"], "url": "https://arxiv.org/abs/1911.00667"}, {"title": "model specification test with unlabeled data: approach from covariate   shift", "id": "1911.00688", "abstract": "we propose a novel framework of the model specification test in regression using unlabeled test data. in many cases, we have conducted statistical inferences based on the assumption that we can correctly specify a model. however, it is difficult to confirm whether a model is correctly specified. to overcome this problem, existing works have devised statistical tests for model specification. existing works have defined a correctly specified model in regression as a model with zero conditional mean of the error term over train data only. extending the definition in conventional statistical tests, we define a correctly specified model as a model with zero conditional mean of the error term over any distribution of the explanatory variable. this definition is a natural consequence of the orthogonality of the explanatory variable and the error term. if a model does not satisfy this condition, the model might lack robustness with regards to the distribution shift. the proposed method would enable us to reject a misspecified model under our definition. by applying the proposed method, we can obtain a model that predicts the label for the unlabeled test data well without losing the interpretability of the model. in experiments, we show how the proposed method works for synthetic and real-world datasets.", "categories": "stat.me econ.em stat.ml", "created": "2019-11-02", "updated": "2020-02-23", "authors": ["masahiro kato", "hikaru kawarazaki"], "url": "https://arxiv.org/abs/1911.00688"}, {"title": "aggregation for potentially infinite populations without continuity or   completeness", "id": "1911.00872", "abstract": "we present an abstract social aggregation theorem. society, and each individual, has a preorder that may be interpreted as expressing values or beliefs. the preorders are allowed to violate both completeness and continuity, and the population is allowed to be infinite. the preorders are only assumed to be represented by functions with values in partially ordered vector spaces, and whose product has convex range. this includes all preorders that satisfy strong independence. any pareto indifferent social preorder is then shown to be represented by a linear transformation of the representations of the individual preorders. further pareto conditions on the social preorder correspond to positivity conditions on the transformation. when all the pareto conditions hold and the population is finite, the social preorder is represented by a sum of individual preorder representations. we provide two applications. the first yields an extremely general version of harsanyi's social aggregation theorem. the second generalizes a classic result about linear opinion pooling.", "categories": "econ.th", "created": "2019-11-03", "updated": "", "authors": ["david mccarthy", "kalle mikkola", "teruji thomas"], "url": "https://arxiv.org/abs/1911.00872"}, {"title": "decision making under uncertainty: an experimental study in market   settings", "id": "1911.00946", "abstract": "we design and implement a novel experimental test of subjective expected utility theory and its generalizations. our experiments are implemented in the laboratory with a student population and pushed out through a large-scale panel to a general sample of the u.s.\\ population. we find that a majority of subjects' choices are consistent with the maximization of {\\em some} utility function, but not with subjective utility theory. the theory is tested by gauging how subjects respond to price changes. a majority of subjects respond to price changes in the direction predicted by the theory, but not to a degree that makes them fully consistent with subjective expected utility. surprisingly, maxmin expected utility adds no explanatory power to subjective expected utility.   our findings remain the same regardless of whether we look at laboratory data or the panel survey, even though the two subject populations are very different. the degree of violations of subjective expected utility theory is not affected by age nor cognitive ability, but it is correlated with financial literacy.", "categories": "econ.gn q-fin.ec", "created": "2019-11-03", "updated": "2019-12-07", "authors": ["federico echenique", "taisuke imai", "kota saito"], "url": "https://arxiv.org/abs/1911.00946"}, {"title": "the survival of start-ups in time of crisis. a machine learning approach   to measure innovation", "id": "1911.01073", "abstract": "this paper shows how data science can contribute to improving empirical research in economics by leveraging on large datasets and extracting information otherwise unsuitable for a traditional econometric approach. as a test-bed for our framework, machine learning algorithms allow us to create a new holistic measure of innovation built on a 2012 italian law aimed at boosting new high-tech firms. we adopt this measure to analyse the impact of innovativeness on a large population of italian firms which entered the market at the beginning of the 2008 global crisis. the methodological contribution is organised in different steps. first, we train seven supervised learning algorithms to recognise innovative firms on 2013 firmographics data and select a combination of those with best predicting power. second, we apply the former on the 2008 dataset and predict which firms would have been labelled as innovative according to the definition of the law. finally, we adopt this new indicator as regressor in a survival model to explain firms' ability to remain in the market after 2008. results suggest that the group of innovative firms are more likely to survive than the rest of the sample, but the survival premium is likely to depend on location.", "categories": "econ.gn q-fin.ec", "created": "2019-11-04", "updated": "", "authors": ["marco guerzoni", "consuelo r. nava", "massimiliano nuccio"], "url": "https://arxiv.org/abs/1911.01073"}, {"title": "cheating with (recursive) models", "id": "1911.01251", "abstract": "to what extent can agents with misspecified subjective models predict false correlations? we study an \"analyst\" who utilizes models that take the form of a recursive system of linear regression equations. the analyst fits each equation to minimize the sum of squared errors against an arbitrarily large sample. we characterize the maximal pairwise correlation that the analyst can predict given a generic objective covariance matrix, subject to the constraint that the estimated model does not distort the mean and variance of individual variables. we show that as the number of variables in the model grows, the false pairwise correlation can become arbitrarily close to one, regardless of the true correlation.", "categories": "econ.th econ.em", "created": "2019-11-04", "updated": "", "authors": ["kfir eliaz", "ran spiegler", "yair weiss"], "url": "https://arxiv.org/abs/1911.01251"}, {"title": "icurrency?", "id": "1911.01272", "abstract": "we discuss the idea of a purely algorithmic universal world icurrency set forth in [kakushadze and liew, 2014] (https://ssrn.com/abstract=2542541) and expanded in [kakushadze and liew, 2017] (https://ssrn.com/abstract=3059330) in light of recent developments, including libra. is libra a contender to become icurrency? among other things, we analyze the libra proposal, including the stability and volatility aspects, and discuss various issues that must be addressed. for instance, one cannot expect a cryptocurrency such as libra to trade in a narrow band without a robust monetary policy. the presentation in the main text of the paper is intentionally nontechnical. it is followed by an extensive appendix with a mathematical description of the dynamics of (crypto)currency exchange rates in target zones, mechanisms for keeping the exchange rate from breaching the band, the role of volatility, etc.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.mf", "created": "2019-10-28", "updated": "2019-11-11", "authors": ["zura kakushadze", "willie yu"], "url": "https://arxiv.org/abs/1911.01272"}, {"title": "engel's law in the commodity composition of exports", "id": "1911.01568", "abstract": "different shares of distinct commodity sectors in production, trade, and consumption illustrate how resources and capital are allocated and invested. economic progress has been claimed to change the share distribution in a universal manner as exemplified by the engel's law for the household expenditure and the shift from primary to manufacturing and service sector in the three sector model. searching for large-scale quantitative evidence of such correlation, we analyze the gross-domestic product (gdp) and international trade data based on the standard international trade classification (sitc) in the period 1962 to 2000. three categories, among ten in the sitc, are found to have their export shares significantly correlated with the gdp over countries and time; the machinery category has positive and food and crude materials have negative correlations. the export shares of commodity categories of a country are related to its gdp by a power-law with the exponents characterizing the gdp-elasticity of their export shares. the distance between two countries in terms of their export portfolios is measured to identify several clusters of countries sharing similar portfolios in 1962 and 2000. we show that the countries whose gdp is increased significantly in the period are likely to transit to the clusters displaying large share of the machinery category.", "categories": "q-fin.gn econ.gn physics.soc-ph q-fin.ec", "created": "2019-11-04", "updated": "", "authors": ["sung-gook choi", "deok-sun lee"], "url": "https://arxiv.org/abs/1911.01568"}, {"title": "nonparametric quantile regressions for panel data models with large t", "id": "1911.01824", "abstract": "this paper considers panel data models where the conditional quantiles of the dependent variables are additively separable as unknown functions of the regressors and the individual effects. we propose two estimators of the quantile partial effects while controlling for the individual heterogeneity. the first estimator is based on local linear quantile regressions, and the second is based on local linear smoothed quantile regressions, both of which are easy to compute in practice. within the large t framework, we provide sufficient conditions under which the two estimators are shown to be asymptotically normally distributed. in particular, for the first estimator, it is shown that $n<<t^{2/(d+4)}$ is needed to ignore the incidental parameter biases, where $d$ is the dimension of the regressors. for the second estimator, we are able to derive the analytical expression of the asymptotic biases under the assumption that $n\\approx th^{d}$, where $h$ is the bandwidth parameter in local linear approximations. our theoretical results provide the basis of using split-panel jackknife for bias corrections. a monte carlo simulation shows that the proposed estimators and the bias-correction method perform well in finite samples.", "categories": "econ.em", "created": "2019-11-05", "updated": "2020-09-28", "authors": ["liang chen"], "url": "https://arxiv.org/abs/1911.01824"}, {"title": "a regulated market under sanctions: on tail dependence between oil,   gold, and tehran stock exchange index", "id": "1911.01826", "abstract": "we demonstrate that the tail dependence should always be taken into account as a proxy for systematic risk of loss for investments. we provide the clear statistical evidence of that the structure of investment portfolios on a regulated market should be adjusted to the price of gold. our finding suggests that the active bartering of oil for goods would prevent collapsing the national market facing international sanctions.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2019-11-01", "updated": "", "authors": ["abootaleb shirvani", "dimitri volchenkov"], "url": "https://arxiv.org/abs/1911.01826"}, {"title": "quantile factor models", "id": "1911.02173", "abstract": "quantile factor models (qfm) represent a new class of factor models for high-dimensional panel data. unlike approximate factor models (afm), where only location-shifting factors can be extracted, qfm also allow to recover unobserved factors shifting other relevant parts of the distributions of observed variables. a quantile regression approach, labeled quantile factor analysis (qfa), is proposed to consistently estimate all the quantile-dependent factors and loadings. their asymptotic distribution is then derived using a kernel-smoothed version of the qfa estimators. two consistent model selection criteria, based on information criteria and rank minimization, are developed to determine the number of factors at each quantile. moreover, in contrast to the conditions required for the use of principal components analysis in afm, qfa estimation remains valid even when the idiosyncratic errors have heavy-tailed distributions. three empirical applications (regarding macroeconomic, climate and finance panel data) provide evidence that extra factors shifting the quantiles other than the means could be relevant in practice.", "categories": "econ.em", "created": "2019-11-05", "updated": "2020-09-22", "authors": ["liang chen", "juan jose dolado", "jesus gonzalo"], "url": "https://arxiv.org/abs/1911.02173"}, {"title": "relative maximum likelihood updating of ambiguous beliefs", "id": "1911.02678", "abstract": "this paper proposes and axiomatizes a new updating rule: relative maximum likelihood (rml) for ambiguous beliefs represented by a set of priors (c). this rule takes the form of applying bayes' rule to a subset of the set c. this subset is a linear contraction of c towards its subset assigning a maximal probability to the observed event. the degree of contraction captures the extent of willingness to discard priors based on likelihood when updating. two well-known updating rules of multiple priors, full bayesian (fb) and maximum likelihood (ml), are included as special cases of rml.   an axiomatic characterization of conditional preferences generated by rml updating is provided when the preferences admit maxmin expected utility representations. the axiomatization relies on weakening the axioms characterizing fb and ml. the axiom characterizing ml is identified for the first time in this paper, addressing a long-standing open question in the literature.", "categories": "econ.th", "created": "2019-11-06", "updated": "2020-06-07", "authors": ["xiaoyu cheng"], "url": "https://arxiv.org/abs/1911.02678"}, {"title": "group average treatment effects for observational studies", "id": "1911.02688", "abstract": "the paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (gates) for non-randomised experiments. the groups can be understood as a broader aggregation of the conditional average treatment effect (cate) where the number of groups is set in advance. in economics, this approach is similar to pre-analysis plans. observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. to control for a potential selection-bias, we implement a doubly-robust estimator in the first stage. we use machine learning methods to learn the conditional mean functions as well as the propensity score. the group average treatment effect is then estimated via a linear projection model. the linear model is easy to interpret, provides p-values and confidence intervals, and limits the danger of finding spurious heterogeneity due to small subgroups in the cate. to control for confounding in the linear model, we use neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. the result is a best linear predictor for effect heterogeneity based on impact groups. we find that our proposed method has lower absolute errors as well as smaller bias than the benchmark doubly-robust estimator. we further introduce a bagging type averaging for the cate function for each observation to avoid biases through sample splitting. the advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.", "categories": "econ.em stat.ml", "created": "2019-11-06", "updated": "2020-03-27", "authors": ["daniel jacob"], "url": "https://arxiv.org/abs/1911.02688"}, {"title": "behavioral equivalence of extensive game structures", "id": "1911.02918", "abstract": "two extensive game structures with imperfect information are said to be behaviorally equivalent if they share the same map (up to relabelings) from profiles of structurally reduced strategies to induced terminal paths. we show that this is the case if and only if one can be transformed into the other through a composition of two elementary transformations, commonly known as \\textquotedblleft interchanging of simultaneous moves\\textquotedblright\\ and \\textquotedblleft coalescing moves/sequential agent splitting.\\textquotedblright", "categories": "econ.th", "created": "2019-11-07", "updated": "", "authors": ["pierpaolo battigalli", "paolo leonetti", "fabio maccheroni"], "url": "https://arxiv.org/abs/1911.02918"}, {"title": "tit-for-tat dynamics and market volatility", "id": "1911.03629", "abstract": "we study the tit-for-tat dynamic in production markets, where each player can make a good given as input various amounts of goods in the system. in the tit-for-tat dynamic, each player allocates its good to its neighbors in fractions proportional to how much they contributed in its production in the last round. tit-for-tat does not use money and was studied before in pure exchange settings.   we study the phase transitions of this dynamic when the valuations are symmetric (i.e. each good has the same worth to everyone) by characterizing which players grow or vanish over time. we also study how the fractions of their investments evolve in the long term, showing that in the limit the players invest only on players with optimal production capacity.", "categories": "cs.gt econ.th", "created": "2019-11-09", "updated": "", "authors": ["simina br\u00e2nzei"], "url": "https://arxiv.org/abs/1911.03629"}, {"title": "optimal experimental design for staggered rollouts", "id": "1911.03764", "abstract": "experimentation has become an increasingly prevalent tool for guiding decision-making and policy choices. a common hurdle in designing experiments is the lack of statistical power. in this paper, we study the optimal multi-period experimental design under the constraint that the treatment cannot be easily removed once implemented; for example, a government might implement a public health intervention in different geographies at different times, where the treatment cannot be easily removed due to practical constraints. the treatment design problem is to select which geographies (referred by units) to treat at which time, intending to test hypotheses about the effect of the treatment. when the potential outcome is a linear function of unit and time effects, and discrete observed/latent covariates, we provide an analytically feasible solution to the optimal treatment design problem where the variance of the treatment effect estimator is at most 1+o(1/n^2) times the variance using the optimal treatment design, where n is the number of units. this solution assigns units in a staggered treatment adoption pattern - if the treatment only affects one period, the optimal fraction of treated units in each period increases linearly in time; if the treatment affects multiple periods, the optimal fraction increases non-linearly in time, smaller at the beginning and larger at the end. in the general setting where outcomes depend on latent covariates, we show that historical data can be utilized in designing experiments. we propose a data-driven local search algorithm to assign units to treatment times. we demonstrate that our approach improves upon benchmark experimental designs via synthetic interventions on the influenza occurrence rate and synthetic experiments on interventions for in-home medical services and grocery expenditure.", "categories": "econ.em stat.me stat.ml", "created": "2019-11-09", "updated": "2020-08-16", "authors": ["ruoxuan xiong", "susan athey", "mohsen bayati", "guido imbens"], "url": "https://arxiv.org/abs/1911.03764"}, {"title": "an asymptotically f-distributed chow test in the presence of   heteroscedasticity and autocorrelation", "id": "1911.03771", "abstract": "this study proposes a simple, trustworthy chow test in the presence of heteroscedasticity and autocorrelation. the test is based on a series heteroscedasticity and autocorrelation robust variance estimator with judiciously crafted basis functions. like the chow test in a classical normal linear regression, the proposed test employs the standard f distribution as the reference distribution, which is justified under fixed-smoothing asymptotics. monte carlo simulations show that the null rejection probability of the asymptotic f test is closer to the nominal level than that of the chi-square test.", "categories": "econ.em", "created": "2019-11-09", "updated": "", "authors": ["yixiao sun", "xuexin wang"], "url": "https://arxiv.org/abs/1911.03771"}, {"title": "measuring the time-varying market efficiency in the prewar japanese   stock market", "id": "1911.04059", "abstract": "this study explores the time-varying structure of market efficiency of the prewar japanese stock market based on lo's (2004) adaptive market hypothesis (amh). in particular, we measure the time-varying degree of market efficiency using new datasets of the stock price index estimated by hirayama (2017a,b, 2018, 2019a, 2020). the empirical results show that (1) the degree of market efficiency in the prewar japanese stock market varied with time and that its variations corresponded with major historical events, (2) lo's (2004) the amh is supported in the prewar japanese stock market, (3) the differences in market efficiency between the old and new tokyo stock exchange (tse) shares and the equity performance index (eqpi) depends on the manner in which the price index is constructed, and (4) the price control policy beginning in the early 1930s suppressed price volatility and improved market efficiency.", "categories": "q-fin.st econ.gn q-fin.ec q-fin.gn", "created": "2019-11-10", "updated": "2020-07-07", "authors": ["akihiko noda"], "url": "https://arxiv.org/abs/1911.04059"}, {"title": "quantitative earnings enhancement from share buybacks", "id": "1911.04199", "abstract": "this paper aims to explore the mechanical effect of a company's share repurchase on earnings per share (eps). in particular, while a share repurchase scheme will reduce the overall number of shares, suggesting that the eps may increase, clearly the expenditure will reduce the net earnings of a company, introducing a trade-off between these competing effects. we first of all review accretive share repurchases, then characterise the increase in eps as a function of price paid by the company. subsequently, we analyse and quantify the estimated difference in earnings growth between a company's natural growth in the absence of buyback scheme to that with its earnings altered as a result of the buybacks. we conclude with an examination of the effect of share repurchases in two cases studies in the us stock-market.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2019-11-11", "updated": "", "authors": ["lawrence middleton", "james dodd", "graham baird"], "url": "https://arxiv.org/abs/1911.04199"}, {"title": "semi-discrete optimal transport", "id": "1911.04348", "abstract": "in the current book i suggest an off-road path to the subject of optimal transport. i tried to avoid prior knowledge of analysis, pde theory and functional analysis, as much as possible. thus i concentrate on discrete and semi-discrete cases, and always assume compactness for the underlying spaces. however, some fundamental knowledge of measure theory and convexity is unavoidable. in order to make it as self-contained as possible i included an appendix with some basic definitions and results. i believe that any graduate student in mathematics, as well as advanced undergraduate students, can read and understand this book. some chapters (in particular in parts ii\\&iii ) can also be interesting for experts. starting with the the most fundamental, fully discrete problem i attempted to place optimal transport as a particular case of the celebrated stable marriage problem. from there we proceed to the partition problem, which can be formulated as a transport from a continuous space to a discrete one. applications to information theory and game theory (cooperative and non-cooperative) are introduced as well.   finally, the general case of transport between two compact measure spaces is introduced as a coupling between two semi-discrete transports.", "categories": "math.oc cs.it econ.th math.it", "created": "2019-11-11", "updated": "2020-09-14", "authors": ["gershon wolansky"], "url": "https://arxiv.org/abs/1911.04348"}, {"title": "a many-to-many assignment game and stable outcome algorithm to evaluate   collaborative mobility-as-a-service platforms", "id": "1911.04435", "abstract": "as mobility as a service (maas) systems become increasingly popular, travel is changing from unimodal trips to personalized services offered by a platform of mobility operators. evaluation of maas platforms depends on modeling both user route decisions as well as operator service and pricing decisions. we adopt a new paradigm for traffic assignment in a maas network of multiple operators using the concept of stable matching to allocate costs and determine prices offered by operators corresponding to user route choices and operator service choices without resorting to nonconvex bilevel programming formulations. unlike our prior work, the proposed model allows travelers to make multimodal, multi-operator trips, resulting in stable cost allocations between competing network operators to provide maas for users. an algorithm is proposed to efficiently generate stability conditions for the stable outcome model. extensive computational experiments demonstrate the use of the model to handling pricing responses of maas operators in technological and capacity changes, government acquisition, consolidation, and firm entry, using the classic sioux falls network. the proposed algorithm replicates the same stability conditions as explicit path enumeration while taking only 17 seconds compared to explicit path enumeration timing out over 2 hours.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2019-11-11", "updated": "2020-06-28", "authors": ["theodoros p. pantelidis", "joseph y. j. chow", "saeid rasulkhani"], "url": "https://arxiv.org/abs/1911.04435"}, {"title": "identification and inference in discrete choice models with imperfect   information", "id": "1911.04529", "abstract": "we study identification of preferences in a single-agent, static, discrete choice model where the decision maker may be imperfectly informed about the utility generated by the available alternatives. we impose no restrictions on the information frictions the decision maker may face and impose weak assumptions on how the decision maker deals with the uncertainty induced by those frictions. we leverage on the notion of one-player bayes correlated equilibrium in bergemann and morris (2013; 2016) to provide a tractable characterisation of the identified set and discuss inference. we use our methodology and data on the 2017 uk general election to estimate a spatial model of voting under weak assumptions on the information that voters have about the returns to voting. we find that the assumptions on the information environment can drive the interpretation of voter preferences. counterfactual exercises quantify the consequences of imperfect information in politics.", "categories": "econ.em", "created": "2019-11-11", "updated": "2020-07-15", "authors": ["cristina gualdani", "shruti sinha"], "url": "https://arxiv.org/abs/1911.04529"}, {"title": "extended minp tests of multiple hypotheses", "id": "1911.04696", "abstract": "much empirical research in economics and finance involves simultaneously testing multiple hypotheses. this paper proposes extended minp (eminp) tests by expanding the minimand set of the minp test statistic to include the $p$% -value of a global test such as a likelihood ratio test. we show that, compared with minp tests, eminp tests may considerably improve the global power in rejecting the intersection of all individual hypotheses. compared with closed tests eminp tests have the computational advantage by sharing the benefit of the stepdown procedure of minp tests and can have a better global power over the tests used to construct closed tests. furthermore, we argue that eminp tests may be viewed as a tool to prevent data snooping when two competing tests that have distinct global powers are exploited. finally, the proposed tests are applied to an empirical application on testing the effects of exercise.", "categories": "econ.em", "created": "2019-11-12", "updated": "", "authors": ["zeng-hua lu"], "url": "https://arxiv.org/abs/1911.04696"}, {"title": "a simple estimator for quantile panel data models using smoothed   quantile regressions", "id": "1911.04729", "abstract": "canay (2011)'s two-step estimator of quantile panel data models, due to its simple intuition and low computational cost, has been widely used in empirical studies in recent years. in this paper, we revisit the estimator of canay (2011) and point out that in his asymptotic analysis the bias of his estimator due to the estimation of the fixed effects is mistakenly omitted, and that such omission will lead to invalid inference on the coefficients. to solve this problem, we propose a similar easy-to-implement estimator based on smoothed quantile regressions. the asymptotic distribution of the new estimator is established and the analytical expression of its asymptotic bias is derived. based on these results, we show how to make asymptotically valid inference based on both analytical and split-panel jackknife bias corrections. finally, finite sample simulations are used to support our theoretical analysis and to illustrate the importance of bias correction in quantile regressions for panel data.", "categories": "econ.em", "created": "2019-11-12", "updated": "", "authors": ["liang chen", "yulong huo"], "url": "https://arxiv.org/abs/1911.04729"}, {"title": "dynamical approach to zipf's law", "id": "1911.04844", "abstract": "the rank-size plots of a large number of different physical and socio-economic systems are usually said to follow zipf's law, but a unique framework for the comprehension of this ubiquitous scaling law is still lacking. here we show that a dynamical approach is crucial: during their evolution, some systems are attracted towards zipf's law, while others presents zipf's law only temporarily and, therefore, spuriously. a truly zipfian dynamics is characterized by a dynamical constraint, or coherence, among the parameters of the generating pdf, and the number of elements in the system. a clear-cut example of such coherence is natural language. our framework allows us to derive some quantitative results that go well beyond the usual zipf's law: i) earthquakes can evolve only incoherently and thus show zipf's law spuriously; this allows an assessment of the largest possible magnitude of an earthquake occurring in a geographical region. ii) we prove that zipfian dynamics are not additive, explaining analytically why us cities evolve coherently, while world cities do not. iii) our concept of coherence can be used for model selection, for example, the yule-simon process can describe the dynamics of world countries' gdp. iv) world cities present spurious zipf's law and we use this property for estimating the maximal population of an urban agglomeration.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2019-11-12", "updated": "2020-07-13", "authors": ["giordano de marzo", "andrea gabrielli", "andrea zaccaria", "luciano pietronero"], "url": "https://arxiv.org/abs/1911.04844"}, {"title": "analytical solution of $k$th price auction", "id": "1911.04865", "abstract": "we provide an exact analytical solution of the nash equilibrium for the $k$th price auction by using inverse of distribution functions. as applications, we identify the unique symmetric equilibrium where the valuations have polynomial distribution, fat tail distribution and exponential distributions.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-11-11", "updated": "2020-06-19", "authors": ["martin mihelich", "yan shu"], "url": "https://arxiv.org/abs/1911.04865"}, {"title": "combinatorial models of cross-country dual meets: what is a big victory?", "id": "1911.05044", "abstract": "combinatorial/probabilistic models for cross-country dual-meets are proposed. the first model assumes that all runners are equally likely to finish in any possible order. the second model assumes that each team is selected from a large identically distributed population of potential runners and with each potential runner's ranking determined by the initial draw from the combined population.", "categories": "stat.ap econ.em", "created": "2019-11-12", "updated": "", "authors": ["kurt s. riedel"], "url": "https://arxiv.org/abs/1911.05044"}, {"title": "beveridgean unemployment gap", "id": "1911.05271", "abstract": "this paper proposes a new method to estimate the unemployment gap (the actual unemployment rate minus the efficient rate). while lowering unemployment puts more people into work, it forces firms to post more vacancies and devote more resources to recruiting. this unemployment-vacancy tradeoff, governed by the beveridge curve, determines the efficient unemployment rate. accordingly, the unemployment gap can be measured from three sufficient statistics: the elasticity of the beveridge curve, cost of recruiting, and social cost of unemployment. in the united states the unemployment gap is countercyclical, reaching 1.5--6.5 percentage points in slumps. thus the us labor market appears inefficient---especially inefficiently slack in slumps.", "categories": "econ.gn q-fin.ec", "created": "2019-11-12", "updated": "2020-04-20", "authors": ["pascal michaillat", "emmanuel saez"], "url": "https://arxiv.org/abs/1911.05271"}, {"title": "randomization tests of copula symmetry", "id": "1911.05307", "abstract": "new nonparametric tests of copula exchangeability and radial symmetry are proposed. the novel aspect of the tests is a resampling procedure that exploits group invariance conditions associated with the relevant symmetry hypothesis. they may be viewed as feasible versions of randomization tests of symmetry, the latter being inapplicable due to the unobservability of margins. our tests are simple to compute, control size asymptotically, consistently detect arbitrary forms of asymmetry, and do not require the specification of a tuning parameter. simulations indicate excellent small sample properties compared to existing procedures involving the multiplier bootstrap.", "categories": "econ.em stat.me", "created": "2019-11-13", "updated": "", "authors": ["brendan k. beare", "juwon seo"], "url": "https://arxiv.org/abs/1911.05307"}, {"title": "how do scientific disciplines evolve in applied sciences? the properties   of scientific fission and ambidextrous scientific drivers", "id": "1911.05363", "abstract": "one of the fundamental questions in science is how scientific disciplines evolve and sustain progress in society. no studies to date allows us to explain the endogenous processes that support the evolution of scientific disciplines and emergence of new scientific fields in applied sciences of physics. this study confronts this problem here by investigating the evolution of experimental physics to explain and generalize some characteristics of the dynamics of applied sciences. empirical analysis suggests properties about the evolution of experimental physics and in general of applied sciences, such as: a) scientific fission, the evolution of scientific disciplines generates a process of division into two or more research fields that evolve as autonomous entities over time; b) ambidextrous drivers of science, the evolution of science via scientific fission is due to scientific discoveries or new technologies; c) new driving research fields, the drivers of scientific disciplines are new research fields rather than old ones; d) science driven by development of general purpose technologies, the evolution of experimental physics and applied sciences is due to the convergence of experimental and theoretical branches of physics associated with the development of computer, information systems and applied computational science. results also reveal that average duration of the upwave of scientific production in scientific fields supporting experimental physics is about 80 years. overall, then, this study begins the process of clarifying and generalizing, as far as possible, some characteristics of the evolutionary dynamics of scientific disciplines that can lay a foundation for the development of comprehensive properties explaining the evolution of science as a whole for supporting fruitful research policy implications directed to advancement of science and technological progress in society.", "categories": "econ.gn q-fin.ec", "created": "2019-11-13", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/1911.05363"}, {"title": "bayesian state-space modeling for analyzing heterogeneous network   effects of us monetary policy", "id": "1911.06206", "abstract": "understanding disaggregate channels in the transmission of monetary policy is of crucial importance for effectively implementing policy measures. we extend the empirical econometric literature on the role of production networks in the propagation of shocks along two dimensions. first, we allow for industry-specific responses that vary over time, reflecting non-linearities and cross-sectional heterogeneities in direct transmission channels. second, we allow for time-varying network structures and dependence. this feature captures both variation in the structure of the production network, but also differences in cross-industry demand elasticities. we find that impacts vary substantially over time and the cross-section. higher-order effects appear to be particularly important in periods of economic and financial uncertainty, often coinciding with tight credit market conditions and financial stress. differentials in industry-specific responses can be explained by how close the respective industries are to end-consumers.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-11-14", "updated": "2020-09-10", "authors": ["niko hauzenberger", "michael pfarrhofer"], "url": "https://arxiv.org/abs/1911.06206"}, {"title": "tracking the circulation routes of fresh coins in bitcoin: a way of   identifying coin miners with transaction network structural properties", "id": "1911.06400", "abstract": "bitcoin draws the highest degree of attention among cryptocurrencies, while coin mining is one of the most important fashion of profiting in the bitcoin ecosystem. this paper constructs fresh coin circulation networks by tracking the fresh coin transfer routes with transaction referencing in bitcoin blockchain. this paper proposes a heuristic algorithm to identifying coin miners by comparing coin circulation networks from different mining pools and thereby inferring the common profit distribution schemes of bitcoin mining pools. furthermore, this paper characterizes the increasing trend of bitcoin miner numbers during recent years.", "categories": "q-fin.gn cs.cr cs.ir econ.gn q-fin.ec", "created": "2019-10-03", "updated": "", "authors": ["zeng-xian lin", "xiao fan liu"], "url": "https://arxiv.org/abs/1911.06400"}, {"title": "weak monotone comparative statics", "id": "1911.06442", "abstract": "we develop a theory of monotone comparative statics based on weak set order, or in short weak monotone comparative statics, and identify the enabling conditions in the context of individual choices, pareto optimal choices for a coalition of agents, and nash equilibria of games. compared with the existing theory based on strong set order, the conditions for weak monotone comparative statics are weaker, sometimes considerably, in terms of the structure of the choice environment and underlying preferences of agents. we apply the theory to establish existence and monotone comparative statics of nash equilibria in games with strategic complementarities and of stable many-to-one matchings in two-sided matching problems, allowing for general preferences that accommodate indifferences and incomplete preferences.", "categories": "econ.th cs.gt", "created": "2019-11-14", "updated": "", "authors": ["yeon-koo che", "jinwoo kim", "fuhito kojima"], "url": "https://arxiv.org/abs/1911.06442"}, {"title": "a generalized markov chain model to capture dynamic preferences and   choice overload", "id": "1911.06716", "abstract": "assortment optimization is an important problem that arises in many industries such as retailing and online advertising where the goal is to find a subset of products from a universe of substitutable products which maximize seller's expected revenue. one of the key challenges in this problem is to model the customer substitution behavior. many parametric random utility maximization (rum) based choice models have been considered in the literature. however, in all these models, probability of purchase increases as we include more products to an assortment. this is not true in general and in many settings more choices hurt sales. this is commonly referred to as the choice overload. in this paper we attempt to address this limitation in rum through a generalization of the markov chain based choice model considered in blanchet et al. (2016). as a special case, we show that our model reduces to a generalization of mnl with no-purchase attractions dependent on the assortment s and strictly increasing with the size of assortment s. while we show that the assortment optimization under this model is np-hard, we present fully polynomial-time approximation scheme (fptas) under reasonable assumptions.", "categories": "econ.th cs.lg stat.ml", "created": "2019-11-15", "updated": "2020-07-15", "authors": ["kumar goutam", "vineet goyal", "agathe soret"], "url": "https://arxiv.org/abs/1911.06716"}, {"title": "semiparametric estimation of correlated random coefficient models   without instrumental variables", "id": "1911.06857", "abstract": "we study a linear random coefficient model where slope parameters may be correlated with some continuous covariates. such a model specification may occur in empirical research, for instance, when quantifying the effect of a continuous treatment observed at two time periods. we show one can carry identification and estimation without instruments. we propose a semiparametric estimator of average partial effects and of average treatment effects on the treated. we showcase the small sample properties of our estimator in an extensive simulation study. among other things, we reveal that it compares favorably with a control function estimator. we conclude with an application to the effect of malaria eradication on economic development in colombia.", "categories": "econ.em stat.ap", "created": "2019-11-15", "updated": "", "authors": ["samuele centorrino", "aman ullah", "jing xue"], "url": "https://arxiv.org/abs/1911.06857"}, {"title": "innovation and strategic network formation", "id": "1911.06872", "abstract": "we study a model of innovation with a large number of firms that create new technologies by combining several discrete ideas. these ideas can be acquired by private investment or via social learning. firms face a choice between secrecy, which protects existing intellectual property, and openness, which facilitates learning from others. their decisions determine interaction rates between firms, and these interaction rates enter our model as link probabilities in a learning network. higher interaction rates impose both positive and negative externalities on other firms, as there is more learning but also more competition. we show that the equilibrium learning network is at a critical threshold between sparse and dense networks. at equilibrium, the positive externality from interaction dominates: the innovation rate and even average firm profits would be dramatically higher if the network were denser. so there are large returns to increasing interaction rates above the critical threshold. nevertheless, several natural types of interventions fail to move the equilibrium away from criticality. one policy solution is to introduce informational intermediaries, such as public innovators who do not have incentives to be secretive. these intermediaries can facilitate a high-innovation equilibrium by transmitting ideas from one private firm to another.", "categories": "econ.th cs.si", "created": "2019-11-15", "updated": "2020-02-09", "authors": ["krishna dasaratha"], "url": "https://arxiv.org/abs/1911.06872"}, {"title": "causal inference under approximate neighborhood interference", "id": "1911.07085", "abstract": "this paper studies causal inference in randomized experiments under network interference. most of the literature assumes a model of interference under which treatments assigned to alters beyond a certain network distance from the ego have no effect on the ego's response. however, many models of social interactions do not satisfy this assumption. this paper proposes a substantially weaker model of \"approximate neighborhood interference\" (ani), under which treatments assigned to alters further from the ego have a smaller, but potentially nonzero, impact on the ego's response. we show that ani is satisfied in well-known models of social interactions. we also prove that, under ani, standard inverse-probability weighting estimators can consistently estimate useful exposure effects and are asymptotically normal under asymptotics taking the network size large. for inference, we consider a network hac variance estimator. under a finite population model, we show the estimator is biased but that the bias can be interpreted as the variance of unit-level exposure effects. this generalizes neyman's well-known result on conservative variance estimation to settings with interference.", "categories": "econ.em stat.me", "created": "2019-11-16", "updated": "2020-06-23", "authors": ["michael p. leung"], "url": "https://arxiv.org/abs/1911.07085"}, {"title": "distributionally robust optimal auction design under mean constraints", "id": "1911.07103", "abstract": "we study a seller who sells a single good to multiple bidders with uncertainty over the joint distribution of bidders' valuations, as well as bidders' higher-order beliefs about their opponents. the seller only knows the mean of the marginal distribution of each bidder's valuation and the range, and an adversarial nature chooses the worst-case distribution within this ambiguity set. we find that a second-price auction with an optimal, random reserve price obtains the optimal revenue guarantee within a broad class of mechanisms that include all the standard auction formats. we find that as the number of bidders grows large, the seller's optimal reserve price converges in probability to a non-binding reserve price.", "categories": "econ.th", "created": "2019-11-16", "updated": "2020-03-10", "authors": ["ethan che"], "url": "https://arxiv.org/abs/1911.07103"}, {"title": "inference in models of discrete choice with social interactions using   network data", "id": "1911.07106", "abstract": "this paper studies inference in models of discrete choice with social interactions when the data consists of a single large network. we provide theoretical justification for the use of spatial and network hac variance estimators in applied work, the latter constructed by using network path distance in place of spatial distance. toward this end, we prove new central limit theorems for network moments in a large class of social interactions models. the results are applicable to discrete games on networks and dynamic models where social interactions enter through lagged dependent variables. we illustrate our results in an empirical application and simulation study.", "categories": "econ.em stat.me", "created": "2019-11-16", "updated": "", "authors": ["michael p. leung"], "url": "https://arxiv.org/abs/1911.07106"}, {"title": "an analysis framework for metric voting based on lp duality", "id": "1911.07162", "abstract": "distortion-based analysis has established itself as a fruitful framework for comparing voting mechanisms. m voters and n candidates are jointly embedded in an (unknown) metric space, and the voters submit rankings of candidates by non-decreasing distance from themselves. based on the submitted rankings, the social choice rule chooses a winning candidate; the quality of the winner is the sum of the (unknown) distances to the voters. the rule's choice will in general be suboptimal, and the worst-case ratio between the cost of its chosen candidate and the optimal candidate is called the rule's distortion. it was shown in prior work that every deterministic rule has distortion at least 3, while the copeland rule and related rules guarantee worst-case distortion at most 5; a very recent result gave a rule with distortion $2+\\sqrt{5} \\approx 4.236$.   we provide a framework based on lp-duality and flow interpretations of the dual which provides a simpler and more unified way for proving upper bounds on the distortion of social choice rules. we illustrate the utility of this approach with three examples. first, we give a fairly simple proof of a strong generalization of the upper bound of 5 on the distortion of copeland, to social choice rules with short paths from the winning candidate to the optimal candidate in generalized weak preference graphs. a special case of this result recovers the recent $2+\\sqrt{5}$ guarantee. second, using this generalized bound, we show that the ranked pairs and schulze rules have distortion $\\theta(\\sqrt(n))$. finally, our framework naturally suggests a combinatorial rule that is a strong candidate for achieving distortion 3, which had also been proposed in recent work. we prove that the distortion bound of 3 would follow from any of three combinatorial conjectures we formulate.", "categories": "cs.gt cs.dm econ.th", "created": "2019-11-17", "updated": "2019-12-14", "authors": ["david kempe"], "url": "https://arxiv.org/abs/1911.07162"}, {"title": "optimal search and discovery", "id": "1911.07773", "abstract": "this paper studies a search problem where a consumer initially is aware of only a few products. to find a good match, the consumer sequentially decides between searching among alternatives he is already aware of and discovering more products. i show that the optimal policy for this search and discovery problem is fully characterized by tractable reservation values. moreover, i prove that a predetermined index fully specifies the purchase decision of a consumer following the optimal search policy. finally, a comparison highlights differences to classical random and directed search.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-11-18", "updated": "2020-09-18", "authors": ["rafael p. greminger"], "url": "https://arxiv.org/abs/1911.07773"}, {"title": "on the price of satisficing in network user equilibria", "id": "1911.07914", "abstract": "when network users are satisficing decision-makers, the resulting traffic pattern attains a satisficing user equilibrium, which may deviate from the (perfectly rational) user equilibrium. in a satisficing user equilibrium traffic pattern, the total system travel time can be worse than in the case of the prue. we show how bad the worst-case satisficing user equilibrium traffic pattern can be, compared to the perfectly rational user equilibrium. we call the ratio between the total system travel times of the two traffic patterns the price of satisficing, for which we provide an analytical bound. we compare the analytical bound with numerical bounds for several transportation networks.", "categories": "cs.gt econ.th", "created": "2019-11-18", "updated": "", "authors": ["mahdi takalloo", "changhyun kwon"], "url": "https://arxiv.org/abs/1911.07914"}, {"title": "strongly budget balanced auctions for multi-sided markets", "id": "1911.08094", "abstract": "in two-sided markets, myerson and satterthwaite's impossibility theorem states that one can not maximize the gain-from-trade while also satisfying truthfulness, individual-rationality and no deficit. attempts have been made to circumvent myerson and satterthwaite's result by attaining approximately-maximum gain-from-trade: the double-sided auctions of mcafee (1992) is truthful and has no deficit, and the one by segal-halevi et al. (2016) additionally has no surplus --- it is strongly-budget-balanced. they consider two categories of agents --- buyers and sellers, where each trade set is composed of a single buyer and a single seller. the practical complexity of applications such as supply chain require one to look beyond two-sided markets. common requirements are for: buyers trading with multiple sellers of different or identical items, buyers trading with sellers through transporters and mediators, and sellers trading with multiple buyers. we attempt to address these settings. we generalize segal-halevi et al. (2016)'s strongly-budget-balanced double-sided auction setting to a multilateral market where each trade set is composed of any number of agent categories. our generalization refines the notion of competition in multi-sided auctions by introducing the concepts of external competition and trade reduction. we also show an obviously-truthful implementation of our auction using multiple ascending prices.", "categories": "cs.gt econ.th", "created": "2019-11-18", "updated": "2019-12-08", "authors": ["rica gonen", "erel segal-halevi"], "url": "https://arxiv.org/abs/1911.08094"}, {"title": "communication, distortion, and randomness in metric voting", "id": "1911.08129", "abstract": "in distortion-based analysis of social choice rules over metric spaces, one assumes that all voters and candidates are jointly embedded in a common metric space. voters rank candidates by non-decreasing distance. the mechanism, receiving only this ordinal (comparison) information, should select a candidate approximately minimizing the sum of distances from all voters. it is known that while the copeland rule and related rules guarantee distortion at most 5, many other standard voting rules, such as plurality, veto, or $k$-approval, have distortion growing unboundedly in the number $n$ of candidates.   plurality, veto, or $k$-approval with small $k$ require less communication from the voters than all deterministic social choice rules known to achieve constant distortion. this motivates our study of the tradeoff between the distortion and the amount of communication in deterministic social choice rules.   we show that any one-round deterministic voting mechanism in which each voter communicates only the candidates she ranks in a given set of $k$ positions must have distortion at least $\\frac{2n-k}{k}$; we give a mechanism achieving an upper bound of $o(n/k)$, which matches the lower bound up to a constant. for more general communication-bounded voting mechanisms, in which each voter communicates $b$ bits of information about her ranking, we show a slightly weaker lower bound of $\\omega(n/b)$ on the distortion.   for randomized mechanisms, it is known that random dictatorship achieves expected distortion strictly smaller than 3, almost matching a lower bound of $3-\\frac{2}{n}$ for any randomized mechanism that only receives each voter's top choice. we close this gap, by giving a simple randomized social choice rule which only uses each voter's first choice, and achieves expected distortion $3-\\frac{2}{n}$.", "categories": "cs.gt cs.dm econ.th", "created": "2019-11-19", "updated": "2019-11-20", "authors": ["david kempe"], "url": "https://arxiv.org/abs/1911.08129"}, {"title": "a multicriteria macroeconomic model with intertemporal equity and   spatial spillovers", "id": "1911.08247", "abstract": "we analyze a macroeconomic model with intergenerational equity considerations and spatial spillovers, which gives rise to a multicriteria optimization problem. intergenerational equity requires to add in the definition of social welfare a long run sustainability criterion to the traditional discounted utilitarian criterion. the spatial structure allows for the possibility of heterogeneiity and spatial diffusion implies that all locations within the spatial domain are interconnected via spatial spillovers. we rely on different techniques (scalarization, $\\epsilon$-constraint method and goal programming) to analyze such a spatial multicriteria problem, relying on numerical approaches to illustrate the nature of the trade-off between the discounted utilitarian and the sustainability criteria.", "categories": "econ.th econ.gn math.oc q-fin.ec", "created": "2019-11-19", "updated": "", "authors": ["herb kunze", "davide la torre", "simone marsiglio"], "url": "https://arxiv.org/abs/1911.08247"}, {"title": "synthetic controls with imperfect pre-treatment fit", "id": "1911.08521", "abstract": "we analyze the properties of the synthetic control (sc) and related estimators when the pre-treatment fit is imperfect. in this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre-treatment periods goes to infinity. still, we also show that a modified version of the sc method can substantially improve in terms of bias and variance relative to the difference-in-difference estimator. we also consider the properties of these estimators in settings with non-stationary common factors.", "categories": "econ.em", "created": "2019-11-19", "updated": "", "authors": ["bruno ferman", "cristine pinto"], "url": "https://arxiv.org/abs/1911.08521"}, {"title": "structural stability of infinite-order regression", "id": "1911.08637", "abstract": "we develop a class of tests for the structural stability of infinite-order models such as the infinite-order autoregressive model and the nonparametric sieve regression. when the number $ p $ of restrictions diverges, the traditional tests based on the suprema of wald, lm and lr statistics or their exponentially weighted averages diverge as well. we introduce a suitable transformation of these tests and obtain proper weak limits under the condition that $p $ grows to infinity as the sample size $n $ goes to infinity. in general, this limit distribution is different from the sequential limit, which can be obtained by increasing the order of the standardized tied-down bessel process in andrews (1993). in particular, our joint asymptotic analysis discovers a nonlinear high order serial correlation, for which we provide a consistent estimator. our monte carlo simulation illustrates the importance of robustifying the structural break test against the nonlinear serial correlation even when $ p $ is moderate. furthermore, we also establish a weighted power optimality property of our tests under some regularity conditions. we examine finite-sample performance in a monte carlo study and illustrate the test with a number of empirical examples.", "categories": "econ.em", "created": "2019-11-19", "updated": "", "authors": ["abhimanyu gupta", "myung hwan seo"], "url": "https://arxiv.org/abs/1911.08637"}, {"title": "strategy-proof and non-wasteful multi-unit auction via social network", "id": "1911.08809", "abstract": "auctions via social network, pioneered by li et al. (2017), have been attracting considerable attention in the literature of mechanism design for auctions. however, no known mechanism has satisfied strategy-proofness, non-deficit, non-wastefulness, and individual rationality for the multi-unit unit-demand auction, except for some naive ones. in this paper, we first propose a mechanism that satisfies all the above properties. we then make a comprehensive comparison with two naive mechanisms, showing that the proposed mechanism dominates them in social surplus, seller's revenue, and incentive of buyers for truth-telling. we also analyze the characteristics of the social surplus and the revenue achieved by the proposed mechanism, including the constant approximability of the worst-case efficiency loss and the complexity of optimizing revenue from the seller's perspective.", "categories": "cs.gt econ.th", "created": "2019-11-20", "updated": "", "authors": ["takehiro kawasaki", "nathanael barrot", "seiji takanashi", "taiki todo", "makoto yokoo"], "url": "https://arxiv.org/abs/1911.08809"}, {"title": "statistical inference on partially linear panel model under unobserved   linearity", "id": "1911.08830", "abstract": "a new statistical procedure, based on a modified spline basis, is proposed to identify the linear components in the panel data model with fixed effects. under some mild assumptions, the proposed procedure is shown to consistently estimate the underlying regression function, correctly select the linear components, and effectively conduct the statistical inference. when compared to existing methods for detection of linearity in the panel model, our approach is demonstrated to be theoretically justified as well as practically convenient. we provide a computational algorithm that implements the proposed procedure along with a path-based solution method for linearity detection, which avoids the burden of selecting the tuning parameter for the penalty term. monte carlo simulations are conducted to examine the finite sample performance of our proposed procedure with detailed findings that confirm our theoretical results in the paper. applications to aggregate production and environmental kuznets curve data also illustrate the necessity for detecting linearity in the partially linear panel model.", "categories": "econ.em stat.me", "created": "2019-11-20", "updated": "", "authors": ["ruiqi liu", "ben boukai", "zuofeng shang"], "url": "https://arxiv.org/abs/1911.08830"}, {"title": "competition of noise and collectivity in global cryptocurrency trading:   route to a self-contained market", "id": "1911.08944", "abstract": "cross-correlations in fluctuations of the daily exchange rates within the basket of the 100 highest-capitalization cryptocurrencies over the period october 1, 2015, through march 31, 2019, are studied. the corresponding dynamics predominantly involve one leading eigenvalue of the correlation matrix, while the others largely coincide with those of wishart random matrices. however, the magnitude of the principal eigenvalue, and thus the degree of collectivity, strongly depends on which cryptocurrency is used as a base. it is largest when the base is the most peripheral cryptocurrency; when more significant ones are taken into consideration, its magnitude systematically decreases, nevertheless preserving a sizable gap with respect to the random bulk, which in turn indicates that the organization of correlations becomes more heterogeneous. this finding provides a criterion for recognizing which currencies or cryptocurrencies play a dominant role in the global crypto-market. the present study shows that over the period under consideration, the bitcoin (btc) predominates, hallmarking exchange rate dynamics at least as influential as the us dollar. the btc started dominating around the year 2017, while further cryptocurrencies, like the ethereum (eth) and even ripple (xrp), assumed similar trends. at the same time, the usd, an original value determinant for the cryptocurrency market, became increasingly disconnected, its related characteristics eventually approaching those of a fictitious currency. these results are strong indicators of incipient independence of the global cryptocurrency market, delineating a self-contained trade resembling the forex.", "categories": "q-fin.st cs.ce econ.em", "created": "2019-11-20", "updated": "2020-02-11", "authors": ["stanis\u0142aw dro\u017cd\u017c", "ludovico minati", "pawe\u0142 o\u015bwi\u0119cimka", "marek stanuszek", "marcin w\u0105torek"], "url": "https://arxiv.org/abs/1911.08944"}, {"title": "a scrambled method of moments", "id": "1911.09128", "abstract": "quasi-monte carlo (qmc) methods are a powerful alternative to classical monte-carlo (mc) integration. under certain conditions, they can approximate the desired integral at a faster rate than the usual central limit theorem, resulting in more accurate estimates. this paper explores these methods in a simulation-based estimation setting with an emphasis on the scramble of owen (1995). for cross-sections and short-panels, the resulting scrambled method of moments simply replaces the random number generator with the scramble (available in most softwares) to reduce simulation noise. scrambled indirect inference estimation is also considered. for time series, qmc may not apply directly because of a curse of dimensionality on the time dimension. a simple algorithm and a class of moments which circumvent this issue are described. asymptotic results are given for each algorithm. monte-carlo examples illustrate these results in finite samples, including an income process with \"lots of heterogeneity.\"", "categories": "econ.em stat.me", "created": "2019-11-20", "updated": "", "authors": ["jean-jacques forneron"], "url": "https://arxiv.org/abs/1911.09128"}, {"title": "a flexible mixed-frequency vector autoregression with a steady-state   prior", "id": "1911.09151", "abstract": "we propose a bayesian vector autoregressive (var) model for mixed-frequency data. our model is based on the mean-adjusted parametrization of the var and allows for an explicit prior on the 'steady states' (unconditional means) of the included variables. based on recent developments in the literature, we discuss extensions of the model that improve the flexibility of the modeling approach. these extensions include a hierarchical shrinkage prior for the steady-state parameters, and the use of stochastic volatility to model heteroskedasticity. we put the proposed model to use in a forecast evaluation using us data consisting of 10 monthly and 3 quarterly variables. the results show that the predictive ability typically benefits from using mixed-frequency data, and that improvements can be obtained for both monthly and quarterly variables. we also find that the steady-state prior generally enhances the accuracy of the forecasts, and that accounting for heteroskedasticity by means of stochastic volatility usually provides additional improvements, although not for all variables.", "categories": "econ.em", "created": "2019-11-20", "updated": "", "authors": ["sebastian ankargren", "m\u00e5ns unosson", "yukai yang"], "url": "https://arxiv.org/abs/1911.09151"}, {"title": "manipulable outcomes within the class of scoring voting rules", "id": "1911.09173", "abstract": "coalitional manipulation in voting is considered to be any scenario in which a group of voters decide to misrepresent their vote in order to secure an outcome they all prefer to the first outcome of the election when they vote honestly. the present paper is devoted to study coalitional manipulability within the class of scoring voting rules. for any such rule and any number of alternatives, we introduce a new approach allowing to characterize all the outcomes that can be manipulable by a coalition of voters. this gives us the possibility to find the probability of manipulable outcomes for some well-studied scoring voting rules in case of small number of alternatives and large electorates under a well-known assumption on individual preference profiles.", "categories": "econ.th math.oc", "created": "2019-11-01", "updated": "2020-09-24", "authors": ["mostapha diss", "boris tsvelikhovskiy"], "url": "https://arxiv.org/abs/1911.09173"}, {"title": "regression discontinuity design under self-selection", "id": "1911.09248", "abstract": "in regression discontinuity (rd) design, self-selection leads to different distributions of covariates on two sides of the policy intervention, which essentially violates the continuity of potential outcome assumption. the standard rd estimand becomes difficult to interpret due to the existence of some indirect effect, i.e. the effect due to self selection. we show that the direct causal effect of interest can still be recovered under a class of estimands. specifically, we consider a class of weighted average treatment effects tailored for potentially different target populations. we show that a special case of our estimands can recover the average treatment effect under the conditional independence assumption per angrist and rokkanen (2015), and another example is the estimand recently proposed in fr\\\"olich and huber (2018). we propose a set of estimators through a weighted local linear regression framework and prove the consistency and asymptotic normality of the estimators. our approach can be further extended to the fuzzy rd case. in simulation exercises, we compare the performance of our estimator with the standard rd estimator. finally, we apply our method to two empirical data sets: the u.s. house elections data in lee (2008) and a novel data set from microsoft bing on generalized second price (gsp) auction.", "categories": "stat.me econ.em", "created": "2019-11-20", "updated": "", "authors": ["sida peng", "yang ning"], "url": "https://arxiv.org/abs/1911.09248"}, {"title": "on the disclosure of promotion value in platforms with learning sellers", "id": "1911.09256", "abstract": "we consider a platform facilitating trade between sellers and buyers with the objective of maximizing consumer surplus. even though in many such marketplaces prices are set by revenue-maximizing sellers, platforms can influence prices through (i) price-dependent promotion policies that can increase demand for a product by featuring it in a prominent position on the webpage and (ii) the information revealed to sellers about the value of being promoted. identifying effective joint information design and promotion policies is a challenging dynamic problem as sellers can sequentially learn the promotion value from sales observations and update prices accordingly. we introduce the notion of confounding promotion policies, which are designed to prevent a bayesian seller from learning the promotion value (at the expense of the short-run loss of diverting consumers from the best product offering). leveraging these policies, we characterize the maximum long-run average consumer surplus that is achievable through joint information design and promotion policies when the seller sets prices myopically. we then establish a bayesian nash equilibrium by showing that the seller's best response to the platform's optimal policy is to price myopically at every history. moreover, the equilibrium we identify is platform-optimal within the class of horizon-maximin equilibria, in which strategies are not predicated on precise knowledge of the horizon length, and are designed to maximize payoff over the worst-case horizon. our analysis allows one to identify practical long-run average optimal platform policies in a broad range of demand models.", "categories": "econ.th", "created": "2019-11-20", "updated": "2020-04-03", "authors": ["yonatan gur", "gregory macnamara", "daniela saban"], "url": "https://arxiv.org/abs/1911.09256"}, {"title": "hybrid quantile estimation for asymmetric power garch models", "id": "1911.09343", "abstract": "asymmetric power garch models have been widely used to study the higher order moments of financial returns, while their quantile estimation has been rarely investigated. this paper introduces a simple monotonic transformation on its conditional quantile function to make the quantile regression tractable. the asymptotic normality of the resulting quantile estimators is established under either stationarity or non-stationarity. moreover, based on the estimation procedure, new tests for strict stationarity and asymmetry are also constructed. this is the first try of the quantile estimation for non-stationary arch-type models in the literature. the usefulness of the proposed methodology is illustrated by simulation results and real data analysis.", "categories": "econ.em stat.me", "created": "2019-11-21", "updated": "", "authors": ["guochang wang", "ke zhu", "guodong li", "wai keung li"], "url": "https://arxiv.org/abs/1911.09343"}, {"title": "a practical introduction to regression discontinuity designs:   foundations", "id": "1911.09511", "abstract": "in this element and its accompanying element, matias d. cattaneo, nicolas idrobo, and rocio titiunik provide an accessible and practical guide for the analysis and interpretation of regression discontinuity (rd) designs that encourages the use of a common set of practices and facilitates the accumulation of rd-based empirical evidence. in this element, the authors discuss the foundations of the canonical sharp rd design, which has the following features: (i) the score is continuously distributed and has only one dimension, (ii) there is only one cutoff, and (iii) compliance with the treatment assignment is perfect. in the accompanying element, the authors discuss practical and conceptual extensions to the basic rd setup.", "categories": "stat.me econ.em stat.ap stat.co", "created": "2019-11-21", "updated": "", "authors": ["matias d. cattaneo", "nicolas idrobo", "rocio titiunik"], "url": "https://arxiv.org/abs/1911.09511"}, {"title": "the artefact of the natural resources curse", "id": "1911.09681", "abstract": "this paper reexamines the validity of the natural resource curse hypothesis, using the database of mineral exporting countries. our findings are as follows: (i) resource-rich countries (rrcs) do not necessarily exhibit poor political, economic and social performance; (ii) rrcs that perform poorly have a low diversified exports portfolio; (iii) in contrast, rrcs with a low diversified exports portfolio do not necessarily perform poorly. then, we develop a model of strategic interaction from a bayesian game setup to study the role of leadership and governance in the management of natural resources. we show that an improvement in the leadership-governance binomial helps to discipline the behavior of lobby groups (theorem 1) and generate a pareto improvement in the management of natural resources (theorem 2). evidence from the world bank group's cpia data confirms the later finding. our results remain valid after some robustness checks.", "categories": "econ.gn q-fin.ec", "created": "2019-11-21", "updated": "", "authors": ["matata ponyo mapon", "jean-paul k. tsasa"], "url": "https://arxiv.org/abs/1911.09681"}, {"title": "facility location problem with capacity constraints: algorithmic and   mechanism design perspectives", "id": "1911.09813", "abstract": "we consider the facility location problem in the one-dimensional setting where each facility can serve a limited number of agents from the algorithmic and mechanism design perspectives. from the algorithmic perspective, we prove that the corresponding optimization problem, where the goal is to locate facilities to minimize either the total cost to all agents or the maximum cost of any agent is np-hard. however, we show that the problem is fixed-parameter tractable, and the optimal solution can be computed in polynomial time whenever the number of facilities is bounded, or when all facilities have identical capacities. we then consider the problem from a mechanism design perspective where the agents are strategic and need not reveal their true locations. we show that several natural mechanisms studied in the uncapacitated setting either lose strategyproofness or a bound on the solution quality for the total or maximum cost objective. we then propose new mechanisms that are strategyproof and achieve approximation guarantees that almost match the lower bounds.", "categories": "cs.gt cs.ai econ.th", "created": "2019-11-21", "updated": "", "authors": ["haris aziz", "hau chan", "barton e. lee", "bo li", "toby walsh"], "url": "https://arxiv.org/abs/1911.09813"}, {"title": "guarantees in fair division: general or monotone preferences", "id": "1911.10009", "abstract": "to divide a \"manna\" {\\omega} of private items (commodities, workloads, land, time intervals) between n agents, the worst case measure of fairness is the welfare guaranteed to each agent, irrespective of others' preferences. if the manna is non atomic and utilities are continuous (not necessarily monotone or convex), we can guarantee the minmax utility: that of our agent's best share in her worst partition of the manna; and implement it by kuhn's generalisation of divide and choose. the larger maxmin utility -- of her worst share in her best partition -- cannot be guaranteed, even for two agents. if for all agents more manna is better than less (or less is better than more), our bid & choose rules implement guarantees between minmax and maxmin by letting agents bid for the smallest (or largest) size of a share they find acceptable.", "categories": "econ.th", "created": "2019-11-22", "updated": "2020-09-16", "authors": ["anna bogomolnaia", "herve moulin"], "url": "https://arxiv.org/abs/1911.10009"}, {"title": "speculative trading, prospect theory and transaction costs", "id": "1911.10106", "abstract": "a speculative agent with prospect theory preference chooses the optimal time to purchase and then to sell an indivisible risky asset as to maximize the expected utility of the round-trip profit net of transaction costs. the optimization problem is formulated as a sequential optimal stopping problem and we provide a complete characterization of the solution. depending on the preference and market parameters as well as the initial price of the asset, the optimal strategy can be \"buy and hold\", \"buy low sell high\", \"buy high sell higher\" or \"no trading\". transaction costs do not necessarily curb speculative trading. for example, while a large proportional transaction cost on sale can unambiguously suppress trading participation, introducing a fixed market entry fee will indeed encourage trading when the asset price level is high.", "categories": "q-fin.mf econ.gn q-fin.ec q-fin.gn", "created": "2019-11-22", "updated": "", "authors": ["alex s. l. tse", "harry zheng"], "url": "https://arxiv.org/abs/1911.10106"}, {"title": "linear social learning in networks with rational agents", "id": "1911.10116", "abstract": "we consider a sequential social-learning environment with rational agents and gaussian private signals, focusing on how the observation network affects the speed of learning. agents learn about a binary state and take turns choosing actions based on own signals and network neighbors' behavior. equilibrium learning may be slow when agents do not observe all predecessors, as agents compromise between incorporating the signals of the observed neighbors and not over-counting the confounding signals of the unobserved early movers. we show that on any network, equilibrium actions are a log-linear function of observations and each agent's accuracy admits a signal-counting interpretation. adding links to the observation network can harm agents even without introducing new confounds. we then consider a network structure where agents move in generations and observe some members of the previous generation. when this observation structure is sufficiently symmetric, the additional information aggregated by each generation is asymptotically equivalent to fewer than two independent signals, even when generations are arbitrarily large. when agents observe all predecessors from the previous generation, social learning aggregates no more than three signals per generation starting from the third generation, and the long-run learning rate is slower when generations are larger.", "categories": "econ.th cs.si econ.gn q-fin.ec", "created": "2019-11-22", "updated": "2020-02-11", "authors": ["krishna dasaratha", "kevin he"], "url": "https://arxiv.org/abs/1911.10116"}, {"title": "uniform inference for value functions", "id": "1911.10215", "abstract": "this paper develops a novel approach to uniform inference for the optimal value function, that is, the function that results from optimizing an objective function marginally over one of its arguments. the marginal optimization map is nonlinear and not differentiable, which complicates inference procedures, since statistical inference methods for nonlinear maps usually rely on regularity through a type of differentiability. we show that the map from objective function to uniform test statistics applied to the value functions - such as kolmogorov-smirnov or cram\\'er-von mises statistics - are directionally differentiable. we establish consistency and weak convergence of nonparametric plug-in estimates of the test statistics and show how they can be used to conduct uniform inference. because the limiting distribution of sample value functions is not generally tractable, to conduct practical inference, we develop detailed resampling techniques that combine a bootstrap procedure with estimates of the directional derivatives. in addition, we formally establish uniform size control of the resampling procedure for testing. monte carlo simulations assess the finite-sample properties of the proposed methods and show accurate empirical size of the procedures. finally, we apply our methods to the evaluation of a job training program using bounds for the distribution function of treatment effects.", "categories": "econ.em", "created": "2019-11-22", "updated": "2020-07-03", "authors": ["sergio firpo", "antonio f. galvao", "thomas parker"], "url": "https://arxiv.org/abs/1911.10215"}, {"title": "a singular stochastic control approach for optimal pairs trading with   proportional transaction costs", "id": "1911.10450", "abstract": "optimal trading strategies for pairs trading have been studied by models that try to find either optimal shares of stocks by assuming no transaction costs or optimal timing of trading fixed numbers of shares of stocks with transaction costs. to find optimal strategies which determine optimally both trade times and number of shares in pairs trading process, we use a singular stochastic control approach to study an optimal pairs trading problem with proportional transaction costs. assuming a cointegrated relationship for a pair of stock log-prices, we consider a portfolio optimization problem which involves dynamic trading strategies with proportional transaction costs. we show that the value function of the control problem is the unique viscosity solution of a nonlinear quasi-variational inequality, which is equivalent to a free boundary problem for the singular stochastic control value function. we then develop a discrete time dynamic programming algorithm to compute the transaction regions, and show the convergence of the discretization scheme. we illustrate our approach with numerical examples and discuss the impact of different parameters on transaction regions. we study the out-of-sample performance in an empirical study that consists of six pairs of u.s. stocks selected from different industry sectors, and demonstrate the efficiency of the optimal strategy.", "categories": "q-fin.tr econ.em", "created": "2019-11-23", "updated": "", "authors": ["haipeng xing"], "url": "https://arxiv.org/abs/1911.10450"}, {"title": "topologically mapping the macroeconomy", "id": "1911.10476", "abstract": "an understanding of the economic landscape in a world of ever increasing data necessitates representations of data that can inform policy, deepen understanding and guide future research. topological data analysis offers a set of tools which deliver on all three calls. abstract two-dimensional snapshots of multi-dimensional space readily capture non-monotonic relationships, inform of similarity between points of interest in parameter space, mapping such to outcomes. specific examples show how some, but not all, countries have returned to great depression levels, and reappraise the links between real private capital growth and the performance of the economy. theoretical and empirical expositions alike remind on the dangers of assuming monotonic relationships and discounting combinations of factors as determinants of outcomes; both dangers topological data analysis addresses. policy-makers can look at outcomes and target areas of the input space where such are not satisfactory, academics may additionally find evidence to motivate theoretical development, and practitioners can gain a rapid and robust base for decision making.", "categories": "econ.em", "created": "2019-11-24", "updated": "", "authors": ["pawel dlotko", "simon rudkin", "wanling qiu"], "url": "https://arxiv.org/abs/1911.10476"}, {"title": "high-dimensional forecasting in the presence of unit roots and   cointegration", "id": "1911.10552", "abstract": "we investigate how the possible presence of unit roots and cointegration affects forecasting with big data. as most macroeoconomic time series are very persistent and may contain unit roots, a proper handling of unit roots and cointegration is of paramount importance for macroeconomic forecasting. the high-dimensional nature of big data complicates the analysis of unit roots and cointegration in two ways. first, transformations to stationarity require performing many unit root tests, increasing room for errors in the classification. second, modelling unit roots and cointegration directly is more difficult, as standard high-dimensional techniques such as factor models and penalized regression are not directly applicable to (co)integrated data and need to be adapted. we provide an overview of both issues and review methods proposed to address these issues. these methods are also illustrated with two empirical applications.", "categories": "econ.em", "created": "2019-11-24", "updated": "", "authors": ["stephan smeekes", "etienne wijler"], "url": "https://arxiv.org/abs/1911.10552"}, {"title": "predicting bubble bursts in oil prices using mixed causal-noncausal   models", "id": "1911.10916", "abstract": "this paper investigates oil price series using mixed causal-noncausal autoregressive (mar) models, namely dynamic processes that depend not only on their lags but also on their leads. mar models have been successfully implemented on commodity prices as they allow to generate nonlinear features such as speculative bubbles. we estimate the probabilities that bubbles in oil price series burst once the series enter an explosive phase. to do so we first evaluate how to adequately detrend nonstationary oil price series while preserving the bubble patterns observed in the raw data. the impact of different filters on the identification of mar models as well as on forecasting bubble events is investigated using monte carlo simulations. we illustrate our findings on wti and brent monthly series.", "categories": "econ.em", "created": "2019-11-25", "updated": "", "authors": ["alain hecq", "elisa voisin"], "url": "https://arxiv.org/abs/1911.10916"}, {"title": "income growth with high inequality implies loss of well-being: a   mathematical model", "id": "1911.11205", "abstract": "a mathematical model of measurement of the perception of well-being for groups with increasing incomes, but proportionally unequal is proposed. assuming that welfare grows with own income and decreases with relative inequality (income of the other concerning one's own), possible scenarios for long-term behavior in welfare functions are concluded. also, it is proved that a high relative inequality (parametric definition) always implies the loss of the self-perception of the well-being of the most disadvantaged group.", "categories": "econ.gn math.ds q-fin.ec", "created": "2019-11-25", "updated": "", "authors": ["fernando c\u00f3rdova-lepe"], "url": "https://arxiv.org/abs/1911.11205"}, {"title": "a new set of cluster driven composite development indicators", "id": "1911.11226", "abstract": "composite development indicators used in policy making often subjectively aggregate a restricted set of indicators. we show, using dimensionality reduction techniques, including principal component analysis (pca) and for the first time information filtering and hierarchical clustering, that these composite indicators miss key information on the relationship between different indicators. in particular, the grouping of indicators via topics is not reflected in the data at a global and local level. we overcome these issues by using the clustering of indicators to build a new set of cluster driven composite development indicators that are objective, data driven, comparable between countries, and retain interpretabilty. we discuss their consequences on informing policy makers about country development, comparing them with the top pagerank indicators as a benchmark. finally, we demonstrate that our new set of composite development indicators outperforms the benchmark on a dataset reconstruction task.", "categories": "econ.gn physics.soc-ph q-fin.ec q-fin.st", "created": "2019-11-25", "updated": "2020-03-26", "authors": ["anshul verma", "orazio angelini", "tiziana di matteo"], "url": "https://arxiv.org/abs/1911.11226"}, {"title": "direct and indirect transactions and requirements", "id": "1911.11569", "abstract": "the indirect transactions between sectors of an economic system has been a long-standing open problem. there have been numerous attempts to define and mathematically formulate this concept in various other scientific fields in literature as well. the existing indirect effects formulations, however, can neither determine the direct and indirect transactions separately nor quantify these transactions between two individual sectors of interest in an economic system. the novel concepts of the direct, indirect and total transactions between any two sectors are introduced, and the corresponding requirements matrices are systematically formulated relative to both final demands and gross outputs, based on the system decomposition theory. it is demonstrated theoretically and through illustrative examples that the proposed requirements matrices accurately define and quantify the corresponding direct, indirect, and total interactions and relationships. the proposed requirements matrices for the us economy using aggregated input-output tables for multiple years are then presented and briefly analyzed.", "categories": "econ.gn q-fin.ec", "created": "2019-11-23", "updated": "2020-03-19", "authors": ["husna betul coskun"], "url": "https://arxiv.org/abs/1911.11569"}, {"title": "proportionality and the limits of welfarism", "id": "1911.11747", "abstract": "we study two influential voting rules proposed in the 1890s by phragm\\'en and thiele, which elect a committee or parliament of k candidates which proportionally represents the voters. voters provide their preferences by approving an arbitrary number of candidates. previous work has proposed proportionality axioms satisfied by thiele's rule (now known as proportional approval voting, pav) but not by phragm\\'en's rule. by proposing two new proportionality axioms (laminar proportionality and priceability) satisfied by phragm\\'en but not thiele, we show that the two rules achieve two distinct forms of proportional representation. phragm\\'en's rule ensures that all voters have a similar amount of influence on the committee, and thiele's rule ensures a fair utility distribution.   thiele's rule is a welfarist voting rule (one that maximizes a function of voter utilities). we show that no welfarist rule can satisfy our new axioms, and we prove that no such rule can satisfy the core. conversely, some welfarist fairness properties cannot be guaranteed by phragm\\'en-type rules. this formalizes the difference between the two types of proportionality. we then introduce an attractive committee rule which satisfies a property intermediate between the core and extended justified representation (ejr). it satisfies laminar proportionality, priceability, and is computable in polynomial time. we show that our new rule provides a logarithmic approximation to the core. on the other hand, pav provides a factor-2 approximation to the core, and this factor is optimal for rules that are fair in the sense of the pigou--dalton principle.", "categories": "cs.gt econ.th", "created": "2019-11-26", "updated": "", "authors": ["dominik peters", "piotr skowron"], "url": "https://arxiv.org/abs/1911.11747"}, {"title": "a contribution to theory of factor income distribution, cambridge   capital controversy and equity premium puzzle", "id": "1911.12490", "abstract": "under very general conditions, we construct a micro-macro model for closed economy with a large number of heterogeneous agents. by introducing both financial capital (i.e. valued capital---- equities of firms) and physical capital (i.e. capital goods), our framework gives a logically consistent, complete factor income distribution theory with micro-foundation. the model shows factor incomes obey different distribution rules at the micro and macro levels, while marginal distribution theory and no-arbitrage princi-ple are unified into a common framework. our efforts solve the main problems of cambridge capital controversy, and reasonably explain the equity premium puzzle. strong empirical evidences support our results.", "categories": "econ.th", "created": "2019-11-27", "updated": "2019-12-07", "authors": ["xiaofeng liu"], "url": "https://arxiv.org/abs/1911.12490"}, {"title": "an integrated early warning system for stock market turbulence", "id": "1911.12596", "abstract": "this study constructs an integrated early warning system (ews) that identifies and predicts stock market turbulence. based on switching arch (swarch) filtering probabilities of the high volatility regime, the proposed ews first classifies stock market crises according to an indicator function with thresholds dynamically selected by the two-peak method. a hybrid algorithm is then developed in the framework of a long short-term memory (lstm) network to make daily predictions that alert turmoils. in the empirical evaluation based on ten-year chinese stock data, the proposed ews yields satisfying results with the test-set accuracy of $96.6\\%$ and on average $2.4$ days of the forewarned period. the model's stability and practical value in real-time decision-making are also proven by the cross-validation and back-testing.", "categories": "econ.em q-fin.mf", "created": "2019-11-28", "updated": "", "authors": ["peiwan wang", "lu zong", "ye ma"], "url": "https://arxiv.org/abs/1911.12596"}, {"title": "a principal-agent approach to capacity remuneration mechanisms", "id": "1911.12623", "abstract": "we propose to study electricity capacity remuneration mechanism design through a principal-agent approach. the principal represents the aggregation of electricity consumers (or a representative entity), subject to the physical risk of shortage, and the agent represents the electricity capacity owners, who invest in capacity and produce electricity to satisfy consumers' demand, and are subject to financial risks. following the methodology of cvitanic et al. (2017), we propose an optimal contract, from consumers' perspective, which complements the revenue capacity owners achieved from the spot energy market, and incentivizes both parties to perform an optimal level of investments while sharing the physical and financial risks. numerical results provide insights on the necessity of a capacity remuneration mechanism and also show how this is especially true when the level of uncertainties on demand or production side increases.", "categories": "econ.gn q-fin.ec", "created": "2019-11-28", "updated": "2020-09-01", "authors": ["cl\u00e9mence alasseur", "heythem farhat", "marcelo saguan"], "url": "https://arxiv.org/abs/1911.12623"}, {"title": "inference under random limit bootstrap measures", "id": "1911.12779", "abstract": "asymptotic bootstrap validity is usually understood as consistency of the distribution of a bootstrap statistic, conditional on the data, for the unconditional limit distribution of a statistic of interest. from this perspective, randomness of the limit bootstrap measure is regarded as a failure of the bootstrap. we show that such limiting randomness does not necessarily invalidate bootstrap inference if validity is understood as control over the frequency of correct inferences in large samples. we first establish sufficient conditions for asymptotic bootstrap validity in cases where the unconditional limit distribution of a statistic can be obtained by averaging a (random) limiting bootstrap distribution. further, we provide results ensuring the asymptotic validity of the bootstrap as a tool for conditional inference, the leading case being that where a bootstrap distribution estimates consistently a conditional (and thus, random) limit distribution of a statistic. we apply our framework to several inference problems in econometrics, including linear models with possibly non-stationary regressors, functional cusum statistics, conditional kolmogorov-smirnov specification tests, the `parameter on the boundary' problem and tests for constancy of parameters in dynamic econometric models.", "categories": "econ.em", "created": "2019-11-28", "updated": "2019-12-05", "authors": ["giuseppe cavaliere", "iliyan georgiev"], "url": "https://arxiv.org/abs/1911.12779"}, {"title": "dynamic optimal choice when rewards are unbounded below", "id": "1911.13025", "abstract": "we propose a new approach to solving dynamic decision problems with rewards that are unbounded below. the approach involves transforming the bellman equation in order to convert an unbounded problem into a bounded one. the major advantage is that, when the conditions stated below are satisfied, the transformed problem can be solved by iterating with a contraction mapping. while the method is not universal, we show by example that many common decision problems do satisfy our conditions.", "categories": "econ.th math.oc", "created": "2019-11-29", "updated": "", "authors": ["qingyin ma", "john stachurski"], "url": "https://arxiv.org/abs/1911.13025"}, {"title": "semiparametric quantile models for ascending auctions with asymmetric   bidders", "id": "1911.13063", "abstract": "the paper proposes a parsimonious and flexible semiparametric quantile regression specification for asymmetric bidders within the independent private value framework. asymmetry is parameterized using powers of a parent private value distribution, which is generated by a quantile regression specification. as noted in cantillon (2008) , this covers and extends models used for efficient collusion, joint bidding and mergers among homogeneous bidders. the specification can be estimated for ascending auctions using the winning bids and the winner's identity. the estimation is in two stage. the asymmetry parameters are estimated from the winner's identity using a simple maximum likelihood procedure. the parent quantile regression specification can be estimated using simple modifications of gimenes (2017). specification testing procedures are also considered. a timber application reveals that weaker bidders have $30\\%$ less chances to win the auction than stronger ones. it is also found that increasing participation in an asymmetric ascending auction may not be as beneficial as using an optimal reserve price as would have been expected from a result of bulowklemperer (1996) valid under symmetry.", "categories": "econ.em", "created": "2019-11-29", "updated": "2020-09-18", "authors": ["jayeeta bhattacharya", "nathalie gimenes", "emmanuel guerre"], "url": "https://arxiv.org/abs/1911.13063"}, {"title": "heuristic strategies in uncertain approval voting environments", "id": "1912.00011", "abstract": "in many collective decision making situations, agents vote to choose an alternative that best represents the preferences of the group. agents may manipulate the vote to achieve a better outcome by voting in a way that does not reflect their true preferences. in real world voting scenarios, people often do not have complete information about other voter preferences and it can be computationally complex to identify a strategy that will maximize their expected utility. in such situations, it is often assumed that voters will vote truthfully rather than expending the effort to strategize. however, being truthful is just one possible heuristic that may be used. in this paper, we examine the effectiveness of heuristics in single winner and multi-winner approval voting scenarios with missing votes. in particular, we look at heuristics where a voter ignores information about other voting profiles and makes their decisions based solely on how much they like each candidate. in a behavioral experiment, we show that people vote truthfully in some situations and prioritize high utility candidates in others. we examine when these behaviors maximize expected utility and show how the structure of the voting environment affects both how well each heuristic performs and how humans employ these heuristics.", "categories": "cs.gt cs.ai cs.ma econ.gn q-fin.ec", "created": "2019-11-29", "updated": "", "authors": ["jaelle scheuerman", "jason l. harman", "nicholas mattei", "k. brent venable"], "url": "https://arxiv.org/abs/1912.00011"}, {"title": "one for all, all for one---von neumann, wald, rawls, and pareto", "id": "1912.00211", "abstract": "applications of the maximin criterion extend beyond economics to statistics, computer science, politics, and operations research. however, the maximin criterion---be it von neumann's, wald's, or rawls'---draws fierce criticism due to its extremely pessimistic stance. i propose a novel concept, dubbed the optimin criterion, which is based on (pareto) optimizing the worst-case payoffs of tacit agreements. the optimin criterion generalizes and unifies results in various fields: it not only coincides with (i) wald's statistical decision-making criterion when nature is antagonistic, (ii) the core in cooperative games when the core is nonempty, though it exists even if the core is empty, but it also generalizes (iii) nash equilibrium in $n$-person constant-sum games, (iv) stable matchings in matching models, and (v) competitive equilibrium in the arrow-debreu economy. moreover, every nash equilibrium satisfies the optimin criterion in an auxiliary game.", "categories": "econ.th", "created": "2019-11-30", "updated": "2020-05-28", "authors": ["mehmet s. ismail"], "url": "https://arxiv.org/abs/1912.00211"}, {"title": "optimal forest rotation under carbon pricing and forest damage risk", "id": "1912.00269", "abstract": "forests will have two notable economic roles in the future: providing renewable raw material and storing carbon to mitigate climate change. the pricing of forest carbon leads to longer rotation times and consequently larger carbon stocks, but also exposes landowners to a greater risk of forest damage. this paper investigates optimal forest rotation under carbon pricing and forest damage risk. i provide the optimality conditions for this problem and illustrate the setting with numerical calculations representing boreal forests under a range of carbon prices and damage probabilities. the relation between damage probability and carbon price towards the optimal rotation length is nearly linear, with carbon pricing having far greater impact. as such, increasing forest carbon stocks by lengthening rotations is an economically attractive method for climate change mitigation, despite the forest damage risk. carbon pricing also increases land expectation value and reduces the economic risks of the landowner. the production possibility frontier under optimal rotation suggests that significantly larger forests carbon stocks are achievable, but imply lower harvests. however, forests' societally optimal role between these two activities is not yet clear-cut; but rests on the future development of relative prices between timber, carbon and other commodities dependent on land-use.", "categories": "econ.gn q-fin.ec", "created": "2019-11-30", "updated": "", "authors": ["tommi ekholm"], "url": "https://arxiv.org/abs/1912.00269"}, {"title": "fair division with bounded sharing", "id": "1912.00459", "abstract": "a set of objects is to be divided fairly among agents with different tastes, modeled by additive value functions. if the objects cannot be shared, so that each of them must be entirely allocated to a single agent, then fair division may not exist. how many objects must be shared between two or more agents in order to attain a fair division? the paper studies various notions of fairness, such as proportionality, envy-freeness and equitability. it also studies consensus division, in which each agent assigns the same value to all bundles --- a notion that is useful in truthful fair division mechanisms. it proves upper bounds on the number of required sharings. however, it shows that finding the minimum number of sharings is, in general, np-hard even for generic instances. many problems remain open.", "categories": "cs.gt econ.th", "created": "2019-12-01", "updated": "", "authors": ["erel segal-halevi"], "url": "https://arxiv.org/abs/1912.00459"}, {"title": "a multifactor regime-switching model for inter-trade durations in the   limit order market", "id": "1912.00764", "abstract": "this paper studies inter-trade durations in the nasdaq limit order market and finds that inter-trade durations in ultra-high frequency have two modes. one mode is to the order of approximately 10^{-4} seconds, and the other is to the order of 1 second. this phenomenon and other empirical evidence suggest that there are two regimes associated with the dynamics of inter-trade durations, and the regime switchings are driven by the changes of high-frequency traders (hfts) between providing and taking liquidity. to find how the two modes depend on information in the limit order book (lob), we propose a two-state multifactor regime-switching (mf-rsd) model for inter-trade durations, in which the probabilities transition matrices are time-varying and depend on some lagged lob factors. the mf-rsd model has good in-sample fitness and the superior out-of-sample performance, compared with some benchmark duration models. our findings of the effects of lob factors on the inter-trade durations help to understand more about the high-frequency market microstructure.", "categories": "econ.em", "created": "2019-12-02", "updated": "", "authors": ["zhicheng li", "haipeng xing", "xinyun chen"], "url": "https://arxiv.org/abs/1912.00764"}, {"title": "clustering and external validity in randomized controlled trials", "id": "1912.01052", "abstract": "in the literature studying randomized controlled trials (rcts), it is often assumed that the potential outcomes of units participating in the experiment are deterministic. this assumption is unlikely to hold, as stochastic shocks may take place during the experiment. in this paper, we consider the case of an rct with individual-level treatment assignment, and we allow for individual-level and cluster-level (e.g. village-level) shocks to affect the potential outcomes. we show that one can draw inference on two estimands: the ate conditional on the realizations of the cluster-level shocks, using heteroskedasticity-robust standard errors; the ate netted out of those shocks, using cluster-robust standard errors. by clustering, researchers can test if the treatment would still have had an effect, had the stochastic shocks that occurred during the experiment been different. then, the decision to cluster or not depends on the level of external validity one would like to achieve.", "categories": "econ.em", "created": "2019-12-02", "updated": "2020-07-27", "authors": ["antoine deeb", "cl\u00e9ment de chaisemartin"], "url": "https://arxiv.org/abs/1912.01052"}, {"title": "mean-shift least squares model averaging", "id": "1912.01194", "abstract": "this paper proposes a new estimator for selecting weights to average over least squares estimates obtained from a set of models. our proposed estimator builds on the mallows model average (mma) estimator of hansen (2007), but, unlike mma, simultaneously controls for location bias and regression error through a common constant. we show that our proposed estimator-- the mean-shift mallows model average (msa) estimator-- is asymptotically optimal to the original mma estimator in terms of mean squared error. a simulation study is presented, where we show that our proposed estimator uniformly outperforms the mma estimator.", "categories": "econ.em math.st stat.me stat.th", "created": "2019-12-03", "updated": "", "authors": ["kenichiro mcalinn", "kosaku takanashi"], "url": "https://arxiv.org/abs/1912.01194"}, {"title": "refuting samuelson's capitulation on the re-switching of techniques in   the cambridge capital controversy", "id": "1912.01250", "abstract": "paul a. samuelson's (1966) capitulation during the so-called cambridge controversy on the re-switching of techniques in capital theory had implications not only in pointing at supposed internal contradiction of the marginal theory of production and distribution, but also in preserving vested interests in the academic and political world. based on a new non-switching theorem, the present paper demonstrates that samuelson's capitulation was logically groundless from the point of view of the economic theory of production.", "categories": "econ.th", "created": "2019-12-03", "updated": "2019-12-19", "authors": ["carlo milana"], "url": "https://arxiv.org/abs/1912.01250"}, {"title": "bilinear form test statistics for extremum estimation", "id": "1912.01410", "abstract": "this paper develops a set of test statistics based on bilinear forms in the context of the extremum estimation framework with particular interest in nonlinear hypothesis. we show that the proposed statistic converges to a conventional chi-square limit. a monte carlo experiment suggests that the test statistic works well in finite samples.", "categories": "econ.em", "created": "2019-12-03", "updated": "", "authors": ["federico crudu", "felipe osorio"], "url": "https://arxiv.org/abs/1912.01410"}, {"title": "examination of the correlation between working time reduction and   employment", "id": "1912.01605", "abstract": "in recent years, it has been debated whether a reduction in working hours would be a viable solution to tackle the unemployment caused by technological change. the improvement of existing production technology is gradually being seen to reduce labor demand. although this debate has been at the forefront for many decades, the high and persistent unemployment encountered in the european union has renewed interest in implementing this policy in order to increase employment. according to advocates of reducing working hours, this policy will increase the number of workers needed during the production process, increasing employment. however, the contradiction expressed by advocates of working time reduction is that the increase in labor costs will lead to a reduction in business activity and ultimately to a reduction in demand for human resources. in this article, we will attempt to answer the question of whether reducing working hours is a way of countering the potential decline in employment due to technological change. in order to answer this question, the aforementioned conflicting views will be examined. as we will see during our statistical examination of the existing empirical studies, the reduction of working time does not lead to increased employment and cannot be seen as a solution to the long-lasting unemployment.", "categories": "econ.gn q-fin.ec", "created": "2019-12-02", "updated": "", "authors": ["virginia tsoukatou"], "url": "https://arxiv.org/abs/1912.01605"}, {"title": "celebrating three decades of worldwide stock market manipulation", "id": "1912.01708", "abstract": "as the decade turns, we reflect on nearly thirty years of successful manipulation of the world's public equity markets. this reflection highlights a few of the key enabling ingredients and lessons learned along the way. a quantitative understanding of market impact and its decay, which we cover briefly, lets you move long-term market prices to your advantage at acceptable cost. hiding your footprints turns out to be less important than moving prices in the direction most people want them to move. widespread (if misplaced) trust of market prices -- buttressed by overestimates of the cost of manipulation and underestimates of the benefits to certain market participants -- makes price manipulation a particularly valuable and profitable tool. of the many recent stories heralding the dawn of the present golden age of misinformation, the manipulation leading to the remarkable increase in the market capitalization of the world's publicly traded companies over the past three decades is among the best.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.tr", "created": "2019-11-20", "updated": "", "authors": ["bruce knuteson"], "url": "https://arxiv.org/abs/1912.01708"}, {"title": "high dimensional latent panel quantile regression with an application to   asset pricing", "id": "1912.02151", "abstract": "we propose a generalization of the linear panel quantile regression model to accommodate both \\textit{sparse} and \\textit{dense} parts: sparse means while the number of covariates available is large, potentially only a much smaller number of them have a nonzero impact on each conditional quantile of the response variable; while the dense part is represent by a low-rank matrix that can be approximated by latent factors and their loadings. such a structure poses problems for traditional sparse estimators, such as the $\\ell_1$-penalised quantile regression, and for traditional latent factor estimator, such as pca. we propose a new estimation procedure, based on the admm algorithm, consists of combining the quantile loss function with $\\ell_1$ \\textit{and} nuclear norm regularization. we show, under general conditions, that our estimator can consistently estimate both the nonzero coefficients of the covariates and the latent low-rank matrix.   our proposed model has a \"characteristics + latent factors\" asset pricing model interpretation: we apply our model and estimator with a large-dimensional panel of financial data and find that (i) characteristics have sparser predictive power once latent factors were controlled (ii) the factors and coefficients at upper and lower quantiles are different from the median.", "categories": "econ.em math.st stat.me stat.th", "created": "2019-12-04", "updated": "", "authors": ["alexandre belloni", "mingli chen", "oscar hernan madrid padilla", "n/a zixuan", "n/a wang"], "url": "https://arxiv.org/abs/1912.02151"}, {"title": "estimating large mixed-frequency bayesian var models", "id": "1912.02231", "abstract": "we discuss the issue of estimating large-scale vector autoregressive (var) models with stochastic volatility in real-time situations where data are sampled at different frequencies. in the case of a large var with stochastic volatility, the mixed-frequency data warrant an additional step in the already computationally challenging markov chain monte carlo algorithm used to sample from the posterior distribution of the parameters. we suggest the use of a factor stochastic volatility model to capture a time-varying error covariance structure. because the factor stochastic volatility model renders the equations of the var conditionally independent, settling for this particular stochastic volatility model comes with major computational benefits. first, we are able to improve upon the mixed-frequency simulation smoothing step by leveraging a univariate and adaptive filtering algorithm. second, the regression parameters can be sampled equation-by-equation in parallel. these computational features of the model alleviate the computational burden and make it possible to move the mixed-frequency var to the high-dimensional regime. we illustrate the model by an application to us data using our mixed-frequency var with 20, 34 and 119 variables.", "categories": "econ.em", "created": "2019-12-04", "updated": "", "authors": ["sebastian ankargren", "paulina jon\u00e9us"], "url": "https://arxiv.org/abs/1912.02231"}, {"title": "quality selection in two-sided markets: a constrained price   discrimination approach", "id": "1912.02251", "abstract": "online platforms collect rich information about participants and then share some of this information with their participants to improve market outcomes. in this paper we study the following information disclosure problem in a two-sided market: which sellers should the platform allow to participate and how much of its available information about participant sellers' quality should the platform share with buyers to maximize its own revenue. to study this information disclosure problem, we introduce two distinct two-sided market models: one in which the platform chooses prices and the sellers choose quantities (similar to ride-sharing), and one in which the sellers choose prices (similar to e-commerce). our main results provide conditions under which simple information structures commonly observed in practice, such as banning certain sellers from the platform while not distinguishing between participating sellers, maximize the platform's revenue. an important innovation in our analysis that we leverage to obtain our structural results is to transform the study of the two-sided market platform's optimal information disclosure policy into a constrained price discrimination problem.", "categories": "econ.th math.oc", "created": "2019-12-04", "updated": "2020-09-29", "authors": ["ramesh johari", "bar light", "gabriel weintraub"], "url": "https://arxiv.org/abs/1912.02251"}, {"title": "modeling and prediction of iran's steel consumption based on economic   activity using support vector machines", "id": "1912.02373", "abstract": "the steel industry has great impacts on the economy and the environment of both developed and underdeveloped countries. the importance of this industry and these impacts have led many researchers to investigate the relationship between a country's steel consumption and its economic activity resulting in the so-called intensity of use model. this paper investigates the validity of the intensity of use model for the case of iran's steel consumption and extends this hypothesis by using the indexes of economic activity to model the steel consumption. we use the proposed model to train support vector machines and predict the future values for iran's steel consumption. the paper provides detailed correlation tests for the factors used in the model to check for their relationships with the steel consumption. the results indicate that iran's steel consumption is strongly correlated with its economic activity following the same pattern as the economy has been in the last four decades.", "categories": "econ.gn cs.lg q-fin.ec stat.ml", "created": "2019-12-04", "updated": "", "authors": ["hossein kamalzadeh", "saeid nassim sobhan", "azam boskabadi", "mohsen hatami", "amin gharehyakheh"], "url": "https://arxiv.org/abs/1912.02373"}, {"title": "stylized facts and agent-based modeling", "id": "1912.02684", "abstract": "the existence of stylized facts in financial data has been documented in many studies. in the past decade the modeling of financial markets by agent-based computational economic market models has become a frequently used modeling approach. the main purpose of these models is to replicate stylized facts and to identify sufficient conditions for their creations. in this paper we introduce the most prominent examples of stylized facts and especially present stylized facts of financial data. furthermore, we given an introduction to agent-based modeling. here, we not only provide an overview of this topic but introduce the idea of universal building blocks for agent-based economic market models.", "categories": "q-fin.gn econ.em", "created": "2019-12-02", "updated": "", "authors": ["simon cramer", "torsten trimborn"], "url": "https://arxiv.org/abs/1912.02684"}, {"title": "one dollar each eliminates envy", "id": "1912.02797", "abstract": "we study the fair division of a collection of $m$ indivisible goods amongst a set of $n$ agents. whilst envy-free allocations typically do not exist in the indivisible goods setting, envy-freeness can be achieved if some amount of a divisible good (money) is introduced. specifically, halpern and shah (sagt 2019, pp.374-389) showed that, given additive valuation functions where the marginal value of each item is at most one dollar for each agent, there always exists an envy-free allocation requiring a subsidy of at most $(n-1)\\cdot m$ dollars. the authors also conjectured that a subsidy of $n-1$ dollars is sufficient for additive valuations. we prove this conjecture. in fact, a subsidy of at most one dollar per agent is sufficient to guarantee the existence of an envy-free allocation. further, we prove that for general monotonic valuation functions an envy-free allocation always exists with a subsidy of at most $2(n-1)$ dollars per agent. in particular, the total subsidy required for monotonic valuations is independent of the number of items.", "categories": "cs.gt econ.th", "created": "2019-12-05", "updated": "", "authors": ["johannes brustle", "jack dippel", "vishnu v. narayan", "mashbat suzuki", "adrian vetta"], "url": "https://arxiv.org/abs/1912.02797"}, {"title": "the choice of when to buy and when to sell", "id": "1912.02869", "abstract": "a consumer who wants to consume a good at a particular period may nevertheless attempt to buy it earlier if he is concerned that the good will otherwise be sold. we analyze the behavior of consumers in equilibrium and the price a profit-maximizing firm would charge. we show that a firm profits by not selling early. if, however, the firm is obligated to also offer the good early, then the firm may maximize profits by setting a price which induces consumers to all arrive early, or all arrive late, depending on the good's value to the customer.", "categories": "econ.gn math.oc q-fin.ec", "created": "2019-12-05", "updated": "", "authors": ["amihai glazer", "refael hassin", "irit nowik"], "url": "https://arxiv.org/abs/1912.02869"}, {"title": "triple the gamma -- a unifying shrinkage prior for variance and variable   selection in sparse state space and tvp models", "id": "1912.03100", "abstract": "time-varying parameter (tvp) models are very flexible in capturing gradual changes in the effect of a predictor on the outcome variable. however, in particular when the number of predictors is large, there is a known risk of overfitting and poor predictive performance, since the effect of some predictors is constant over time. we propose a prior for variance shrinkage in tvp models, called triple gamma. the triple gamma prior encompasses a number of priors that have been suggested previously, such as the bayesian lasso, the double gamma prior and the horseshoe prior. we present the desirable properties of such a prior and its relationship to bayesian model averaging for variance selection. the features of the triple gamma prior are then illustrated in the context of time varying parameter vector autoregressive models, both for simulated datasets and for a series of macroeconomics variables in the euro area.", "categories": "econ.em stat.me", "created": "2019-12-06", "updated": "", "authors": ["annalisa cadonna", "sylvia fr\u00fchwirth-schnatter", "peter knaus"], "url": "https://arxiv.org/abs/1912.03100"}, {"title": "the international effects of central bank information shocks", "id": "1912.03158", "abstract": "we explore the international transmission of monetary policy and central bank information shocks by the federal reserve and the european central bank. identification of these shocks is achieved by using a combination of high-frequency market surprises around announcement dates of policy decisions and sign restrictions. we propose a high-dimensional macroeconometric framework for modeling aggregate quantities alongside country-specific variables to study international shock propagation and spillover effects. our results are in line with the established literature focusing on individual economies, and moreover suggest substantial international spillover effects in both directions for monetary policy and central bank information shocks. in addition, we detect heterogeneities in the transmission of ecb policy actions to individual member states.", "categories": "econ.em econ.gn q-fin.ec", "created": "2019-12-06", "updated": "", "authors": ["michael pfarrhofer", "anna stelzer"], "url": "https://arxiv.org/abs/1912.03158"}, {"title": "synthetic controls and weighted event studies with staggered adoption", "id": "1912.03290", "abstract": "staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. the synthetic control method (scm) is a recent addition to the evaluation toolkit but is designed to study a single treated unit and does not easily accommodate staggered adoption. in this paper, we generalize scm to the staggered adoption setting. current practice involves fitting scm separately for each treated unit and then averaging. we show that the average of separate scm fits does not necessarily achieve good balance for the average of the treated units, leading to possible bias in the estimated effect. we propose \"partially pooled\" scm weights that instead minimize both average and state-specific imbalance, and show that the resulting estimator controls bias under a linear factor model. we also combine our partially pooled scm weights with traditional fixed effects methods to obtain an augmented estimator that improves over both scm weighting and fixed effects estimation alone. we assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. we implement the proposed method in the augsynth r package.", "categories": "stat.me econ.em", "created": "2019-12-06", "updated": "", "authors": ["eli ben-michael", "avi feller", "jesse rothstein"], "url": "https://arxiv.org/abs/1912.03290"}, {"title": "investigating the investment behaviors in cryptocurrency", "id": "1912.03311", "abstract": "this study investigates the socio-demographic characteristics that individual cryptocurrency investors exhibit and the factors which go into their investment decisions in different initial coin offerings. a web based revealed preference survey was conducted among australian and chinese blockchain and cryptocurrency followers, and a multinomial logit model was applied to inferentially analyze the characteristics of cryptocurrency investors and the determinants of the choice of investment in cryptocurrency coins versus other types of ico tokens. the results show a difference between the determinant of these two choices among australian and chinese cryptocurrency folks. the significant factors of these two choices include age, gender, education, occupation, and investment experience, and they align well with the behavioural literature. furthermore, alongside differences in how they rank the attributes of icos, there is further variance between how chinese and australian investors rank deterrence factors and investment strategies.", "categories": "econ.gn q-fin.ec", "created": "2019-12-06", "updated": "", "authors": ["dingli xi", "timothy ian o'brien", "elnaz irannezhad"], "url": "https://arxiv.org/abs/1912.03311"}, {"title": "perfect bidder collusion through bribe and request", "id": "1912.03607", "abstract": "we study collusion in a second price auction with two bidders in a dynamic environment. one bidder can make a take-it-or-leave-it collusion proposal, which consists of both an offer and a request of bribes, to the opponent. we show there always exists a robust equilibrium in which the collusion success probability is one. in the equilibrium, the interim expected payoff of the collusion initiator pareto dominates the counterpart in any robust equilibria of the single-option model (es\\\"{o} and schummer (2004)) and any other separating equilibria in our model.", "categories": "econ.th", "created": "2019-12-07", "updated": "", "authors": ["jingfeng lu", "zongwei lu", "christian riis"], "url": "https://arxiv.org/abs/1912.03607"}, {"title": "vat tax gap prediction: a 2-steps gradient boosting approach", "id": "1912.03781", "abstract": "tax evasion is the illegal evasion of taxes by individuals, corporations, and trusts. the revenue loss from tax avoidance can undermine the effectiveness and equity of the government policies. a standard measure of tax evasion is the tax gap, that can be estimated as the difference between the total amounts of tax theoretically collectable and the total amounts of tax actually collected in a given period. this paper presents an original contribution to bottom-up approach, based on results from fiscal audits, through the use of machine learning. the major disadvantage of bottom-up approaches is represented by selection bias when audited taxpayers are not randomly selected, as in the case of audits performed by the italian revenue agency. our proposal, based on a 2-steps gradient boosting model, produces a robust tax gap estimate and, embeds a solution to correct for the selection bias which do not require any assumptions on the underlying data distribution. the 2-steps gradient boosting approach is used to estimate the italian value-added tax (vat) gap on individual firms on the basis of fiscal and administrative data income tax returns gathered from tax administration data base, for the fiscal year 2011. the proposed method significantly boost the performance in predicting with respect to the classical parametric approaches.", "categories": "stat.ap econ.gn q-fin.ec stat.me stat.ml", "created": "2019-12-08", "updated": "2020-06-03", "authors": ["giovanna tagliaferri", "daria scacciatelli", "pierfrancesco alaimo di loro"], "url": "https://arxiv.org/abs/1912.03781"}, {"title": "energy scenario exploration with modeling to generate alternatives (mga)", "id": "1912.03788", "abstract": "energy system optimization models (esoms) should be used in an interactive way to uncover knife-edge solutions, explore alternative system configurations, and suggest different ways to achieve policy objectives under conditions of deep uncertainty. in this paper, we do so by employing an existing optimization technique called modeling to generate alternatives (mga), which involves a change in the model structure in order to systematically explore the near-optimal decision space. the mga capability is incorporated into tools for energy model optimization and analysis (temoa), an open source framework that also includes a technology rich, bottom up esom. in this analysis, temoa is used to explore alternative energy futures in a simplified single region energy system that represents the u.s. electric sector and a portion of the light duty transport sector. given the dataset limitations, we place greater emphasis on the methodological approach rather than specific results.", "categories": "physics.soc-ph econ.em", "created": "2019-12-08", "updated": "", "authors": ["joseph f. decarolis", "samaneh babaee", "binghui li", "suyash kanungo"], "url": "https://arxiv.org/abs/1912.03788"}, {"title": "approximate factor models with strongly correlated idiosyncratic errors", "id": "1912.04123", "abstract": "we consider the estimation of approximate factor models for time series data, where strong serial and cross-sectional correlations amongst the idiosyncratic component are present. this setting comes up naturally in many applications, but existing approaches in the literature rely on the assumption that such correlations are weak, leading to mis-specification of the number of factors selected and consequently inaccurate inference. in this paper, we explicitly incorporate the dependent structure present in the idiosyncratic component through lagged values of the observed multivariate time series. we formulate a constrained optimization problem to estimate the factor space and the transition matrices of the lagged values {\\em simultaneously}, wherein the constraints reflect the low rank nature of the common factors and the sparsity of the transition matrices. we establish theoretical properties of the obtained estimates, and introduce an easy-to-implement computational procedure for empirical work. the performance of the model and the implementation procedure is evaluated on synthetic data and compared with competing approaches, and further illustrated on a data set involving weekly log-returns of 75 us large financial institutions for the 2001-2016 period.", "categories": "stat.me econ.em", "created": "2019-12-09", "updated": "", "authors": ["jiahe lin", "george michailidis"], "url": "https://arxiv.org/abs/1912.04123"}, {"title": "regularized estimation of high-dimensional factor-augmented vector   autoregressive (favar) models", "id": "1912.04146", "abstract": "a factor-augmented vector autoregressive (favar) model is defined by a var equation that captures lead-lag correlations amongst a set of observed variables $x$ and latent factors $f$, and a calibration equation that relates another set of observed variables $y$ with $f$ and $x$. the latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the var system. the favar model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the var equation. however, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. hence, in this paper we investigate the favar model under high-dimensional scaling. we introduce an appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. further, we address a number of technical challenges introduced by the fact that estimates of the var system model parameters are based on estimated rather than directly observed quantities. the performance of the proposed estimators is evaluated on synthetic data. further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators.", "categories": "stat.me econ.em", "created": "2019-12-09", "updated": "2020-05-31", "authors": ["jiahe lin", "george michailidis"], "url": "https://arxiv.org/abs/1912.04146"}, {"title": "willingness to pay for community-based health insurance among rural   households of southwest ethiopia", "id": "1912.04281", "abstract": "use of healthcare services is inadequate in ethiopia in spite of the high burden of diseases. user-fee charges are the most important factor for this deficiency in healthcare utilization. hence, the country is introducing community based and social health insurances since 2010 to tackle such problems. this study was conducted cross-sectionally, in march 2013, to assess willingness of rural households to pay for community-based health insurance in debub bench district of southwest ethiopia. two-stage sampling technique was used to select 845 households. selected households were contacted using simple random sampling technique. double bounded dichotomous choice method was used to illicit the willingness to pay. data were analyzed with stata 11. krinsky and rob method was used to calculate the mean/median with 95% ci willingness to pay after the predictors have been estimated using seemingly unrelated bivariate probit regression. eight hundred and eight (95.6%) of the sampled households were interviewed. among them 629(77.8%) households were willing to join the proposed cbhi scheme. about 54% of the households in the district were willing to pay either the initial or second bids presented. on average, these households were willingness to pay was 162.61 birr per household (8.9 us$) annually. if the community based health insurance is rolled out in the district, about half of households will contribute 163 birr (8.9 us$) annually. if the premium exceeds the amount specified, majority of the households would not join the scheme. key words: community based health insurance, willingness to pay, contingent valuation method, double bounded dichotomous choice, krinsky and robb, rural households, ethiopia.", "categories": "econ.gn q-fin.ec", "created": "2019-12-09", "updated": "", "authors": ["melaku haile likka", "shimeles ololo sinkie", "berhane megerssa"], "url": "https://arxiv.org/abs/1912.04281"}, {"title": "market price of trading liquidity risk and market depth", "id": "1912.04565", "abstract": "price impact of a trade is an important element in pre-trade and post-trade analyses. we introduce a framework to analyze the market price of liquidity risk, which allows us to derive an inhomogeneous bernoulli ordinary differential equation. we obtain two closed form solutions, one of which reproduces the linear function of the order flow in kyle (1985) for informed traders. however, when traders are not as asymmetrically informed, an s-shape function of the order flow is obtained. we perform an empirical intra-day analysis on nikkei futures to quantify the price impact of order flow and compare our results with industry's heuristic price impact functions. our model of order flow yields a rich framework for not only to estimate the liquidity risk parameters, but also to provide a plausible cause of why volatility and correlation are stochastic in nature. finally, we find that the market depth encapsulates the market price of liquidity risk.", "categories": "q-fin.tr econ.em q-fin.cp q-fin.mf stat.co", "created": "2019-12-10", "updated": "", "authors": ["masaaki kijima", "christopher ting"], "url": "https://arxiv.org/abs/1912.04565"}, {"title": "adaptive dynamic model averaging with an application to house price   forecasting", "id": "1912.04661", "abstract": "dynamic model averaging (dma) combines the forecasts of a large number of dynamic linear models (dlms) to predict the future value of a time series. the performance of dma critically depends on the appropriate choice of two forgetting factors. the first of these controls the speed of adaptation of the coefficient vector of each dlm, while the second enables time variation in the model averaging stage. in this paper we develop a novel, adaptive dynamic model averaging (adma) methodology. the proposed methodology employs a stochastic optimisation algorithm that sequentially updates the forgetting factor of each dlm, and uses a state-of-the-art non-parametric model combination algorithm from the prediction with expert advice literature, which offers finite-time performance guarantees. an empirical application to quarterly uk house price data suggests that adma produces more accurate forecasts than the benchmark autoregressive model, as well as competing dma specifications.", "categories": "econ.em stat.me", "created": "2019-12-10", "updated": "", "authors": ["alisa yusupova", "nicos g. pavlidis", "efthymios g. pavlidis"], "url": "https://arxiv.org/abs/1912.04661"}, {"title": "voluntary disclosure and personalized pricing", "id": "1912.04774", "abstract": "central to privacy concerns is that firms may use consumer data to price discriminate. a common policy response is that consumers should be given control over which firms access their data and how. since firms learn about a consumer's preferences based on the data seen and the consumer's disclosure choices, the equilibrium implications of consumer control are unclear. we study whether such measures improve consumer welfare in monopolistic and competitive markets. we find that consumer control can improve consumer welfare relative to both perfect price discrimination and no personalized pricing. first, consumers can use disclosure to amplify competitive forces. second, consumers can disclose information to induce even a monopolist to lower prices. whether consumer control improves welfare depends on the disclosure technology and market competitiveness. simple disclosure technologies suffice in competitive markets. when facing a monopolist, a consumer needs partial disclosure possibilities to obtain any welfare gains.", "categories": "econ.th", "created": "2019-12-10", "updated": "2020-08-17", "authors": ["s. nageeb ali", "greg lewis", "shoshana vasserman"], "url": "https://arxiv.org/abs/1912.04774"}, {"title": "endogenous agglomeration in a many-region world", "id": "1912.05113", "abstract": "we study a general family of economic geography models that features endogenous agglomeration. in many-region settings, the spatial scale---\"global\" or \"local\"---of the dispersion force(s) in a model plays a key role in determining the resulting endogenous spatial patterns and comparative statics. a global dispersion force accrues from competition between different locations and leads to the formation of multiple economic clusters (or cities). a local dispersion force is caused by crowding effects within each location and induces the flattening of each city. by distinguishing local and global dispersion forces, we can define three prototypical classes, namely, models that have only global, only local, and both local and global dispersion forces. the three model classes engender qualitatively different spatial patterns. multiple cities are formed only when a global dispersion force is at work; otherwise, only a unimodal distribution can be formed. a city can have its spatial extent only when a local dispersion force is at work. a wide variety of extant models are reduced into the three prototypical classes. our framework adds consistent interpretations to the empirical literature and also provides general \\red{insights into} treatment effects in structural economic geography models.", "categories": "econ.gn math.ds nlin.ps q-fin.ec", "created": "2019-12-10", "updated": "2020-10-01", "authors": ["takashi akamatsu", "tomoya mori", "minoru osawa", "yuki takayama"], "url": "https://arxiv.org/abs/1912.05113"}, {"title": "third-degree price discrimination versus uniform pricing", "id": "1912.05164", "abstract": "we compare the revenue of the optimal third-degree price discrimination policy against a uniform pricing policy. a uniform pricing policy offers the same price to all segments of the market. our main result establishes that for a broad class of third-degree price discrimination problems with concave revenue functions and common support, a uniform price is guaranteed to achieve one half of the optimal monopoly profits. this revenue bound holds for any arbitrary number of segments and prices that the seller would use with third-degree price discrimination. we further establish that these conditions are tight and that weakening either common support or concavity leads to arbitrarily poor revenue comparisons.", "categories": "econ.gn cs.gt econ.th q-fin.ec", "created": "2019-12-11", "updated": "2020-07-22", "authors": ["dirk bergemann", "francisco castro", "gabriel weintraub"], "url": "https://arxiv.org/abs/1912.05164"}, {"title": "fuzzy group identification problems", "id": "1912.05540", "abstract": "we present a fuzzy version of the group identification problem (\"who is a j?\") introduced by kasher and rubinstein (1997). we consider a class $n = \\{1,2,\\ldots,n\\}$ of agents, each one with an opinion about the membership to a group j of the members of the society, consisting in a function $\\pi : n \\to [0; 1]$, indicating for each agent, including herself, the degree of membership to j. we consider the problem of aggregating those functions, satisfying different sets of axioms and characterizing different aggregators. while some results are analogous to those of the originally crisp model, the fuzzy version is able to overcome some of the main impossibility results of kasher and rubinstein.", "categories": "econ.th", "created": "2019-12-11", "updated": "", "authors": ["federico fioravanti", "fernando tohm\u00e9"], "url": "https://arxiv.org/abs/1912.05540"}, {"title": "the rise of multiple institutional affiliations in academia", "id": "1912.05576", "abstract": "this study provides the first systematic, international, large-scale evidence on the extent and nature of multiple institutional affiliations on journal publications. studying more than 13.6m authors and 20.5m articles from 40 countries we document that: in 2019, almost one in three articles was (co-)authored by authors with multiple affiliations and the share of authors with multiple affiliations increased from 6.7% to 11.8% since 1996. the growth of multiple affiliations is prevalent in all disciplines and it is stronger in high impact journals. about 60% of multiple affiliations involve institutions from within the academic sector and international co-affiliations often involve institutions from the united states, germany and the united kingdom as well as institutions in neighboring countries. we discuss potential causes and show that the timing of the rise in multiple affiliations can be linked to the introduction of more competitive funding structures such as 'excellence initiatives' in a number of countries. we discuss implications for science and science policy.", "categories": "econ.gn cs.dl q-fin.ec", "created": "2019-12-11", "updated": "2020-09-22", "authors": ["hanna hottenrott", "michael rose", "cornelia lawson"], "url": "https://arxiv.org/abs/1912.05576"}, {"title": "a bilateral river bargaining problem with negative externality", "id": "1912.05844", "abstract": "this article is addressing the problem of river sharing between two agents along a river in the presence of negative externalities. where, each agent claims river water based on the hydrological characteristics of the territories. the claims can be characterized by some international framework (principles) of entitlement. these international principles are appears to be inequitable by the other agents in the presence of negative externalities. the negotiated treaties address sharing water along with the issue of negative externalities imposed by the upstream agent on the downstream agents. the market based bargaining mechanism is used for modeling and for characterization of agreement points.", "categories": "econ.th", "created": "2019-12-12", "updated": "", "authors": ["shivshanker singh patel", "parthasarathy ramachandran"], "url": "https://arxiv.org/abs/1912.05844"}, {"title": "alternative axioms in group identification problems", "id": "1912.05961", "abstract": "kasher and rubinstein (1997) introduced the problem of classifying the members of a group in terms of the opinions of their potential members. this involves a finite set of agents $n = \\{1,2,\\ldots,n\\}$, each one having an opinion about which agents should be classified as belonging to a specific subgroup j. a collective identity function (cif) aggregates those opinions yielding the class of members deemed $j$. kasher and rubinstein postulate axioms, intended to ensure fair and socially desirable outcomes, characterizing different cifs. we follow their lead by replacing their liberal axiom by other axioms, constraining the spheres of influence of the agents. we show that some of them lead to different cifs while in another instance we find an impossibility result.", "categories": "econ.th", "created": "2019-12-11", "updated": "", "authors": ["federico fioravanti", "fernando tohm\u00e9"], "url": "https://arxiv.org/abs/1912.05961"}, {"title": "a regularized factor-augmented vector autoregressive model", "id": "1912.06049", "abstract": "we propose a regularized factor-augmented vector autoregressive (favar) model that allows for sparsity in the factor loadings. in this framework, factors may only load on a subset of variables which simplifies the factor identification and their economic interpretation. we identify the factors in a data-driven manner without imposing specific relations between the unobserved factors and the underlying time series. using our approach, the effects of structural shocks can be investigated on economically meaningful factors and on all observed time series included in the favar model. we prove consistency for the estimators of the factor loadings, the covariance matrix of the idiosyncratic component, the factors, as well as the autoregressive parameters in the dynamic model. in an empirical application, we investigate the effects of a monetary policy shock on a broad range of economically relevant variables. we identify this shock using a joint identification of the factor model and the structural innovations in the var model. we find impulse response functions which are in line with economic rationale, both on the factor aggregates and observed time series level.", "categories": "econ.em stat.ap", "created": "2019-12-12", "updated": "", "authors": ["maurizio daniele", "julie schnaitmann"], "url": "https://arxiv.org/abs/1912.06049"}, {"title": "inference for high-dimensional regressions with heteroskedasticity and   autocorrelation", "id": "1912.06307", "abstract": "time series regression analysis relies on the heteroskedasticity- and autocorrelation-consistent (hac) estimation of the asymptotic variance to conduct proper inference. this paper develops such inferential methods for high-dimensional time series regressions. to recognize the time series data structures we focus on the sparse-group lasso estimator. we establish the debiased central limit theorem for low dimensional groups of regression coefficients and study the hac estimator of the long-run variance based on the sparse-group lasso residuals. the treatment relies on a new fuk-nagaev inequality for a class of $\\tau$-dependent processes with heavier than gaussian tails, which is of independent interest.", "categories": "econ.em math.st stat.me stat.ml stat.th", "created": "2019-12-12", "updated": "2020-05-28", "authors": ["andrii babii", "eric ghysels", "jonas striaukas"], "url": "https://arxiv.org/abs/1912.06307"}, {"title": "synthetic control inference for staggered adoption: estimating the   dynamic effects of board gender diversity policies", "id": "1912.06320", "abstract": "we introduce a synthetic control methodology to study policies with staggered adoption. many policies, such as the board gender quota, are replicated by other policy setters at different time frames. our method estimates the dynamic average treatment effects on the treated using variation introduced by the staggered adoption of policies. our method gives asymptotically unbiased estimators of many interesting quantities and delivers asymptotically valid inference. by using the proposed method and national labor data in europe, we find evidence that quota regulation on board diversity leads to a decrease in part-time employment, and an increase in full-time employment for female professionals.", "categories": "econ.em", "created": "2019-12-12", "updated": "", "authors": ["jianfei cao", "shirley lu"], "url": "https://arxiv.org/abs/1912.06320"}, {"title": "network data", "id": "1912.06346", "abstract": "many economic activities are embedded in networks: sets of agents and the (often) rivalrous relationships connecting them to one another. input sourcing by firms, interbank lending, scientific research, and job search are four examples, among many, of networked economic activities. motivated by the premise that networks' structures are consequential, this chapter describes econometric methods for analyzing them. i emphasize (i) dyadic regression analysis incorporating unobserved agent-specific heterogeneity and supporting causal inference, (ii) techniques for estimating, and conducting inference on, summary network parameters (e.g., the degree distribution or transitivity index); and (iii) empirical models of strategic network formation admitting interdependencies in preferences. current research challenges and open questions are also discussed.", "categories": "econ.em stat.me", "created": "2019-12-13", "updated": "", "authors": ["bryan s. graham"], "url": "https://arxiv.org/abs/1912.06346"}, {"title": "the role of low temperature waste heat recovery in achieving 2050 goals:   a policy positioning paper", "id": "1912.06558", "abstract": "urban waste heat recovery, in which low temperature heat from urban sources is recovered for use in a district heat network, has a great deal of potential in helping to achieve 2050 climate goals. for example, heat from data centres, metro systems, public sector buildings and waste water treatment plants could be used to supply ten percent of europe's heat demand. despite this, at present, urban waste heat recovery is not widespread and is an immature technology. to help achieve greater uptake, three policy recommendations are made. first, policy raising awareness of waste heat recovery and creating a legal framework is suggested. second, it is recommended that pilot projects are promoted to help demonstrate technical and economic feasibility. finally, a pilot credit facility is proposed aimed at bridging the gap between potential investors and heat recovery projects.", "categories": "econ.gn q-fin.ec", "created": "2019-12-13", "updated": "", "authors": ["edward wheatcroft", "henry wynn", "kristina lygnerud", "giorgio bonvicini"], "url": "https://arxiv.org/abs/1912.06558"}, {"title": "periodic attractor in the discrete time best-response dynamics of the   rock-paper-scissors game", "id": "1912.06831", "abstract": "the rock-paper-scissors (rps) game is a classic non-cooperative game widely studied in terms of its theoretical analysis as well as in its applications, ranging from sociology and biology to economics. many experimental results of the rps game indicate that this game is better modelled by the discretized best-response dynamics rather than continuous time dynamics. in this work we show that the attractor of the discrete time best-response dynamics of the rps game is finite and periodic. moreover we also describe the bifurcations of the attractor and determine the exact number, period and location of the periodic strategies.", "categories": "math.ds cs.gt econ.th", "created": "2019-12-14", "updated": "", "authors": ["jos\u00e9 pedro gaiv\u00e3o", "telmo peixe"], "url": "https://arxiv.org/abs/1912.06831"}, {"title": "the crawler: two equivalence results for object (re)allocation problems   when preferences are single-peaked", "id": "1912.06909", "abstract": "for object reallocation problems, if preferences are strict but otherwise unrestricted, the top trading cycle rule (ttc) is the leading rule: it is the only rule satisfying efficiency, the endowment lower bound, and strategy-proofness; moreover, ttc coincides with the core. however, on the subdomain of single-peaked preferences, bade (2019a) defines a new rule, the \"crawler\", which also satisfies the first three properties. our first theorem states that the crawler and a naturally defined \"dual\" rule are actually the same. next, for object allocation problems, we define a probabilistic version of the crawler by choosing an endowment profile at random according to a uniform distribution, and applying the original definition. our second theorem states that this rule is the same as the \"random priority rule\" which, as proved by knuth (1996) and abdulkadiroglu and s\\\"onmez (1998), is equivalent to the \"core from random endowments\".", "categories": "econ.th cs.gt cs.ma", "created": "2019-12-14", "updated": "", "authors": ["yuki tamura", "hadi hosseini"], "url": "https://arxiv.org/abs/1912.06909"}, {"title": "eu economic modelling system", "id": "1912.07115", "abstract": "this is the first study that attempts to assess the regional economic impacts of the european institute of innovation and technology (eit) investments in a spatially explicit macroeconomic model, which allows us to take into account all key direct, indirect and spatial spillover effects of eit investments via inter-regional trade and investment linkages and a spatial diffusion of technology via an endogenously determined global knowledge frontier with endogenous growth engines driven by investments in knowledge and human capital. our simulation results of highly detailed eit expenditure data suggest that, besides sizable direct effects in those regions that receive the eit investment support, there are also significant spatial spillover effects to other (non-supported) eu regions. taking into account all key indirect and spatial spillover effects is a particular strength of the adopted spatial general equilibrium methodology; our results suggest that they are important indeed and need to be taken into account when assessing the impacts of eit investment policies on regional economies.", "categories": "econ.gn q-fin.ec", "created": "2019-12-15", "updated": "", "authors": ["olga ivanova", "d'artis kancs", "mark thissen"], "url": "https://arxiv.org/abs/1912.07115"}, {"title": "prediction intervals for synthetic control methods", "id": "1912.07120", "abstract": "uncertainty quantification is a fundamental problem in the analysis and interpretation of synthetic control (sc) methods. we develop prediction intervals in the canonical sc framework, and provide conditions under which these intervals offer finite-sample probability guarantees. our construction begins by noting that the statistical uncertainty of the sc prediction is governed by two distinct sources of randomness: one coming from the construction of the (likely misspecified) sc weights in the pre-treatment period, and the other coming from the unobservable stochastic error in the post-treatment period when the treatment effect is analyzed. accordingly, our proposed prediction intervals are constructed taking into account both sources of randomness. for implementation, we propose a multiplier bootstrap approach along with finite-sample-based probability bound arguments. we illustrate the performance of our proposed prediction intervals in the context of three empirical applications from the sc literature.", "categories": "stat.me econ.em", "created": "2019-12-15", "updated": "", "authors": ["matias d. cattaneo", "yingjie feng", "rocio titiunik"], "url": "https://arxiv.org/abs/1912.07120"}, {"title": "an economical business-cycle model", "id": "1912.07163", "abstract": "in recent decades, in developed economies, slack on the product and labor markets has fluctuated a lot over the business cycle, while inflation has been very stable. at the same time, these economies have been prone to enter long-lasting liquidity traps with stable positive inflation and high unemployment. motivated by these observations, this paper develops a simple policy-oriented business-cycle model in which (1) fluctuations in aggregate demand and supply lead to fluctuations in slack but not in inflation; and (2) the aggregate demand structure is consistent with permanent liquidity traps. the model extends the money-in-the-utility-function model by introducing matching frictions and including real wealth into the utility function. matching frictions allow us to represent slack and to consider a general equilibrium with constant inflation. wealth in the utility function enriches the aggregate demand structure to be consistent with permanent liquidity traps. we use the model to study the effects of various aggregate demand and supply shocks, and to analyze several stabilization policies---such as conventional monetary policy, helicopter drop of money, tax on wealth, and government spending.", "categories": "econ.th econ.gn q-fin.ec", "created": "2019-12-15", "updated": "", "authors": ["pascal michaillat", "emmanuel saez"], "url": "https://arxiv.org/abs/1912.07163"}, {"title": "analysis of regression discontinuity designs with multiple cutoffs or   multiple scores", "id": "1912.07346", "abstract": "we introduce the \\texttt{stata} (and \\texttt{r}) package \\texttt{rdmulti}, which includes three commands (\\texttt{rdmc}, \\texttt{rdmcplot}, \\texttt{rdms}) for analyzing regression discontinuity (rd) designs with multiple cutoffs or multiple scores. the command \\texttt{rdmc} applies to non-cumulative and cumulative multi-cutoff rd settings. it calculates pooled and cutoff-specific rd treatment effects, and provides robust bias-corrected inference procedures. post estimation and inference is allowed. the command \\texttt{rdmcplot} offers rd plots for multi-cutoff settings. finally, the command \\texttt{rdms} concerns multi-score settings, covering in particular cumulative cutoffs and two running variables contexts. it also calculates pooled and cutoff-specific rd treatment effects, provides robust bias-corrected inference procedures, and allows for post-estimation estimation and inference. these commands employ the \\texttt{stata} (and \\texttt{r}) package \\texttt{rdrobust} for plotting, estimation, and inference. companion \\texttt{r} functions with the same syntax and capabilities are provided.", "categories": "stat.co econ.em", "created": "2019-12-16", "updated": "2020-04-25", "authors": ["matias d. cattaneo", "rocio titiunik", "gonzalo vazquez-bare"], "url": "https://arxiv.org/abs/1912.07346"}, {"title": "estimation of auction models with shape restrictions", "id": "1912.07466", "abstract": "we introduce several new estimation methods that leverage shape constraints in auction models to estimate various objects of interest, including the distribution of a bidder's valuations, the bidder's ex ante expected surplus, and the seller's counterfactual revenue. the basic approach applies broadly in that (unlike most of the literature) it works for a wide range of auction formats and allows for asymmetric bidders. though our approach is not restrictive, we focus our analysis on first--price, sealed--bid auctions with independent private valuations. we highlight two nonparametric estimation strategies, one based on a least squares criterion and the other on a maximum likelihood criterion. we also provide the first direct estimator of the strategy function. we establish several theoretical properties of our methods to guide empirical analysis and inference. in addition to providing the asymptotic distributions of our estimators, we identify ways in which methodological choices should be tailored to the objects of their interest. for objects like the bidders' ex ante surplus and the seller's counterfactual expected revenue with an additional symmetric bidder, we show that our input--parameter--free estimators achieve the semiparametric efficiency bound. for objects like the bidders' inverse strategy function, we provide an easily implementable boundary--corrected kernel smoothing and transformation method in order to ensure the squared error is integrable over the entire support of the valuations. an extensive simulation study illustrates our analytical results and demonstrates the respective advantages of our least--squares and maximum likelihood estimators in finite samples. compared to estimation strategies based on kernel density estimation, the simulations indicate that the smoothed versions of our estimators enjoy a large degree of robustness to the choice of an input parameter.", "categories": "econ.em stat.ap stat.co stat.me", "created": "2019-12-16", "updated": "", "authors": ["joris pinkse", "karl schurter"], "url": "https://arxiv.org/abs/1912.07466"}, {"title": "estimating a behavioral new keynesian model", "id": "1912.07601", "abstract": "this paper analyzes identification issues of a behavorial new keynesian model and estimates it using likelihood-based and limited-information methods with identification-robust confidence sets. the model presents some of the same difficulties that exist in simple benchmark dsge models, but the analytical solution is able to indicate in what conditions the cognitive discounting parameter (attention to the future) can be identified and the robust estimation methods is able to confirm its importance for explaining the proposed behavioral model.", "categories": "econ.gn q-fin.ec", "created": "2019-12-16", "updated": "", "authors": ["joaquim andrade", "pedro cordeiro", "guilherme lambais"], "url": "https://arxiv.org/abs/1912.07601"}, {"title": "econometrics for decision making: building foundations sketched by   haavelmo and wald", "id": "1912.08726", "abstract": "in the early 1940s, haavelmo proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. his fundamental contribution has become thoroughly embedded in subsequent econometric research, yet it could not fully answer all the deep issues that the author raised. notably, haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. in the same period, wald initiated his own seminal development of statistical decision theory. haavelmo favorably cited wald, but econometrics did not embrace statistical decision theory. instead, it focused on study of identification, estimation, and statistical inference. this paper proposes statistical decision theory as a framework for evaluation of the performance of models in decision making. i particularly consider the common practice of as-if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. a central theme is that one should evaluate as-if optimization or any other model-based decision rule by its performance across the state space listing all states of nature that one believes feasible, not the model space. i apply the theme to prediction and treatment choice. statistical decision theory is conceptually simple, but application is often challenging. advancement of computation is the primary task to continue building the foundations sketched by haavelmo and wald.", "categories": "econ.em", "created": "2019-12-17", "updated": "2020-07-16", "authors": ["charles f. manski"], "url": "https://arxiv.org/abs/1912.08726"}, {"title": "a simple way to assess inference methods", "id": "1912.08772", "abstract": "we propose a simple way to assess the quality of asymptotic approximations required for inference methods. our assessment can detect problems when the asymptotic theory that justifies the inference method is invalid and/or provides a poor approximation given the design of the empirical application. it can be easily applied to a wide range of applications. if widely used by applied researchers, this assessment has the potential of substantially reducing the number of papers that are published based on misleading inference. we analyze in detail the cases of differences in differences with few treated cluster, stratified experiments, shift-share designs, and matching estimators.", "categories": "econ.em", "created": "2019-12-18", "updated": "2020-08-11", "authors": ["bruno ferman"], "url": "https://arxiv.org/abs/1912.08772"}, {"title": "hybrid threats as an exogenous economic shock", "id": "1912.08916", "abstract": "the aim of this study is to contribute to the theory of exogenous economic shocks and their equivalents in an attempt to explain business cycle fluctuations, which still do not have a clear explanation. to this end the author has developed an econometric model based on a regression analysis. another objective is to tackle the issue of hybrid threats, which have not yet been subjected to a cross-disciplinary research. these were reviewed in terms of their economic characteristics in order to complement research in the fields of defence and security.", "categories": "econ.gn q-fin.ec", "created": "2019-12-17", "updated": "", "authors": ["shteryo nozharov"], "url": "https://arxiv.org/abs/1912.08916"}, {"title": "inefficiencies in digital advertising markets", "id": "1912.09012", "abstract": "digital advertising markets are growing and attracting increased scrutiny. this paper explores four market inefficiencies that remain poorly understood: ad effect measurement, frictions between and within advertising channel members, ad blocking and ad fraud. these topics are not unique to digital advertising, but each manifests in new ways in markets for digital ads. we identify relevant findings in the academic literature, recent developments in practice, and promising topics for future research.", "categories": "econ.gn q-fin.ec", "created": "2019-12-18", "updated": "2020-02-22", "authors": ["brett r gordon", "kinshuk jerath", "zsolt katona", "sridhar narayanan", "jiwoong shin", "kenneth c wilbur"], "url": "https://arxiv.org/abs/1912.09012"}, {"title": "causal inference and data-fusion in econometrics", "id": "1912.09104", "abstract": "learning about cause and effect is arguably the main goal in applied econometrics. in practice, the validity of these causal inferences is contingent on a number of critical assumptions regarding the type of data that has been collected and the substantive knowledge that is available. for instance, unobserved confounding factors threaten the internal validity of estimates, data availability is often limited to non-random, selection-biased samples, causal effects need to be learned from surrogate experiments with imperfect compliance, and causal knowledge has to be extrapolated across structurally heterogeneous populations. a powerful causal inference framework is required to tackle these challenges, which plague most data analysis to varying degrees. building on the structural approach to causality introduced by haavelmo (1943) and the graph-theoretic framework proposed by pearl (1995), the artificial intelligence (ai) literature has developed a wide array of techniques for causal learning that allow to leverage information from various imperfect, heterogeneous, and biased data sources (bareinboim and pearl, 2016). in this paper, we discuss recent advances in this literature that have the potential to contribute to econometric methodology along three dimensions. first, they provide a unified and comprehensive framework for causal inference, in which the aforementioned problems can be addressed in full generality. second, due to their origin in ai, they come together with sound, efficient, and complete algorithmic criteria for automatization of the corresponding identification task. and third, because of the nonparametric description of structural models that graph-theoretic approaches build on, they combine the strengths of both structural econometrics as well as the potential outcomes framework, and thus offer a perfect middle ground between these two competing literature streams.", "categories": "econ.em", "created": "2019-12-19", "updated": "2019-12-20", "authors": ["paul h\u00fcnermund", "elias bareinboim"], "url": "https://arxiv.org/abs/1912.09104"}, {"title": "temporal-difference estimation of dynamic discrete choice models", "id": "1912.09509", "abstract": "we propose a new algorithm to estimate the structural parameters in dynamic discrete choice models. the algorithm is based on the conditional choice probability approach, but uses the idea of temporal-difference learning from the reinforcement learning literature to estimate the different terms in the value functions. in estimating these terms with functional approximations using basis functions, our approach has the advantage of naturally allowing for continuous state spaces. furthermore, it does not require specification of transition probabilities, and even estimation of choice probabilities can be avoided using a recursive procedure. computationally, our algorithm only requires solving a low dimensional linear equation. we find that it is substantially faster than existing approaches when the finite dependence property does not hold, and comparable in speed to approaches that exploit this property. for the estimation of dynamic games, our procedure does not require integrating over the actions of other players, which further heightens the computational advantage. we show that our estimator is consistent, and efficient under discrete state spaces. in settings with continuous states, we propose easy to implement locally robust corrections in order to achieve parametric rates of convergence. preliminary monte carlo simulations confirm the workings of our algorithm.", "categories": "econ.em", "created": "2019-12-19", "updated": "", "authors": ["karun adusumilli", "dita eckardt"], "url": "https://arxiv.org/abs/1912.09509"}, {"title": "robust multi-product pricing under general extreme value models", "id": "1912.09552", "abstract": "we study robust versions of pricing problems where customers choose products according to a general extreme value (gev) choice model, and the choice parameters are not given exactly but lie in an uncertainty set. we show that, when the robust problem is unconstrained and the price sensitivity parameters are homogeneous, the robust optimal prices have a constant markup over products and we provide formulas that allow to compute this constant markup by binary search. we also show that, in the case that the price sensitivity parameters are only homogeneous in each subset of the products and the uncertainty set is rectangular, the robust problem can be converted into a deterministic pricing problem and the robust optimal prices have a constant markup in each subset, and we also provide explicit formulas to compute them. for constrained pricing problems, we propose a formulation where, instead of requiring that the expected sale constraints be satisfied, we add a penalty cost to the objective function for violated constraints. we then show that the robust pricing problem with over-expected-sale penalties can be reformulated as a convex optimization program where the purchase probabilities are the decision variables. we provide numerical results for the logit and nested logit model to illustrate the advantages of our approach. our results generally hold for any arbitrary gev model, including the multinomial logit, nested or cross-nested logit.", "categories": "math.oc cs.ds econ.em", "created": "2019-12-19", "updated": "", "authors": ["tien mai", "patrick jaillet"], "url": "https://arxiv.org/abs/1912.09552"}, {"title": "from disequilibrium markets to equilibrium", "id": "1912.09679", "abstract": "the modeling of financial markets as disequilibrium models by ordinary differential equations has become a popular modeling tool. one famous example of such a model is the beja-goldman model(the journal of finance, 1980) which we consider in this paper. we study the passage from disequilibrium dynamics to equilibrium. mathematically, this limit corresponds to an asymptotic limit also known as a tikhonov-fenichel reduction. furthermore, we analyze the stability of the reduced equilibrium model and discuss the economic implications. we conduct several numerical examples to visualize and support our analysis.", "categories": "econ.gn math.ds q-fin.ec q-fin.mf q-fin.tr", "created": "2019-12-20", "updated": "", "authors": ["christian lax", "torsten trimborn"], "url": "https://arxiv.org/abs/1912.09679"}, {"title": "monetary policy and wealth inequalities in great britain: assessing the   role of unconventional policies for a decade of household data", "id": "1912.09702", "abstract": "this paper explores whether unconventional monetary policy operations have redistributive effects on household wealth. drawing on household balance sheet data from the wealth and asset survey, we construct monthly time series indicators on the distribution of different asset types held by british households for the period that the monetary policy switched as the policy rate reached the zero lower bound (2006-2016). using this series, we estimate the response of wealth inequalities on monetary policy, taking into account the effect of unconventional policies conducted by the bank of england in response to the global financial crisis. our evidence reveals that unconventional monetary policy shocks have significant long-lasting effects on wealth inequality: an expansionary monetary policy in the form of asset purchases raises wealth inequality across households, as measured by their gini coefficients of net wealth, housing wealth, and financial wealth. the evidence of our analysis helps to raise awareness of central bankers about the redistributive effects of their monetary policy decisions.", "categories": "econ.gn q-fin.ec", "created": "2019-12-20", "updated": "", "authors": ["anastasios evgenidis", "apostolos fasianos"], "url": "https://arxiv.org/abs/1912.09702"}, {"title": "optimal dynamic treatment regimes and partial welfare ordering", "id": "1912.10014", "abstract": "dynamic treatment regimes are treatment allocations tailored to heterogeneous individuals. the optimal dynamic treatment regime is a regime that maximizes counterfactual welfare. we introduce a framework in which we can partially learn the optimal dynamic regime from observational data, relaxing the sequential randomization assumption commonly employed in the literature but instead using (binary) instrumental variables. we propose the notion of sharp partial ordering of counterfactual welfares with respect to dynamic regimes and establish mapping from data to partial ordering via a set of linear programs. we then characterize the identified set of the optimal regime as the set of maximal elements associated with the partial ordering. one main contribution of this paper is that we develop simple analytical conditions to establish the ordering, which bypass solving a large number of large-scale linear programs, and thus facilitate estimation and inference. this paper's analytical framework has broader applicability beyond the current context, e.g., in establishing signs of various treatment effects and rankings of policies across different counterfactual scenarios.", "categories": "econ.em", "created": "2019-12-20", "updated": "2020-09-14", "authors": ["sukjin han"], "url": "https://arxiv.org/abs/1912.10014"}, {"title": "reslogit: a residual neural network logit model", "id": "1912.10058", "abstract": "we present a residual logit (reslogit) model for seamlessly integrating a data-driven deep neural network (dnn) architecture in the random utility maximization paradigm. dnn models such as the multi-layer perceptron (mlp) have shown remarkable success in modelling complex data accurately, but recent studies have consistently demonstrated that their black-box properties are incompatible with discrete choice analysis for the purpose of interpreting decision making behaviour. our proposed machine learning choice model is a departure from the conventional feed-forward mlp framework by using a dynamic residual neural network learning based approach. our proposed method can be formulated as a generalized extreme value (gev) random utility maximization model for greater flexibility in capturing unobserved heterogeneity. it can generate choice model structures where the covariance between random utilities is estimated and incorporated into the random error terms, allowing for a richer set of higher-order substitution patterns than a standard logit might be able to achieve. we describe the process of our model estimation and examine the relative empirical performance and econometric implications on two mode choice experiments. we analyzed the behavioural and theoretical properties of our methodology. we showed how model interpretability is possible, while also capturing the underlying complex and unobserved behavioural heterogeneity effects in the residual covariance matrices.", "categories": "econ.em", "created": "2019-12-20", "updated": "", "authors": ["melvin wong", "bilal farooq"], "url": "https://arxiv.org/abs/1912.10058"}, {"title": "mining the automotive industry: a network analysis of corporate   positioning and technological trends", "id": "1912.10097", "abstract": "the digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. following the need for monitoring shifting industries, we present a network-centred analysis of car manufacturer web pages. solely exploiting publicly-available information, we construct large networks from web pages and hyperlinks. the network properties disclose the internal corporate positioning of the three largest automotive manufacturers, toyota, volkswagen and hyundai with respect to innovative trends and their international outlook. we tag web pages concerned with topics like e-mobility and environment or autonomous driving, and investigate their relevance in the network. sentiment analysis on individual web pages uncovers a relationship between page linking and use of positive language, particularly with respect to innovative trends. web pages of the same country domain form clusters of different size in the network that reveal strong correlations with sales market orientation. our approach maintains the web content's hierarchical structure imposed by the web page networks. it, thus, presents a method to reveal hierarchical structures of unstructured text content obtained from web scraping. it is highly transparent, reproducible and data driven, and could be used to gain complementary insights into innovative strategies of firms and competitive landscapes, which would not be detectable by the analysis of web content alone.", "categories": "cs.si econ.gn q-fin.ec", "created": "2019-12-20", "updated": "2020-01-08", "authors": ["niklas stoehr", "fabian braesemann", "michael frommelt", "shi zhou"], "url": "https://arxiv.org/abs/1912.10097"}, {"title": "efficient and convergent sequential pseudo-likelihood estimation of   dynamic discrete games", "id": "1912.10488", "abstract": "we propose a new sequential efficient pseudo-likelihood (epl) estimator for structural economic models with an equality constraint, particularly dynamic discrete choice games of incomplete information. each iteration in the epl sequence is consistent and asymptotically efficient, and iterating to convergence improves finite sample performance. for dynamic single-agent models, we show that aguirregabiria and mira's (2002, 2007) nested pseudo-likelihood (npl) estimator arises as a special case of epl. in dynamic games, epl maintains its efficiency properties, although npl does not. and a convenient change of variable in the equilibrium fixed point equation ensures epl iterations have the same computational simplicity as npl iterations. furthermore, epl iterations are stable and locally convergent to the finite-sample maximum likelihood estimator at a nearly-quadratic rate for all regular markov perfect equilibria, including unstable equilibria where npl encounters convergence problems. monte carlo simulations confirm the theoretical results and demonstrate epl's good performance in finite samples.", "categories": "econ.em", "created": "2019-12-22", "updated": "", "authors": ["adam dearing", "jason r. blevins"], "url": "https://arxiv.org/abs/1912.10488"}, {"title": "building and testing yield curve generators for p&c insurance", "id": "1912.10526", "abstract": "interest-rate risk is a key factor for property-casualty insurer capital. p&c companies tend to be highly leveraged, with bond holdings much greater than capital. for gaap capital, bonds are marked to market but liabilities are not, so shifts in the yield curve can have a significant impact on capital. yield-curve scenario generators are one approach to quantifying this risk. they produce many future simulated evolutions of the yield curve, which can be used to quantify the probabilities of bond-value changes that would result from various maturity-mix strategies. some of these generators are provided as black-box models where the user gets only the projected scenarios. one focus of this paper is to provide methods for testing generated scenarios from such models by comparing to known distributional properties of yield curves.   p&c insurers hold bonds to maturity and manage cash-flow risk by matching asset and liability flows. derivative pricing and stochastic volatility are of little concern over the relevant time frames. this requires different models and model testing than what is common in the broader financial markets.   to complicate things further, interest rates for the last decade have not been following the patterns established in the sixty years following wwii. we are now coming out of the period of very low rates, yet are still not returning to what had been thought of as normal before that. modeling and model testing are in an evolving state while new patterns emerge.   our analysis starts with a review of the literature on interest-rate model testing, with a p&c focus, and an update of the tests for current market behavior. we then discuss models, and use them to illustrate the fitting and testing methods. the testing discussion does not require the model-building section.", "categories": "q-fin.rm econ.em", "created": "2019-12-22", "updated": "", "authors": ["gary venter", "kailan shang"], "url": "https://arxiv.org/abs/1912.10526"}, {"title": "improved central limit theorem and bootstrap approximations in high   dimensions", "id": "1912.10529", "abstract": "this paper deals with the gaussian and bootstrap approximations to the distribution of the max statistic in high dimensions. this statistic takes the form of the maximum over components of the sum of independent random vectors and its distribution plays a key role in many high-dimensional econometric problems. using a novel iterative randomized lindeberg method, the paper derives new bounds for the distributional approximation errors. these new bounds substantially improve upon existing ones and simultaneously allow for a larger class of bootstrap methods.", "categories": "math.st econ.em stat.th", "created": "2019-12-22", "updated": "", "authors": ["victor chernozhukov", "denis chetverikov", "kengo kato", "yuta koike"], "url": "https://arxiv.org/abs/1912.10529"}, {"title": "probability assessments of an ice-free arctic: comparing statistical and   climate model projections", "id": "1912.10774", "abstract": "the downward trend in arctic sea ice is a key factor determining the pace and intensity of future global climate change; moreover, declines in sea ice can have a wide range of additional environmental and economic consequences. based on several decades of satellite data, we provide statistical forecasts of arctic sea ice extent during the rest of this century. the best-fitting statistical model indicates that sea ice is diminishing at an increasing rate. by contrast, average projections from the cmip5 global climate models foresee a gradual slowing of sea ice loss even in high carbon emissions scenarios. our long-range statistical projections also deliver probability assessments of the timing of an ice-free arctic. this analysis indicates almost a 60 percent chance of an effectively ice-free arctic ocean in the 2030s -- much earlier than the average projection from global climate models.", "categories": "stat.ap econ.em", "created": "2019-12-23", "updated": "", "authors": ["francis x. diebold", "glenn d. rudebusch"], "url": "https://arxiv.org/abs/1912.10774"}, {"title": "variable-lag granger causality for time series analysis", "id": "1912.10829", "abstract": "granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. typical operationalizations of granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. however, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. to address this issue, we develop variable-lag granger causality, a generalization of granger causality that relaxes the assumption of the fixed time delay and allows causes to influence effects with arbitrary time delays. in addition, we propose a method for inferring variable-lag granger causality relations. we demonstrate our approach on an application for studying coordinated collective behavior and show that it performs better than several existing methods in both simulated and real-world datasets. our approach can be applied in any domain of time series analysis.", "categories": "cs.lg econ.em stat.me stat.ml", "created": "2019-12-18", "updated": "", "authors": ["chainarong amornbunchornvej", "elena zheleva", "tanya y. berger-wolf"], "url": "https://arxiv.org/abs/1912.10829"}, {"title": "economic complexity: why we like \"complexity weighted diversification\"", "id": "1912.10955", "abstract": "a recent paper by hausmann and collaborators (1) reaches the important conclusion that complexity-weighted diversification is the essential element to predict country growth. we like this result because complexity-weighted diversification is precisely the first equation of the fitness algorithm that we introduced in 2012 (2,3). however, contrary to what is claimed in (1), it is incorrect to say that diversification is contained also in the eci algorithm (4). we discuss the origin of this misunderstanding and show that the eci algorithm contains exactly zero diversification. this is actually one of the reasons for the poor performances of eci which leads to completely unrealistic results, as for instance, the derivation that qatar or saudi arabia are industrially more competitive than china (5,6). another important element of our new approach is the representation of the economic dynamics of countries as trajectories in the gdppc-fitness space (7-10). in some way also this has been rediscovered by hausmann and collaborators and renamed as \"stream plots\", but, given their weaker metrics and methods, they propose it to use it only for a qualitative insight, while ours led to quantitative and successful forecasting. the fitness approach has paved the way to a robust and testable framework for economic complexity resulting in a highly competitive scheme for growth forecasting (7-10). according to a recent report by bloomberg (9): the new fitness method, \"systematically outperforms standard methods, despite requiring much less data\".", "categories": "econ.gn q-fin.ec", "created": "2019-12-23", "updated": "", "authors": ["luciano pietronero", "andrea gabrielli", "andrea zaccaria"], "url": "https://arxiv.org/abs/1912.10955"}, {"title": "electoral crime under democracy: information effects from judicial   decisions in brazil", "id": "1912.10958", "abstract": "this paper examines voters' responses to the disclosure of electoral crime information in large democracies. i focus on brazil, where the electoral court makes candidates' criminal records public before every election. using a sample of local candidates running for office between 2004 and 2016, i find that a conviction for an electoral crime reduces candidates' probability of election and vote share by 10.3 and 12.9 percentage points (p.p.), respectively. these results are not explained by (potential) changes in judge, voter, or candidate behavior over the electoral process. i additionally perform machine classification of court documents to estimate heterogeneous punishment for severe and trivial crimes. i document a larger electoral penalty (6.5 p.p.) if candidates are convicted for severe crimes. these results supplement the information shortcut literature by examining how judicial information influences voters' decisions and showing that voters react more strongly to more credible sources of information.", "categories": "econ.gn q-fin.ec", "created": "2019-12-23", "updated": "", "authors": ["andre assumpcao"], "url": "https://arxiv.org/abs/1912.10958"}, {"title": "quantifying the effects of the 2008 recession using the zillow dataset", "id": "1912.11341", "abstract": "this report explores the use of zillow's housing metrics dataset to investigate the effects of the 2008 us subprime mortgage crisis on various us locales. we begin by exploring the causes of the recession and the metrics available to us in the dataset. we settle on using the zillow home value index (zhvi) because it is seasonally adjusted and able to account for a variety of inventory factors. then, we explore three methodologies for quantifying recession impact: (a) principal components analysis, (b) area under baseline, and (c) arima modeling and confidence intervals. while pca does not yield useable results, we ended up with six cities from both aub and arima analysis, the top 3 \"losers\" and \"gainers\" of the 2008 recession, as determined by each analysis. this gave us 12 cities in total. finally, we tested the robustness of our analysis against three \"common knowledge\" metrics for the recession: geographic clustering, population trends, and unemployment rate. while we did find some overlap between the results of our analysis and geographic clustering, there was no positive regression outcome from comparing our methodologies to population trends and the unemployment rate.", "categories": "econ.gn cs.lg cs.na math.na q-fin.ec", "created": "2019-12-22", "updated": "", "authors": ["arunav gupta", "lucas nguyen", "camille dunning", "ka ming chan"], "url": "https://arxiv.org/abs/1912.11341"}, {"title": "healthy access for healthy places: a multidimensional food access   measure", "id": "1912.11351", "abstract": "when it comes to preventive healthcare, place matters. it is increasingly clear that social factors, particularly reliable access to healthy food, are as determinant to health and health equity as medical care. however, food access studies often only present one-dimensional measurements of access. we hypothesize that food access is a multidimensional concept and evaluated penchansky and thomas's 1981 definition of access. in our approach, we identify ten variables contributing to food access in the city of chicago and use principal component analysis to identify vulnerable populations with low access. our results indicate that within the urban environment of the case study site, affordability is the most important factor in low food accessibility, followed by urban youth, reduced mobility, and higher immigrant population.", "categories": "econ.gn q-fin.ec", "created": "2019-12-22", "updated": "", "authors": ["irena gao", "marynia kolak"], "url": "https://arxiv.org/abs/1912.11351"}, {"title": "pareto models for risk management", "id": "1912.11736", "abstract": "the pareto model is very popular in risk management, since simple analytical formulas can be derived for financial downside risk measures (value-at-risk, expected shortfall) or reinsurance premiums and related quantities (large claim index, return period). nevertheless, in practice, distributions are (strictly) pareto only in the tails, above (possible very) large threshold. therefore, it could be interesting to take into account second order behavior to provide a better fit. in this article, we present how to go from a strict pareto model to pareto-type distributions. we discuss inference, and derive formulas for various measures and indices, and finally provide applications on insurance losses and financial risks.", "categories": "econ.em stat.me", "created": "2019-12-25", "updated": "", "authors": ["arthur charpentier", "emmanuel flachaire"], "url": "https://arxiv.org/abs/1912.11736"}, {"title": "semicooperation under curved strategy spacetime", "id": "1912.12146", "abstract": "mutually beneficial cooperation is a common part of economic systems as firms in partial cooperation with others can often make a higher sustainable profit. though cooperative games were popular in 1950s, recent interest in non-cooperative games is prevalent despite the fact that cooperative bargaining seems to be more useful in economic and political applications. in this paper we assume that the strategy space and time are inseparable with respect to a contract. under this assumption we show that the strategy spacetime is a dynamic curved liouville-like 2-brane quantum gravity surface under asymmetric information and that traditional euclidean geometry fails to give a proper feedback nash equilibrium. cooperation occurs when two firms' strategies fall into each other's influence curvature in this strategy spacetime. small firms in an economy dominated by large firms are subject to the influence of large firms. we determine an optimal feedback semi-cooperation of the small firm in this case using a liouville-feynman path integral method.", "categories": "math.oc econ.th", "created": "2019-12-27", "updated": "", "authors": ["paramahansa pramanik", "alan m. polansky"], "url": "https://arxiv.org/abs/1912.12146"}, {"title": "minimax semiparametric learning with approximate sparsity", "id": "1912.12213", "abstract": "many objects of interest can be expressed as a linear, mean square continuous functional of a least squares projection (regression). often the regression may be high dimensional, depending on many variables. this paper gives minimal conditions for root-n consistent and efficient estimation of such objects when the regression and the riesz representer of the functional are approximately sparse and the sum of the absolute value of the coefficients is bounded. the approximately sparse functions we consider are those where an approximation by some $t$ regressors has root mean square error less than or equal to $ct^{-\\xi}$ for $c,$ $\\xi>0.$ we show that a necessary condition for efficient estimation is that the sparse approximation rate $\\xi_{1}$ for the regression and the rate $\\xi_{2}$ for the riesz representer satisfy $\\max\\{\\xi_{1} ,\\xi_{2}\\}>1/2.$ this condition is stronger than the corresponding condition $\\xi_{1}+\\xi_{2}>1/2$ for holder classes of functions. we also show that lasso based, cross-fit, debiased machine learning estimators are asymptotically efficient under these conditions. in addition we show efficiency of an estimator without cross-fitting when the functional depends on the regressors and the regression sparse approximation rate satisfies $\\xi_{1}>1/2$.", "categories": "math.st econ.em stat.ml stat.th", "created": "2019-12-27", "updated": "", "authors": ["jelena bradic", "victor chernozhukov", "whitney k. newey", "yinchu zhu"], "url": "https://arxiv.org/abs/1912.12213"}, {"title": "reading macroeconomics from the yield curve: the turkish case", "id": "1912.12351", "abstract": "this paper aims to analyze the relationship between yield curve -being a line of the interests in various maturities at a given time- and gdp growth in turkey. the paper focuses on analyzing the yield curve in relation to its predictive power on turkish macroeconomic dynamics using the linear regression model. to do so, the interest rate spreads of different maturities are used as a proxy of the yield curve. findings of the ols regression are similar to that found in the literature and supports the positive relation between slope of yield curve and gdp growth in turkey. moreover, the predicted values of the gdp growth from interest rate spread closely follow the actual gdp growth in turkey, indicating its predictive power on the economic activity.", "categories": "econ.gn q-fin.ec", "created": "2019-12-23", "updated": "", "authors": ["ipek turker", "bayram cakir"], "url": "https://arxiv.org/abs/1912.12351"}, {"title": "bayesian estimation of large dimensional time varying vars using copulas", "id": "1912.12527", "abstract": "this paper provides a simple, yet reliable, alternative to the (bayesian) estimation of large multivariate vars with time variation in the conditional mean equations and/or in the covariance structure. with our new methodology, the original multivariate, n dimensional model is treated as a set of n univariate estimation problems, and cross-dependence is handled through the use of a copula. thus, only univariate distribution functions are needed when estimating the individual equations, which are often available in closed form, and easy to handle with mcmc (or other techniques). estimation is carried out in parallel for the individual equations. thereafter, the individual posteriors are combined with the copula, so obtaining a joint posterior which can be easily resampled. we illustrate our approach by applying it to a large time-varying parameter var with 25 macroeconomic variables.", "categories": "econ.em stat.ot", "created": "2019-12-28", "updated": "", "authors": ["mike tsionas", "marwan izzeldin", "lorenzo trapani"], "url": "https://arxiv.org/abs/1912.12527"}, {"title": "focused bayesian prediction", "id": "1912.12571", "abstract": "we propose a new method for conducting bayesian prediction that delivers accurate predictions without correctly specifying the unknown true data generating process. a prior is defined over a class of plausible predictive models. after observing data, we update the prior to a posterior over these models, via a criterion that captures a user-specified measure of predictive accuracy. under regularity, this update yields posterior concentration onto the element of the predictive class that maximizes the expectation of the accuracy measure. in a series of simulation experiments and empirical examples we find notable gains in predictive accuracy relative to conventional likelihood-based prediction.", "categories": "stat.me econ.gn q-fin.ec stat.ap", "created": "2019-12-28", "updated": "2020-08-21", "authors": ["ruben loaiza-maya", "gael m. martin", "david t. frazier"], "url": "https://arxiv.org/abs/1912.12571"}, {"title": "credit risk: simple closed form approximate maximum likelihood estimator", "id": "1912.12611", "abstract": "we consider discrete default intensity based and logit type reduced form models for conditional default probabilities for corporate loans where we develop simple closed form approximations to the maximum likelihood estimator (mle) when the underlying covariates follow a stationary gaussian process. in a practically reasonable asymptotic regime where the default probabilities are small, say 1-3% annually, the number of firms and the time period of data available is reasonably large, we rigorously show that the proposed estimator behaves similarly or slightly worse than the mle when the underlying model is correctly specified. for more realistic case of model misspecification, both estimators are seen to be equally good, or equally bad. further, beyond a point, both are more-or-less insensitive to increase in data. these conclusions are validated on empirical and simulated data. the proposed approximations should also have applications outside finance, where logit-type models are used and probabilities of interest are small.", "categories": "econ.em q-fin.pm", "created": "2019-12-29", "updated": "", "authors": ["anand deo", "sandeep juneja"], "url": "https://arxiv.org/abs/1912.12611"}, {"title": "on an extension of a theorem of eilenberg and a characterization of   topological connectedness", "id": "1912.12787", "abstract": "on taking a non-trivial and semi-transitive bi-relation constituted by two (hard and soft) binary relations, we report a (i) p-continuity assumption that guarantees the completeness and transitivity of its soft part, and a (ii) characterization of a connected topological space in terms of its attendant properties on the space. our work generalizes antecedent results in applied mathematics, all following eilenberg (1941), and now framed in the context of a parametrized-topological space. this re-framing is directly inspired by the continuity assumption in wold (1943-44) and the mixture-space structure proposed in herstein and milnor (1953), and the unifying synthesis of these pioneering but neglected papers that it affords may have independent interest.", "categories": "econ.th math.gn", "created": "2019-12-29", "updated": "", "authors": ["m. ali khan", "metin uyanik"], "url": "https://arxiv.org/abs/1912.12787"}, {"title": "priority to unemployed immigrants? a causal machine learning evaluation   of training in belgium", "id": "1912.12864", "abstract": "based on administrative data of unemployed in belgium, we estimate the labour market effects of three training programmes at various aggregation levels using modified causal forests, a causal machine learning estimator. while all programmes have positive effects after the lock-in period, we find substantial heterogeneity across programmes and unemployed. simulations show that 'black-box' rules that reassign unemployed to programmes that maximise estimated individual gains can considerably improve effectiveness: up to 20 percent more (less) time spent in (un)employment within a 30 months window. a shallow policy tree delivers a simple rule that realizes about 70 percent of this gain.", "categories": "econ.em econ.gn q-fin.ec stat.ml", "created": "2019-12-30", "updated": "2020-05-06", "authors": ["bart cockx", "michael lechner", "joost bollens"], "url": "https://arxiv.org/abs/1912.12864"}, {"title": "adaptive discrete smoothing for high-dimensional and nonlinear panel   data", "id": "1912.12867", "abstract": "in this paper we develop a data-driven smoothing technique for high-dimensional and non-linear panel data models. we allow for individual specific (non-linear) functions and estimation with econometric or machine learning methods by using weighted observations from other individuals. the weights are determined by a data-driven way and depend on the similarity between the corresponding functions and are measured based on initial estimates. the key feature of such a procedure is that it clusters individuals based on the distance / similarity between them, estimated in a first stage. our estimation method can be combined with various statistical estimation procedures, in particular modern machine learning methods which are in particular fruitful in the high-dimensional case and with complex, heterogeneous data. the approach can be interpreted as a \\textquotedblleft soft-clustering\\textquotedblright\\ in comparison to traditional\\textquotedblleft\\ hard clustering\\textquotedblright that assigns each individual to exactly one group. we conduct a simulation study which shows that the prediction can be greatly improved by using our estimator. finally, we analyze a big data set from didichuxing.com, a leading company in transportation industry, to analyze and predict the gap between supply and demand based on a large set of covariates. our estimator clearly performs much better in out-of-sample prediction compared to existing linear panel data estimators.", "categories": "stat.me cs.lg econ.em stat.ml", "created": "2019-12-30", "updated": "2020-01-03", "authors": ["xi chen", "ye luo", "martin spindler"], "url": "https://arxiv.org/abs/1912.12867"}, {"title": "robustness of equilibria in games with many players", "id": "1912.12908", "abstract": "this paper proposes a new equilibrium concept \"robust perfect equilibrium\" for non-cooperative games with many players. such an equilibrium is shown to exist and satisfy the important behavioral principles of admissibility, trembling-hand perfection, and ex post perfection. these robustness properties strengthen relevant equilibrium results in the large literature on strategic interactions of many agents. illustrative applications to congestion games and potential games are presented. in the particular case of a congestion game with strictly increasing cost functions, we show that there is a unique equilibrium traffic flow induced by robust perfect equilibria.", "categories": "econ.th cs.gt", "created": "2019-12-30", "updated": "2020-10-05", "authors": ["enxian chen", "lei qiao", "xiang sun", "yeneng sun"], "url": "https://arxiv.org/abs/1912.12908"}, {"title": "effect of franchised business models on fast food company stock prices   in recession and recovery with weibull analysis", "id": "1912.12940", "abstract": "at the initial stages of this research, the assumption was that the franchised businesses perhaps should not be affected much by recession as there are multiple cash pools available inherent to the franchised business model. however, after analyzing the available data, it indicated otherwise, the stock price performance as discussed indicates a different pattern. the stock price data is analyzed with an unconventional tool, weibull distribution and observations confirmed the presence of either a reverse trend in franchised business than what is observed for non-franchised or the franchised stock followed large food suppliers. there is a layered ownership and cash flow in a franchised business model. the parent company run by franchiser depends on the performance of child companies run by franchisees. both parent and child companies are run as independent businesses but only the parent company is listed as a stock ticker in stock exchange. does this double layer of vertical operation, cash reserve, and cash flow protect them better in recession? the data analyzed in this paper indicates that the recession effect can be more severe; and if it dives with the average market, expect a slower recovery of stock prices in a franchised business model. this paper characterizes the differences and explains the natural experiment with available financial data.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2019-12-30", "updated": "", "authors": ["sandip dutta", "vignesh prabhu"], "url": "https://arxiv.org/abs/1912.12940"}, {"title": "recovering latent variables by matching", "id": "1912.13081", "abstract": "we propose an optimal-transport-based matching method to nonparametrically estimate linear models with independent latent variables. the method consists in generating pseudo-observations from the latent variables, so that the euclidean distance between the model's predictions and their matched counterparts in the data is minimized. we show that our nonparametric estimator is consistent, and we document that it performs well in simulated data. we apply this method to study the cyclicality of permanent and transitory income shocks in the panel study of income dynamics. we find that the dispersion of income shocks is approximately acyclical, whereas the skewness of permanent shocks is procyclical. by comparison, we find that the dispersion and skewness of shocks to hourly wages vary little with the business cycle.", "categories": "econ.em", "created": "2019-12-30", "updated": "", "authors": ["manuel arellano", "stephane bonhomme"], "url": "https://arxiv.org/abs/1912.13081"}, {"title": "generalized rental harmony", "id": "1912.13249", "abstract": "rental harmony is the problem of assigning rooms in a rented house to tenants with different preferences, and simultaneously splitting the rent among them, such that no tenant envies the bundle (room+price) given to another tenant. different papers have studied this problem under two incompatible assumptions: the miserly tenants assumption is that each tenant prefers a free room to a non-free room; the quasilinear tenants assumption is that each tenant attributes a monetary value to each room, and prefers a room of which the difference between value and price is maximum. this note shows how to adapt the main technique used for rental harmony with miserly tenants, using sperner's lemma, to a much more general class of preferences, that contains both miserly and quasilinear tenants as special cases. this implies that some recent results derived for miserly tenants apply to this more general preference class too.", "categories": "cs.gt econ.th", "created": "2019-12-31", "updated": "2020-02-29", "authors": ["erel segal-halevi"], "url": "https://arxiv.org/abs/1912.13249"}, {"title": "regulatory markets for ai safety", "id": "2001.00078", "abstract": "we propose a new model for regulation to achieve ai safety: global regulatory markets. we first sketch the model in general terms and provide an overview of the costs and benefits of this approach. we then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on ai models employed in commercial drones.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2019-12-11", "updated": "", "authors": ["jack clark", "gillian k. hadfield"], "url": "https://arxiv.org/abs/2001.00078"}, {"title": "entropic decision making", "id": "2001.00122", "abstract": "using results from neurobiology on perceptual decision making and value-based decision making, the problem of decision making between lotteries is reformulated in an abstract space where uncertain prospects are mapped to corresponding active neuronal representations. this mapping allows us to maximize non-extensive entropy in the new space with some constraints instead of a utility function. to achieve good agreements with behavioral data, the constraints must include at least constraints on the weighted average of the stimulus and on its variance. both constraints are supported by the adaptability of neuronal responses to an external stimulus. by analogy with thermodynamic and information engines, we discuss the dynamics of choice between two lotteries as they are being processed simultaneously in the brain by rate equations that describe the transfer of attention between lotteries and within the various prospects of each lottery. this model is able to give new insights on risk aversion and on behavioral anomalies not accounted for by prospect theory.", "categories": "q-bio.nc econ.gn q-fin.ec", "created": "2019-12-31", "updated": "", "authors": ["adnan rebei"], "url": "https://arxiv.org/abs/2001.00122"}, {"title": "prediction in locally stationary time series", "id": "2001.00419", "abstract": "we develop an estimator for the high-dimensional covariance matrix of a locally stationary process with a smoothly varying trend and use this statistic to derive consistent predictors in non-stationary time series. in contrast to the currently available methods for this problem the predictor developed here does not rely on fitting an autoregressive model and does not require a vanishing trend. the finite sample properties of the new methodology are illustrated by means of a simulation study and a financial indices study.", "categories": "stat.me econ.em math.st stat.th", "created": "2020-01-02", "updated": "2020-01-03", "authors": ["holger dette", "weichi wu"], "url": "https://arxiv.org/abs/2001.00419"}, {"title": "growth and inequalities in a physicist's view", "id": "2001.00478", "abstract": "it is still common wisdom amongst economists, politicians and lay people that economic growth is a necessity of our social systems, at least to avoid distributional conflicts. this paper challenges such belief moving from a purely physical theoretical perspective. it formally considers the constraints imposed by a finite environment on the prospect of continuous growth, including the dynamics of costs. as costs grow faster than production it is easy to deduce a final unavoidable global collapse. then, analyzing and discussing the evolution of the unequal share of wealth under the premises of growth and competition, it is shown that the increase of inequalities is a necessary consequence of the premises.", "categories": "econ.gn q-fin.ec", "created": "2019-12-30", "updated": "2020-05-07", "authors": ["angelo tartaglia"], "url": "https://arxiv.org/abs/2001.00478"}, {"title": "logical differencing in dyadic network formation models with   nontransferable utilities", "id": "2001.00691", "abstract": "this paper considers a semiparametric model of dyadic network formation under nontransferable utilities (ntu). ntu arises frequently in real-world social interactions that require bilateral consent, but by its nature induces additive non-separability. we show how unobserved individual heterogeneity in our model can be canceled out without additive separability, using a novel method we call logical differencing. the key idea is to construct events involving the intersection of two mutually exclusive restrictions on the unobserved heterogeneity, based on multivariate monotonicity. we provide a consistent estimator and analyze its performance via simulation, and apply our method to the nyakatoke risk-sharing networks.", "categories": "econ.em", "created": "2020-01-02", "updated": "2020-07-17", "authors": ["wayne yuan gao", "ming li", "sheng xu"], "url": "https://arxiv.org/abs/2001.00691"}, {"title": "judicial favoritism of politicians: evidence from small claims court", "id": "2001.00889", "abstract": "multiple studies have documented racial, gender, political ideology, or ethnical biases in comparative judicial systems. supplementing this literature, we investigate whether judges rule cases differently when one of the litigants is a politician. we suggest a theory of power collusion, according to which judges might use rulings to buy cooperation or threaten members of the other branches of government. we test this theory using a sample of small claims cases in the state of s\\~ao paulo, brazil, where no collusion should exist. the results show a negative bias of 3.7 percentage points against litigant politicians, indicating that judges punish, rather than favor, politicians in court. this punishment in low-salience cases serves as a warning sign for politicians not to cross the judiciary when exercising checks and balances, suggesting yet another barrier to judicial independence in development settings.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2020-01-03", "updated": "2020-01-31", "authors": ["andre assumpcao", "julio trecenti"], "url": "https://arxiv.org/abs/2001.00889"}, {"title": "a socioeconomic well-being index", "id": "2001.01036", "abstract": "an annual well-being index constructed from thirteen socioeconomic factors is proposed in order to dynamically measure the mood of the us citizenry. econometric models are fitted to the log-returns of the index in order to quantify its tail risk and perform option pricing and risk budgeting. by providing a statistically sound assessment of socioeconomic content, the index is consistent with rational finance theory, enabling the construction and valuation of insurance-type financial instruments to serve as contracts written against it. endogenously, the vxo volatility measure of the stock market appears to be the greatest contributor to tail risk. exogenously, \"stress-testing\" the index against the politically important factors of trade imbalance and legal immigration, quantify the systemic risk. for probability levels in the range of 5% to 10%, values of trade below these thresholds are associated with larger downward movements of the index than for immigration at the same level. the main intent of the index is to provide early-warning for negative changes in the mood of citizens, thus alerting policy makers and private agents to potential future market downturns.", "categories": "econ.gn q-fin.ec", "created": "2020-01-03", "updated": "", "authors": ["a. alexandre trindade", "abootaleb shirvani", "xiaohan ma"], "url": "https://arxiv.org/abs/2001.01036"}, {"title": "bayesian median autoregression for robust time series forecasting", "id": "2001.01116", "abstract": "we develop a bayesian median autoregressive (bayesmar) model for time series forecasting. the proposed method utilizes time-varying quantile regression at the median, favorably inheriting the robustness of median regression in contrast to the widely used mean-based methods. motivated by a working laplace likelihood approach in bayesian quantile regression, bayesmar adopts a parametric model bearing the same structure of autoregressive (ar) models by altering the gaussian error to laplace, leading to a simple, robust, and interpretable modeling strategy for time series forecasting. we estimate model parameters by markov chain monte carlo. bayesian model averaging (bma) is used to account for model uncertainty including the uncertainty in the autoregressive order, in addition to a bayesian model selection approach. the proposed methods are illustrated using simulation and real data applications. an application to u.s. macroeconomic data forecasting shows that bayesmar leads to favorable and often superior predictive performances than the selected mean-based alternatives under various loss functions. the proposed methods are generic and can be used to complement a rich class of methods that builds on the ar models.", "categories": "stat.ap econ.em stat.me", "created": "2020-01-04", "updated": "", "authors": ["zijian zeng", "meng li"], "url": "https://arxiv.org/abs/2001.01116"}, {"title": "classifying ecosystem disservices and comparing their effects with   ecosystem services in beijing, china", "id": "2001.01605", "abstract": "to completely understand the effects of urban ecosystems, the effects of ecosystem disservices should be considered along with the ecosystem services and require more research attention. in this study, we tried to better understand its formation through the use of cascade flowchart and classification systems and compare their effects with ecosystem services. it is vitally important to differentiate final and intermediate ecosystem disservices for understanding the negative effects of the ecosystem on human well-being. the proposed functional classification of eds (i.e. provisioning, regulating and cultural eds) should also help better bridging eds and es studies. in addition, we used beijing as a case study area to value the eds caused by urban ecosystems and compare the findings with es values. the results suggested that although eds caused great financial loss the potential economic gain from ecosystem services still significantly outweigh the loss. our study only sheds light on valuating the net effects of urban ecosystems. in the future, we believe that eds valuation should be at least equally considered in ecosystem valuation studies to create more comprehensive and sustainable development policies, land use proposals and management plans.", "categories": "econ.gn q-fin.ec", "created": "2020-01-06", "updated": "", "authors": ["shuyao wu", "jiao huang", "shuangcheng li"], "url": "https://arxiv.org/abs/2001.01605"}, {"title": "housing investment, stock market participation and household portfolio   choice: evidence from china's urban areas", "id": "2001.01641", "abstract": "this paper employs the survey data of chfs (2013) to investigate the impact of housing investment on household stock market participation and portfolio choice. the results show that larger housing investment encourages the household participation in the stock market, but reduces the proportion of their stockholding. the above conclusion remains true even when the endogeneity problem is controlled with risk attitude classification, heckman model test and subsample regression. this study shows that the growth in the housing market will not lead to stock market development because of lack of household financial literacy and the low expected yield on stock market.", "categories": "econ.gn q-fin.ec", "created": "2020-01-06", "updated": "", "authors": ["huirong liu"], "url": "https://arxiv.org/abs/2001.01641"}, {"title": "passengers' travel behavior in response to unplanned transit disruptions", "id": "2001.01718", "abstract": "public transit disruption is becoming more common across different transit services, which can have a destructive influence on the resiliency and reliability of the transportation system. utilizing a recently collected data of transit users in the chicago metropolitan area, the current study aims to analyze how transit users respond to unplanned service disruption and disclose the factors that affect their behavior.", "categories": "econ.gn q-fin.ec", "created": "2020-01-06", "updated": "2020-07-23", "authors": ["nima golshani", "ehsan rahimi", "ramin shabanpour", "kouros mohammadian", "joshua auld", "hubert ley"], "url": "https://arxiv.org/abs/2001.01718"}, {"title": "efficient allocations in double auction markets", "id": "2001.02071", "abstract": "this paper proposes a simple descriptive model for discrete-time double auction markets of divisible assets. as in the classical models of exchange economics, we consider a finite set of agents described by their initial endowments and preferences. instead of the classical walrasian-type market models, however, we assume that all trades take place in double auctions where the agents communicate through sealed limit orders for buying and selling. we find that, in repeated call auctions, nonstrategic bidding leads to a sequence of allocations that converges to individually rational pareto allocations.", "categories": "econ.th", "created": "2020-01-05", "updated": "", "authors": ["teemu pennanen"], "url": "https://arxiv.org/abs/2001.02071"}, {"title": "optimal insurance contract with benefits in kind under adverse selection", "id": "2001.02099", "abstract": "an income loss can have a negative impact on households, forcing them to reduce their consumption of some staple goods. this can lead to health issues and, consequently, generate significant costs for society. we suggest that consumers can, to prevent these negative consequences, buy insurance to secure sufficient consumption of a staple good if they lose part of their income. we develop a two-period/two-good principal-agent problem with adverse selection and endogenous reservation utility to model insurance with in-kind benefits. this model allows us to obtain semi-explicit solutions for the insurance contract and is applied to the context of fuel poverty. for this application, our model allows to conclude that, even in the least efficient scenario from the households point of view, i.e., when the insurance is provided by a monopoly, this mechanism improves significantly the living conditions of the riskiest households by ensuring them a sufficient consumption of energy.", "categories": "econ.gn q-fin.ec", "created": "2020-01-06", "updated": "2020-04-28", "authors": ["cl\u00e9mence alasseur", "corinne chaton", "emma hubert"], "url": "https://arxiv.org/abs/2001.02099"}, {"title": "understanding the great recession using machine learning algorithms", "id": "2001.02115", "abstract": "nyman and ormerod (2017) show that the machine learning technique of random forests has the potential to give early warning of recessions. applying the approach to a small set of financial variables and replicating as far as possible a genuine ex ante forecasting situation, over the period since 1990 the accuracy of the four-step ahead predictions is distinctly superior to those actually made by the professional forecasters. here we extend the analysis by examining the contributions made to the great recession of the late 2000s by each of the explanatory variables. we disaggregate private sector debt into its household and non-financial corporate components. we find that both household and non-financial corporate debt were key determinants of the great recession. we find a considerable degree of non-linearity in the explanatory models. in contrast, the public sector debt to gdp ratio appears to have made very little contribution. it did rise sharply during the great recession, but this was as a consequence of the sharp fall in economic activity rather than it being a cause. we obtain similar results for both the united states and the united kingdom.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-01-02", "updated": "", "authors": ["rickard nyman", "paul ormerod"], "url": "https://arxiv.org/abs/2001.02115"}, {"title": "whos ditching the bus?", "id": "2001.02200", "abstract": "this paper uses stop-level passenger count data in four cities to understand the nation-wide bus ridership decline between 2012 and 2018. the local characteristics associated with ridership change are evaluated in portland, miami, minneapolis/st-paul, and atlanta. poisson models explain ridership as a cross-section and the change thereof as a panel. while controlling for the change in frequency, jobs, and population, the correlation with local socio-demographic characteristics are investigated using data from the american community survey. the effect of changing neighborhood demographics on bus ridership are modeled using longitudinal employer-household dynamics data. at a point in time, neighborhoods with high proportions of non-white, carless, and most significantly, high-school-educated residents are the most likely to have high ridership. over time, white neighborhoods are losing the most ridership across all four cities. in miami and atlanta, places with high concentrations of residents with college education and without access to a car also lose ridership at a faster rate. in minneapolis/st-paul, the proportion of college-educated residents is linked to ridership gain. the sign and significance of these results remain consistent even when controlling for intra-urban migration. although bus ridership is declining across neighborhood characteristics, these results suggest that the underlying cause of bus ridership decline must be primarily affecting the travel behavior of white bus riders.", "categories": "physics.soc-ph cs.si econ.gn q-fin.ec", "created": "2020-01-07", "updated": "2020-03-11", "authors": ["simon j. berrebi", "kari e. watkins"], "url": "https://arxiv.org/abs/2001.02200"}, {"title": "external threats, political turnover and fiscal capacity", "id": "2001.02322", "abstract": "in most of the recent literature on state capacity, the significance of wars in state-building assumes that threats from foreign countries generate common interests among domestic groups, leading to larger investments in state capacity. however, many countries that have suffered external conflicts don't experience increased unity. instead, they face factional politics that often lead to destructive civil wars. this paper develops a theory of the impact of interstate conflicts on fiscal capacity in which fighting an external threat is not always a common-interest public good, and in which interstate conflicts can lead to civil wars. the theory identifies conditions under which an increased risk of external conflict decreases the chance of civil war, which in turn results in a government with a longer political life and with more incentives to invest in fiscal capacity. these conditions depend on the cohesiveness of institutions, but in a non-trivial and novel way: a higher risk of an external conflict that results in lower political turnover, but that also makes a foreign invasion more likely, contributes to state-building only if institutions are sufficiently incohesive.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["hector galindo-silva"], "url": "https://arxiv.org/abs/2001.02322"}, {"title": "bilateral tariffs under international competition", "id": "2001.02426", "abstract": "this paper explores the gain maximization problem of two nations engaging in non-cooperative bilateral trade. probabilistic model of an exchange of commodities under different price systems is considered. volume of commodities exchanged determines the demand each nation has over the counter party's currency. however, each nation can manipulate this quantity by imposing a tariff on imported commodities. as long as the gain from trade is determined by the balance between imported and exported commodities, such a scenario results in a two party game where nash equilibrium tariffs are determined for various foreign currency demand functions and ultimately, the exchange rate based on optimal tariffs is obtained.", "categories": "econ.th", "created": "2020-01-08", "updated": "", "authors": ["tsotne kutalia", "revaz tevzadze"], "url": "https://arxiv.org/abs/2001.02426"}, {"title": "pursuing more sustainable energy consumption by analyzing sectoral   direct and indirect energy use in malaysia: an input-output analysis", "id": "2001.02508", "abstract": "malaysia is experiencing ever increasing domestic energy consumption. this study is an attempt at analyzing the changes in sectoral energy intensities in malaysia for the period 1995 to 2011. the study quantifies the sectoral total, direct, and indirect energy intensities to track the sectors that are responsible for the increasing energy consumption. the energy input-output model which is a frontier method for examining resource embodiments in goods and services on a sectoral scale that is popular among scholars has been applied in this study.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.02508"}, {"title": "ways to reduce cost of living: a case study among low income household   in kubang pasu, kedah, malaysia", "id": "2001.02509", "abstract": "this study was conducted to examine and understand the spending behavior of low income households (b40), namely households with income of rm3800 and below. the study focused on the area kubang pasu district, kedah.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.02509"}, {"title": "infrastructure disparities in northern malaysia", "id": "2001.02510", "abstract": "this study examines the disparities of infrastructure in four states in northern peninsular malaysia. this study used a primer data which is collected by using a face to face interview with a structure questionnaire on head of household at kedah, perlis, penang and perak. the list of respondents is provided by the department of statistics of malaysia (dos). the department of statistics of malaysia (dos) uses the population observation in 2010 to determine the respondents is provided by the department of statistics of malaysia (dos).", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.02510"}, {"title": "if the prospect of some occupations are stagnating with technological   advancement? a task attribute approach to detect employment vulnerability", "id": "2001.02783", "abstract": "two distinct trends can prove the existence of technological unemployment in the us. first, there are more open jobs than the number of unemployed persons looking for a job, and second, the shift of the beveridge curve. there have been many attempts to find the cause of technological unemployment. however, all of these approaches fail when it comes to evaluating the impact of modern technologies on employment future. this study hypothesizes that rather than looking into skill requirement or routine non-routine discrimination of tasks, a holistic approach is required to predict which occupations are going to be vulnerable with the advent of this 4th industrial revolution, i.e., widespread application of ai, ml algorithms, and robotics. three critical attributes are considered: bottleneck, hazardous, and routine. forty-five relevant attributes are chosen from the o*net database that can define these three types of tasks. performing principal axis factor analysis, and k-medoid clustering, the study discovers a list of 367 vulnerable occupations. the study further analyzes the last nine years of national employment data and finds that over the previous four years, the growth of vulnerable occupations is only half than that of non-vulnerable ones despite the long rally of economic expansion.", "categories": "econ.gn q-fin.ec", "created": "2020-01-08", "updated": "", "authors": ["iftekhairul islam", "fahad shaon"], "url": "https://arxiv.org/abs/2001.02783"}, {"title": "institutions and china's comparative development", "id": "2001.02804", "abstract": "robust assessment of the institutionalist account of comparative development is hampered by problems of omitted variable bias and reverse causation, since institutional quality is not randomly assigned with respect to geographic and human capital endowments. a recent series of papers has applied spatial regression discontinuity designs to estimate the impact of institutions on incomes at international borders, drawing inference from the abrupt discontinuity in governance at borders, whereas other determinants of income vary smoothly across borders. i extend this literature by assessing the importance of sub-national variation in institutional quality at provincial borders in china. employing nighttime lights emissions as a proxy for income, across multiple specifications i find no evidence in favour of an institutionalist account of the comparative development of chinese provinces.", "categories": "econ.gn q-fin.ec", "created": "2020-01-08", "updated": "", "authors": ["paul minard"], "url": "https://arxiv.org/abs/2001.02804"}, {"title": "china's first workforce skill taxonomy", "id": "2001.02863", "abstract": "china is the world's second largest economy. after four decades of economic miracles, china's economy is transitioning into an advanced, knowledge-based economy. yet, we still lack a detailed understanding of the skills that underly the chinese labor force, and the development and spatial distribution of these skills. for example, the us standardized skill taxonomy o*net played an important role in understanding the dynamics of manufacturing and knowledge-based work, as well as potential risks from automation and outsourcing. here, we use machine learning techniques to bridge this gap, creating china's first workforce skill taxonomy, and map it to o*net. this enables us to reveal workforce skill polarization into social-cognitive skills and sensory-physical skills, and to explore the china's regional inequality in light of workforce skills, and compare it to traditional metrics such as education. we build an online tool for the public and policy makers to explore the skill taxonomy: skills.sysu.edu.cn. we will also make the taxonomy dataset publicly available for other researchers upon publication.", "categories": "econ.gn q-fin.ec", "created": "2020-01-09", "updated": "", "authors": ["weipan xu", "xiaozhen qin", "xun li", "haohui\"caron\" chen", "morgan frank", "alex rutherford", "andrew reeson", "iyad rahwan"], "url": "https://arxiv.org/abs/2001.02863"}, {"title": "segregation with social linkages: evaluating schelling's model with   networked individuals", "id": "2001.02959", "abstract": "this paper generalizes the original schelling (1969, 1971a,b, 2006) model of racial and residential segregation to a context of variable externalities due to social linkages. in a setting in which individuals' utility function is a convex combination of a heuristic function \u007fa la schelling, of the distance to friends, and of the cost of moving, the prediction of the original model gets attenuated: the segregation equilibria are not the unique solutions. while the cost of distance has a monotonic pro-status-quo effect, equivalent to that of models of migration and gravity models, if friends and neighbours are formed following independent processes the location of friends in space generates an externality that reinforces the initial configuration if the distance to friends is minimal, and if the degree of each agent is high. the effect on segregation equilibria crucially depends on the role played by network externalities.", "categories": "econ.gn q-fin.ec", "created": "2020-01-09", "updated": "", "authors": ["roy cerqueti", "luca de benedictis", "valerio leone sciabolazza"], "url": "https://arxiv.org/abs/2001.02959"}, {"title": "determinants of social-economic mobility in the northern region of   malaysia", "id": "2001.03043", "abstract": "colleting the data through a survey in the northern region of malaysia; kedah, perlis, penang and perak, this study investigates intergenerational social mobility in malaysia. we measure and analyzed the factors that influence social-economic mobility by using binary choice model (logit model). social mobility can be measured in several ways, by income, education, occupation or social class. more often, economic research has focused on some measure of income. social mobility variable is measured using the difference between educational achievement between a father and son. if there is a change of at least of two educational levels between a father and son, then this study will assign the value one which means that social mobility has occurred.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.03043"}, {"title": "estimating the impact of gst implementation on cost of production and   cost of living in malaysia", "id": "2001.03045", "abstract": "the implementation of goods and services tax(gst) is often attributed as the main cause of the rising prices of goods and services. the main objective of this study is to estimate the extent of gst implementation impact on the costs of production, which in turn have implication on households living costs.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.03045"}, {"title": "relationship between type of risks and income of the rural households in   the pattani province of thailand", "id": "2001.03046", "abstract": "this study examines the relationship between type of risks and income of the rural households in pattani province,thailand using the standard multiple regression analysis.a multi-stage sampling technique is employed to select 600 households of 12 districts in the rural pattani province and a structured questionnaire is used for data collection.evidences from descriptive analysis show that the type of risks faced by households in rural pattani province are job loss,reduction of salary,household member died,household members who work have accident,marital problem and infection of crops or livestock.in addition,result from the regression analysis suggests that job loss,household member died and marital problem have significant negative effects on the households income.the result suggests that job loss has adverse impact on households income.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun"], "url": "https://arxiv.org/abs/2001.03046"}, {"title": "targeting in social networks with anonymized information", "id": "2001.03122", "abstract": "this paper studies whether a planner who only has information about the network topology can discriminate among agents according to their network position. the planner proposes a simple menu of contracts, one for each location, in order to maximize total welfare, and agents choose among the menu. this mechanism is immune to deviations by single agents, and to deviations by groups of agents of sizes 2, 3 and 4 if side-payments are ruled out. however, if compensations are allowed, groups of agents may have an incentive to jointly deviate from the optimal contract in order to exploit other agents. we identify network topologies for which the optimal contract is group incentive compatible with transfers: undirected networks and regular oriented trees, and network topologies for which the planner must assign uniform quantities: single root and nested neighborhoods directed networks.", "categories": "econ.th", "created": "2020-01-09", "updated": "", "authors": ["francis bloch", "shaden shabayek"], "url": "https://arxiv.org/abs/2001.03122"}, {"title": "a game of hide and seek in networks", "id": "2001.03132", "abstract": "we propose and study a strategic model of hiding in a network, where the network designer chooses the links and his position in the network facing the seeker who inspects and disrupts the network. we characterize optimal networks for the hider, as well as equilibrium hiding and seeking strategies on these networks. we show that optimal networks are either equivalent to cycles or variants of a core-periphery networks where every node in the periphery is connected to a single node in the core.", "categories": "econ.th cs.gt", "created": "2020-01-09", "updated": "", "authors": ["francis bloch", "bhaskar dutta", "marcin dziubinski"], "url": "https://arxiv.org/abs/2001.03132"}, {"title": "dynamic interaction between shared autonomous vehicles and public   transit: a competitive perspective", "id": "2001.03197", "abstract": "the emerging autonomous vehicles (av) can either supplement the public transportation (pt) system or be a competitor with it. this paper focuses on this competition in a hypothetical scenario--\"if both av and pt operators are profit-oriented,\" and uses an abm to quantitatively evaluate the system performance in this competition from the perspectives of four stakeholders--av operator, pt operator, passengers, and public authority. in our model, av operator updates its supply by changing fleet sizes while pt by adjusting headways, and both use heuristic approaches to update supply in order to increase profits. we implement the model in the first-mile scenario in tampines. in four regulation scenarios--two by two combinations regarding whether av and pt are allowed to change supplies--we find that since av can release the bus operator from low-demand routes, the competition can lead to higher profits of both, and higher system efficiency, simultaneously, rather than a one-sided loss-gain result. for pt, after supply updates, spatially the services are concentrated to short feeder routes directly to the subway station, and temporally concentrated to peak hours. for passengers, the competition reduces their travel time but increases their travel costs. nonetheless, the generalized travel cost is still reduced when counting the value of time. for system efficiency and sustainability, bus supply adjustment can increase the bus average load and reduce total pce, while the av supply adjustment shows the opposite effect. for policy implications, the paper suggests that pt should be allowed to optimize its supply strategies under specific operation goal constraints, and av operation should be regulated to relieve its externality on the system, including limiting the number of licenses, operation time, and service areas, which makes av operate like a complementary mode to pt.", "categories": "physics.soc-ph cs.gt econ.gn q-fin.ec", "created": "2020-01-09", "updated": "2020-02-03", "authors": ["baichuan mo", "zhejing cao", "hongmou zhang", "yu shen", "jinhua zhao"], "url": "https://arxiv.org/abs/2001.03197"}, {"title": "behavioral and game-theoretic security investments in interdependent   systems modeled by attack graphs", "id": "2001.03213", "abstract": "we consider a system consisting of multiple interdependent assets, and a set of defenders, each responsible for securing a subset of the assets against an attacker. the interdependencies between the assets are captured by an attack graph, where an edge from one asset to another indicates that if the former asset is compromised, an attack can be launched on the latter asset. each edge has an associated probability of successful attack, which can be reduced via security investments by the defenders. in such scenarios, we investigate the security investments that arise under certain features of human decision-making that have been identified in behavioral economics. in particular, humans have been shown to perceive probabilities in a nonlinear manner, typically overweighting low probabilities and underweighting high probabilities. we show that suboptimal investments can arise under such weighting in certain network topologies. we also show that pure strategy nash equilibria exist in settings with multiple (behavioral) defenders, and study the inefficiency of the equilibrium investments by behavioral defenders compared to a centralized socially optimal solution.", "categories": "eess.sy cs.gt cs.sy econ.gn q-fin.ec", "created": "2020-01-09", "updated": "2020-05-09", "authors": ["mustafa abdallah", "parinaz naghizadeh", "ashish r. hota", "timothy cason", "saurabh bagchi", "shreyas sundaram"], "url": "https://arxiv.org/abs/2001.03213"}, {"title": "the logic of strategic assets: from oil to artificial intelligence", "id": "2001.03246", "abstract": "what resources and technologies are strategic? this question is often the focus of policy and theoretical debates, where the label \"strategic\" designates those assets that warrant the attention of the highest levels of the state. but these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of \"strategic\" to advance particular agendas. we aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. we distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. we then employ these logics to clarify three important cases: the avon 2 engine in the 1950s, the u.s.-japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-01-09", "updated": "", "authors": ["jeffrey ding", "allan dafoe"], "url": "https://arxiv.org/abs/2001.03246"}, {"title": "how to cut a cake fairly: a generalization to groups", "id": "2001.03327", "abstract": "a fundamental result in cake cutting states that for any number of players with arbitrary preferences over a cake, there exists a division of the cake such that every player receives a single contiguous piece and no player is left envious. we generalize this result by showing that it is possible to partition the players into groups of any desired sizes and divide the cake among the groups, so that each group receives a single contiguous piece and no player finds the piece of another group better than that of the player's own group.", "categories": "econ.th cs.gt math.co", "created": "2020-01-10", "updated": "2020-04-02", "authors": ["erel segal-halevi", "warut suksompong"], "url": "https://arxiv.org/abs/2001.03327"}, {"title": "text as data: real-time measurement of economic welfare", "id": "2001.03401", "abstract": "economists are showing increasing interest in the use of text as an input to economic research. here, we analyse online text to construct a real time metric of welfare. for purposes of description, we call it the feel good factor (fgf). the particular example used to illustrate the concept is confined to data from the london area, but the methodology is readily generalisable to other geographical areas. the fgf illustrates the use of online data to create a measure of welfare which is not based, as gdp is, on value added in a market-oriented economy. there is already a large literature which measures wellbeing/happiness. but this relies on conventional survey approaches, and hence on the stated preferences of respondents. in unstructured online media text, users reveal their emotions in ways analogous to the principle of revealed preference in consumer demand theory. the analysis of online media offers further advantages over conventional survey-based measures of sentiment or well-being. it can be carried out in real time rather than with the lags which are involved in survey approaches. in addition, it is very much cheaper.", "categories": "econ.gn q-fin.ec", "created": "2020-01-10", "updated": "", "authors": ["rickard nyman", "paul ormerod"], "url": "https://arxiv.org/abs/2001.03401"}, {"title": "associating ridesourcing with road safety outcomes: insights from austin   texas", "id": "2001.03461", "abstract": "improving road safety and setting targets for reducing traffic-related crashes and deaths are highlighted as part of the united nation's sustainable development goals and vision zero efforts around the globe. the advent of transportation network companies, such as ridesourcing, expands mobility options in cities and may impact road safety outcomes. in this study, we analyze the effects of ridesourcing use on road crashes, injuries, fatalities, and driving while intoxicated (dwi) offenses in travis county texas. our approach leverages real-time ridesourcing volume to explain variation in road safety outcomes. spatial panel data models with fixed effects are deployed to examine whether the use of ridesourcing is significantly associated with road crashes and other safety metrics. our results suggest that for a 10% increase in ridesourcing trips, we expect a 0.12% decrease in road crashes (p<0.05), a 0.25% decrease in road injuries (p<0.001), and a 0.36% decrease in dwi offenses (p<0.0001) in travis county. ridesourcing use is not associated with road fatalities at a 0.05 significance level. this study augments existing work because it moves beyond binary indicators of ridesourcing presence or absence and analyzes patterns within an urbanized area rather than metropolitan-level variation. contributions include developing a data-rich approach for assessing the impacts of ridesourcing use on our transportation system's safety, which may serve as a template for future analyses of other us cities. our findings provide feedback to policymakers by clarifying associations between ridesourcing use and traffic safety, while helping identify sets of actions to achieve safer and more efficient shared mobility systems.", "categories": "econ.gn q-fin.ec", "created": "2020-01-08", "updated": "2020-06-16", "authors": ["eleftheria kontou", "noreen c. mcdonald"], "url": "https://arxiv.org/abs/2001.03461"}, {"title": "macroeconomic instability and fiscal decentralization: an empirical   analysis", "id": "2001.03486", "abstract": "the main objective of this paper is to fill a critical gap in the literature by analyzing the effects of decentralization on the macroeconomic stability. a survey of the voluminous literature on decentralization suggests that the question of the links between decentralization and macroeconomic stability has been relatively scantily analyzed. even though there is still a lot of room for analysis as far as the effects of decentralization on other aspects of the economy are concerned, we believe that it is in this area that a more thorough analyses are mostly called for. through this paper, we will try to shed more light on the issue notably by looking at other dimension of macroeconomic stability than the ones usually employed in previous studies as well as by examining other factors that might accentuate or diminish the effects of decentralization on macroeconomic stability. our results found that decentralization appears to lead to a decrease in inflation rate. however, we do not find any correlation between decentralization with the level of fiscal deficit. our results also show that the impact of decentralization on inflation is conditional on the level of perceived corruption and political institutions.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["ahmad zafarullah abdul jalil", "mukaramah harun", "siti hadijah che mat"], "url": "https://arxiv.org/abs/2001.03486"}, {"title": "does non-farm income improve the poverty and income inequality among   agricultural household in rural kedah?", "id": "2001.03487", "abstract": "this paper used a primary data collected through a surveys among farmers in rural kedah to examine the effect of non farm income on poverty and income inequality. this paper employed two method, for the first objective which is to examine the impact of non farm income to poverty, we used poverty decomposition techniques - foster, greer and thorbecke (fgt) as has been done by adams (2004). for the second objective, which is to examine the impact of non farm income to income inequality, we used gini decomposition techniques.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["siti hadijah che mata", "ahmad zafarullah abdul jalil", "mukaramah harun"], "url": "https://arxiv.org/abs/2001.03487"}, {"title": "constructing a social accounting matrix framework to analyse the impact   of public expenditure on income distribution in malaysia", "id": "2001.03488", "abstract": "the use of the social accounting matrix (sam) in income distribution analysis is a method recommended by economists. however, until now, there have only been a few sam developed in malaysia. the last sam produced for malaysia was developed in 1984 based upon data from 1970 and has not been updated since this time despite the significance changes in the structure of the malaysian economy. the paper proposes a new malaysian sam framework to analyse public expenditure impact on income distribution in malaysia. the sam developed in the present paper is based on more recent data, providing an up-to date and coherent picture of the complexity of the malaysian economy. the paper describes the structure of the sam framework with a detailed aggregation and disaggregation of accounts related to public expenditure and income distribution issues. in the sam utilized in the present study, the detailed framework of the different components of public expenditure in the production sectors and household groups is essential in the analysis of the different effects of the various public expenditure programmes on the incomes of households among different groups.", "categories": "econ.gn q-fin.ec", "created": "2020-01-07", "updated": "", "authors": ["mukaramah harun", "a. r. zakariah", "m. azali"], "url": "https://arxiv.org/abs/2001.03488"}, {"title": "commuting service platform: concept and analysis", "id": "2001.03646", "abstract": "we propose and investigate the concept of commuting service platforms (csp) that leverage emerging mobility services to provide commuting services and connect directly commuters (employees) and their worksites (employers). by applying the two-sided market analysis framework, we show under what conditions a csp may present the two-sidedness. both the monopoly and duopoly csps are then analyzed. we showhowthe price allocation, i.e., the prices charged to commuters and worksites, can impact the participation and profit of the csps. we also add demand constraints to the duopoly model so that the participation rates ofworksites and employees are (almost) the same. with demand constraints, the competition between the two csps becomes less intense in general. discussions are presented on how the results and findings in this paper may help build csp in practice and how to develop new, csp-based travel demand management strategies.", "categories": "econ.gn q-fin.ec", "created": "2020-01-10", "updated": "", "authors": ["rong fan", "n/a xuegang", "n/a ban"], "url": "https://arxiv.org/abs/2001.03646"}, {"title": "estimation of large network formation games", "id": "2001.03838", "abstract": "this paper develops estimation methods for network formation models using observed data from a single large network. the model allows for utility externalities from friends of friends and friends in common, so the expected utility is nonlinear in the link choices of an agent. we propose a novel method that uses the legendre transform to express the expected utility as a linear function of the individual link choices. this implies that the optimal link decision is that for an agent who myopically chooses to establish links or not to the other members of the network. the dependence between the agent's link choices is through an auxiliary variable. we propose a two-step estimation procedure that requires weak assumptions on equilibrium selection, is simple to compute, and has consistent and asymptotically normal estimators for the parameters. monte carlo results show that the estimation procedure performs well.", "categories": "econ.em", "created": "2020-01-11", "updated": "", "authors": ["geert ridder", "shuyang sheng"], "url": "https://arxiv.org/abs/2001.03838"}, {"title": "supply network formation and fragility", "id": "2001.03853", "abstract": "we model the production of complex goods in a large supply network. each firm sources several essential inputs through relationships with other firms. due to the risk of such supply relationships being idiosyncratically disrupted, firms multisource inputs and strategically invest to make relationships with suppliers stronger. aggregate production is robust to idiosyncratic disruptions. however, there is a regime in which equilibrium supply networks are fragile, with small aggregate shocks to relationships causing arbitrarily steep drops in output. the endogenous configuration of supply networks provides a new channel for the powerful amplification of shocks.", "categories": "econ.th cs.gt physics.soc-ph", "created": "2020-01-12", "updated": "2020-09-15", "authors": ["matthew elliott", "benjamin golub", "matthew v. leduc"], "url": "https://arxiv.org/abs/2001.03853"}, {"title": "a multi-country dynamic factor model with stochastic volatility for euro   area business cycle analysis", "id": "2001.03935", "abstract": "this paper develops a dynamic factor model that uses euro area (ea) country-specific information on output and inflation to estimate an area-wide measure of the output gap. our model assumes that output and inflation can be decomposed into country-specific stochastic trends and a common cyclical component. comovement in the trends is introduced by imposing a factor structure on the shocks to the latent states. we moreover introduce flexible stochastic volatility specifications to control for heteroscedasticity in the measurement errors and innovations to the latent states. carefully specified shrinkage priors allow for pushing the model towards a homoscedastic specification, if supported by the data. our measure of the output gap closely tracks other commonly adopted measures, with small differences in magnitudes and timing. to assess whether the model-based output gap helps in forecasting inflation, we perform an out-of-sample forecasting exercise. the findings indicate that our approach yields superior inflation forecasts, both in terms of point and density predictions.", "categories": "econ.em", "created": "2020-01-12", "updated": "", "authors": ["florian huber", "michael pfarrhofer", "philipp piribauer"], "url": "https://arxiv.org/abs/2001.03935"}, {"title": "food supply chain and business model innovation", "id": "2001.03982", "abstract": "this paper investigates the contribution of business model innovations in improvement of food supply chains. through a systematic literature review, the notable business model innovations in the food industry are identified, surveyed, and evaluated. findings reveal that the innovations in value proposition, value creation processes, and value delivery processes of business models are the successful strategies proposed in food industry. it is further disclosed that rural female entrepreneurs, social movements, and also urban conditions are the most important driving forces inducing the farmers to reconsider their business models. in addition, the new technologies and environmental factors are the secondary contributors in business model innovation for the food processors. it is concluded that digitalization has disruptively changed the food distributors models. e-commerce models and internet of things are reported as the essential factors imposing the retailers to innovate their business models. furthermore, the consumption demand and the product quality are two main factors affecting the business models of all the firms operating in the food supply chain regardless of their positions in the chain. the findings of the current study provide an insight into the food industry to design a sustainable business model to bridge the gap between food supply and food demand.", "categories": "econ.gn q-fin.ec", "created": "2020-01-12", "updated": "", "authors": ["saeed nosratabadi", "amirhosein mosavi", "zoltan lakner"], "url": "https://arxiv.org/abs/2001.03982"}, {"title": "reconstruction of interbank network using ridge entropy maximization   model", "id": "2001.04097", "abstract": "we develop a network reconstruction model based on the entropy maximization considering the sparsity of network. here the reconstruction is to estimate network's adjacency matrix from node's local information. we reconstruct the interbank network in japan from financial data in balance sheets of individual banks using the developed reconstruction model in the period from 2000 to 2016. the sparsity of the interbank network is successfully reproduced in the reconstructed network. we examine the accuracy of the reconstructed interbank network by comparing the actual data and analyze the characteristics of the interbank network. the comparison confirms that the accuracy of the reconstruction model is acceptably good. for the reconstructed interbank network, we obtain the following characteristics which are consistent with the previously known stylized facts: the short path length, the small clustering coefficient, the disassortative property, and the core and peripheral structure. community analysis shows that the number of communities is 2-3 in the normal period, 1 in the economic crisis (2003, 2008-2013). the major nodes in each community have been the major commercial banks. since 2013, the major commercial banks have lost the average pagerank and the leading regional banks have obtained both the average degree and the average pagerank. the observed changing role of banks is considered as a result of the quantitative and qualitative easing monetary policy started by bank of japan in april of 2013.", "categories": "econ.gn q-fin.ec", "created": "2020-01-13", "updated": "", "authors": ["yuichi ikeda", "hidetoshi takeda"], "url": "https://arxiv.org/abs/2001.04097"}, {"title": "electricity prices and tariffs to keep everyone happy: a framework for   compatible fixed and nodal structures to increase efficiency", "id": "2001.04283", "abstract": "some consumers, such as householders, are unwilling to face volatile electricity prices, and perceive as unfair price differentiations based on location. for these reasons, nodal prices in distribution networks are rarely employed. however, the increasing availability of renewable resources in distribution grids, and emerging price-elastic behaviour, pave the way for the effective introduction of marginal nodal pricing schemes in distribution networks. the aim of the proposed framework is to show how traditional non-flexible consumers can coexist with flexible users in a local distribution area, where the latter pay nodal prices whereas the former are charged a fixed price, which is derived by the underlying nodal prices. in addition, it determines how the distribution system operator should manage the local grid by optimally determining the lines to be expanded, and the collected network tariff levied on network users, while accounting for both congestion rent and investment costs. the proposed framework is formulated as a non-linear integer bilevel model, which is then recast as an equivalent single optimization problem, by using integer algebra and complementarity relations. the power flows in the distribution area are modelled by resorting to a second-order cone relaxation, whose solution is exact for radial networks under mild assumptions. the final model results in a mixed-integer quadratically constrained program, which can be solved with off-the-shelf solvers. numerical test cases based on a 5-bus and a 33-bus networks are reported to show the effectiveness of the proposed method.", "categories": "econ.gn cs.sy eess.sy q-fin.ec", "created": "2020-01-08", "updated": "", "authors": ["iacopo savelli", "thomas morstyn"], "url": "https://arxiv.org/abs/2001.04283"}, {"title": "a higher-order correct fast moving-average bootstrap for dependent data", "id": "2001.04867", "abstract": "we develop and implement a novel fast bootstrap for dependent data. our scheme is based on the i.i.d. resampling of the smoothed moment indicators. we characterize the class of parametric and semi-parametric estimation problems for which the method is valid. we show the asymptotic refinements of the proposed procedure, proving that it is higher-order correct under mild assumptions on the time series, the estimating functions, and the smoothing kernel. we illustrate the applicability and the advantages of our procedure for generalized empirical likelihood estimation. as a by-product, our fast bootstrap provides higher-order correct asymptotic confidence distributions. monte carlo simulations on an autoregressive conditional duration model provide numerical evidence that the novel bootstrap yields higher-order accurate confidence intervals. a real-data application on dynamics of trading volume of stocks illustrates the advantage of our method over the routinely-applied first-order asymptotic theory, when the underlying distribution of the test statistic is skewed or fat-tailed.", "categories": "stat.me econ.em q-fin.st", "created": "2020-01-14", "updated": "", "authors": ["davide la vecchia", "alban moor", "olivier scaillet"], "url": "https://arxiv.org/abs/2001.04867"}, {"title": "sparse covariance estimation in logit mixture models", "id": "2001.05034", "abstract": "this paper introduces a new data-driven methodology for estimating sparse covariance matrices of the random coefficients in logit mixture models. researchers typically specify covariance matrices in logit mixture models under one of two extreme assumptions: either an unrestricted full covariance matrix (allowing correlations between all random coefficients), or a restricted diagonal matrix (allowing no correlations at all). our objective is to find optimal subsets of correlated coefficients for which we estimate covariances. we propose a new estimator, called misc, that uses a mixed-integer optimization (mio) program to find an optimal block diagonal structure specification for the covariance matrix, corresponding to subsets of correlated coefficients, for any desired sparsity level using markov chain monte carlo (mcmc) posterior draws from the unrestricted full covariance matrix. the optimal sparsity level of the covariance matrix is determined using out-of-sample validation. we demonstrate the ability of misc to correctly recover the true covariance structure from synthetic data. in an empirical illustration using a stated preference survey on modes of transportation, we use misc to obtain a sparse covariance matrix indicating how preferences for attributes are related to one another.", "categories": "stat.me econ.em stat.ml", "created": "2020-01-14", "updated": "", "authors": ["youssef m aboutaleb", "mazen danaf", "yifei xie", "moshe ben-akiva"], "url": "https://arxiv.org/abs/2001.05034"}, {"title": "how social interactions matter when distance dies?", "id": "2001.05095", "abstract": "we consider an economic geography model with two inter-regional proximity structures, one due to trade linkages and the other due to social interactions. we investigate how the network structure of social interactions, or the social proximity structure, affects the timing of endogenous agglomeration and the spatial distribution of workers across regions. endogenous agglomeration emerges when inter-regional trade and/or social interactions incur high transportation costs, and the uniform dispersion occurs when these costs become negligibly small (i.e., when distance dies). in many-region geography, the network structure of social proximity emerges as the determinant of the geographical distribution of workers when trade becomes freer. if social proximity is governed by geographical distance (as in ground transportation), a mono-centric concentration emerges. if geographically distant pairs of regions are ``socially close'' (due to, e.g., passenger transportation modes with strong distance economy such as regional airlines), then geographically multi-centric spatial distribution can be sustainable.", "categories": "econ.gn math.ds nlin.ps q-fin.ec", "created": "2020-01-14", "updated": "", "authors": ["minoru osawa", "jos\u00e9 m. gaspar"], "url": "https://arxiv.org/abs/2001.05095"}, {"title": "examining the correlation of the level of wage inequality with labor   market institutions", "id": "2001.06003", "abstract": "technological change is responsible for major changes in the labor market. one of the offspring of technological change is the sbtc, which is for many economists the leading cause of the increasing wage inequality. however, despite that the technological change affected similarly the majority of the developed countries, nevertheless, the level of the increase of wage inequality wasn't similar. following the predictions of the sbtc theory, the different levels of inequality could be due to varying degrees of skill inequality between economies, possibly caused by variations in the number of skilled workers available. however, recent research shows that the difference mentioned above can explain a small percentage of the difference between countries. therefore, most of the resulting inequality could be due to the different ways in which the higher level of skills is valued in each labor market. the position advocated in this article is that technological change is largely given for all countries without much scope to reverse. therefore, in order to illustrate the changes in the structure of wage distribution that cause wage inequality, we need to understand how technology affects labor market institutions.in this sense, the pay inequality caused by technological progress is not a phenomenon we passively accept. on the contrary, recognizing that the structure and the way labor market institutions function is largely influenced by the way institutions respond to technological change, we can understand and maybe reverse this underlying wage inequality.", "categories": "econ.gn q-fin.ec", "created": "2020-01-16", "updated": "2020-01-24", "authors": ["virginia tsoukatou"], "url": "https://arxiv.org/abs/2001.06003"}, {"title": "recovering network structure from aggregated relational data using   penalized regression", "id": "2001.06052", "abstract": "social network data can be expensive to collect. breza et al. (2017) propose aggregated relational data (ard) as a low-cost substitute that can be used to recover the structure of a latent social network when it is generated by a specific parametric random effects model. our main observation is that many economic network formation models produce networks that are effectively low-rank. as a consequence, network recovery from ard is generally possible without parametric assumptions using a nuclear-norm penalized regression. we demonstrate how to implement this method and provide finite-sample bounds on the mean squared error of the resulting estimator for the distribution of network links. computation takes seconds for samples with hundreds of observations. easy-to-use code in r and python can be found at https://github.com/mpleung/ard.", "categories": "econ.em econ.gn q-fin.ec stat.ap", "created": "2020-01-16", "updated": "", "authors": ["hossein alidaee", "eric auerbach", "michael p. leung"], "url": "https://arxiv.org/abs/2001.06052"}, {"title": "distributional synthetic controls", "id": "2001.06118", "abstract": "this article extends the method of synthetic controls to probability measures. the distribution of the synthetic control group is obtained as the optimally weighted barycenter in wasserstein space of the distributions of the control groups which minimizes the distance to the distribution of the treatment group. it can be applied to settings with disaggregated- or aggregated (functional) data. the method produces a generically unique counterfactual distribution when the data are continuously distributed. a basic representation of the barycenter provides a computationally efficient implementation via a straightforward tensor-variate regression approach. in addition, identification results are provided that also shed new light on the classical synthetic controls estimator. as an illustration the approach estimates the aggregate effect of the legalization of cannabis on the distribution of household income in colorado one year after amendment 64.", "categories": "econ.em stat.me", "created": "2020-01-16", "updated": "2020-03-31", "authors": ["florian gunsilius"], "url": "https://arxiv.org/abs/2001.06118"}, {"title": "comparing school choice and college admission mechanisms by their   immunity to strategic admissions", "id": "2001.06166", "abstract": "recently dozens of school districts and college admissions systems around the world have reformed their admission rules. as a main motivation for these reforms the policymakers cited strategic flaws of the rules: students had strong incentives to game the system, which caused dramatic consequences for non-strategic students. however, almost none of the new rules were strategy-proof. we explain this puzzle. we show that after the reforms the rules became more immune to strategic admissions: each student received a smaller set of schools that he can get in using a strategy, weakening incentives to manipulate. simultaneously, the admission to each school became strategy-proof to a larger set of students, making the schools more available for non-strategic students. we also show that the existing explanation of the puzzle due to pathak and s\\\"onmez (2013) is incomplete.", "categories": "econ.th econ.gn q-fin.ec", "created": "2020-01-17", "updated": "2020-01-25", "authors": ["somouaoga bonkoungou", "alexander s. nesterov"], "url": "https://arxiv.org/abs/2001.06166"}, {"title": "entropy balancing for continuous treatments", "id": "2001.06281", "abstract": "this paper introduces entropy balancing for continuous treatments (ebct) by extending the original entropy balancing methodology of hainm\\\"uller (2012). in order to estimate balancing weights, the proposed approach solves a globally convex constrained optimization problem. ebct weights reliably eradicate pearson correlations between covariates and the continuous treatment variable. this is the case even when other methods based on the generalized propensity score tend to yield insufficient balance due to strong selection into different treatment intensities. moreover, the optimization procedure is more successful in avoiding extreme weights attached to a single unit. extensive monte-carlo simulations show that treatment effect estimates using ebct display similar or lower bias and uniformly lower root mean squared error. these properties make ebct an attractive method for the evaluation of continuous treatments.", "categories": "econ.em stat.me", "created": "2020-01-17", "updated": "2020-05-28", "authors": ["stefan t\u00fcbbicke"], "url": "https://arxiv.org/abs/2001.06281"}, {"title": "who voted for a no deal brexit? a composition model of great britains   2019 european parliamentary elections", "id": "2001.06548", "abstract": "the purpose of this paper is to use the votes cast at the 2019 european elections held in united kingdom to re-visit the analysis conducted subsequent to its 2016 european union referendum vote. this exercise provides a staging post on public opinion as the united kingdom moves to leave the european union during 2020. a composition data analysis in a seemingly unrelated regression framework is adopted that respects the compositional nature of the vote outcome; each outcome is a share that adds up to 100% and each outcome is related to the alternatives. contemporary explanatory data for each counting area is sourced from the themes of socio-demographics, employment, life satisfaction and place. the study find that there are still strong and stark divisions in the united kingdom, defined by age, qualifications, employment and place. the use of a compositional analysis approach produces challenges in regards to the interpretation of these models, but marginal plots are seen to aid the interpretation somewhat.", "categories": "physics.soc-ph econ.gn q-fin.ec stat.ap", "created": "2020-01-16", "updated": "", "authors": ["stephen clark"], "url": "https://arxiv.org/abs/2001.06548"}, {"title": "a tail dependence-based mst and their topological indicators in   modelling systemic risk in the european insurance sector", "id": "2001.06567", "abstract": "in the present work we analyse the dynamics of indirect connections between insurance companies that result from market price channels. in our analysis we assume that the stock quotations of insurance companies reflect market sentiments which constitute a very important systemic risk factor. interlinkages between insurers and their dynamics have a direct impact on systemic risk contagion in the insurance sector. we propose herein a new hybrid approach to the analysis of interlinkages dynamics based on combining the copula-dcc-garch model and minimum spanning trees (mst). using the copula-dcc-garch model we determine the tail dependence coefficients. then, for each analysed period we construct mst based on these coefficients. the dynamics is analysed by means of time series of selected topological indicators of the msts in the years 2005-2019. our empirical results show the usefulness of the proposed approach to the analysis of systemic risk in the insurance sector. the times series obtained from the proposed hybrid approach reflect the phenomena occurring on the market. the analysed mst topological indicators can be considered as systemic risk predictors.", "categories": "q-fin.st econ.em", "created": "2020-01-17", "updated": "2020-03-09", "authors": ["anna denkowska", "stanis\u0142aw wanat"], "url": "https://arxiv.org/abs/2001.06567"}, {"title": "generalized local iv with unordered multiple treatment levels:   identification, efficient estimation, and testable implication", "id": "2001.06746", "abstract": "this paper studies the econometric aspects of the generalized local iv framework defined using the unordered monotonicity condition, which accommodates multiple levels of treatment and instrument in program evaluations. the framework is explicitly developed to allow for conditioning covariates. nonparametric identification results are obtained for a wide range of policy-relevant parameters. semiparametric efficiency bounds are computed for these identified structural parameters, including the local average structural function and local average structural function on the treated. two semiparametric estimators are introduced that achieve efficiency. one is the conditional expectation projection estimator defined through the nonparametric identification equation. the other is the double/debiased machine learning estimator defined through the efficient influence function, which is suitable for high-dimensional settings. more generally, for parameters implicitly defined by possibly non-smooth and overidentifying moment conditions, this study provides the calculation for the corresponding semiparametric efficiency bounds and proposes efficient semiparametric gmm estimators again using the efficient influence functions. then an optimal set of testable implications of the model assumption is proposed. previous results developed for the binary local iv model and the multivalued treatment model under unconfoundedness are encompassed as special cases in this more general framework. the theoretical results are illustrated by an empirical application investigating the return to schooling across different fields of study, and a monte carlo experiment.", "categories": "econ.em", "created": "2020-01-18", "updated": "", "authors": ["haitian xie"], "url": "https://arxiv.org/abs/2001.06746"}, {"title": "incentive-compatible diffusion auctions", "id": "2001.06975", "abstract": "diffusion auction is a new model in auction design. it can incentivize the buyers who have already joined in the auction to further diffuse the sale information to others via social relations, whereby both the seller's revenue and the social welfare can be improved. diffusion auctions are essentially non-typical multidimensional mechanism design problems and agents' social relations are complicatedly involved with their bids. in such auctions, incentive-compatibility (ic) means it is best for every agent to honestly report her valuation and fully diffuse the sale information to all her neighbors. existing work identified some specific mechanisms for diffusion auctions, while a general theory characterizing all incentive-compatible diffusion auctions is still missing. in this work, we identify a sufficient and necessary condition for all dominant-strategy incentive-compatible (dsic) diffusion auctions. we formulate the monotonic allocation policies in such multidimensional problems and show that any monotonic allocation policy can be implemented in a dsic diffusion auction mechanism. moreover, given any monotonic allocation policy, we obtain the optimal payment policy to maximize the seller's revenue.", "categories": "cs.gt econ.th", "created": "2020-01-20", "updated": "2020-04-26", "authors": ["bin li", "dong hao", "dengji zhao"], "url": "https://arxiv.org/abs/2001.06975"}, {"title": "fundamental limits of testing the independence of irrelevant   alternatives in discrete choice", "id": "2001.07042", "abstract": "the multinomial logit (mnl) model and the axiom it satisfies, the independence of irrelevant alternatives (iia), are together the most widely used tools of discrete choice. the mnl model serves as the workhorse model for a variety of fields, but is also widely criticized, with a large body of experimental literature claiming to document real-world settings where iia fails to hold. statistical tests of iia as a modelling assumption have been the subject of many practical tests focusing on specific deviations from iia over the past several decades, but the formal size properties of hypothesis testing iia are still not well understood. in this work we replace some of the ambiguity in this literature with rigorous pessimism, demonstrating that any general test for iia with low worst-case error would require a number of samples exponential in the number of alternatives of the choice problem. a major benefit of our analysis over previous work is that it lies entirely in the finite-sample domain, a feature crucial to understanding the behavior of tests in the common data-poor settings of discrete choice. our lower bounds are structure-dependent, and as a potential cause for optimism, we find that if one restricts the test of iia to violations that can occur in a specific collection of choice sets (e.g., pairs), one obtains structure-dependent lower bounds that are much less pessimistic. our analysis of this testing problem is unorthodox in being highly combinatorial, counting eulerian orientations of cycle decompositions of a particular bipartite graph constructed from a data set of choices. by identifying fundamental relationships between the comparison structure of a given testing problem and its sample efficiency, we hope these relationships will help lay the groundwork for a rigorous rethinking of the iia testing problem as well as other testing problems in discrete choice.", "categories": "math.st econ.em stat.me stat.ml stat.th", "created": "2020-01-20", "updated": "", "authors": ["arjun seshadri", "johan ugander"], "url": "https://arxiv.org/abs/2001.07042"}, {"title": "investor experiences and international capital flows", "id": "2001.07790", "abstract": "we propose a novel explanation for classic international macro puzzles regarding capital flows and portfolio investment, which builds on modern macro-finance models of experience-based belief formation. individual experiences of past macroeconomic outcomes have been shown to exert a long-lasting influence on beliefs about future realizations, and to explain domestic stock-market investment. we argue that experience effects can explain the tendency of investors to hold an over proportional fraction of their equity wealth in domestic stocks (home bias), to invest in domestic equity markets in periods of domestic crises (retrenchment), and to withdraw capital from foreign equity markets in periods of foreign crises (fickleness). experience-based learning generates additional implications regarding the strength of these puzzles in times of higher or lower economic activity and depending on the demographic composition of market participants. we test and confirm these predictions in the data.", "categories": "econ.gn q-fin.ec", "created": "2020-01-21", "updated": "", "authors": ["ulrike malmendier", "demian pouzo", "victoria vanasco"], "url": "https://arxiv.org/abs/2001.07790"}, {"title": "oracle efficient estimation of structural breaks in cointegrating   regressions", "id": "2001.07949", "abstract": "in this paper, we propose an adaptive group lasso procedure to efficiently estimate structural breaks in cointegrating regressions. it is well-known that the group lasso estimator is not simultaneously estimation consistent and model selection consistent in structural break settings. hence, we use a first step group lasso estimation of a diverging number of breakpoint candidates to produce weights for a second adaptive group lasso estimation. we prove that parameter changes are estimated consistently by group lasso and show that the number of estimated breaks is greater than the true number but still sufficiently close to it. then, we use these results and prove that the adaptive group lasso has oracle properties if weights are obtained from our first step estimation. simulation results show that the proposed estimator delivers the expected results. an economic application to the long-run us money demand function demonstrates the practical importance of this methodology.", "categories": "econ.em stat.ml", "created": "2020-01-22", "updated": "2020-03-31", "authors": ["karsten schweikert"], "url": "https://arxiv.org/abs/2001.07949"}, {"title": "measuring the input rank in global supply networks", "id": "2001.08003", "abstract": "we introduce the input rank as a measure of relevance of direct and indirect suppliers in global value chains. we conceive an intermediate input to be more relevant for a downstream buyer if a decrease in that input's productivity affects that buyer more. in particular, in our framework, the relevance of any input depends: i) on the network position of the supplier relative to the buyer, ii) the patterns of intermediate inputs vs labor intensities connecting the buyer and the supplier, iii) and the competitive pressures along supply chains. after we compute the input rank from both u.s. and world input-output tables, we provide useful insights on the crucial role of services inputs as well as on the relatively higher relevance of domestic suppliers and suppliers coming from regionally integrated partners. finally, we test that the input rank is a good predictor of vertical integration choices made by 20,489 u.s. parent companies controlling 154,836 subsidiaries worldwide.", "categories": "econ.gn q-fin.ec", "created": "2020-01-22", "updated": "2020-09-06", "authors": ["armando rungi", "loredana fattorini", "kenan huremovic"], "url": "https://arxiv.org/abs/2001.08003"}, {"title": "complexity, stability properties of mixed games and dynamic algorithms,   and learning in the sharing economy", "id": "2001.08192", "abstract": "the sharing economy (which includes airbnb, apple, alibaba, uber, wework, ebay, didi chuxing, amazon) blossomed across the world, triggered structural changes in industries and significantly affected international capital flows primarily by disobeying a wide variety of statutes and laws in many countries. they also illegally reduced and changing the nature of competition in many industries often to the detriment of social welfare. this article develops new dynamic pricing models for the seos and derives some stability properties of mixed games and dynamic algorithms which eliminate antitrust liability and also reduce deadweight losses, greed, regret and gps manipulation. the new dynamic pricing models contravene the myerson satterthwaite impossibility theorem.", "categories": "cs.gt cs.ai econ.th math.ds nlin.ao", "created": "2020-01-17", "updated": "", "authors": ["michael c. nwogugu"], "url": "https://arxiv.org/abs/2001.08192"}, {"title": "comments are welcome", "id": "2001.08376", "abstract": "scholars present their new research at seminars and conferences, and send drafts to peers, hoping to receive comments and suggestions that will improve the quality of their work. using a dataset of papers published in economics journals, this article measures how much peers' individual and collective comments improve the quality of research. controlling for the quality of the research idea and author, i find that a one standard deviation increase in the number of peers' individual and collective comments increases the quality of the journal in which the research is published by 47%.", "categories": "econ.gn q-fin.ec", "created": "2020-01-21", "updated": "2020-02-14", "authors": ["asier minondo"], "url": "https://arxiv.org/abs/2001.08376"}, {"title": "effects of the institutional change based on democratization on origin   and diffusion of technological innovation", "id": "2001.08432", "abstract": "political systems shape institutions and govern institutional change supporting economic performance, production and diffusion of technological innovation. this study shows, using global data of countries, that institutional change, based on a progressive democratization of countries, is a driving force of inventions, adoption and diffusion of innovations in society. the relation between technological innovation and level of democracy can be explained with following factors: higher economic freedom in society, effective regulation, higher economic and political stability, higher investments in r&d and higher education, good economic governance and higher level of education system for training high-skilled human resources. overall, then, the positive associations between institutional change, based on a process of democratization, and paths of technological innovation can sustain best practices of political economy for the development of economies in the presence of globalization and geographical expansion of markets.", "categories": "econ.gn q-fin.ec", "created": "2020-01-23", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/2001.08432"}, {"title": "knowledge graphs for innovation ecosystems", "id": "2001.08615", "abstract": "innovation ecosystems can be naturally described as a collection of networked entities, such as experts, institutions, projects, technologies and products. representing in a machine-readable form these entities and their relations is not entirely attainable, due to the existence of abstract concepts such as knowledge and due to the confidential, non-public nature of this information, but even its partial depiction is of strong interest. the representation of innovation ecosystems incarnated as knowledge graphs would enable the generation of reports with new insights, the execution of advanced data analysis tasks. an ontology to capture the essential entities and relations is presented, as well as the description of data sources, which can be used to populate innovation knowledge graphs. finally, the application case of the universidad politecnica de madrid is presented, as well as an insight of future applications.", "categories": "cs.ir cs.ai cs.lg econ.gn q-fin.ec", "created": "2020-01-09", "updated": "", "authors": ["alberto tejero", "victor rodriguez-doncel", "ivan pau"], "url": "https://arxiv.org/abs/2001.08615"}, {"title": "big data based research on mechanisms of sharing economy restructuring   the world", "id": "2001.08926", "abstract": "many researches have discussed the phenomenon and definition of sharing economy, but an understanding of sharing economy's reconstructions of the world remains elusive. we illustrate the mechanism of sharing economy's reconstructions of the world in detail based on big data including the mechanism of sharing economy's reconstructions of society, time and space, users, industry, and self-reconstruction in the future, which is very important for society to make full use of the reconstruction opportunity to upgrade our world through sharing economy. on the one hand, we established the mechanisms for sharing economy rebuilding society, industry, space-time, and users through qualitative analyses, and on the other hand, we demonstrated the rationality of the mechanisms through quantitative analyses of big data.", "categories": "econ.gn q-fin.ec", "created": "2020-01-24", "updated": "", "authors": ["dingju zhu"], "url": "https://arxiv.org/abs/2001.08926"}, {"title": "social cost of carbon: what do the numbers really mean?", "id": "2001.08935", "abstract": "the social cost of carbon (scc) is estimated by integrated assessment models and is widely used by government agencies to value the climate impacts of rulemakings, however, the core discussion around scc so far was focused on validity of obtained numerical estimates and related uncertainties while largely neglecting a deeper discussion of the scc applicability limits stemming from the calculation method. this work provides a conceptual mathematical background and the economic interpretation that is behind the scc calculation in the three widely used integrated assessment models. policy makers need to be aware of the difference between the commonly implied and the actual meaning of scc that substantially limits its applicability in practice. the presented results call for a critical revision of the scc concept and the scc calculation methods in integrated assessment models.", "categories": "econ.gn q-fin.ec", "created": "2020-01-24", "updated": "2020-05-07", "authors": ["nikolay khabarov", "alexey smirnov", "michael obersteiner"], "url": "https://arxiv.org/abs/2001.08935"}, {"title": "integrated ridesharing services with chance-constrained dynamic pricing   and demand learning", "id": "2001.09151", "abstract": "the design of integrated mobility-on-demand services requires jointly considering the interactions between traveler choice behavior and operators' operation policies to design a financially sustainable pricing scheme. however, most existing studies focus on the supply side perspective, disregarding the impact of customer choice behavior in the presence of co-existing transport networks. we propose a modeling framework for dynamic integrated mobility-on-demand service operation policy evaluation with two service options: door-to-door rideshare and rideshare with transit transfer. a new constrained dynamic pricing model is proposed to maximize operator profit, taking into account the correlated structure of different modes of transport. user willingness to pay is considered as a stochastic constraint, resulting in a more realistic ticket price setting while maximizing operator profit. unlike most studies, which assume that travel demand is known, we propose a demand learning process to calibrate customer demand over time based on customers' historical purchase data. we evaluate the proposed methodology through simulations under different scenarios on a test network by considering the interactions of supply and demand in a multimodal market. different scenarios in terms of customer arrival intensity, vehicle capacity, and the variance of user willingness to pay are tested. results suggest that the proposed chance-constrained assortment price optimization model allows increasing operator profit while keeping the proposed ticket prices acceptable.", "categories": "econ.gn q-fin.ec", "created": "2020-01-23", "updated": "2020-06-07", "authors": ["tai-yu ma", "sylvain klein"], "url": "https://arxiv.org/abs/2001.09151"}, {"title": "bayesian panel quantile regression for binary outcomes with correlated   random effects: an application on crime recidivism in canada", "id": "2001.09295", "abstract": "this article develops a bayesian approach for estimating panel quantile regression with binary outcomes in the presence of correlated random effects. we construct a working likelihood using an asymmetric laplace (al) error distribution and combine it with suitable prior distributions to obtain the complete joint posterior distribution. for posterior inference, we propose two markov chain monte carlo (mcmc) algorithms but prefer the algorithm that exploits the blocking procedure to produce lower autocorrelation in the mcmc draws. we also explain how to use the mcmc draws to calculate the marginal effects, relative risk and odds ratio. the performance of our preferred algorithm is demonstrated in multiple simulation studies and shown to perform extremely well. furthermore, we implement the proposed framework to study crime recidivism in quebec, a canadian province, using a novel data from the administrative correctional files. our results suggest that the recently implemented \"tough-on-crime\" policy of the canadian government has been largely successful in reducing the probability of repeat offenses in the post-policy period. besides, our results support existing findings on crime recidivism and offer new insights at various quantiles.", "categories": "econ.em stat.me", "created": "2020-01-25", "updated": "", "authors": ["georges bresson", "guy lacroix", "mohammad arshad rahman"], "url": "https://arxiv.org/abs/2001.09295"}, {"title": "in simple communication games, when does ex ante fact-finding benefit   the receiver?", "id": "2001.09387", "abstract": "always, if the number of states is equal to two; or if the number of receiver actions is equal to two and i. the number of states is three or fewer, or ii. the game is cheap talk, or ii. there are just two available messages for the sender. a counterexample is provided for each failure of these conditions.", "categories": "econ.th", "created": "2020-01-25", "updated": "", "authors": ["mark whitmeyer"], "url": "https://arxiv.org/abs/2001.09387"}, {"title": "estimating marginal treatment effects under unobserved group   heterogeneity", "id": "2001.09560", "abstract": "this paper studies endogenous treatment effect models in which individuals are classified into unobserved groups based on heterogeneous treatment choice rules. such heterogeneity may arise, for example, when multiple treatment eligibility criteria and different preference patterns exist. using a finite mixture approach, we propose a marginal treatment effect (mte) framework in which the treatment choice and outcome equations can be heterogeneous across groups. under the availability of valid instrumental variables specific to each group, we show that the mte for each group can be separately identified using the local instrumental variable method. based on our identification result, we propose a two-step semiparametric procedure for estimating the group-wise mte parameters. we first estimate the finite-mixture treatment choice model by a maximum likelihood method and then estimate the mtes using a series approximation method. we prove that the proposed mte estimator is consistent and asymptotically normally distributed. we illustrate the usefulness of the proposed method with an application to economic returns to college education.", "categories": "econ.em stat.me", "created": "2020-01-26", "updated": "2020-04-20", "authors": ["tadao hoshino", "takahide yanagi"], "url": "https://arxiv.org/abs/2001.09560"}, {"title": "the network paradigm as a modeling tool in regional economy: the case of   interregional commuting in greece", "id": "2001.09664", "abstract": "network science is an emerging discipline using the network paradigm to model communication systems as pair-sets of interconnected nodes and their linkages (edges). this paper applies this paradigm to study an interacting system in regional economy consisting of daily road transportation flows for labor purposes, the so-called commuting phenomenon. in particular, the commuting system in greece including 39 non-insular prefectures is modeled into a complex network and it is studied using measures and methods of complex network analysis and empirical techniques. the study aims to detect the structural characteristics of the greek interregional commuting network (gcn) and to interpret how this network is related to the regional development. the analysis highlights the effect of the spatial constraints in the structure of the gcn, it provides insights about the major road transport projects constructed the last decade, and it outlines a populationcontrolled (gravity) pattern of commuting, illustrating that high-populated regions attract larger volumes of the commuting activity, which consequently affects their productivity. overall, this paper highlights the effectiveness of complex network analysis in the modeling of systems of regional economy, such as the systems of spatial interaction and the transportation networks, and it promotes the use of the network paradigm to the regional research.", "categories": "econ.gn q-fin.ec", "created": "2020-01-27", "updated": "", "authors": ["dimitrios tsiotas", "labros sdrolias", "dimitrios belias"], "url": "https://arxiv.org/abs/2001.09664"}, {"title": "regional airports in greece, their characteristics and their importance   for the local economic development", "id": "2001.09666", "abstract": "technological developments worldwide are contributing to the improvement of transport infrastructures and they are helping to reduce the overall transport costs. at the same time, such developments along with the reduction in transport costs are affecting the spatial interdependence between the regions and countries, a fact inducing significant effects on their economies and, in general, on their growth-rates. a specific class of transport infrastructures contributing significantly to overcoming the spatial constraints is the airtransport infrastructures. nowadays, the importance of air-transport infrastructures in the economic development is determinative, especially for the geographically isolated regions, such as for the island regions of greece. within this context, this paper studies the greek airports and particularly the evolution of their overall transportation imprint, their geographical distribution, and the volume of the transport activity of each airport. also, it discusses, in a broad context, the seasonality of the greek airport activity, the importance of the airports for the local and regional development, and it formulates general conclusions.", "categories": "econ.gn q-fin.ec", "created": "2020-01-27", "updated": "", "authors": ["serafeim polyzos", "dimitrios tsiotas"], "url": "https://arxiv.org/abs/2001.09666"}, {"title": "risk fluctuation characteristics of internet finance: combining industry   characteristics with ecological value", "id": "2001.09798", "abstract": "the internet plays a key role in society and is vital to economic development. due to the pressure of competition, most technology companies, including internet finance companies, continue to explore new markets and new business. funding subsidies and resource inputs have led to significant business income tendencies in financial statements. this tendency of business income is often manifested as part of the business loss or long-term unprofitability. we propose a risk change indicator (rfr) and compare the risk indicator of fourteen representative companies. this model combines extreme risk value with slope, and the combination method is simple and effective. the results of experiment show the potential of this model. the risk volatility of technology enterprises including internet finance enterprises is highly cyclical, and the risk volatility of emerging internet fintech companies is much higher than that of other technology companies.", "categories": "econ.em", "created": "2020-01-27", "updated": "", "authors": ["runjie xu", "chuanmin mi", "nan ye", "tom marshall", "yadong xiao", "hefan shuai"], "url": "https://arxiv.org/abs/2001.09798"}, {"title": "objective social choice: using auxiliary information to improve voting   outcomes", "id": "2001.10092", "abstract": "how should one combine noisy information from diverse sources to make an inference about an objective ground truth? this frequently recurring, normative question lies at the core of statistics, machine learning, policy-making, and everyday life. it has been called \"combining forecasts\", \"meta-analysis\", \"ensembling\", and the \"mle approach to voting\", among other names. past studies typically assume that noisy votes are identically and independently distributed (i.i.d.), but this assumption is often unrealistic. instead, we assume that votes are independent but not necessarily identically distributed and that our ensembling algorithm has access to certain auxiliary information related to the underlying model governing the noise in each vote. in our present work, we: (1) define our problem and argue that it reflects common and socially relevant real world scenarios, (2) propose a multi-arm bandit noise model and count-based auxiliary information set, (3) derive maximum likelihood aggregation rules for ranked and cardinal votes under our noise model, (4) propose, alternatively, to learn an aggregation rule using an order-invariant neural network, and (5) empirically compare our rules to common voting rules and naive experience-weighted modifications. we find that our rules successfully use auxiliary information to outperform the naive baselines.", "categories": "cs.ma cs.lg econ.th", "created": "2020-01-27", "updated": "", "authors": ["silviu pitis", "michael r. zhang"], "url": "https://arxiv.org/abs/2001.10092"}, {"title": "smart city governance in developing countries: a systematic literature   review", "id": "2001.10173", "abstract": "smart cities that make broad use of digital technologies have been touted as possible solutions for the population pressures faced by many cities in developing countries and may help meet the rising demand for services and infrastructure. nevertheless, the high financial cost involved in infrastructure maintenance, the substantial size of the informal economies, and various governance challenges are curtailing government idealism regarding smart cities. this review examines the state of smart city development in developing countries, which includes understanding the conceptualisations, motivations, and unique drivers behind (and barriers to) smarty city development. a total of 56 studies were identified from a systematic literature review from an initial pool of 3928 social sciences literature identified from two academic databases. data were analysed using thematic synthesis and thematic analysis. the review found that technology-enabled smart cities in developing countries can only be realised when concurrent socioeconomic, human, legal, and regulatory reforms are instituted. governments need to step up their efforts to fulfil the basic infrastructure needs of citizens, raise more revenue, construct clear regulatory frameworks to mitigate the technological risks involved, develop human capital, ensure digital inclusivity, and promote environmental sustainability. a supportive ecosystem that encourages citizen participation, nurtures start-ups, and promotes public-private partnerships needs to be created to realise their smart city vision.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-01-28", "updated": "", "authors": ["si ying tan", "araz taeihagh"], "url": "https://arxiv.org/abs/2001.10173"}, {"title": "saddlepoint approximations for spatial panel data models", "id": "2001.10377", "abstract": "we develop new higher-order asymptotic techniques for the gaussian maximum likelihood estimator in a spatial panel data model, with fixed effects, time-varying covariates, and spatially correlated errors. our saddlepoint density and tail area approximation feature relative error of order $o(m^{-1})$ for $m=n(t-1)$ with $n$ being the cross-sectional dimension and $t$ the time-series dimension. the main theoretical tool is the tilted-edgeworth technique in a non-identically distributed setting. the density approximation is always non-negative, does not need resampling, and is accurate in the tails. we provide an algorithm and monte carlo experiments illustrating its good performance over first-order asymptotics and edgeworth expansions, while preserving analytical tractability. an empirical application on the investment-saving relationship in oecd countries shows disagreement between testing results based on first-order asymptotics and saddlepoint techniques.", "categories": "math.st econ.em stat.me stat.th", "created": "2020-01-22", "updated": "2020-01-31", "authors": ["chaonan jiang", "davide la vecchia", "elvezio ronchetti", "olivier scaillet"], "url": "https://arxiv.org/abs/2001.10377"}, {"title": "skills to not fall behind in school", "id": "2001.10519", "abstract": "many recent studies emphasize how important the role of cognitive and social-emotional skills can be in determining people's quality of life. although skills are of great importance in many aspects, in this paper we will focus our efforts to better understand the relationship between several types of skills with academic progress delay. our dataset contains the same students in 2012 and 2017, and we consider that there was a academic progress delay for a specific student if he or she progressed less than expected in school grades. our methodology primarily includes the use of a bayesian logistic regression model and our results suggest that both cognitive and social-emotional skills may impact the conditional probability of falling behind in school, and the magnitude of the impact between the two types of skills can be comparable.", "categories": "stat.ap econ.em", "created": "2020-01-28", "updated": "", "authors": ["felipe maia polo"], "url": "https://arxiv.org/abs/2001.10519"}, {"title": "who presents and where? an analysis of research seminars in us economics   departments", "id": "2001.10561", "abstract": "using a large dataset of research seminars held at us economics departments in 2018, i explore the factors that determine who is invited to present at a research seminar and whether the invitation is accepted. i find that high-quality scholars have a higher probability of being invited than low-quality scholars, and researchers are more likely to accept an invitation if it is issued by a top economics department. the probability of being invited increases with the size of the host department. young and low-quality scholars have a higher probability of accepting an invitation. the distance between the host department and invited scholar reduces the probability of being invited and accepting the invitation. female scholars do not have a lower probability of being invited to give a research seminar than men.", "categories": "econ.gn q-fin.ec", "created": "2020-01-28", "updated": "2020-05-07", "authors": ["asier minondo"], "url": "https://arxiv.org/abs/2001.10561"}, {"title": "frequentist shrinkage under inequality constraints", "id": "2001.10586", "abstract": "this paper shows how to shrink extremum estimators towards inequality constraints motivated by economic theory. we propose an inequality constrained shrinkage estimator (icse) which takes the form of a weighted average between the unconstrained and inequality constrained estimators with the data dependent weight. the weight drives both the direction and degree of shrinkage. we use a local asymptotic framework to derive the asymptotic distribution and risk of the icse. we provide conditions under which the asymptotic risk of the icse is strictly less than that of the unrestricted extremum estimator. the degree of shrinkage cannot be consistently estimated under the local asymptotic framework. to address this issue, we propose a feasible plug-in estimator and investigate its finite sample behavior. we also apply our framework to gasoline demand estimation under the slutsky restriction.", "categories": "econ.em", "created": "2020-01-28", "updated": "", "authors": ["edvard bakhitov"], "url": "https://arxiv.org/abs/2001.10586"}, {"title": "stable and efficient structures in multigroup network formation", "id": "2001.10627", "abstract": "in this work we present a strategic network formation model predicting the emergence of multigroup structures. individuals decide to form or remove links based on the benefits and costs those connections carry; we focus on bilateral consent for link formation. an exogenous system specifies the frequency of coordination issues arising among the groups. we are interested in structures that arise to resolve coordination issues and, specifically, structures in which groups are linked through bridging, redundant, and co-membership interconnections. we characterize the conditions under which certain structures are stable and study their efficiency as well as the convergence of formation dynamics.", "categories": "cs.si econ.th", "created": "2020-01-28", "updated": "", "authors": ["shadi mohagheghi", "jingying ma", "francesco bullo"], "url": "https://arxiv.org/abs/2001.10627"}, {"title": "functional sequential treatment allocation with covariates", "id": "2001.10996", "abstract": "we consider a multi-armed bandit problem with covariates. given a realization of the covariate vector, instead of targeting the treatment with highest conditional expectation, the decision maker targets the treatment which maximizes a general functional of the conditional potential outcome distribution, e.g., a conditional quantile, trimmed mean, or a socio-economic functional such as an inequality, welfare or poverty measure. we develop expected regret lower bounds for this problem, and construct a near minimax optimal assignment policy.", "categories": "stat.ml cs.lg econ.em math.st stat.th", "created": "2020-01-29", "updated": "", "authors": ["anders bredahl kock", "david preinerstorfer", "bezirgen veliyev"], "url": "https://arxiv.org/abs/2001.10996"}, {"title": "blocked clusterwise regression", "id": "2001.11130", "abstract": "a recent literature in econometrics models unobserved cross-sectional heterogeneity in panel data by assigning each cross-sectional unit a one-dimensional, discrete latent type. such models have been shown to allow estimation and inference by regression clustering methods. this paper is motivated by the finding that the clustered heterogeneity models studied in this literature can be badly misspecified, even when the panel has significant discrete cross-sectional structure. to address this issue, we generalize previous approaches to discrete unobserved heterogeneity by allowing each unit to have multiple, imperfectly-correlated latent variables that describe its response-type to different covariates. we give inference results for a k-means style estimator of our model and develop information criteria to jointly select the number clusters for each latent variable. monte carlo simulations confirm our theoretical results and give intuition about the finite-sample performance of estimation and model selection. we also contribute to the theory of clustering with an over-specified number of clusters and derive new convergence rates for this setting. our results suggest that over-fitting can be severe in k-means style estimators when the number of clusters is over-specified.", "categories": "econ.em stat.me stat.ml", "created": "2020-01-29", "updated": "", "authors": ["max cytrynbaum"], "url": "https://arxiv.org/abs/2001.11130"}, {"title": "fictitious play outperforms counterfactual regret minimization", "id": "2001.11165", "abstract": "we compare the performance of two popular algorithms, fictitious play and counterfactual regret minimization, in approximating nash equilibrium in multiplayer games. despite recent success of counterfactual regret minimization in multiplayer poker and conjectures of its superiority, we show that fictitious play leads to improved nash equilibrium approximation over a variety of game classes and sizes.", "categories": "cs.gt cs.ai cs.ma econ.th", "created": "2020-01-29", "updated": "2020-07-23", "authors": ["sam ganzfried"], "url": "https://arxiv.org/abs/2001.11165"}, {"title": "agenda-manipulation in ranking", "id": "2001.11341", "abstract": "a committee ranks a set of alternatives by sequentially voting on pairs, in an order chosen by the committee's chair. although the chair has no knowledge of voters' preferences, we show that she can do as well as if she had perfect information. we characterise strategies with this 'regret-freeness' property in two ways: (1) they are efficient, and (2) they avoid two intuitive errors. one regret-free strategy is a sorting algorithm called insertion sort. we show that it is characterised by a lexicographic property, and is outcome-equivalent to a recursive variant of the much-studied amendment procedure.", "categories": "econ.th", "created": "2020-01-30", "updated": "2020-07-01", "authors": ["gregorio curello", "ludvig sinander"], "url": "https://arxiv.org/abs/2001.11341"}, {"title": "spatial competition with unit-demand functions", "id": "2001.11422", "abstract": "this paper studies a spatial competition game between two firms that sell a homogeneous good at some pre-determined fixed price. a population of consumers is spread out over the real line, and the two firms simultaneously choose location in this same space. when buying from one of the firms, consumers incur the fixed price plus some transportation costs, which are increasing with their distance to the firm. under the assumption that each consumer is ready to buy one unit of the good whatever the locations of the firms, firms converge to the median location: there is \"minimal differentiation\". in this article, we relax this assumption and assume that there is an upper limit to the distance a consumer is ready to cover to buy the good. we show that the game always has at least one nash equilibrium in pure strategy. under this more general assumption, the \"minimal differentiation principle\" no longer holds in general. at equilibrium, firms choose \"minimal\", \"intermediate\" or \"full\" differentiation, depending on this critical distance a consumer is ready to cover and on the shape of the distribution of consumers' locations.", "categories": "math.oc econ.th", "created": "2020-01-30", "updated": "", "authors": ["ga\u00ebtan fournier", "karine van der straeten", "j\u00f6rgen weibull"], "url": "https://arxiv.org/abs/2001.11422"}, {"title": "housing search in the age of big data: smarter cities or the same old   blind spots?", "id": "2001.11585", "abstract": "housing scholars stress the importance of the information environment in shaping housing search behavior and outcomes. rental listings have increasingly moved online over the past two decades and, in turn, online platforms like craigslist are now central to the search process. do these technology platforms serve as information equalizers or do they reflect traditional information inequalities that correlate with neighborhood sociodemographics? we synthesize and extend analyses of millions of us craigslist rental listings and find they supply significantly different volumes, quality, and types of information in different communities. technology platforms have the potential to broaden, diversify, and equalize housing search information, but they rely on landlord behavior and, in turn, likely will not reach this potential without a significant redesign or policy intervention. smart cities advocates hoping to build better cities through technology must critically interrogate technology platforms and big data for systematic biases.", "categories": "stat.ap cs.cy econ.gn q-fin.ec", "created": "2020-01-30", "updated": "", "authors": ["geoff boeing", "max besbris", "ariela schachter", "john kuk"], "url": "https://arxiv.org/abs/2001.11585"}, {"title": "an interacting agent model of economic crisis", "id": "2001.11843", "abstract": "most national economies are linked by international trade. consequently, economic globalization forms a massive and complex economic network with strong links, that is, interactions arising from increasing trade. various interesting collective motions are expected to emerge from strong economic interactions in a global economy under trade liberalization. among the various economic collective motions, economic crises are our most intriguing problem. in our previous studies, we have revealed that the kuramoto's coupled limit-cycle oscillator model and the ising-like spin model on networks are invaluable tools for characterizing the economic crises. in this study, we develop a mathematical theory to describe an interacting agent model that derives the kuramoto model and the ising-like spin model by using appropriate approximations. our interacting agent model suggests phase synchronization and spin ordering during economic crises. we confirm the emergence of the phase synchronization and spin ordering during economic crises by analyzing various economic time series data. we also develop a network reconstruction model based on entropy maximization that considers the sparsity of the network. here network reconstruction means estimating a network's adjacency matrix from a node's local information. the interbank network is reconstructed using the developed model, and a comparison is made of the reconstructed network with the actual data. we successfully reproduce the interbank network and the known stylized facts. in addition, the exogenous shock acting on an industry community in a supply chain network and financial sector are estimated. estimation of exogenous shocks acting on communities of in the real economy in the supply chain network provide evidence of the channels of distress propagating from the financial sector to the real economy through the supply chain network.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-01-29", "updated": "", "authors": ["yuichi ikeda"], "url": "https://arxiv.org/abs/2001.11843"}, {"title": "estimating the welfare effects of school vouchers", "id": "2002.00103", "abstract": "we analyze the welfare effects of voucher provision in the dc opportunity scholarship program (osp), a school voucher program in washington, dc, that randomly allocated vouchers to students. to do so, we develop new discrete choice tools to show how to use data with random allocation of school vouchers to characterize what we can learn about the welfare benefits of providing a voucher of a given amount, as measured by the average willingness to pay for that voucher, and these benefits net of the costs of providing that voucher. a novel feature of our tools is that they allow specifying the relationship of the demand for the various schools with respect to prices to be entirely nonparametric or to be parameterized in a flexible manner, both of which do not necessarily imply that the welfare parameters are point identified. applying our tools to the osp data, we find that provision of the status-quo as well as a wide range of counterfactual voucher amounts has a positive net average benefit. we find these positive results arise due to the presence of many low-tuition schools in the program, removing these schools from the program can result in a negative net average benefit.", "categories": "econ.gn econ.em q-fin.ec", "created": "2020-01-31", "updated": "", "authors": ["vishal kamat", "samuel norris"], "url": "https://arxiv.org/abs/2002.00103"}, {"title": "natural experiments", "id": "2002.00202", "abstract": "the term natural experiment is used inconsistently. in one interpretation, it refers to an experiment where a treatment is randomly assigned by someone other than the researcher. in another interpretation, it refers to a study in which there is no controlled random assignment, but treatment is assigned by some external factor in a way that loosely resembles a randomized experiment---often described as an \"as if random\" assignment. in yet another interpretation, it refers to any non-randomized study that compares a treatment to a control group, without any specific requirements on how the treatment is assigned. i introduce an alternative definition that seeks to clarify the integral features of natural experiments and at the same time distinguish them from randomized controlled experiments. i define a natural experiment as a research study where the treatment assignment mechanism (i) is neither designed nor implemented by the researcher, (ii) is unknown to the researcher, and (iii) is probabilistic by virtue of depending on an external factor. the main message of this definition is that the difference between a randomized controlled experiment and a natural experiment is not a matter of degree, but of essence, and thus conceptualizing a natural experiment as a research design akin to a randomized experiment is neither rigorous nor a useful guide to empirical analysis. using my alternative definition, i discuss how a natural experiment differs from a traditional observational study, and offer practical recommendations for researchers who wish to use natural experiments to study causal effects.", "categories": "stat.me econ.em", "created": "2020-02-01", "updated": "", "authors": ["rocio titiunik"], "url": "https://arxiv.org/abs/2002.00202"}, {"title": "variable-lag granger causality and transfer entropy for time series   analysis", "id": "2002.00208", "abstract": "granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. typical operationalizations of granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. the assumption of fixed time delay also exists in transfer entropy, which is considered to be a non-linear version of granger causality. however, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. to address this issue, we develop variable-lag granger causality and variable-lag transfer entropy, generalizations of both granger causality and transfer entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays. in addition, we propose methods for inferring both variable-lag granger causality and transfer entropy relations. in our approaches, we utilize an optimal warping path of dynamic time warping (dtw) to infer variable-lag causal relations. we demonstrate our approaches on an application for studying coordinated collective behavior and other real-world casual-inference datasets and show that our proposed approaches perform better than several existing methods in both simulated and real-world datasets. our approaches can be applied in any domain of time series analysis. the software of this work is available in the r-cran package: vltimecausality.", "categories": "cs.lg econ.em physics.data-an stat.me stat.ml", "created": "2020-02-01", "updated": "2020-06-01", "authors": ["chainarong amornbunchornvej", "elena zheleva", "tanya berger-wolf"], "url": "https://arxiv.org/abs/2002.00208"}, {"title": "insights on the theory of robust games", "id": "2002.00225", "abstract": "a robust game is a distribution-free model to handle ambiguity generated by a bounded set of possible realizations of the values of players' payoff functions. the players are worst-case optimizers and a solution, called robust-optimization equilibrium, is guaranteed by standard regularity conditions. the paper investigates the sensitivity to the level of uncertainty of this equilibrium. specifically, we prove that it is an epsilon-nash equilibrium of the nominal counterpart game, where the epsilon-approximation measures the extra profit that a player would obtain by reducing his level of uncertainty. moreover, given an epsilon-nash equilibrium of a nominal game, we prove that it is always possible to introduce uncertainty such that the epsilon-nash equilibrium is a robust-optimization equilibrium. an example shows that a robust cournot duopoly model can admit multiple and asymmetric robust-optimization equilibria despite only a symmetric nash equilibrium exists for the nominal counterpart game.", "categories": "econ.th cs.gt", "created": "2020-02-01", "updated": "", "authors": ["giovanni paolo crespi", "davide radi", "matteo rocca"], "url": "https://arxiv.org/abs/2002.00225"}, {"title": "efficient representation of supply and demand curves on day-ahead   electricity markets", "id": "2002.00507", "abstract": "our paper aims to model supply and demand curves of electricity day-ahead auction in a parsimonious way. our main task is to build an appropriate algorithm to present the information about electricity prices and demands with far less parameters than the original one. we represent each curve using mesh-free interpolation techniques based on radial basis function approximation. we describe results of this method for the day-ahead ipex spot price of italy.", "categories": "q-fin.pr econ.em", "created": "2020-02-02", "updated": "", "authors": ["mariia soloviova", "tiziano vargiolu"], "url": "https://arxiv.org/abs/2002.00507"}, {"title": "naples;mining the lead-lag relationship from non-synchronous and   high-frequency data", "id": "2002.00724", "abstract": "in time-series analysis, the term \"lead-lag effect\" is used to describe a delayed effect on a given time series caused by another time series. lead-lag effects are ubiquitous in practice and are specifically critical in formulating investment strategies in high-frequency trading. at present, there are three major challenges in analyzing the lead-lag effects. first, in practical applications, not all time series are observed synchronously. second, the size of the relevant dataset and rate of change of the environment is increasingly faster, and it is becoming more difficult to complete the computation within a particular time limit. third, some lead-lag effects are time-varying and only last for a short period, and their delay lengths are often affected by external factors. in this paper, we propose naples (negative and positive lead-lag estimator), a new statistical measure that resolves all these problems. through experiments on artificial and real datasets, we demonstrate that naples has a strong correlation with the actual lead-lag effects, including those triggered by significant macroeconomic announcements.", "categories": "q-fin.st econ.em", "created": "2020-02-03", "updated": "", "authors": ["katsuya ito", "kei nakagawa"], "url": "https://arxiv.org/abs/2002.00724"}, {"title": "a neural-embedded choice model: tastenet-mnl modeling taste   heterogeneity with flexibility and interpretability", "id": "2002.00922", "abstract": "discrete choice models (dcms) and neural networks (nns) can complement each other. we propose a neural network embedded choice model - tastenet-mnl, to improve the flexibility in modeling taste heterogeneity while keeping model interpretability. the hybrid model consists of a tastenet module: a feed-forward neural network that learns taste parameters as flexible functions of individual characteristics; and a choice module: a multinomial logit model (mnl) with manually specified utility. tastenet and mnl are fully integrated and jointly estimated. by embedding a neural network into a dcm, we exploit a neural network's function approximation capacity to reduce specification bias. through special structure and parameter constraints, we incorporate expert knowledge to regularize the neural network and maintain interpretability. on synthetic data, we show that tastenet-mnl can recover the underlying non-linear utility function, and provide predictions and interpretations as accurate as the true model; while examples of logit or random coefficient logit models with misspecified utility functions result in large parameter bias and low predictability. in the case study of swissmetro mode choice, tastenet-mnl outperforms benchmarking mnls' predictability; and discovers a wider spectrum of taste variations within the population, and higher values of time on average. this study takes an initial step towards developing a framework to combine theory-based and data-driven approaches for discrete choice modeling.", "categories": "econ.em cs.lg stat.me", "created": "2020-02-03", "updated": "", "authors": ["yafei han", "christopher zegras", "francisco camara pereira", "moshe ben-akiva"], "url": "https://arxiv.org/abs/2002.00922"}, {"title": "can one hear the size of a target zone?", "id": "2002.00948", "abstract": "we develop a target zone model with realistic features such as finite exit time, non-stationary dynamics and heavy tails. our rigorous characterization of risk corresponds to the dynamic counterpart of a mean-preserving spread. we explicitly solve for both stationary and transient exchange rate paths, and show how they are influenced by the distance to both the time horizon and the target zone bands. this enables us to show how central bank intervention is endogenous to both the distance of the fundamental to the band and the underlying risk. we discuss how the credibility of the target zone is shaped by the set horizon and the degree of underlying risk, and we determine a minimum time at which the required parity can be reached. we prove that the interplay of the diffusive component and the destabilizing risk component can yield an endogenous regime shift characterized by a threshold level of risk above which the target zone ceases to exist. all the previous results cannot obtain by means of the standard gaussian and affine models. we recover by numerical simulations the different exchange rate densities established by the target zone literature.", "categories": "econ.gn q-fin.ec", "created": "2020-02-03", "updated": "", "authors": ["jean-louis arcand", "max-olivier hongler", "shekhar hari kumar", "daniele rinaldo"], "url": "https://arxiv.org/abs/2002.00948"}, {"title": "profit-oriented sales forecasting: a comparison of forecasting   techniques from a business perspective", "id": "2002.00949", "abstract": "choosing the technique that is the best at forecasting your data, is a problem that arises in any forecasting application. decades of research have resulted into an enormous amount of forecasting methods that stem from statistics, econometrics and machine learning (ml), which leads to a very difficult and elaborate choice to make in any forecasting exercise. this paper aims to facilitate this process for high-level tactical sales forecasts by comparing a large array of techniques for 35 times series that consist of both industry data from the coca-cola company and publicly available datasets. however, instead of solely focusing on the accuracy of the resulting forecasts, this paper introduces a novel and completely automated profit-driven approach that takes into account the expected profit that a technique can create during both the model building and evaluation process. the expected profit function that is used for this purpose, is easy to understand and adaptable to any situation by combining forecasting accuracy with business expertise. furthermore, we examine the added value of ml techniques, the inclusion of external factors and the use of seasonal models in order to ascertain which type of model works best in tactical sales forecasting. our findings show that simple seasonal time series models consistently outperform other methodologies and that the profit-driven approach can lead to selecting a different forecasting model.", "categories": "econ.em cs.lg stat.ml", "created": "2020-02-03", "updated": "", "authors": ["tine van calster", "filip van den bossche", "bart baesens", "wilfried lemahieu"], "url": "https://arxiv.org/abs/2002.00949"}, {"title": "quid pro quo allocations in production-inventory games", "id": "2002.00953", "abstract": "the concept of owen point, introduced in guardiola et al. (2009), is an appealing solution concept that for production-inventory games (pi-games) always belongs to their core. the owen point allows all the players in the game to operate at minimum cost but it does not take into account the cost reduction induced by essential players over their followers (fans). thus, it may be seen as an altruistic allocation for essential players what can be criticized. the aim this paper is two-fold: to study the structure and complexity of the core of pi-games and to introduce new core allocations for pi-games improving the weaknesses of the owen point. regarding the first goal, we advance further on the analysis of pi-games and we analyze its core structure and algorithmic complexity. specifically, we prove that the number of extreme points of the core of pi-games is exponential on the number of players. on the other hand, we propose and characterize a new core-allocation, the omega point, which compensates the essential players for their role on reducing the costs of their fans. moreover, we define another solution concept, the quid pro quo set (qpq-set) of allocations, which is based on the owen and omega points. among all the allocations in this set, we emphasize what we call the solomonic qpq allocation and we provide some necessary conditions for the coincidence of that allocation with the shapley value and the nucleolus.", "categories": "cs.gt econ.th", "created": "2020-02-03", "updated": "", "authors": ["luis guardiola", "ana meca", "justo puerto"], "url": "https://arxiv.org/abs/2002.00953"}, {"title": "rental housing spot markets: how online information exchanges can   supplement transacted-rents data", "id": "2002.01578", "abstract": "traditional us rental housing data sources such as the american community survey and the american housing survey report on the transacted market - what existing renters pay each month. they do not explicitly tell us about the spot market - i.e., the asking rents that current homeseekers must pay to acquire housing - though they are routinely used as a proxy. this study compares governmental data to millions of contemporaneous rental listings and finds that asking rents diverge substantially from these most recent estimates. conventional housing data understate current market conditions and affordability challenges, especially in cities with tight and expensive rental markets.", "categories": "econ.gn cs.cy q-fin.ec stat.ap", "created": "2020-02-04", "updated": "", "authors": ["geoff boeing", "jake wegmann", "junfeng jiao"], "url": "https://arxiv.org/abs/2002.01578"}, {"title": "sharpe ratio in high dimensions: cases of maximum out of sample,   constrained maximum, and optimal portfolio choice", "id": "2002.01800", "abstract": "in this paper, we analyze maximum sharpe ratio when the number of assets in a portfolio is larger than its time span. one obstacle in this large dimensional setup is the singularity of the sample covariance matrix of the excess asset returns. to solve this issue, we benefit from a technique called nodewise regression, which was developed by meinshausen and buhlmann (2006). it provides a sparse/weakly sparse and consistent estimate of the precision matrix, using the lasso method. we analyze three issues. one of the key results in our paper is that mean-variance efficiency for the portfolios in large dimensions is established. then tied to that result, we also show that the maximum out-of-sample sharpe ratio can be consistently estimated in this large portfolio of assets. furthermore, we provide convergence rates and see that the number of assets slow down the convergence up to a logarithmic factor. then, we provide consistency of maximum sharpe ratio when the portfolio weights add up to one, and also provide a new formula and an estimate for constrained maximum sharpe ratio. finally, we provide consistent estimates of the sharpe ratios of global minimum variance portfolio and markowitz's (1952) mean variance portfolio. in terms of assumptions, we allow for time series data. simulation and out-of-sample forecasting exercise shows that our new method performs well compared to factor and shrinkage based techniques.", "categories": "q-fin.pm econ.em math.st q-fin.st stat.ml stat.th", "created": "2020-02-05", "updated": "2020-06-29", "authors": ["mehmet caner", "marcelo medeiros", "gabriel vasconcelos"], "url": "https://arxiv.org/abs/2002.01800"}, {"title": "dependence-robust inference using resampled statistics", "id": "2002.02097", "abstract": "we develop inference procedures robust to general forms of weak dependence. these involve test statistics constructed by resampling data in a manner that does not depend on the unknown correlation structure of the data. the statistics are simple to compute and asymptotically normal under the weak requirement that the target parameter can be consistently estimated at the parametric rate. this requirement holds for regular estimators under many well-known forms of weak dependence and justifies the claim of dependence-robustness. we consider applications to settings with unknown or complicated forms of dependence, with various forms network dependence as leading examples. we develop tests for both moment equalities and inequalities.", "categories": "econ.em stat.me", "created": "2020-02-05", "updated": "2020-06-23", "authors": ["michael p. leung"], "url": "https://arxiv.org/abs/2002.02097"}, {"title": "on ridership and frequency", "id": "2002.02493", "abstract": "in 2018, bus ridership attained its lowest level since 1973. if transit agencies hope to reverse this trend, they must understand how their service allocation policies affect ridership. this paper is among the first to model ridership trends on a hyper-local level over time. a poisson fixed-effects model is developed to evaluate the ridership elasticity to frequency using passenger count data from portland, miami, minneapolis/st-paul, and atlanta between 2012 and 2018. in every agency, ridership is found to be elastic to frequency when observing the variation between individual route-segments at one point in time. in other words, the most frequent routes are already the most productive. when observing the variation within each route-segment over time, however, ridership is inelastic; each additional vehicle-trip is expected to generate less ridership than the average bus already on the route. in three of the four agencies, the elasticity is a decreasing function of prior frequency, meaning that low-frequency routes are the most sensitive to frequency change. this paper can help transit agencies anticipate the marginal effect of shifting service throughout the network. as the quality and availability of passenger count data improve, this paper can serve as the methodological basis to explore the dynamics of bus ridership.", "categories": "physics.soc-ph cs.si econ.em stat.ap", "created": "2020-02-06", "updated": "", "authors": ["simon berrebi", "taylor gibbs", "sanskruti joshi", "kari e watkins"], "url": "https://arxiv.org/abs/2002.02493"}, {"title": "all-pay auctions with different forfeits", "id": "2002.02599", "abstract": "in an auction each party bids a certain amount and the one which bids the highest is the winner. interestingly, auctions can also be used as models for other real-world systems. in an all pay auction all parties must pay a forfeit for bidding. in the most commonly studied all pay auction, parties forfeit their entire bid, and this has been considered as a model for expenditure on political campaigns. here we consider a number of alternative forfeits which might be used as models for different real-world competitions, such as preparing bids for defense or infrastructure contracts.", "categories": "econ.th", "created": "2020-02-06", "updated": "", "authors": ["benjamin kang", "james unwin"], "url": "https://arxiv.org/abs/2002.02599"}, {"title": "a polynomial algorithm for maxmin and minmax envy-free rent division on   a soft budget", "id": "2002.02966", "abstract": "the current practice of envy-free rent division, lead by the fair allocation website spliddit, is based on quasi-linear preferences. these preferences rule out agents' well documented financial constraints. to resolve this issue we consider piece-wise linear budget constrained preferences. these preferences admit differences in agents' marginal disutility of paying rent below and above a given reference, i.e., a soft budget. we construct a polynomial algorithm to calculate a maxmin utility envy-free allocation, and other related solutions, in this domain.", "categories": "cs.gt econ.th", "created": "2020-02-07", "updated": "", "authors": ["rodrigo a. velez"], "url": "https://arxiv.org/abs/2002.02966"}, {"title": "fairness and efficiency in cake-cutting with single-peaked preferences", "id": "2002.03174", "abstract": "we study the cake-cutting problem when agents have single-peaked preferences over the cake. we show that a recently proposed mechanism by wang-wu (2019) to obtain envy-free allocations can yield large welfare losses. using a simplifying assumption, we characterize all pareto optimal allocations, which have a simple structure: are peak-preserving and non-wasteful. finally, we provide simple alternative mechanisms that pareto dominate that of wang-wu, and which achieve envy-freeness or pareto optimality.", "categories": "cs.gt econ.th", "created": "2020-02-08", "updated": "2020-03-04", "authors": ["bhavook bhardwaj", "rajnish kumar", "josue ortega"], "url": "https://arxiv.org/abs/2002.03174"}, {"title": "asymptotically optimal control of a centralized dynamic matching market   with general utilities", "id": "2002.03205", "abstract": "we consider a matching market where buyers and sellers arrive according to independent poisson processes at the same rate and independently abandon the market if not matched after an exponential amount of time with the same mean. in this centralized market, the utility for the system manager from matching any buyer and any seller is a general random variable. we consider a sequence of systems indexed by $n$ where the arrivals in the $n^{\\mathrm{th}}$ system are sped up by a factor of $n$. we analyze two families of one-parameter policies: the population threshold policy immediately matches an arriving agent to its best available mate only if the number of mates in the system is above a threshold, and the utility threshold policy matches an arriving agent to its best available mate only if the corresponding utility is above a threshold. using an asymptotic fluid analysis of the two-dimensional markov process of buyers and sellers, we show that when the matching utility distribution is light-tailed, % (i.e., the expected value of the maximum of many random variables is a regularly varying function with $\\alpha=0$) the population threshold policy with threshold $\\frac{n}{\\ln n}$ is asymptotically optimal among all policies that make matches only at agent arrival epochs. in the heavy-tailed case% (i.e., $\\alpha\\in(0,1)$), we characterize the optimal threshold level for both policies. %although they do not attain the performance of our loose upper bound. we also study the utility threshold policy in an unbalanced matching market with heavy-tailed matching utilities, and find that the buyers and sellers have the same asymptotically optimal utility threshold.", "categories": "math.pr cs.si econ.em", "created": "2020-02-08", "updated": "", "authors": ["jose h. blanchet", "martin i. reiman", "viragh shah", "lawrence m. wein"], "url": "https://arxiv.org/abs/2002.03205"}, {"title": "all-pay auctions as models for trade wars and military annexation", "id": "2002.03492", "abstract": "we explore an application of all-pay auctions to model trade wars and territorial annexation. specifically, in the model we consider the expected resource, production, and aggressive (military/tariff) power are public information, but actual resource levels are private knowledge. we consider the resource transfer at the end of such a competition which deprives the weaker country of some fraction of its original resources. in particular, we derive the quasi-equilibria strategies for two country conflicts under different scenarios. this work is relevant for the ongoing us-china trade war, and the recent russian capture of crimea, as well as historical and future conflicts.", "categories": "econ.th", "created": "2020-02-09", "updated": "", "authors": ["benjamin kang", "james unwin"], "url": "https://arxiv.org/abs/2002.03492"}, {"title": "notes on a social transmission model with a continuum of agents", "id": "2002.03569", "abstract": "this note presents a simple overlapping-generations (olg) model of the transmission of a trait, such as a culture. initially, some fraction of agents carry the trait. in each time period, young agents are \"born\" and are influenced by some older agents. agents adopt the trait only if at least a certain number of their influencers have the trait. this influence may occur due to rational choice (e.g., because the young agents are playing a coordination game with old agents who are already committed to a strategy), or for some other reason. our interest is in how the process of social influence unfolds over time, and whether a trait will persist or die out. we characterize the dynamics of the fraction of active agents and relate the analysis to classic results on branching processes and random graphs.", "categories": "physics.soc-ph econ.th", "created": "2020-02-10", "updated": "2020-04-16", "authors": ["benjamin golub"], "url": "https://arxiv.org/abs/2002.03569"}, {"title": "markov switching", "id": "2002.03598", "abstract": "markov switching models are a popular family of models that introduces time-variation in the parameters in the form of their state- or regime-specific values. importantly, this time-variation is governed by a discrete-valued latent stochastic process with limited memory. more specifically, the current value of the state indicator is determined only by the value of the state indicator from the previous period, thus the markov property, and the transition matrix. the latter characterizes the properties of the markov process by determining with what probability each of the states can be visited next period, given the state in the current period. this setup decides on the two main advantages of the markov switching models. namely, the estimation of the probability of state occurrences in each of the sample periods by using filtering and smoothing methods and the estimation of the state-specific parameters. these two features open the possibility for improved interpretations of the parameters associated with specific regimes combined with the corresponding regime probabilities, as well as for improved forecasting performance based on persistent regimes and parameters characterizing them.", "categories": "econ.em", "created": "2020-02-10", "updated": "", "authors": ["yong song", "tomasz wo\u017aniak"], "url": "https://arxiv.org/abs/2002.03598"}, {"title": "the effect of weather conditions on fertilizer applications: a spatial   dynamic panel data analysis", "id": "2002.03922", "abstract": "given the extreme dependence of agriculture on weather conditions, this paper analyses the effect of climatic variations on this economic sector, by considering both a huge dataset and a flexible spatio-temporal model specification. in particular, we study the response of n-fertilizer application to abnormal weather conditions, while accounting for gdp as a control variable. the dataset consists of gridded data spanning over 21 years (1993--2013), while the methodological strategy makes use of a spatial dynamic panel data (sdpd) model that accounts for both space and time fixed effects, besides dealing with both space and time dependences. time-invariant short and long term effects, as well as time-varying marginal effects are also properly defined, revealing interesting results on the impact of both gdp and weather conditions on fertilizer utilizations. the analysis considers four macro-regions - europe, south america, south-east asia and africa - to allow for comparisons among different socio-economic societies. in addition to finding a good support for the existence of an environmental kuznets curve for fertilizer application, it shows peculiar responses of n-fertilization to deviations from normal conditions of moisture for each selected region, calling for ad hoc policy interventions.", "categories": "econ.em", "created": "2020-02-10", "updated": "", "authors": ["anna gloria bill\u00e8", "marco rogna"], "url": "https://arxiv.org/abs/2002.03922"}, {"title": "sequential monitoring of changes in housing prices", "id": "2002.04101", "abstract": "we propose a sequential monitoring scheme to find structural breaks in real estate markets. the changes in the real estate prices are modeled by a combination of linear and autoregressive terms. the monitoring scheme is based on a detector and a suitably chosen boundary function. if the detector crosses the boundary function, a structural break is detected. we provide the asymptotics for the procedure under the stability null hypothesis and the stopping time under the change point alternative. monte carlo simulation is used to show the size and the power of our method under several conditions. we study the real estate markets in boston, los angeles and at the national u.s. level. we find structural breaks in the markets, and we segment the data into stationary segments. it is observed that the autoregressive parameter is increasing but stays below 1.", "categories": "econ.em stat.ap stat.me", "created": "2020-02-10", "updated": "", "authors": ["lajos horv\u00e1th", "zhenya liu", "shanglin lu"], "url": "https://arxiv.org/abs/2002.04101"}, {"title": "identifiability and estimation of possibly non-invertible svarma models:   a new parametrisation", "id": "2002.04346", "abstract": "this paper deals with parameterisation, identifiability, and maximum likelihood (ml) estimation of possibly non-invertible structural vector autoregressive moving average (svarma) models driven by independent and non-gaussian shocks. we introduce a new parameterisation of the ma polynomial matrix based on the wiener-hopf factorisation (whf) and show that the model is identified in this parametrisation for a generic set in the parameter space (when certain just-identifying restrictions are imposed). when the svarma model is driven by gaussian errors, neither the static shock transmission matrix, nor the location of the determinantal zeros of the ma polynomial matrix can be identified without imposing further identifying restrictions on the parameters. we characterise the classes of observational equivalence with respect to second moment information at different stages of the modelling process. subsequently, cross-sectional and temporal independence and non-gaussianity of the shocks is used to solve these identifiability problems and identify the true root location of the ma polynomial matrix as well as the static shock transmission matrix (up to permutation and scaling).typically imposed identifying restrictions on the shock transmission matrix as well as on the determinantal root location are made testable. furthermore, we provide low level conditions for asymptotic normality of the ml estimator. the estimation procedure is illustrated with various examples from the economic literature and implemented as r-package.", "categories": "econ.em", "created": "2020-02-11", "updated": "", "authors": ["bernd funovits"], "url": "https://arxiv.org/abs/2002.04346"}, {"title": "the dimension of the set of causal solutions of linear multivariate   rational expectations models", "id": "2002.04369", "abstract": "this paper analyses the number of free parameters and solutions of the structural difference equation obtained from a linear multivariate rational expectations model. first, it is shown that the number of free parameters depends on the structure of the zeros at zero of a certain matrix polynomial of the structural difference equation and the number of inputs of the rational expectations model. second, the implications of requiring that some components of the endogenous variables be predetermined are analysed. third, a condition for existence and uniqueness of a causal stationary solution is given.", "categories": "econ.em", "created": "2020-02-11", "updated": "", "authors": ["bernd funovits"], "url": "https://arxiv.org/abs/2002.04369"}, {"title": "generalized poisson difference autoregressive processes", "id": "2002.04470", "abstract": "this paper introduces a new stochastic process with values in the set z of integers with sign. the increments of process are poisson differences and the dynamics has an autoregressive structure. we study the properties of the process and exploit the thinning representation to derive stationarity conditions and the stationary distribution of the process. we provide a bayesian inference method and an efficient posterior approximation procedure based on monte carlo. numerical illustrations on both simulated and real data show the effectiveness of the proposed inference.", "categories": "stat.me econ.em", "created": "2020-02-11", "updated": "", "authors": ["giulia carallo", "roberto casarin", "christian p. robert"], "url": "https://arxiv.org/abs/2002.04470"}, {"title": "ramsey optimal policy versus multiple equilibria with fiscal and   monetary interactions", "id": "2002.04508", "abstract": "we consider a frictionless constant endowment economy based on leeper (1991). in this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc fiscal rule, there are two equilibria. one has active monetary policy and passive fiscal policy, while the other has passive monetary policy and active fiscal policy. we consider an extended setup in which the policy maker minimizes a loss function under quasi-commitment, as in schaumburg and tambalotti (2007). under this formulation there exists a unique ramsey equilibrium, with an interest rate peg and a passive fiscal policy. we thank john p. conley, luis de araujo and one referree for their very helpful comments.", "categories": "econ.gn math.oc q-fin.ec", "created": "2020-02-11", "updated": "", "authors": ["jean-bernard chatelain", "kirsten ralf"], "url": "https://arxiv.org/abs/2002.04508"}, {"title": "fast complete algorithm for multiplayer nash equilibrium", "id": "2002.04734", "abstract": "we describe a new complete algorithm for computing nash equilibrium in multiplayer general-sum games, based on a quadratically-constrained feasibility program formulation. we demonstrate that the algorithm runs significantly faster than the prior fastest complete algorithm on several game classes previously studied and that its runtimes even outperform the best incomplete algorithms.", "categories": "cs.gt cs.ai cs.ma econ.th math.oc", "created": "2020-02-11", "updated": "2020-07-27", "authors": ["sam ganzfried"], "url": "https://arxiv.org/abs/2002.04734"}, {"title": "bifurcations in economic growth model with distributed time delay   transformed to ode", "id": "2002.05016", "abstract": "we consider the model of economic growth with time delayed investment function. assuming the investment is time distributed we can use the linear chain trick technique to transform delay differential equation system to equivalent system of ordinary differential system (ode). the time delay parameter is a mean time delay of gamma distribution. we reduce the system with distribution delay to both three and four-dimensional odes. we study the hopf bifurcation in these systems with respect to two parameters: the time delay parameter and the rate of growth parameter. we derive the results from the analytical as well as numerical investigations. from the former we obtain the sufficient criteria on the existence and stability of a limit cycle solution through the hopf bifurcation. in numerical studies with the dana and malgrange investment function we found two hopf bifurcations with respect to the rate growth parameter and detect the existence of stable long-period cycles in the economy. we find that depending on the time delay and adjustment speed parameters the range of admissible values of the rate of growth parameter breaks down into three intervals. first we have stable focus, then the limit cycle and again the stable solution with two hopf bifurcations. such behaviour appears for some middle interval of admissible range of values of the rate of growth parameter.", "categories": "econ.th econ.gn math.ds q-fin.ec", "created": "2020-02-12", "updated": "", "authors": ["luca guerrini", "adam krawiec", "marek szydlowski"], "url": "https://arxiv.org/abs/2002.05016"}, {"title": "efficient policy learning from surrogate-loss classification reductions", "id": "2002.05153", "abstract": "recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. but, efficient policy evaluation need not yield efficient estimation of policy parameters. we consider the estimation problem given by a weighted surrogate-loss classification reduction of policy learning with any score function, either direct, inverse-propensity weighted, or doubly robust. we show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. we draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semiparametric model. in light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. we propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically.", "categories": "cs.lg econ.em math.st stat.ml stat.th", "created": "2020-02-12", "updated": "", "authors": ["andrew bennett", "nathan kallus"], "url": "https://arxiv.org/abs/2002.05153"}, {"title": "a hierarchy of limitations in machine learning", "id": "2002.05193", "abstract": "\"all models are wrong, but some are useful\", wrote george e. p. box (1979). machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. this paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. the limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.", "categories": "cs.cy cs.lg econ.em math.st stat.ml stat.th", "created": "2020-02-12", "updated": "2020-02-29", "authors": ["momin m. malik"], "url": "https://arxiv.org/abs/2002.05193"}, {"title": "decreasing market value of variable renewables is a result of policy,   not variability", "id": "2002.05209", "abstract": "although recent studies have shown that electricity systems with shares of wind and solar above 80% can be affordable, economists have raised concerns about market integration. correlated generation from variable renewable sources depresses market prices, which can cause wind and solar to cannibalize their own revenues and prevent them from covering their costs from the market. this cannibalization appears to set limits on the integration of wind and solar, and thus contradict studies that show that high shares are cost effective. here we show from theory and with numerical examples how policies interact with prices, revenue and costs for renewable electricity systems. the decline in average revenue seen in some recent literature is due to an implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas. if instead the driving policy is a carbon dioxide cap or tax, wind and solar shares can rise without cannibalising their own market revenue, even at penetrations of wind and solar above 80%. policy is thus the primary factor driving lower market values; the variability of wind and solar is only a secondary factor that accelerates the decline if they are subsidised. the strong dependence of market value on the policy regime means that market value needs to be used with caution as a measure of market integration.", "categories": "q-fin.gn econ.gn math.oc q-fin.ec", "created": "2020-02-07", "updated": "", "authors": ["t. brown", "l. reichenberg"], "url": "https://arxiv.org/abs/2002.05209"}, {"title": "the multiplayer colonel blotto game", "id": "2002.05240", "abstract": "we initiate the study of the natural multiplayer generalization of the classic continuous colonel blotto game. the two-player blotto game, introduced by borel as a model of resource competition across $n$ simultaneous fronts, has been studied extensively for a century and seen numerous applications throughout the social sciences. our work defines the multiplayer colonel blotto game and derives nash equilibria for various settings of $k$ (number of players) and $n$. we also introduce a \"boolean\" version of blotto that becomes interesting in the multiplayer setting. the main technical difficulty of our work, as in the two-player theoretical literature, is the challenge of coupling various marginal distributions into a joint distribution satisfying a strict sum constraint. in contrast to previous works in the continuous setting, we derive our couplings algorithmically in the form of efficient sampling algorithms.", "categories": "cs.gt econ.th", "created": "2020-02-12", "updated": "2020-02-24", "authors": ["enric boix-adser\u00e0", "benjamin l. edelman", "siddhartha jayanti"], "url": "https://arxiv.org/abs/2002.05240"}, {"title": "bounds on direct and indirect effects under treatment/mediator   endogeneity and outcome attrition", "id": "2002.05253", "abstract": "causal mediation analysis aims at disentangling a treatment effect into an indirect mechanism operating through an intermediate outcome or mediator, as well as the direct effect of the treatment on the outcome of interest. however, the evaluation of direct and indirect effects is frequently complicated by non-ignorable selection into the treatment and/or mediator, even after controlling for observables, as well as sample selection/outcome attrition. we propose a method for bounding direct and indirect effects in the presence of such complications using a method that is based on a sequence of linear programming problems. considering inverse probability weighting by propensity scores, we compute the weights that would yield identification in the absence of complications and perturb them by an entropy parameter reflecting a specific amount of propensity score misspecification to set-identify the effects of interest. we apply our method to data from the national longitudinal survey of youth 1979 to derive bounds on the explained and unexplained components of a gender wage gap decomposition that is likely prone to non-ignorable mediator selection and outcome attrition.", "categories": "econ.em stat.me", "created": "2020-02-12", "updated": "2020-05-03", "authors": ["martin huber", "luk\u00e1\u0161 laff\u00e9rs"], "url": "https://arxiv.org/abs/2002.05253"}, {"title": "adaptive experimental design for efficient treatment effect estimation", "id": "2002.05308", "abstract": "the goal of many scientific experiments including a/b testing is to estimate the average treatment effect (ate), which is defined as the difference between the expected outcomes of two or more treatments. in this paper, we consider a situation where an experimenter can assign a treatment to research subjects sequentially. in adaptive experimental design, the experimenter is allowed to change the probability of assigning a treatment using past observations for estimating the ate efficiently. however, with this approach, it is difficult to apply a standard statistical method to construct an estimator because the observations are not independent and identically distributed. we thus propose an algorithm for efficient experiments with estimators constructed from dependent samples. we also introduce a sequential testing framework using the proposed estimator. to justify our proposed approach, we provide finite and infinite sample analyses. finally, we experimentally show that the proposed algorithm exhibits preferable performance.", "categories": "stat.ml cs.lg econ.em", "created": "2020-02-12", "updated": "2020-09-24", "authors": ["masahiro kato", "takuya ishihara", "junya honda", "yusuke narita"], "url": "https://arxiv.org/abs/2002.05308"}, {"title": "top of the batch: interviews and the match", "id": "2002.05323", "abstract": "most doctors in the nrmp are matched to one of their most-preferred internship programs. since various surveys indicate similarities across doctors' preferences, this suggests a puzzle. how can nearly everyone get a position in a highly-desirable program when positions in each program are scarce? we provide one possible explanation for this puzzle. we show that the patterns observed in the nrmp data may be an artifact of the interview process that precedes the match. our analysis highlights the importance of interactions occurring outside of a matching clearinghouse for resulting outcomes, and casts doubts on analysis of clearinghouses that take reported preferences at face value.", "categories": "econ.gn q-fin.ec", "created": "2020-02-12", "updated": "", "authors": ["federico echenique", "ruy gonzalez", "alistair wilson", "leeat yariv"], "url": "https://arxiv.org/abs/2002.05323"}, {"title": "long-term prediction intervals of economic time series", "id": "2002.05384", "abstract": "we construct long-term prediction intervals for time-aggregated future values of univariate economic time series. we propose computational adjustments of the existing methods to improve coverage probability under a small sample constraint. a pseudo-out-of-sample evaluation shows that our methods perform at least as well as selected alternative methods based on model-implied bayesian approaches and bootstrapping. our most successful method yields prediction intervals for eight macroeconomic indicators over a horizon spanning several decades.", "categories": "econ.em", "created": "2020-02-13", "updated": "", "authors": ["marek chudy", "sayar karmakar", "wei biao wu"], "url": "https://arxiv.org/abs/2002.05384"}, {"title": "experimental design in two-sided platforms: an analysis of bias", "id": "2002.05670", "abstract": "we develop an analytical framework to study experimental design in two-sided marketplaces. many of these experiments exhibit interference, where an intervention applied to one market participant influences the behavior of another participant. this interference leads to biased estimates of the treatment effect of the intervention. we develop a stochastic market model and associated mean field limit to capture dynamics in such experiments, and use our model to investigate how the performance of different designs and estimators is affected by marketplace interference effects. platforms typically use two common experimental designs: demand-side (\"customer\") randomization (cr) and supply-side (\"listing\") randomization (lr), along with their associated estimators. we show that good experimental design depends on market balance: in highly demand-constrained markets, cr is unbiased, while lr is biased; conversely, in highly supply-constrained markets, lr is unbiased, while cr is biased. we also introduce and study a novel experimental design based on two-sided randomization (tsr) where both customers and listings are randomized to treatment and control. we show that appropriate choices of tsr designs can be unbiased in both extremes of market balance, while yielding relatively low bias in intermediate regimes of market balance.", "categories": "stat.me econ.em", "created": "2020-02-13", "updated": "2020-08-20", "authors": ["ramesh johari", "hannah li", "inessa liskovich", "gabriel weintraub"], "url": "https://arxiv.org/abs/2002.05670"}, {"title": "economic complexity of prefectures in japan", "id": "2002.05785", "abstract": "every nation prioritizes the inclusive economic growth and development of all regions. however, we observe that economic activities are clustered in space, which results in a disparity in per-capita income among different regions. a complexity-based method was proposed by hidalgo and hausmann [pnas 106, 10570-10575 (2009)] to explain the large gaps in per-capita income across countries. although there have been extensive studies on countries' economic complexity using international export data, studies on economic complexity at the regional level are relatively less studied. here, we study the industrial sector complexity of prefectures in japan based on the basic information of more than one million firms. we aggregate the data as a bipartite network of prefectures and industrial sectors. we decompose the bipartite network as a prefecture-prefecture network and sector-sector network, which reveals the relationships among them. similarities among the prefectures and among the sectors are measured using a metric. from these similarity matrices, we cluster the prefectures and sectors using the minimal spanning tree technique.the computed economic complexity index from the structure of the bipartite network shows a high correlation with macroeconomic indicators, such as per-capita gross prefectural product and prefectural income per person. we argue that this index reflects the present economic performance and hidden potential of the prefectures for future growth.", "categories": "econ.gn q-fin.ec", "created": "2020-02-11", "updated": "2020-08-31", "authors": ["abhijit chakraborty", "hiroyasu inoue", "yoshi fujiwara"], "url": "https://arxiv.org/abs/2002.05785"}, {"title": "the effect of network adoption subsidies: evidence from digital traces   in rwanda", "id": "2002.05791", "abstract": "governments spend billions of dollars subsidizing the adoption of different goods. however, it is difficult to gauge whether those goods are resold, or are valued by their ultimate recipients. this project studies a program to subsidize the adoption of mobile phones in one of the poorest countries in the world. rwanda subsidized the equivalent of 8% of the stock of mobile phones for select rural areas. we analyze the program using 5.3 billion transaction records from the dominant mobile phone network. transaction records reveal where and how much subsidized handsets were ultimately used, and indicators of resale. some subsidized handsets drifted from the rural areas where they were allocated to urban centers, but the subsidized handsets were used as much as handsets purchased at retail prices, suggesting they were valued. recipients are similar to those who paid for phones, but are highly connected to each other. we then simulate welfare effects using a network demand system that accounts for how each person's adoption affects the rest of the network. spillovers are substantial: 73-76% of the operator revenue generated by the subsidy comes from nonrecipients. we compare the enacted subsidy program to counterfactual targeting based on different network heuristics.", "categories": "econ.gn q-fin.ec", "created": "2020-02-13", "updated": "", "authors": ["daniel bj\u00f6rkegren", "burak ceyhun karaca"], "url": "https://arxiv.org/abs/2002.05791"}, {"title": "fairness through experimentation: inequality in a/b testing as an   approach to responsible design", "id": "2002.05819", "abstract": "as technology continues to advance, there is increasing concern about individuals being left behind. many businesses are striving to adopt responsible design practices and avoid any unintended consequences of their products and services, ranging from privacy vulnerabilities to algorithmic bias. we propose a novel approach to fairness and inclusiveness based on experimentation. we use experimentation because we want to assess not only the intrinsic properties of products and algorithms but also their impact on people. we do this by introducing an inequality approach to a/b testing, leveraging the atkinson index from the economics literature. we show how to perform causal inference over this inequality measure. we also introduce the concept of site-wide inequality impact, which captures the inclusiveness impact of targeting specific subpopulations for experiments, and show how to conduct statistical inference on this impact. we provide real examples from linkedin, as well as an open-source, highly scalable implementation of the computation of the atkinson index and its variance in spark/scala. we also provide over a year's worth of learnings -- gathered by deploying our method at scale and analyzing thousands of experiments -- on which areas and which kinds of product innovations seem to inherently foster fairness through inclusiveness.", "categories": "cs.si econ.em", "created": "2020-02-13", "updated": "", "authors": ["guillaume saint-jacques", "amir sepehri", "nicole li", "igor perisic"], "url": "https://arxiv.org/abs/2002.05819"}, {"title": "the structure of two-valued strategy-proof social choice functions with   indifference", "id": "2002.06341", "abstract": "we give a structure theorem for all coalitionally strategy-proof social choice functions whose range is a subset of cardinality two of a given larger set of alternatives.   we provide this in the case where the voters/agents are allowed to express indifference and the domain consists of profiles of preferences over a society of arbitrary cardinality. the theorem, that takes the form of a representation formula, can be used to construct all functions under consideration.", "categories": "econ.th", "created": "2020-02-15", "updated": "2020-07-03", "authors": ["achille basile", "surekha rao", "k. p. s. bhaskara rao"], "url": "https://arxiv.org/abs/2002.06341"}, {"title": "an optimal mechanism charging for priority in a queue", "id": "2002.06533", "abstract": "we derive a revenue-maximizing scheme that charges customers who are homogeneous with respect to their waiting cost parameter for a random fee in order to become premium customers. this scheme incentivizes all customers to purchase priority, each at his/her drawn price. we also design a revenue-maximizing scheme for the case where customers are heterogeneous with respect to their waiting cost parameter. now lower cost parameter customers are encouraged to join the premium class at a low price: given that, those with high cost parameter would be willing to pay even more for this privilege.", "categories": "econ.th cs.gt", "created": "2020-02-16", "updated": "", "authors": ["moshe haviv", "eyal winter"], "url": "https://arxiv.org/abs/2002.06533"}, {"title": "convex combinatorial auction of pipeline network capacities", "id": "2002.06554", "abstract": "in this paper we propose a mechanism for the allocation of pipeline capacities, assuming that the participants bidding for capacities do have subjective evaluation of various network routes. the proposed mechanism is based on the concept of bidding for route-quantity pairs. each participant defines a limited number of routes and places multiple bids, corresponding to various quantities, on each of these routes. the proposed mechanism assigns a convex combination of the submitted bids to each participant, thus its called convex combinatorial auction. the capacity payments in the proposed model are determined according to the vickrey-clarke-groves principle. we compare the efficiency of the proposed algorithm with a simplified model of the method currently used for pipeline capacity allocation in the eu (simultaneous ascending clock auction of pipeline capacities) via simulation, according to various measures, such as resulting utility of players, utilization of network capacities, total income of the auctioneer and fairness.", "categories": "econ.th", "created": "2020-02-16", "updated": "", "authors": ["d\u00e1vid csercsik"], "url": "https://arxiv.org/abs/2002.06554"}, {"title": "synchronization of endogenous business cycles", "id": "2002.06555", "abstract": "comovement of economic activity across sectors and countries is a defining feature of business cycles. however, standard models that attribute comovement to propagation of exogenous shocks struggle to generate a level of comovement that is as high as in the data. in this paper, we consider models that produce business cycles endogenously, through some form of non-linear dynamics---limit cycles or chaos. these models generate stronger comovement, because they combine shock propagation with synchronization of endogenous dynamics. in particular, we study a demand-driven model in which business cycles emerge from strategic complementarities across sectors in different countries, synchronizing their oscillations through input-output linkages. we first use a combination of analytical methods and extensive numerical simulations to establish a number of theoretical results. we show that the importance that sectors or countries have in setting the common frequency of oscillations depends on their eigenvector centrality in the input-output network, and we develop an eigendecomposition that explores the interplay between non-linear dynamics, shock propagation and network structure. we then calibrate our model to data on 27 sectors and 17 countries, showing that synchronization indeed produces stronger comovement, giving more flexibility to match the data.", "categories": "econ.gn nlin.ao q-fin.ec q-fin.gn", "created": "2020-02-16", "updated": "", "authors": ["marco pangallo"], "url": "https://arxiv.org/abs/2002.06555"}, {"title": "simple, credible, and approximately-optimal auctions", "id": "2002.06702", "abstract": "we identify the first static credible mechanism for multi-item additive auctions that achieves a constant factor of the optimal revenue. this is one instance of a more general framework for designing two-part tariff auctions, adapting the duality framework of cai et al [cdw16]. given a (not necessarily incentive compatible) auction format $a$ satisfying certain technical conditions, our framework augments the auction with a personalized entry fee for each bidder, which must be paid before the auction can be accessed. these entry fees depend only on the prior distribution of bidder types, and in particular are independent of realized bids. our framework can be used with many common auction formats, such as simultaneous first-price, simultaneous second-price, and simultaneous all-pay auctions. if all-pay auctions are used, we prove that the resulting mechanism is credible in the sense that the auctioneer cannot benefit by deviating from the stated mechanism after observing agent bids. if second-price auctions are used, we obtain a truthful $o(1)$-approximate mechanism with fixed entry fees that are amenable to tuning via online learning techniques. our results for first price and all-pay are the first revenue guarantees of non-truthful mechanisms in multi-dimensional environments; an open question in the literature [rst17].", "categories": "cs.gt econ.th", "created": "2020-02-16", "updated": "2020-06-16", "authors": ["constantinos daskalakis", "maxwell fishelson", "brendan lucier", "vasilis syrgkanis", "santhoshini velusamy"], "url": "https://arxiv.org/abs/2002.06702"}, {"title": "from matching with diversity constraints to matching with regional   quotas", "id": "2002.06748", "abstract": "in the past few years, several new matching models have been proposed and studied that take into account complex distributional constraints. relevant lines of work include (1) school choice with diversity constraints where students have (possibly overlapping) types and (2) hospital-doctor matching where various regional quotas are imposed. in this paper, we present a polynomial-time reduction to transform an instance of (1) to an instance of (2) and we show how the feasibility and stability of corresponding matchings are preserved under the reduction. our reduction provides a formal connection between two important strands of work on matching with distributional constraints. we then apply the reduction in two ways. firstly, we show that it is np-complete to check whether a feasible and stable outcome for (1) exists. due to our reduction, these np-completeness results carry over to setting (2). in view of this, we help unify some of the results that have been presented in the literature. secondly, if we have positive results for (2), then we have corresponding results for (1). one key conclusion of our results is that further developments on axiomatic and algorithmic aspects of hospital-doctor matching with regional quotas will result in corresponding results for school choice with diversity constraints.", "categories": "cs.gt cs.ai econ.th", "created": "2020-02-16", "updated": "", "authors": ["haris aziz", "serge gaspers", "zhaohong sun", "toby walsh"], "url": "https://arxiv.org/abs/2002.06748"}, {"title": "fair prediction with endogenous behavior", "id": "2002.07147", "abstract": "there is increasing regulatory interest in whether machine learning algorithms deployed in consequential domains (e.g. in criminal justice) treat different demographic groups \"fairly.\" however, there are several proposed notions of fairness, typically mutually incompatible. using criminal justice as an example, we study a model in which society chooses an incarceration rule. agents of different demographic groups differ in their outside options (e.g. opportunity for legal employment) and decide whether to commit crimes. we show that equalizing type i and type ii errors across groups is consistent with the goal of minimizing the overall crime rate; other popular notions of fairness are not.", "categories": "econ.th cs.ai cs.gt cs.lg econ.em", "created": "2020-02-18", "updated": "", "authors": ["christopher jung", "sampath kannan", "changhwa lee", "mallesh m. pai", "aaron roth", "rakesh vohra"], "url": "https://arxiv.org/abs/2002.07147"}, {"title": "satellite reveals age and extent of oil palm plantations in southeast   asia", "id": "2002.07163", "abstract": "in recent decades, global oil palm production has shown an abrupt increase, with almost 90% produced in southeast asia alone. monitoring oil palm is largely based on national surveys and inventories or one-off mapping studies. however, they do not provide detailed spatial extent or timely updates and trends in oil palm expansion or age. palm oil yields vary significantly with plantation age, which is critical for landscape-level planning. here we show the extent and age of oil palm plantations for the year 2017 across southeast asia using remote sensing. satellites reveal a total of 11.66 (+/- 2.10) million hectares (mha) of plantations with more than 45% located in sumatra. plantation age varies from ~7 years in kalimantan to ~13 in insular malaysia. more than half the plantations on kalimantan are young (<7 years) and not yet in full production compared to insular malaysia where 45% of plantations are older than 15 years, with declining yields. for the first time, these results provide a consistent, independent, and transparent record of oil palm plantation extent and age structure, which are complementary to national statistics.", "categories": "econ.gn q-fin.ec", "created": "2020-02-17", "updated": "", "authors": ["olha danylo", "johannes pirker", "guido lemoine", "guido ceccherini", "linda see", "ian mccallum", "n/a hadi", "florian kraxner", "fr\u00e9d\u00e9ric achard", "steffen fritz"], "url": "https://arxiv.org/abs/2002.07163"}, {"title": "how do expectations affect learning about fundamentals? some   experimental evidence", "id": "2002.07229", "abstract": "individuals' output often depends not just on their ability and actions, but also on external factors or fundamentals, whose effect they cannot separately identify. at the same time, many individuals have incorrect beliefs about their own ability. heidhues et al. (2018) characterise overconfident and underconfident individuals' equilibrium beliefs and learning process in these situations. they argue overconfident individuals will act sub-optimally because of how they learn. we carry out the first experimental test of their theory. subjects take incorrectly marked tests, and we measure how they learn about the marker's accuracy over time. we use machine learning to identify heterogeneous effects. overconfident subjects have lower beliefs about the fundamental, as heidhues et al. predict, and thus would make sub-optimal decisions. but we find no evidence it is because of how they learn.", "categories": "econ.gn q-fin.ec", "created": "2020-02-17", "updated": "2020-02-19", "authors": ["kieran marray", "nikhil krishna", "jarel tang"], "url": "https://arxiv.org/abs/2002.07229"}, {"title": "double/debiased machine learning for dynamic treatment effects", "id": "2002.07285", "abstract": "we consider the estimation of treatment effects in settings when multiple treatments are assigned over time and treatments can have a causal effect on future outcomes. we formulate the problem as a linear state space markov process with a high dimensional state and propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments. our method allows the use of arbitrary machine learning methods to control for the high dimensional state, subject to a mean square error guarantee, while still allowing parametric estimation and construction of confidence intervals for the dynamic treatment effect parameters of interest. our method is based on a sequential regression peeling process, which we show can be equivalently interpreted as a neyman orthogonal moment estimator. this allows us to show root-n asymptotic normality of the estimated causal effects.", "categories": "econ.em cs.lg stat.ml", "created": "2020-02-17", "updated": "2020-06-12", "authors": ["greg lewis", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/2002.07285"}, {"title": "dynamic reserve prices for repeated auctions: learning from bids", "id": "2002.07331", "abstract": "a large fraction of online advertisement is sold via repeated second price auctions. in these auctions, the reserve price is the main tool for the auctioneer to boost revenues. in this work, we investigate the following question: can changing the reserve prices based on the previous bids improve the revenue of the auction, taking into account the long-term incentives and strategic behavior of the bidders? we show that if the distribution of the valuations is known and satisfies the standard regularity assumptions, then the optimal mechanism has a constant reserve. however, when there is uncertainty in the distribution of the valuations, previous bids can be used to learn the distribution of the valuations and to update the reserve price. we present a simple, approximately incentive-compatible, and asymptotically optimal dynamic reserve mechanism that can significantly improve the revenue over the best static reserve.   the paper is from july 2014 (our submission to wine 2014), posted later here on the arxiv to complement the 1-page abstract in the wine 2014 proceedings.", "categories": "cs.gt econ.th", "created": "2020-02-17", "updated": "", "authors": ["yash kanoria", "hamid nazerzadeh"], "url": "https://arxiv.org/abs/2002.07331"}, {"title": "hopf bifurcation from new-keynesian taylor rule to ramsey optimal policy", "id": "2002.07479", "abstract": "this paper compares different implementations of monetary policy in a new-keynesian setting. we can show that a shift from ramsey optimal policy under short-term commitment (based on a negative feedback mechanism) to a taylor rule (based on a positive feedback mechanism) corresponds to a hopf bifurcation with opposite policy advice and a change of the dynamic properties. this bifurcation occurs because of the ad hoc assumption that interest rate is a forward-looking variable when policy targets (inflation and output gap) are forward-looking variables in the new-keynesian theory.", "categories": "econ.em math.oc", "created": "2020-02-18", "updated": "", "authors": ["jean-bernard chatelain", "kirsten ralf"], "url": "https://arxiv.org/abs/2002.07479"}, {"title": "market power in convex hull pricing", "id": "2002.07595", "abstract": "the start up costs in many kinds of generators lead to complex cost structures, which in turn yield severe market loopholes in the locational marginal price (lmp) scheme. convex hull pricing (a.k.a. extended lmp) is proposed to improve the market efficiency by providing the minimal uplift payment to the generators. in this letter, we consider a stylized model where all generators share the same generation capacity. we analyze the generators' possible strategic behaviors in such a setting, and then propose an index for market power quantification in the convex hull pricing schemes.", "categories": "math.oc cs.ce econ.gn q-fin.ec", "created": "2020-02-15", "updated": "", "authors": ["jian sun", "chenye wu"], "url": "https://arxiv.org/abs/2002.07595"}, {"title": "vat compliance incentives", "id": "2002.07862", "abstract": "in this work i clarify vat evasion incentives through a game theoretical approach. traditionally, evasion has been linked to the decreasing risk aversion in higher revenues (allingham and sandmo (1972), cowell (1985) (1990)). i claim tax evasion to be a rational choice when compliance is stochastically more expensive than evading, even in absence of controls and sanctions. i create a framework able to measure the incentives for taxpayers to comply. the incentives here are deductions of specific vat documented expenses from the income tax. the issue is very well known and deduction policies at work in many countries. the aim is to compute the right parameters for each precise class of taxpayers. vat evasion is a collusive conduct between the two counterparts of the transaction. i therefore first explore the convenience for the two private counterparts to agree on the joint evasion and to form a coalition. crucial is that compliance incentives break the agreement among the transaction participants' coalition about evading. the game solution leads to boundaries for marginal tax rates or deduction percentages, depending on parameters, able to create incentives to comply the stylized example presented here for vat policies, already in use in many countries, is an attempt to establish a more general method for tax design, able to make compliance the \"dominant strategy\", satisfying the \"outside option\" constraint represented by evasion, even in absence of audit and sanctions. the theoretical results derived here can be easily applied to real data for precise tax design engineering.", "categories": "econ.th", "created": "2020-02-18", "updated": "", "authors": ["maria-augusta miceli"], "url": "https://arxiv.org/abs/2002.07862"}, {"title": "the interconnectedness of the economic content in the speeches of the us   presidents", "id": "2002.07880", "abstract": "the speeches stated by influential politicians can have a decisive impact on the future of a country. in particular, the economic content of such speeches affects the economy of countries and their financial markets. for this reason, we examine a novel dataset containing the economic content of 951 speeches stated by 45 us presidents from george washington (april 1789) to donald trump (february 2017). in doing so, we use an economic glossary carried out by means of text mining techniques. the goal of our study is to examine the structure of significant interconnections within a network obtained from the economic content of presidential speeches. in such a network, nodes are represented by talks and links by values of cosine similarity, the latter computed using the occurrences of the economic terms in the speeches. the resulting network displays a peculiar structure made up of a core (i.e. a set of highly central and densely connected nodes) and a periphery (i.e. a set of non-central and sparsely connected nodes). the presence of different economic dictionaries employed by the presidents characterize the core-periphery structure. the presidents' talks belonging to the network's core share the usage of generic (non-technical) economic locutions like \"interest\" or \"trade\". while the use of more technical and less frequent terms characterizes the periphery (e.g. \"yield\" ). furthermore, the speeches close in time share a common economic dictionary. these results together with the economics glossary usages during the us periods of boom and crisis provide unique insights on the economic content relationships among presidents' speeches.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-02-18", "updated": "", "authors": ["matteo cinelli", "valerio ficcadenti", "jessica riccioni"], "url": "https://arxiv.org/abs/2002.07880"}, {"title": "tourism demand forecasting with tourist attention: an ensemble deep   learning approach", "id": "2002.07964", "abstract": "the large amount of tourism-related data presents a series of challenges for tourism demand forecasting, including data deficiencies, multicollinearity and long calculation times. a bagging-based multivariate ensemble deep learning approach integrating stacked autoencoders and kernel-based extreme learning machines (b-sake) is proposed to address these challenges in this study. we forecast tourist arrivals in beijing from four countries by adopting historical data on tourist arrivals in beijing, economic indicators and online tourist behavior variables. the results from the cases of four origin countries suggest that our proposed b-sake approach outperforms than benchmark models in terms of horizontal accuracy, directional accuracy and statistical significance. both bagging and stacked autoencoder can improve the forecasting performance of the models. moreover, the forecasting performance of the models is evaluated with consistent results by means of the multi-step-ahead forecasting scheme.", "categories": "stat.ap cs.lg econ.em", "created": "2020-02-18", "updated": "2020-03-10", "authors": ["shaolong sun", "yanzhao li", "shouyang wang", "ju-e guo"], "url": "https://arxiv.org/abs/2002.07964"}, {"title": "seasonal and trend forecasting of tourist arrivals: an adaptive   multiscale ensemble learning approach", "id": "2002.08021", "abstract": "the accurate seasonal and trend forecasting of tourist arrivals is a very challenging task. in the view of the importance of seasonal and trend forecasting of tourist arrivals, and limited research work paid attention to these previously. in this study, a new adaptive multiscale ensemble (ame) learning approach incorporating variational mode decomposition (vmd) and least square support vector regression (lssvr) is developed for short-, medium-, and long-term seasonal and trend forecasting of tourist arrivals. in the formulation of our developed ame learning approach, the original tourist arrivals series are first decomposed into the trend, seasonal and remainders volatility components. then, the arima is used to forecast the trend component, the sarima is used to forecast seasonal component with a 12-month cycle, while the lssvr is used to forecast remainder volatility components. finally, the forecasting results of the three components are aggregated to generate an ensemble forecasting of tourist arrivals by the lssvr based nonlinear ensemble approach. furthermore, a direct strategy is used to implement multi-step-ahead forecasting. taking two accuracy measures and the diebold-mariano test, the empirical results demonstrate that our proposed ame learning approach can achieve higher level and directional forecasting accuracy compared with other benchmarks used in this study, indicating that our proposed approach is a promising model for forecasting tourist arrivals with high seasonality and volatility.", "categories": "stat.ap cs.lg econ.em", "created": "2020-02-19", "updated": "2020-03-10", "authors": ["shaolong suna", "dan bi", "ju-e guo", "shouyang wang"], "url": "https://arxiv.org/abs/2002.08021"}, {"title": "the cointegrated var without unit roots: representation theory and   asymptotics", "id": "2002.08092", "abstract": "it has been known since elliott (1998) that efficient methods of inference on cointegrating relationships break down when autoregressive roots are near but not exactly equal to unity. this paper addresses this problem within the framework of a var with non-unit roots. we develop a characterisation of cointegration, based on the impulse response function implied by the var, that remains meaningful even when roots are not exactly unity. under this characterisation, the long-run equilibrium relationships between the series are identified with a subspace associated to the largest characteristic roots of the var. we analyse the asymptotics of maximum likelihood estimators of this subspace, thereby generalising johansen's (1995) treatment of the cointegrated var with exactly unit roots. inference is complicated by nuisance parameter problems similar to those encountered in the context of predictive regressions, and can be dealt with by approaches familiar from that setting.", "categories": "econ.em math.st stat.th", "created": "2020-02-19", "updated": "", "authors": ["james a. duffy", "jerome r. simons"], "url": "https://arxiv.org/abs/2002.08092"}, {"title": "lattice structure of the random stable set in many-to-many matching   market", "id": "2002.08156", "abstract": "for a many-to-many matching market, we study the lattice structure of the set of random stable matchings. we define a partial order on the random stable set and present two intuitive binary operations to compute the least upper bound and the greatest lower bound for each side of the matching market. then, we prove that with these binary operations the set of random stable matchings forms two dual lattices.", "categories": "econ.th", "created": "2020-02-19", "updated": "2020-06-08", "authors": ["noelia juarez", "pablo a. neme", "jorge oviedo"], "url": "https://arxiv.org/abs/2002.08156"}, {"title": "off-policy bandit and reinforcement learning", "id": "2002.08536", "abstract": "we develop a method for predicting the performance of reinforcement learning and bandit algorithms, given historical data that may have been generated by a different algorithm. our estimator has the property that its prediction converges in probability to the true performance of a counterfactual algorithm at the fast $\\sqrt{n}$ rate, as the sample size $n$ increases. we also show a correct way to estimate the variance of our prediction, thus allowing the analyst to quantify the uncertainty in the prediction. these properties hold even when the analyst does not know which among a large number of potentially important state variables are really important. these theoretical guarantees make our estimator safe to use. we finally apply it to improve advertisement design by a major advertisement company. we find that our method produces smaller mean squared errors than state-of-the-art methods.", "categories": "cs.lg cs.ai econ.em stat.me stat.ml", "created": "2020-02-19", "updated": "2020-06-09", "authors": ["yusuke narita", "shota yasui", "kohei yata"], "url": "https://arxiv.org/abs/2002.08536"}, {"title": "combining shrinkage and sparsity in conjugate vector autoregressive   models", "id": "2002.08760", "abstract": "conjugate priors allow for fast inference in large dimensional vector autoregressive (var) models but, at the same time, introduce the restriction that each equation features the same set of explanatory variables. this paper proposes a straightforward means of post-processing posterior estimates of a conjugate bayesian var to effectively perform equation-specific covariate selection. compared to existing techniques using shrinkage alone, our approach combines shrinkage and sparsity in both the var coefficients and the error variance-covariance matrices, greatly reducing estimation uncertainty in large dimensions while maintaining computational tractability. we illustrate our approach by means of two applications. the first application uses synthetic data to investigate the properties of the model across different data-generating processes, the second application analyzes the predictive gains from sparsification in a forecasting exercise for us data.", "categories": "econ.em", "created": "2020-02-20", "updated": "2020-08-26", "authors": ["niko hauzenberger", "florian huber", "luca onorante"], "url": "https://arxiv.org/abs/2002.08760"}, {"title": "cournot-nash equilibrium and optimal transport in a dynamic setting", "id": "2002.08786", "abstract": "we consider a large population dynamic game in discrete time. the peculiarity of the game is that players are characterized by time-evolving types, and so reasonably their actions should not anticipate the future values of their types. when interactions between players are of mean-field kind, we relate nash equilibria for such games to an asymptotic notion of dynamic cournot-nash equilibria. inspired by the works of blanchet and carlier for the static situation, we interpret dynamic cournot-nash equilibria in the light of causal optimal transport theory. further specializing to games of potential type, we establish existence, uniqueness and characterization of equilibria. moreover we develop, for the first time, a numerical scheme for causal optimal transport, which is then leveraged in order to compute dynamic cournot-nash equilibria. this is illustrated in a detailed case study of a congestion game.", "categories": "math.oc econ.gn q-fin.ec", "created": "2020-02-20", "updated": "", "authors": ["beatrice acciaio", "julio backhoff-veraguas", "junchao jia"], "url": "https://arxiv.org/abs/2002.08786"}, {"title": "heavy tails make happy buyers", "id": "2002.09014", "abstract": "in a second-price auction with i.i.d. (independent identically distributed) bidder valuations, adding bidders increases expected buyer surplus if the distribution of valuations has a sufficiently heavy right tail. while this does not imply that a bidder in an auction should prefer for more bidders to join the auction, it does imply that a bidder should prefer it in exchange for the bidder being allowed to participate in more auctions. also, for a heavy-tailed valuation distribution, marginal expected seller revenue per added bidder remains strong even when there are already many bidders.", "categories": "cs.gt econ.th stat.ap", "created": "2020-02-20", "updated": "", "authors": ["eric bax"], "url": "https://arxiv.org/abs/2002.09014"}, {"title": "rational choice hypothesis as x-point of utility function and norm   function", "id": "2002.09036", "abstract": "towards the realization of a sustainable, fair and inclusive society, we proposed a novel decision-making model that incorporates social norms in a rational choice model from the standpoints of deontology and utilitarianism. we proposed a hypothesis that interprets choice of action as the x-point for individual utility function that increases with actions and social norm function that decreases with actions. this hypothesis is based on humans psychologically balancing the value of utility and norms in selecting actions. using the hypothesis and approximation, we were able to isolate and infer utility function and norm function from real-world measurement data of actions on environmental conditions and elucidate the interaction between the both functions that led from current status to target actions. as examples of collective data that aggregate decision-making of individuals, we looked at the changes in power usage before and after the great east japan earthquake and the correlation between national gdp and co2 emission in different countries. the first example showed that the perceived benefits of power (i.e., utility of power usage) was stronger than the power usage restrictions imposed by norms after the earthquake, contrary to our expectation. the second example showed that a reduction of co2 emission in each country was not related to utility derived from gdp but to norms related to co2 emission. going forward, we will apply this new x-point model to actual social practices involving normative problems, and design the approaches for the diagnosis, prognosis and intervention of social systems by it systems.", "categories": "econ.gn q-fin.ec", "created": "2020-02-19", "updated": "2020-06-10", "authors": ["takeshi kato", "yasuyuki kudo", "junichi miyakoshi", "jun otsuka", "hayato saigo", "kaori karasawa", "hiroyuki yamaguchi", "yasuo deguchi"], "url": "https://arxiv.org/abs/2002.09036"}, {"title": "sustainability and fairness simulations based on decision-making model   of utility function and norm function", "id": "2002.09037", "abstract": "we introduced a decision-making model based on value functions that included individualistic utility function and socio-constructivistic norm function and proposed a norm-fostering process that recursively updates norm function through mutual recognition between the self and others. as an example, we looked at the resource-sharing problem typical of economic activities and assumed the distribution of individual actions to define the (1) norm function fostered through mutual comparison of value/action ratio based on the equity theory (progressive tax-like), (2) norm function proportional to resource utilization (proportional tax-like) and (3) fixed norm function independent of resource utilization (fixed tax-like). by carrying out numerical simulation, we showed that the progressive tax-like norm function (i) does not increase disparity for the distribution of the actions, unlike the other norm functions, and (ii) has high resource productivity and low gini coefficient. therefore the progressive tax-like norm function has the highest sustainability and fairness.", "categories": "econ.gn q-fin.ec", "created": "2020-02-19", "updated": "2020-04-24", "authors": ["takeshi kato", "yasuyuki kudo", "junichi miyakoshi", "jun otsuka", "hayato saigo", "kaori karasawa", "hiroyuki yamaguchi", "yoshinori hiroi", "yasuo deguchi"], "url": "https://arxiv.org/abs/2002.09037"}, {"title": "asymptotic marginal propensity to consume", "id": "2002.09108", "abstract": "we prove that the consumption functions in income fluctuation problems are asymptotically linear if the marginal utility is regularly varying. we also analytically characterize the asymptotic marginal propensities to consume (mpcs) out of wealth and derive necessary and sufficient conditions under which they are 0, 1, or are somewhere in between. when the return process with time-varying volatility is calibrated from data, the asymptotic mpcs can be zero with moderate risk aversion. our results potentially explain why the saving rates among the rich are positive and increasing in wealth.", "categories": "econ.gn q-fin.ec", "created": "2020-02-20", "updated": "", "authors": ["qingyin ma", "alexis akira toda"], "url": "https://arxiv.org/abs/2002.09108"}, {"title": "a new decomposition ensemble approach for tourism demand forecasting:   evidence from major source countries", "id": "2002.09201", "abstract": "the asian-pacific region is the major international tourism demand market in the world, and its tourism demand is deeply affected by various factors. previous studies have shown that different market factors influence the tourism market demand at different timescales. accordingly, the decomposition ensemble learning approach is proposed to analyze the impact of different market factors on market demand, and the potential advantages of the proposed method on forecasting tourism demand in the asia-pacific region are further explored. this study carefully explores the multi-scale relationship between tourist destinations and the major source countries, by decomposing the corresponding monthly tourist arrivals with noise-assisted multivariate empirical mode decomposition. with the china and malaysia as case studies, their respective empirical results show that decomposition ensemble approach significantly better than the benchmarks which include statistical model, machine learning and deep learning model, in terms of the level forecasting accuracy and directional forecasting accuracy.", "categories": "econ.gn q-fin.ec", "created": "2020-02-21", "updated": "", "authors": ["chengyuan zhang", "fuxin jiang", "shouyang wang", "shaolong sun"], "url": "https://arxiv.org/abs/2002.09201"}, {"title": "kernel conditional moment test via maximum moment restriction", "id": "2002.09225", "abstract": "we propose a new family of specification tests called kernel conditional moment (kcm) tests. our tests are built on a novel representation of conditional moment restrictions in a reproducing kernel hilbert space (rkhs) called conditional moment embedding (cmme). after transforming the conditional moment restrictions into a continuum of unconditional counterparts, the test statistic is defined as the maximum moment restriction (mmr) within the unit ball of the rkhs. we show that the mmr not only fully characterizes the original conditional moment restrictions, leading to consistency in both hypothesis testing and parameter estimation, but also has an analytic expression that is easy to compute as well as closed-form asymptotic distributions. our empirical studies show that the kcm test has a promising finite-sample performance compared to existing tests.", "categories": "math.st cs.lg econ.em stat.ml stat.th", "created": "2020-02-21", "updated": "2020-06-19", "authors": ["krikamol muandet", "wittawat jitkrittum", "jonas k\u00fcbler"], "url": "https://arxiv.org/abs/2002.09225"}, {"title": "optimization of a dynamic profit function using euclidean path integral", "id": "2002.09394", "abstract": "a euclidean path integral is used to find an optimal strategy for a firm under a walrasian system, pareto optimality and a non-cooperative feedback nash equilibrium. we define dynamic optimal strategies and develop a feynman type path integration method to capture all non-additive convex strategies. we also show that the method can solve the non-linear case, for example merton-garman-hamiltonian system, which the traditional pontryagin maximum principle cannot solve in closed form. furthermore, under walrasian system we are able to solve for the optimal strategy under a linear constraint with a linear objective function with respect to strategy.", "categories": "econ.th math.oc math.pr", "created": "2020-02-21", "updated": "", "authors": ["p. pramanik", "a. m. polansky"], "url": "https://arxiv.org/abs/2002.09394"}, {"title": "a characterization of proportionally representative committees", "id": "2002.09598", "abstract": "a well-known axiom for proportional representation is proportionality of solid coalitions (psc). we characterize committees satisfying psc as possible outcomes of the minimal demand rule, which generalizes an approach pioneered by michael dummett.", "categories": "cs.gt cs.ai cs.ma econ.th", "created": "2020-02-21", "updated": "", "authors": ["haris aziz", "barton e. lee"], "url": "https://arxiv.org/abs/2002.09598"}, {"title": "survey bandits with regret guarantees", "id": "2002.09814", "abstract": "we consider a variant of the contextual bandit problem. in standard contextual bandits, when a user arrives we get the user's complete feature vector and then assign a treatment (arm) to that user. in a number of applications (like healthcare), collecting features from users can be costly. to address this issue, we propose algorithms that avoid needless feature collection while maintaining strong regret guarantees.", "categories": "cs.lg econ.em stat.ml", "created": "2020-02-22", "updated": "", "authors": ["sanath kumar krishnamurthy", "susan athey"], "url": "https://arxiv.org/abs/2002.09814"}, {"title": "unit-root test within a threshold arma framework", "id": "2002.09968", "abstract": "we propose a new unit-root test based on lagrange multipliers, where we extend the null hypothesis to an integrated moving-average process (ima(1,1)) and the alternative to a first-order threshold autoregressive moving-average process (tarma(1,1)). this new theoretical framework provides tests with good size without pre-modelling steps. moreover, leveraging on the versatile capability of the tarma(1,1), our test has power against a wide range of linear and nonlinear alternatives. we prove the consistency and asymptotic similarity of the test. the proof of tightness of the test is of independent and general theoretical interest. moreover, we propose a wild bootstrap version of the statistic. our proposals outperform most existing tests in many contexts. we support the view that rejection does not necessarily imply nonlinearity so that unit-root tests should not be used uncritically to select a model. finally, we present an application to real exchange rates.", "categories": "stat.me econ.em math.st stat.th", "created": "2020-02-23", "updated": "", "authors": ["kung-sik chan", "simone giannerini", "greta goracci", "howell tong"], "url": "https://arxiv.org/abs/2002.09968"}, {"title": "estimation and inference about tail features with tail censored data", "id": "2002.09982", "abstract": "this paper considers estimation and inference about tail features when the observations beyond some threshold are censored. we first show that ignoring such tail censoring could lead to substantial bias and size distortion, even if the censored probability is tiny. second, we propose a new maximum likelihood estimator (mle) based on the pareto tail approximation and derive its asymptotic properties. third, we provide a small sample modification to the mle by resorting to extreme value theory. the mle with this modification delivers excellent small sample performance, as shown by monte carlo simulations. we illustrate its empirical relevance by estimating (i) the tail index and the extreme quantiles of the us individual earnings with the current population survey dataset and (ii) the tail index of the distribution of macroeconomic disasters and the coefficient of risk aversion using the dataset collected by barro and urs{\\'u}a (2008). our new empirical findings are substantially different from the existing literature.", "categories": "econ.em", "created": "2020-02-23", "updated": "", "authors": ["yulong wang", "zhijie xiao"], "url": "https://arxiv.org/abs/2002.09982"}, {"title": "optimal advertising for information products", "id": "2002.10045", "abstract": "when selling information, sometimes the seller can increase the revenue by giving away some partial information to change the buyers' belief about the information product, so the buyers may be more willing to purchase. this work studies the general problem of advertising information products by revealing some partial information. we consider buyers who need to make a decision, the outcome of which depends on the state of the world that is unknown to the buyers. there is an information seller who has access to the state of the world. the seller can advertise the information by revealing some partial information. we assume that the seller chooses a long-term advertising strategy and then commits to it. the buyers decide whether to purchase the full information product after seeing the partial information. the seller's goal is to maximize the expected revenue. we study the problem in two settings.   1. the seller targets the buyers of a certain type. in this case, we prove that finding the optimal advertising strategy is equivalent to finding the concave closure of a function, which is np-hard in general. based on this observation, we prove some properties of the optimal mechanism, which allow us to solve for the optimal mechanism by a convex program (of exponential size in general, polynomial size for special cases). we also prove some interesting characterizations of the optimal mechanisms based on these properties.   2. for the general problem when the seller faces buyers of different types and only knows the distribution of their types, it is np-hard to find a constant factor approximation. we thus look at special cases and provide an approximation algorithm that finds an $\\varepsilon$-suboptimal mechanism when it is not too hard to predict the possible type of buyers who will make the purchase.", "categories": "cs.gt econ.th", "created": "2020-02-23", "updated": "2020-07-14", "authors": ["shuran zheng", "yiling chen"], "url": "https://arxiv.org/abs/2002.10045"}, {"title": "novel insights in the levy-levy-solomon agent-based economic market   model", "id": "2002.10222", "abstract": "the levy-levy-solomon model (a microscopic model of the stock market: cycles, booms, and crashes, economic letters 45 (1))is one of the most influential agent-based economic market models. in several publications this model has been discussed and analyzed. especially lux and zschischang (some new results on the levy, levy and solomon microscopic stock market model, physica a, 291(1-4)) have shown that the model exhibits finite-size effects. in this study we extend existing work in several directions. first, we show simulations which reveal finite-size effects of the model. secondly, we shed light on the origin of these finite-size effects. furthermore, we demonstrate the sensitivity of the levy-levy-solomon model with respect to random numbers. especially, we can conclude that a low-quality pseudo random number generator has a huge impact on the simulation results. finally, we study the impact of the stopping criteria in the market clearance mechanism of the levy-levy-solomon model.", "categories": "q-fin.tr econ.gn physics.soc-ph q-fin.ec", "created": "2020-02-13", "updated": "", "authors": ["maximilian beikirch", "torsten trimborn"], "url": "https://arxiv.org/abs/2002.10222"}, {"title": "forecasting foreign exchange rate: a multivariate comparative analysis   between traditional econometric, contemporary machine learning & deep   learning techniques", "id": "2002.10247", "abstract": "in todays global economy, accuracy in predicting macro-economic parameters such as the foreign the exchange rate or at least estimating the trend correctly is of key importance for any future investment. in recent times, the use of computational intelligence-based techniques for forecasting macroeconomic variables has been proven highly successful. this paper tries to come up with a multivariate time series approach to forecast the exchange rate (usd/inr) while parallelly comparing the performance of three multivariate prediction modelling techniques: vector auto regression (a traditional econometric technique), support vector machine (a contemporary machine learning technique), and recurrent neural networks (a contemporary deep learning technique). we have used monthly historical data for several macroeconomic variables from april 1994 to december 2018 for usa and india to predict usd-inr foreign exchange rate. the results clearly depict that contemporary techniques of svm and rnn (long short-term memory) outperform the widely used traditional method of auto regression. the rnn model with long short-term memory (lstm) provides the maximum accuracy (97.83%) followed by svm model (97.17%) and var model (96.31%). at last, we present a brief analysis of the correlation and interdependencies of the variables used for forecasting.", "categories": "q-fin.st cs.lg econ.em stat.ml", "created": "2020-02-19", "updated": "", "authors": ["manav kaushik", "a k giri"], "url": "https://arxiv.org/abs/2002.10247"}, {"title": "bayesian inference in high-dimensional time-varying parameter models   using integrated rotated gaussian approximations", "id": "2002.10274", "abstract": "researchers increasingly wish to estimate time-varying parameter (tvp) regressions which involve a large number of explanatory variables. including prior information to mitigate over-parameterization concerns has led to many using bayesian methods. however, bayesian markov chain monte carlo (mcmc) methods can be very computationally demanding. in this paper, we develop computationally efficient bayesian methods for estimating tvp models using an integrated rotated gaussian approximation (irga). this exploits the fact that whereas constant coefficients on regressors are often important, most of the tvps are often unimportant. since gaussian distributions are invariant to rotations we can split the the posterior into two parts: one involving the constant coefficients, the other involving the tvps. approximate methods are used on the latter and, conditional on these, the former are estimated with precision using mcmc methods. in empirical exercises involving artificial data and a large macroeconomic data set, we show the accuracy and computational benefits of irga methods.", "categories": "econ.em", "created": "2020-02-24", "updated": "", "authors": ["florian huber", "gary koop", "michael pfarrhofer"], "url": "https://arxiv.org/abs/2002.10274"}, {"title": "estimating economic models with testable assumptions: theory and   applications", "id": "2002.10415", "abstract": "this paper studies the identification, estimation, and hypothesis testing problem in complete and incomplete economic models with testable assumptions. testable assumptions ($a$) give strong and interpretable empirical content to the models but they also carry the possibility that some distribution of observed outcomes may reject these assumptions. a natural way to avoid this is to find a set of relaxed assumptions ($\\tilde{a}$) that cannot be rejected by any distribution of observed outcome and the identified set of the parameter of interest is not changed when the original assumption is not rejected. the main contribution of this paper is to characterize the properties of such a relaxed assumption $\\tilde{a}$ using a generalized definition of refutability and confirmability. i also propose a general method to construct such $\\tilde{a}$. a general estimation and inference procedure is proposed and can be applied to most incomplete economic models. i apply my methodology to the instrument monotonicity assumption in local average treatment effect (late) estimation and to the sector selection assumption in a binary outcome roy model of employment sector choice. in the late application, i use my general method to construct a set of relaxed assumptions $\\tilde{a}$ that can never be rejected, and the identified set of late is the same as imposing $a$ when $a$ is not rejected. late is point identified under my extension $\\tilde{a}$ in the late application. in the binary outcome roy model, i use my method of incomplete models to relax roy's sector selection assumption and characterize the identified set of the binary potential outcome as a polyhedron.", "categories": "econ.em", "created": "2020-02-24", "updated": "2020-07-11", "authors": ["moyu liao"], "url": "https://arxiv.org/abs/2002.10415"}, {"title": "forecasting the intra-day spread densities of electricity prices", "id": "2002.10566", "abstract": "intra-day price spreads are of interest to electricity traders, storage and electric vehicle operators. this paper formulates dynamic density functions, based upon skewed-t and similar representations, to model and forecast the german electricity price spreads between different hours of the day, as revealed in the day-ahead auctions. the four specifications of the density functions are dynamic and conditional upon exogenous drivers, thereby permitting the location, scale and shape parameters of the densities to respond hourly to such factors as weather and demand forecasts. the best fitting and forecasting specifications for each spread are selected based on the pinball loss function, following the closed-form analytical solutions of the cumulative distribution functions.", "categories": "stat.ap cs.lg econ.em stat.ml", "created": "2020-02-20", "updated": "", "authors": ["ekaterina abramova", "derek bunn"], "url": "https://arxiv.org/abs/2002.10566"}, {"title": "random horizon principal-agent problem", "id": "2002.10982", "abstract": "we consider a general formulation of the random horizon principal-agent problem with a continuous payment and a lump-sum payment at termination. in the european version of the problem, the random horizon is chosen solely by the principal with no other possible action from the agent than exerting effort on the dynamics of the output process. we also consider the american version of the contract, which covers the seminal sannikov's model, where the agent can also quit by optimally choosing the termination time of the contract. our main result reduces such non-zero-sum stochastic differential games to appropriate stochastic control problems which may be solved by standard methods of stochastic control theory. this reduction is obtained by following sannikov's approach, further developed by cvitanic, possamai, and touzi. we first introduce an appropriate class of contracts for which the agent's optimal effort is immediately characterized by the standard verification argument in stochastic control theory. we then show that this class of contracts is dense in an appropriate sense so that the optimization over this restricted family of contracts represents no loss of generality. the result is obtained by using the recent well-posedness result of random horizon second-order backward sde.", "categories": "math.oc econ.gn q-fin.ec", "created": "2020-02-25", "updated": "", "authors": ["yiqing lin", "zhenjie ren", "nizar touzi", "junjian yang"], "url": "https://arxiv.org/abs/2002.10982"}, {"title": "a practical approach to social learning", "id": "2002.11017", "abstract": "models of social learning feature either binary signals or abstract signal structures often deprived of micro-foundations. both models are limited when analyzing interim results or performing empirical analysis. we present a method of generating signal structures which are richer than the binary model, yet are tractable enough to perform simulations and empirical analysis. we demonstrate the method's usability by revisiting two classical papers: (1) we discuss the economic significance of unbounded signals smith and sorensen (2000); (2) we use experimental data from anderson and holt (1997) to perform econometric analysis. additionally, we provide a necessary and sufficient condition for the occurrence of action cascades.", "categories": "econ.th cs.si econ.em econ.gn q-fin.ec", "created": "2020-02-25", "updated": "", "authors": ["amir ban", "moran koren"], "url": "https://arxiv.org/abs/2002.11017"}, {"title": "hours worked and the u.s. distribution of real annual earnings 1976-2016", "id": "2002.11211", "abstract": "we examine the impact of annual hours worked on annual earnings by decomposing changes in the real annual earnings distribution into composition, structural and hours effects. we do so via a nonseparable simultaneous model of hours, wages and earnings. we provide identification results and estimators of the objects required for the decompositions. using the current population survey for the survey years 1976-2016, we find that changes in the level of annual hours of work are important in explaining movements in inequality in female annual earnings. this captures the substantial changes in their employment behavior over this period. the impact of hours on males' earnings inequality operates only through the lower part of the earnings distribution and reflects the sensitivity of these workers' annual hours of work to cyclical factors.", "categories": "econ.em", "created": "2020-02-25", "updated": "", "authors": ["iv\u00e1n fern\u00e1ndez-val", "franco peracchi", "aico van vuuren", "francis vella"], "url": "https://arxiv.org/abs/2002.11211"}, {"title": "feasible joint posterior beliefs", "id": "2002.11362", "abstract": "we study the set of possible joint posterior belief distributions of a group of agents who share a common prior regarding a binary state and who observe some information structure. our main result is that, for the two agent case, a quantitative version of aumann's agreement theorem provides a necessary and sufficient condition for feasibility. for any number of agents, a related \"no trade\" condition likewise provides a characterization of feasibility. we use our characterization to study joint belief distributions in which agents are informed regarding the state, and yet receive no information regarding the other's posterior. we study a related class of bayesian persuasion problems with a single sender and multiple receivers, and explore the extreme points of the set of feasible distributions.", "categories": "econ.th cs.gt math.pr", "created": "2020-02-26", "updated": "2020-07-01", "authors": ["itai arieli", "yakov babichenko", "fedor sandomirskiy", "omer tamuz"], "url": "https://arxiv.org/abs/2002.11362"}, {"title": "econometric issues with laubach and williams' estimates of the natural   rate of interest", "id": "2002.11583", "abstract": "holston, laubach and williams' (2017) estimates of the natural rate of interest are driven by the downward trending behaviour of 'other factor' $z_{t}$. i show that their implementation of stock and watson's (1998) median unbiased estimation (mue) to determine the size of the $\\lambda _{z}$ parameter which drives this downward trend in $z_{t}$ is unsound. it cannot recover the ratio of interest $\\lambda _{z}=a_{r}\\sigma _{z}/\\sigma _{\\tilde{y}}$ from mue required for the estimation of the full structural model. this failure is due to an 'unnecessary' misspecification in holston et al.'s (2017) formulation of the stage 2 model. more importantly, their implementation of mue on this misspecified stage 2 model spuriously amplifies the point estimate of $\\lambda _{z}$. using a simulation experiment, i show that their procedure generates excessively large estimates of $\\lambda _{z}$ when applied to data generated from a model where the true $\\lambda _{z}$ is equal to zero. correcting the misspecification in their stage 2 model and the implementation of mue leads to a substantially smaller $\\lambda _{z}$ estimate, and with this, a more subdued downward trending influence of 'other factor' $z_{t}$ on the natural rate. moreover, the $\\lambda _{z}$ point estimate is statistically highly insignificant, suggesting that there is no role for 'other factor' $z_{t}$ in this model. i also discuss various other estimation issues that arise in holston et al.'s (2017) model of the natural rate that make it unsuitable for policy analysis.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-02-26", "updated": "2020-08-05", "authors": ["daniel buncic"], "url": "https://arxiv.org/abs/2002.11583"}, {"title": "off-policy evaluation and learning for external validity under a   covariate shift", "id": "2002.11642", "abstract": "we consider the evaluation and training of a new policy for the evaluation data by using the historical data obtained from a different policy. the goal of off-policy evaluation (ope) is to estimate the expected reward of a new policy over the evaluation data, and that of off-policy learning (opl) is to find a new policy that maximizes the expected reward over the evaluation data. although the standard ope and opl assume the same distribution of covariate between the historical and evaluation data, there often exists a problem of a covariate shift, i.e., the distribution of the covariate of the historical data is different from that of the evaluation data. in this paper, we derive the efficiency bound of ope under a covariate shift. then, we propose doubly robust and efficient estimators for ope and opl under a covariate shift by using an estimator of the density ratio between the distributions of the historical and evaluation data. we also discuss other possible estimators and compare their theoretical properties. finally, we confirm the effectiveness of the proposed estimators through experiments.", "categories": "stat.ml cs.lg econ.em", "created": "2020-02-26", "updated": "", "authors": ["masahiro kato", "masatoshi uehara", "shota yasui"], "url": "https://arxiv.org/abs/2002.11642"}, {"title": "corrupted multidimensional binary search: learning in the presence of   irrational agents", "id": "2002.11650", "abstract": "standard game-theoretic formulations for settings like contextual pricing and security games assume that agents act in accordance with a specific behavioral model. in practice however, some agents may not prescribe to the dominant behavioral model or may act in ways that are arbitrarily inconsistent. existing algorithms heavily depend on the model being (approximately) accurate for all agents and have poor performance in the presence of even a few such arbitrarily irrational agents. how do we design learning algorithms that are robust to the presence of arbitrarily irrational agents?   we address this question for a number of canonical game-theoretic applications by designing a robust algorithm for the fundamental problem of multidimensional binary search. the performance of our algorithm degrades gracefully with the number of corrupted rounds, which correspond to irrational agents and need not be known in advance. as binary search is the key primitive in algorithms for contextual pricing, stackelberg security games, and other game-theoretic applications, we immediately obtain robust algorithms for these settings.   our techniques draw inspiration from learning theory, game theory, high-dimensional geometry, and convex analysis, and may be of independent algorithmic interest.", "categories": "cs.lg cs.ds cs.gt econ.gn q-fin.ec stat.ml", "created": "2020-02-26", "updated": "2020-02-27", "authors": ["akshay krishnamurthy", "thodoris lykouris", "chara podimata"], "url": "https://arxiv.org/abs/2002.11650"}, {"title": "me, myself and i: a general theory of non-markovian time-inconsistent   stochastic control for sophisticated agents", "id": "2002.12572", "abstract": "we develop a theory for continuous-time non-markovian stochastic control problems which are inherently time-inconsistent. their distinguishing feature is that the classical bellman optimality principle no longer holds. our formulation is cast within the framework of a controlled non-markovian forward stochastic differential equation, and a general objective functional setting. we adopt a game-theoretic approach to study such problems, meaning that we seek for \\emph{sub-game perfect nash equilibrium} points. as a first novelty of this work, we introduce and motivate a new definition of equilibrium that allows us to establish rigorously an \\emph{extended dynamic programming principle}, in the same spirit as in the classical theory. this in turn allows us to introduce a system of backward stochastic differential equations analogous to the classical hjb equation. we prove that this system is fundamental, in the sense that its well-posedness is both necessary and sufficient to characterise the value function and equilibria. as a final step we provide an existence and uniqueness result. some examples and extensions of our results are also presented.", "categories": "math.oc econ.th math.pr", "created": "2020-02-28", "updated": "", "authors": ["camilo hern\u00e1ndez", "dylan possama\u00ef"], "url": "https://arxiv.org/abs/2002.12572"}, {"title": "causal mediation analysis with double machine learning", "id": "2002.12710", "abstract": "this paper combines causal mediation analysis with double machine learning to control for observed confounders in a data-driven way under a selection-on-observables assumption in a high-dimensional setting. we consider the average indirect effect of a binary treatment operating through an intermediate variable (or mediator) on the causal path between the treatment and the outcome, as well as the unmediated direct effect. estimation is based on efficient score functions, which possess a multiple robustness property w.r.t. misspecifications of the outcome, mediator, and treatment models. this property is key for selecting these models by double machine learning, which is combined with data splitting to prevent overfitting in the estimation of the effects of interest. we demonstrate that the direct and indirect effect estimators are asymptotically normal and root-n consistent under specific regularity conditions and investigate the finite sample properties of the suggested methods in a simulation study when considering lasso as machine learner. we also provide an empirical application to the u.s. national longitudinal survey of youth, assessing the indirect effect of health insurance coverage on general health operating via routine checkups as mediator, as well as the direct effect. we find a moderate short term effect of health insurance coverage on general health which is, however, not mediated by routine checkups.", "categories": "econ.em", "created": "2020-02-28", "updated": "2020-08-13", "authors": ["helmut farbmacher", "martin huber", "luk\u00e1\u0161 laff\u00e9rs", "henrika langen", "martin spindler"], "url": "https://arxiv.org/abs/2002.12710"}, {"title": "dynamic beveridge curve accounting", "id": "2003.00033", "abstract": "we develop a dynamic decomposition of the empirical beveridge curve, i.e., the level of vacancies conditional on unemployment. using a standard model, we show that three factors can shift the beveridge curve: reduced-form matching efficiency, changes in the job separation rate, and out-of-steady-state dynamics. we find that the shift in the beveridge curve during and after the great recession was due to all three factors, and each factor taken separately had a large effect. comparing the pre-2010 period to the post-2010 period, a fall in matching efficiency and out-of-steady-state dynamics both pushed the curve upward, while the changes in the separation rate pushed the curve downward. the net effect was the observed upward shift in vacancies given unemployment. in previous recessions changes in matching efficiency were relatively unimportant, while dynamics and the separation rate had more impact. thus, the unusual feature of the great recession was the deterioration in matching efficiency, while separations and dynamics have played significant, partially offsetting roles in most downturns. the importance of these latter two margins contrasts with much of the literature, which abstracts from one or both of them. we show that these factors affect the slope of the empirical beveridge curve, an important quantity in recent welfare analyses estimating the natural rate of unemployment.", "categories": "econ.gn q-fin.ec", "created": "2020-02-28", "updated": "", "authors": ["hie joo ahn", "leland d. crane"], "url": "https://arxiv.org/abs/2003.00033"}, {"title": "determination of latent dimensionality in international trade flow", "id": "2003.00129", "abstract": "currently, high-dimensional data is ubiquitous in data science, which necessitates the development of techniques to decompose and interpret such multidimensional (aka tensor) datasets. finding a low dimensional representation of the data, that is, its inherent structure, is one of the approaches that can serve to understand the dynamics of low dimensional latent features hidden in the data. nonnegative rescal is one such technique, particularly well suited to analyze self-relational data, such as dynamic networks found in international trade flows. nonnegative rescal computes a low dimensional tensor representation by finding the latent space containing multiple modalities. estimating the dimensionality of this latent space is crucial for extracting meaningful latent features. here, to determine the dimensionality of the latent space with nonnegative rescal, we propose a latent dimension determination method which is based on clustering of the solutions of multiple realizations of nonnegative rescal decompositions. we demonstrate the performance of our model selection method on synthetic data and then we apply our method to decompose a network of international trade flows data from international monetary fund and validate the resulting features against empirical facts from economic literature.", "categories": "cs.lg cs.ir econ.gn q-fin.ec stat.ml", "created": "2020-02-28", "updated": "", "authors": ["duc p. truong", "erik skau", "vladimir i. valtchinov", "boian s. alexandrov"], "url": "https://arxiv.org/abs/2003.00129"}, {"title": "identification of random coefficient latent utility models", "id": "2003.00276", "abstract": "this paper provides nonparametric identification results for random coefficient distributions in perturbed utility models. we cover discrete and continuous choice models. we establish identification using variation in mean quantities, and the results apply when an analyst observes aggregate demands but not whether goods are chosen together. we require exclusion restrictions and independence between random slope coefficients and random intercepts. we do not require regressors to have large supports or parametric assumptions.", "categories": "econ.em", "created": "2020-02-29", "updated": "", "authors": ["roy allen", "john rehbeck"], "url": "https://arxiv.org/abs/2003.00276"}, {"title": "technological interdependencies predict innovation dynamics", "id": "2003.00580", "abstract": "we propose a simple model where the innovation rate of a technological domain depends on the innovation rate of the technological domains it relies on. using data on us patents from 1836 to 2017, we make out-of-sample predictions and find that the predictability of innovation rates can be boosted substantially when network effects are taken into account. in the case where a technology$'$s neighborhood future innovation rates are known, the average predictability gain is 28$\\%$ compared to simpler time series model which do not incorporate network effects. even when nothing is known about the future, we find positive average predictability gains of 20$\\%$. the results have important policy implications, suggesting that the effective support of a given technology must take into account the technological ecosystem surrounding the targeted technology.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-01", "updated": "", "authors": ["anton pichler", "fran\u00e7ois lafond", "j. doyne farmer"], "url": "https://arxiv.org/abs/2003.00580"}, {"title": "continuum and thermodynamic limits for a simple random-exchange model", "id": "2003.00930", "abstract": "we discuss various limits of a simple random exchange model that can be used for the distribution of wealth. we start from a discrete state space - discrete time version of this model and, under suitable scaling, we show its functional convergence to a continuous space - discrete time model. then, we show a thermodynamic limit of the empirical distribution to the solution of a kinetic equation of boltzmann type. we solve this equation and we show that the solutions coincide with the appropriate limits of the invariant measure for the markov chain. in this way we complete boltzmann's program of deriving kinetic equations from random dynamics for this simple model. three families of invariant measures for the mean field limit are discovered and we show that only two of those families can be obtained as limits of the discrete system and the third is extraneous. finally, we cast our results in the framework of integer partitions and strengthen some results already available in the literature.", "categories": "math.pr econ.gn q-fin.ec", "created": "2020-03-02", "updated": "", "authors": ["bertram d\u00fcring", "nicos georgiou", "sara merino-aceituno", "enrico scalas"], "url": "https://arxiv.org/abs/2003.00930"}, {"title": "a scalar parameterized mechanism for two-sided markets", "id": "2003.01206", "abstract": "we consider a market in which both suppliers and consumers compete for a product via scalar-parameterized supply offers and demand bids. scalar-parameterized offers/bids are appealing due to their modeling simplicity and desirable mathematical properties with the most prominent being bounded efficiency loss and price markup under strategic interactions. our model incorporates production capacity constraints and minimum inelastic demand requirements. under perfect competition, the market mechanism yields allocations that maximize social welfare. when market participants are price-anticipating, we show that there exists a unique nash equilibrium, and provide an efficient way to compute the resulting market allocation. moreover, we explicitly characterize the bounds on the welfare loss and prices observed at the nash equilibrium.", "categories": "econ.gn cs.sy eess.sy q-fin.ec", "created": "2020-03-02", "updated": "", "authors": ["mariola ndrio", "khaled alshehri", "subhonmesh bose"], "url": "https://arxiv.org/abs/2003.01206"}, {"title": "influence of climate change on the corn yield in ontario and its impact   on corn farms income at the 2068 horizon", "id": "2003.01270", "abstract": "our study aims at quantifying the impact of climate change on corn farming in ontario under several warming scenarios at the 2068 horizon. it is articulated around a discrete-time dynamic model of corn farm income with an annual time-step, corresponding to one agricultural cycle from planting to harvest. at each period, we compute the income given the corn yield, which is highly dependent on weather variables. we also provide a reproducible forecast of the yearly distribution of corn yield for 10 cities in ontario. the price of corn futures at harvest time is taken into account and we fit our model by using 49 years of historical data. we then conduct out-of-sample monte-carlo simulations to obtain the farm income forecasts under a given climate change scenario.", "categories": "econ.gn q-fin.ec", "created": "2020-03-02", "updated": "2020-05-10", "authors": ["antoine kornprobst", "matt davison"], "url": "https://arxiv.org/abs/2003.01270"}, {"title": "a note on solving discretely-constrained nash-cournot games via   complementarity", "id": "2003.01536", "abstract": "discretely-constrained nash-cournot games have attracted attention as they arise in various competitive energy production settings in which players must make one or more discrete decisions. gabriel et al. [\"solving discretely-constrained nash-cournot games with an application to power markets.\" networks and spatial economics 13(3), 2013] claim that the set of equilibria to a discretely-constrained nash-cournot game coincides with the set of solutions to a corresponding discretely-constrained mixed complementarity problem. we show that this claim is false.", "categories": "econ.th math.oc", "created": "2020-02-28", "updated": "", "authors": ["dimitri j. papageorgiou", "francisco trespalacios", "stuart harwood"], "url": "https://arxiv.org/abs/2003.01536"}, {"title": "the role of uncertainty in controlling climate change", "id": "2003.01615", "abstract": "integrated assessment models (iams) of the climate and economy aim to analyze the impact and efficacy of policies that aim to control climate change, such as carbon taxes and subsidies. a major characteristic of iams is that their geophysical sector determines the mean surface temperature increase over the preindustrial level, which in turn determines the damage function. most of the existing iams are perfect-foresight forward-looking models, assuming that we know all of the future information. however, there are significant uncertainties in the climate and economic system, including parameter uncertainty, model uncertainty, climate tipping risks, economic risks, and ambiguity. for example, climate damages are uncertain: some researchers assume that climate damages are proportional to instantaneous output, while others assume that climate damages have a more persistent impact on economic growth. climate tipping risks represent (nearly) irreversible climate events that may lead to significant changes in the climate system, such as the greenland ice sheet collapse, while the conditions, probability of tipping, duration, and associated damage are also uncertain. technological progress in carbon capture and storage, adaptation, renewable energy, and energy efficiency are uncertain too. in the face of these uncertainties, policymakers have to provide a decision that considers important factors such as risk aversion, inequality aversion, and sustainability of the economy and ecosystem. solving this problem may require richer and more realistic models than standard iams, and advanced computational methods. the recent literature has shown that these uncertainties can be incorporated into iams and may change optimal climate policies significantly.", "categories": "econ.gn q-fin.ec", "created": "2020-03-03", "updated": "", "authors": ["yongyang cai"], "url": "https://arxiv.org/abs/2003.01615"}, {"title": "numerical solution of dynamic portfolio optimization with transaction   costs", "id": "2003.01809", "abstract": "we apply numerical dynamic programming techniques to solve discrete-time multi-asset dynamic portfolio optimization problems with proportional transaction costs and shorting/borrowing constraints. examples include problems with multiple assets, and many trading periods in a finite horizon problem. we also solve dynamic stochastic problems, with a portfolio including one risk-free asset, an option, and its underlying risky asset, under the existence of transaction costs and constraints. these examples show that it is now tractable to solve such problems.", "categories": "q-fin.pm econ.gn q-fin.cp q-fin.ec q-fin.mf", "created": "2020-03-03", "updated": "", "authors": ["yongyang cai", "kenneth judd", "rong xu"], "url": "https://arxiv.org/abs/2003.01809"}, {"title": "estimating the effect of central bank independence on inflation using   longitudinal targeted maximum likelihood estimation", "id": "2003.02208", "abstract": "the notion that an independent central bank reduces a country's inflation is a controversial hypothesis. to date, it has not been possible to satisfactorily answer this question because the complex macroeconomic structure that gives rise to the data has not been adequately incorporated into statistical analyses. we develop a causal model that summarizes the economic process of inflation. based on this causal model and recent data, we discuss and identify the assumptions under which the effect of central bank independence on inflation can be identified and estimated. given these and alternative assumptions, we estimate this effect using modern doubly robust effect estimators, i.e., longitudinal targeted maximum likelihood estimators. the estimation procedure incorporates machine learning algorithms and is tailored to address the challenges associated with complex longitudinal macroeconomic data. we do not find strong support for the hypothesis that having an independent central bank for a long period of time necessarily lowers inflation. simulation studies evaluate the sensitivity of the proposed methods in complex settings when certain assumptions are violated and highlight the importance of working with appropriate learning algorithms for estimation.", "categories": "econ.em stat.ap", "created": "2020-03-04", "updated": "2020-07-29", "authors": ["philipp f. m. baumann", "michael schomaker", "enzo rossi"], "url": "https://arxiv.org/abs/2003.02208"}, {"title": "joint estimation of discrete choice model and arrival rate with   unobserved stock-out events", "id": "2003.02313", "abstract": "this paper studies the joint estimation problem of a discrete choice model and the arrival rate of potential customers when unobserved stock-out events occur. in this paper, we generalize [anupindi et al., 1998] and [conlon and mortimer, 2013] in the sense that (1) we work with generic choice models, (2) we allow arbitrary numbers of products and stock-out events, and (3) we consider the existence of the null alternative, and estimates the overall arrival rate of potential customers. in addition, we point out that the modeling in [conlon and mortimer, 2013] is problematic, and present the correct formulation.", "categories": "math.oc econ.em", "created": "2020-03-04", "updated": "", "authors": ["hongzhang shao", "anton j. kleywegt"], "url": "https://arxiv.org/abs/2003.02313"}, {"title": "bow-tie structure and community identification of global supply chain   network", "id": "2003.02343", "abstract": "we study on topological properties of global supply chain network in terms of degree distribution, hierarchical structure, and degree-degree correlation in the global supply chain network. the global supply chain data is constructed by collecting various company data from the web site of standard & poor's capital iq platform in 2018. the in- and out-degree distributions are characterized by a power law with in-degree exponent = 2.42 and out-degree exponent = 2.11. the clustering coefficient decays as power law with an exponent = 0.46. the nodal degree-degree correlation indicates the absence of assortativity. the bow-tie structure of gwcc reveals that the out component is the largest and it consists 41.1% of total firms. the gscc component comprises 16.4% of total firms. we observe that the firms in the upstream or downstream sides are mostly located a few steps away from the gscc. furthermore, we uncover the community structure of the network and characterize them according to their location and industry classification. we observe that the largest community consists of consumer discretionary sector mainly based in the us. these firms belong to the out component in the bow-tie structure of the global supply chain network. finally, we confirm the validity for propositions s1 (short path length), s2 (power-law degree distribution), s3 (high clustering coefficient), s4 (\"fit-gets-richer\" growth mechanism), s5 (truncation of power-law degree distribution), and s7 (community structure with overlapping boundaries) in the global supply chain network.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-04", "updated": "", "authors": ["abhijit chakraborty", "yuichi ikeda"], "url": "https://arxiv.org/abs/2003.02343"}, {"title": "compromise, don't optimize: a prior-free alternative to perfect bayesian   equilibrium", "id": "2003.02539", "abstract": "perfect bayesian equilibrium is the classic solution concept for games with incomplete information, where players optimize under given beliefs over states. we introduce a new concept called perfect compromise equilibrium, where players find compromise decisions that are good in all states. this solution concept is tractable even if states are high dimensional as it does not rely on priors, and it always exists. we demonstrate the power of our solution concept in prominent economic examples, including cournot and bertrand markets, spence's signaling, and bilateral trade with common value.", "categories": "econ.th", "created": "2020-03-05", "updated": "", "authors": ["karl schlag", "andriy zapechelnyuk"], "url": "https://arxiv.org/abs/2003.02539"}, {"title": "impact of congestion charge and minimum wage on tncs: a case study for   san francisco", "id": "2003.02550", "abstract": "this paper describes the impact on transportation network companies (tncs) of the imposition of a congestion charge and a driver minimum wage. the impact is assessed using a market equilibrium model to calculate the changes in the number of passenger trips and trip fare, number of drivers employed, the tnc platform profit, the number of tnc vehicles, and city revenue. two charges are considered: (a) a charge per tnc trip similar to an excise tax, and (b) a charge per vehicle operating hour (whether or not it has a passenger) similar to a road tax. both charges reduce the number of tnc trips, but this reduction is limited by the wage floor, and the number of tnc vehicles reduced is not significant. the time-based charge is preferable to the trip-based charge since, by penalizing idle vehicle time, the former increases vehicle occupancy. in a case study for san francisco, the time-based charge is found to be pareto superior to the trip-based charge as it yields higher passenger surplus, higher platform profits, and higher tax revenue for the city.", "categories": "econ.em math.oc", "created": "2020-03-05", "updated": "2020-09-07", "authors": ["sen li", "kameshwar poolla", "pravin varaiya"], "url": "https://arxiv.org/abs/2003.02550"}, {"title": "backward cusum for testing and monitoring structural change", "id": "2003.02682", "abstract": "it is well known that the conventional cusum test suffers from low power and large detection delay. we therefore propose two alternative detector statistics. the backward cusum detector sequentially cumulates the recursive residuals in reverse chronological order, whereas the stacked backward cusum detector considers a triangular array of backward cumulated residuals. while both the backward cusum detector and the stacked backward cusum detector are suitable for retrospective testing, only the stacked backward cusum detector can be monitored on-line. the limiting distributions of the maximum statistics under suitable sequences of alternatives are derived for retrospective testing and fixed endpoint monitoring. in the retrospective testing context, the local power of the tests is shown to be substantially higher than that for the conventional cusum test if a single break occurs after one third of the sample size. when applied to monitoring schemes, the detection delay of the stacked backward cusum is shown to be much shorter than that of the conventional monitoring cusum procedure. moreover, an infinite horizon monitoring procedure and critical values are presented.", "categories": "econ.em stat.me", "created": "2020-03-05", "updated": "", "authors": ["sven otto", "j\u00f6rg breitung"], "url": "https://arxiv.org/abs/2003.02682"}, {"title": "equal predictive ability tests for panel data with an application to   oecd and imf forecasts", "id": "2003.02803", "abstract": "this paper develops novel tests to compare the predictive ability of two forecasters using panels. we consider two different equal predictive ability (epa) hypotheses. first hypothesis states that the predictive ability of two forecasters is equal on average over all periods and units. under the second one, the epa hypothesis holds jointly for all units. we study the asymptotic properties of proposed tests using sequential limits under strong and weak cross-sectional dependence. their finite sample properties are investigated via monte carlo simulations. they are applied to compare the economic growth forecasts of oecd and imf using data from oecd countries.", "categories": "econ.em", "created": "2020-03-05", "updated": "", "authors": ["oguzhan akgun", "alain pirotte", "giovanni urga", "zhenlin yang"], "url": "https://arxiv.org/abs/2003.02803"}, {"title": "conflict externalization and the quest for peace: theory and case   evidence from colombia", "id": "2003.02990", "abstract": "i study the relationship between the likelihood of a violent domestic conflict and the risk that such a conflict \"externalizes\" (i.e. spreads to another country by creating an international dispute). i consider a situation in which a domestic conflict between a government and a rebel group has the potential to externalize. i show that the risk of externalization increases the likelihood of a peaceful outcome, but only if the government is sufficiently powerful relative to the rebels, the risk of externalization is sufficiently high, and the foreign actor who can intervene in the domestic conflict is sufficiently uninterested in material costs and benefits. i show how this model helps to understand the recent and successful peace process between the colombian government and the country's most powerful rebel group, the revolutionary armed forces of colombia (farc).", "categories": "econ.gn q-fin.ec", "created": "2020-03-05", "updated": "2020-08-25", "authors": ["hector galindo-silva"], "url": "https://arxiv.org/abs/2003.02990"}, {"title": "implementability of honest multi-agent sequential decision-making with   dynamic population", "id": "2003.03173", "abstract": "we study the design of decision-making mechanism for resource allocations over a multi-agent system in a dynamic environment. agents' privately observed preference over resources evolves over time and the population is dynamic due to the adoption of stopping rules. the proposed model designs the rules of encounter for agents participating in the dynamic mechanism by specifying an allocation rule and three payment rules to elicit agents' coupled decision makings of honest preference reporting and optimal stopping over multiple periods. the mechanism provides a special posted-price payment rule that depends only on each agent's realized stopping time to directly influence the population dynamics. this letter focuses on the theoretical implementability of the rules in perfect bayesian nash equilibrium and characterizes necessary and sufficient conditions to guarantee agents' honest equilibrium behaviors over periods. we provide the design principles to construct the payments in terms of the allocation rules and identify the restrictions of the designer's ability to influence the population dynamics. the established conditions make the designer's problem of finding multiple rules to determine an optimal allocation rule.", "categories": "eess.sy cs.sy econ.th", "created": "2020-03-06", "updated": "2020-05-19", "authors": ["tao zhang", "quanyan zhu"], "url": "https://arxiv.org/abs/2003.03173"}, {"title": "double machine learning based program evaluation under unconfoundedness", "id": "2003.03191", "abstract": "this paper consolidates recent methodological developments based on double machine learning (dml) with a focus on program evaluation under unconfoundedness. dml based methods leverage flexible prediction methods to control for confounding in the estimation of (i) standard average effects, (ii) different forms of heterogeneous effects, and (iii) optimal treatment assignment rules. we emphasize that these estimators build all on the same doubly robust score, which allows to utilize computational synergies. an evaluation of multiple programs of the swiss active labor market policy shows how dml based methods enable a comprehensive policy analysis. however, we find evidence that estimates of individualized heterogeneous effects can become unstable.", "categories": "econ.em", "created": "2020-03-06", "updated": "", "authors": ["michael c. knaus"], "url": "https://arxiv.org/abs/2003.03191"}, {"title": "complete subset averaging for quantile regressions", "id": "2003.03299", "abstract": "we propose a novel conditional quantile prediction method based on the complete subset averaging (csa) for quantile regressions. all models under consideration are potentially misspecified and the dimension of regressors goes to infinity as the sample size increases. since we average over the complete subsets, the number of models is much larger than the usual model averaging method which adopts sophisticated weighting schemes. we propose to use an equal weight but select the proper size of the complete subset based on the leave-one-out cross-validation method. building upon the theory of lu and su (2015), we investigate the large sample properties of csa and show the asymptotic optimality in the sense of li (1987). we check the finite sample performance via monte carlo simulations and empirical applications.", "categories": "econ.em stat.me", "created": "2020-03-06", "updated": "", "authors": ["ji hyung lee", "youngki shin"], "url": "https://arxiv.org/abs/2003.03299"}, {"title": "skillcheck: an incentive-based certification system using blockchains", "id": "2003.03540", "abstract": "skill verification is a central problem in workforce hiring. companies and academia often face the difficulty of ascertaining the skills of an applicant since the certifications of the skills claimed by a candidate are generally not immediately verifiable and costly to test. blockchains have been proposed in the literature for skill verification and tamper-proof information storage in a decentralized manner. however, most of these approaches deal with storing the certificates issued by traditional universities on the blockchain. among the few techniques that consider the certification procedure itself, questions like (a) scalability with limited staff, (b) uniformity of grades over multiple evaluators, or (c) honest effort extraction from the evaluators are usually not addressed. we propose a blockchain-based platform named skillcheck, which considers the questions above, and ensure several desirable properties. the platform incentivizes effort in grading via payments with tokens which it generates from the payments of the users of the platform, e.g., the recruiters and test-takers. we provide a detailed description of the design of the platform along with the provable properties of the algorithm.", "categories": "cs.cr econ.gn q-fin.ec", "created": "2020-03-07", "updated": "2020-03-12", "authors": ["jay gupta", "swaprava nath"], "url": "https://arxiv.org/abs/2003.03540"}, {"title": "coronavirus perceptions and economic anxiety", "id": "2003.03848", "abstract": "we provide one of the first systematic assessments of the development and determinants of economic anxiety at the onset of the coronavirus pandemic. using a global dataset on internet searches and two representative surveys from the us, we document a substantial increase in economic anxiety during and after the arrival of the coronavirus. we also document a large dispersion in beliefs about the pandemic risk factors of the coronavirus, and demonstrate that these beliefs causally affect individuals' economic anxieties. finally, we show that individuals' mental models of infectious disease spread understate non-linear growth and shape the extent of economic anxiety.", "categories": "econ.gn q-fin.ec", "created": "2020-03-08", "updated": "2020-07-04", "authors": ["thiemo fetzer", "lukas hensel", "johannes hermle", "christopher roth"], "url": "https://arxiv.org/abs/2003.03848"}, {"title": "favoritism in research assistantship selection in turkish academia", "id": "2003.04060", "abstract": "this article analyzes the procedure for the initial employment of research assistants in turkish universities to see if it complies with the rules and regulations. we manually collected 2409 applicant data from 53 turkish universities to see if applicants are ranked according to the rules suggested by the higher education council of turkey. the rulebook states that applicants should be ranked according to a final score based on the weighted average of their gpa, graduate examination score, academic examination score, and foreign language skills score. thus, the research assistant selection is supposed to be a fair process where each applicant is evaluated based on objective metrics. however, our analysis of data suggests that the final score of the applicants is almost entirely based on the highly subjective academic examination conducted by the hiring institution. thus, the applicants gpa, standardized graduate examination score, standardized foreign language score are irrelevant in the selection process, making it a very unfair process based on favoritism.", "categories": "econ.gn q-fin.ec", "created": "2020-03-09", "updated": "", "authors": ["osman gulseven"], "url": "https://arxiv.org/abs/2003.04060"}, {"title": "unit root testing with slowly varying trends", "id": "2003.04066", "abstract": "a unit root test is proposed for time series with a general nonlinear deterministic trend component. it is shown that asymptotically the pooled ols estimator of overlapping blocks filters out any trend component that satisfies some lipschitz condition. under both fixed-$b$ and small-$b$ block asymptotics, the limiting distribution of the t-statistic for the unit root hypothesis is derived. nuisance parameter corrections provide heteroskedasticity-robust tests, and serial correlation is accounted for by pre-whitening. a monte carlo study that considers slowly varying trends yields both good size and improved power results for the proposed tests when compared to conventional unit root tests.", "categories": "econ.em", "created": "2020-03-09", "updated": "2020-08-06", "authors": ["sven otto"], "url": "https://arxiv.org/abs/2003.04066"}, {"title": "effect of segregation on inequality in kinetic models of wealth exchange", "id": "2003.04129", "abstract": "empirical distributions of wealth and income can be reproduced using simplified agent-based models of economic interactions, analogous to microscopic collisions of gas particles. building upon these models of freely interacting agents, we explore the effect of a segregated economic network in which interactions are restricted to those between agents of similar wealth. agents on a 2d lattice undergo kinetic exchanges with their nearest neighbours, while continuously switching places to minimize local wealth differences. a spatial concentration of wealth leads to a steady state with increased global inequality and a magnified distinction between local and global measures of combatting poverty. individual saving propensity proves ineffective in the segregated economy, while redistributive taxation transcends the spatial inhomogeneity and greatly reduces inequality. adding fluctuations to the segregation dynamics, we observe a sharp phase transition to lower inequality at a critical temperature, accompanied by a sudden change in the distribution of the wealthy elite.", "categories": "physics.soc-ph econ.gn nlin.ao q-fin.ec", "created": "2020-03-04", "updated": "", "authors": ["lennart fernandes", "jacques tempere"], "url": "https://arxiv.org/abs/2003.04129"}, {"title": "fast bayesian record linkage with record-specific disagreement   parameters", "id": "2003.04238", "abstract": "applied researchers are often interested in linking individuals between two datasets that lack unique identifiers. accuracy and computational feasibility are a challenge, particularly when linking large datasets. we develop a bayesian method for automated probabilistic record linkage and show it recovers 40% more true matches, holding accuracy constant, than comparable methods in a matching of union army recruitment data to the 1900 us census for which expert-labelled true matches are known. our approach, which builds on a recent state-of-the-art bayesian method, refines the modelling of comparison data, allowing disagreement probability parameters conditional on non-match status to be record-specific. to make this refinement computationally feasible, we implement a gibbs sampler that achieves significant improvement in speed over comparable recent implementations. we also generalize the notion of comparison data to allow for treatment of very common first names that spuriously produce exact matches in record pairs and show how to estimate true positive rate and positive predictive value when ground truth is unavailable.", "categories": "stat.me econ.em stat.ap", "created": "2020-03-09", "updated": "", "authors": ["thomas stringham"], "url": "https://arxiv.org/abs/2003.04238"}, {"title": "mechanism design for large scale systems", "id": "2003.04263", "abstract": "in this paper, we consider infinite number of non atomic self-interested agents with private valuation of a divisible good. we design a pricing mechanism that is easy to implement, is individually rational, weakly budget balanced and incentive compatible. in this mechanism, agents send reports of their types, based on which, the designer solves a constrained optimization problem through lagrange's mechanism. the resulting optimal allocation and lagrange's multiplier is sent as the allocation and prices to the respective agent. we show that reporting one's type truthfully is a dominant strategy of the players in this mechanism. we then extend this idea to the dynamic case, when player's types are dynamically evolving as a controlled markov process. in this case, in each time period, reporting one's type is a dominant strategy of the players.", "categories": "cs.gt econ.th", "created": "2020-03-09", "updated": "", "authors": ["meng zhang", "deepanshu vasal"], "url": "https://arxiv.org/abs/2003.04263"}, {"title": "optimal trade strategy of a regional economy by food exports", "id": "2003.04307", "abstract": "this paper examines the export promotion of processed foods by a regional economy and regional vitalisation policy. we employ bertrand models that contain a major home producer and a home producer in a local area. in our model, growth in the profit of one producer does not result in an increase in the profit of the other, despite strategic complements. we show that the profit of the producer in the local area decreases because of the deterioration of a location condition, and its profit increases through the reinforcement of the administrative guidance. furthermore, when the inefficiency of the location worsens, the local government should optimally decrease the level of administrative guidance. hence, the local government should strategically eliminate this inefficiency to maintain a sufficient effect of administrative guidance.", "categories": "econ.gn q-fin.ec", "created": "2020-03-08", "updated": "", "authors": ["m. okimoto"], "url": "https://arxiv.org/abs/2003.04307"}, {"title": "identification and estimation of weakly separable models without   monotonicity", "id": "2003.04337", "abstract": "we study the identification and estimation of treatment effect parameters in weakly separable models. in their seminal work, vytlacil and yildiz (2007) showed how to identify and estimate the average treatment effect of a dummy endogenous variable when the outcome is weakly separable in a single index. their identification result builds on a monotonicity condition with respect to this single index. in comparison, we consider similar weakly separable models with multiple indices, and relax the monotonicity condition for identification. unlike vytlacil and yildiz (2007), we exploit the full information in the distribution of the outcome variable, instead of just its mean. indeed, when the outcome distribution function is more informative than the mean, our method is applicable to more general settings than theirs; in particular we do not rely on their monotonicity assumption and at the same time we also allow for multiple indices. to illustrate the advantage of our approach, we provide examples of models where our approach can identify parameters of interest whereas existing methods would fail. these examples include models with multiple unobserved disturbance terms such as the roy model and multinomial choice models with dummy endogenous variables, as well as potential outcome models with endogenous random coefficients. our method is easy to implement and can be applied to a wide class of models. we establish standard asymptotic properties such as consistency and asymptotic normality.", "categories": "econ.em", "created": "2020-03-09", "updated": "2020-04-03", "authors": ["songnian chen", "shakeeb khan", "xun tang"], "url": "https://arxiv.org/abs/2003.04337"}, {"title": "a systematic and analytical review of the socioeconomic and   environmental impact of the deployed high-speed rail (hsr) systems on the   world", "id": "2003.04452", "abstract": "the installation of high-speed rail in the world during the last two decades resulted in significant socioeconomic and environmental changes. the u.s. has the longest rail network in the world, but the focus is on carrying a wide variety of loads including coal, farm crops, industrial products, commercial goods, and miscellaneous mixed shipments. freight and passenger services in the u.s. dates to 1970, with both carried out by private railway companies. railways were the main means of transport between cities from the late 19th century through the middle of the 20th century. however, rapid growth in production and improvements in technologies changed those dynamics. the fierce competition for comfortability and pleasantness in passenger travel and the proliferation of aviation services in the u.s. channeled federal and state budgets towards motor vehicle infrastructure, which brought demand for railroads to a halt in the 1950s. presently, the u.s. has no high-speed trains, aside from sections of amtrak s acela line in the northeast corridor that can reach 150 mph for only 34 miles of its 457-mile span. the average speed between new york and boston is about 65 mph. on the other hand, china has the world s fastest and largest high-speed rail network, with more than 19,000 miles, of which the vast majority was built in the past decade. japan s bullet trains can reach nearly 200 miles per hour and dates to the 1960s. that system moved more than 9 billion people without a single passenger casualty. in this systematic review, we studied the effect of high-speed rail (hsr) on the u.s. and other countries including france, japan, germany, italy, and china in terms of energy consumption, land use, economic development, travel behavior, time use, human health, and quality of life.", "categories": "econ.gn q-fin.ec", "created": "2020-03-09", "updated": "2020-03-18", "authors": ["mohsen momenitabar", "zhila dehdari ebrahimi", "mohammad arani"], "url": "https://arxiv.org/abs/2003.04452"}, {"title": "a new approach for macroscopic analysis to improve the technical and   economic impacts of urban interchanges on traffic networks", "id": "2003.04459", "abstract": "pursuing three important elements including economic, safety, and traffic are the overall objective of decision evaluation across all transport projects. in this study, we investigate the feasibility of the development of city interchanges and road connections for network users. to achieve this goal, a series of minor goals are required to be met in advance including determining benefits, costs of implement-ing new highway interchanges, quantifying the effective parameters, the increase in fuel consumption, the reduction in travel time, and finally influence on travel speed. in this study, geometric advancement of hakim highway, and yadegar-e-emam highway were investigated in the macro view from the cloverleaf inter-section with a low capacity to a three-level directional intersection of the enhanced cloverleaf. for this purpose, the simulation was done by emme software of inro company. the results of the method were evaluated by the objective of net present value (npv), and the benefit and cost of each one was stated precisely in different years. at the end, some suggestion has been provided.", "categories": "econ.gn q-fin.ec", "created": "2020-03-09", "updated": "2020-09-17", "authors": ["seyed hassan hosseini", "ahmad mehrabian", "zhila dehdari ebrahimi", "mohsen momenitabar", "mohammad arani"], "url": "https://arxiv.org/abs/2003.04459"}, {"title": "a mean-field game approach to equilibrium pricing, optimal generation,   and trading in solar renewable energy certificate markets", "id": "2003.04938", "abstract": "solar renewable energy certificate (srec) markets are a market-based system designed to incentivize solar energy generation. a regulatory body imposes a lower bound on the amount of energy each regulated firm must generate via solar means, providing them with a certificate for each mwh generated. regulated firms seek to navigate the market to minimize the cost imposed on them, by modulating their srec generation and trading activities. as such, the srec market can be viewed through the lens of a large stochastic game with heterogeneous agents, where agents interact through the market price of the certificates. we study this stochastic game by solving the mean-field game (mfg) limit with sub-populations of heterogeneous agents. our market participants optimize costs accounting for trading frictions, cost of generation, non-linear non-compliance penalty, and generation uncertainty. moreover, we endogenize srec price through market clearing. using techniques from variational analysis, we characterize firms' optimal controls as the solution of mckean-vlasov (mv) fbsdes and determine the equilibrium srec price. we establish the existence and uniqueness of a solution to this mv-fbsde, and further prove that the mfg strategies have the $\\epsilon$-nash property for the finite player game. finally, we develop a numerical scheme for solving the mv-fbsdes and conclude by demonstrating how firms behave in equilibrium using simulated examples.", "categories": "q-fin.mf cs.sy econ.th eess.sy math.oc q-fin.tr", "created": "2020-03-10", "updated": "2020-05-05", "authors": ["arvind shrivats", "dena firoozi", "sebastian jaimungal"], "url": "https://arxiv.org/abs/2003.04938"}, {"title": "a climate insidium with a price on warming", "id": "2003.05114", "abstract": "in this paper, i introduce a new emissions trading system (ets) design to address the problems with existing etss and carbon taxes. first, existing ets designs inhibit emissions but do not constrain warming to any set level. existing etss have the indirect objective of reducing emissions instead of directly reducing warming. even a global mechanism using an existing ets cannot guarantee a particular warming path. part 1: a price on warming addresses this. my proposed market trades contracts tied to temperature in a double-sided auction of emissions permits and sequestration contracts. unlike existing etss, the mechanism has a consistent timescale and metric tied to warming, with explicit limits on global temperature in every period into the far future. every auction finds prices for emissions into the far future. second, if a jurisdiction does not require firms to manage their emissions, the firms have little incentive to do so. part 2: a climate insidium addresses this. my design incentivizes firms to participate even if their jurisdictions do not join. with sanctions from member jurisdictions and participating firms, the design has bottom-up incentives for joining, and the incentives rise over time under realistic conditions, potentially resulting in a rush to join. third, existing designs have high transaction costs for implementation, requiring international treaties to begin. part 3: a faster path forward addresses this. i propose a path without national or international action to begin. a coalition can implement these rules, creating political force to accelerate participation. full implementation still requires national agreements. this design appears to be closer to \"first best\", with a lower cost of climate mitigation, than any in the literature, while increasing the certainty of avoiding catastrophic global warming. it might also provide a faster pathway to implementation.", "categories": "econ.gn q-fin.ec", "created": "2020-03-11", "updated": "", "authors": ["john f. raffensperger"], "url": "https://arxiv.org/abs/2003.05114"}, {"title": "on the structure of the world economy: an absorbing markov chain   approach", "id": "2003.05204", "abstract": "the expansion of global production networks has raised many important questions about the interdependence among countries and how future changes in the world economy are likely to affect the countries' positioning in global value chains. we are approaching the structure and lengths of value chains from a completely different perspective than has been available so far. by assigning a random endogenous variable to a network linkage representing the number of intermediate sales/purchases before absorption (final use or value added), the discrete-time absorbing markov chains proposed here shed new light on the world input/output networks. the variance of this variable can help assess the risk when shaping the chain length and optimize the level of production. contrary to what might be expected simply on the basis of comparative advantage, the results reveal that both the input and output chains exhibit the same quasi-stationary product distribution. put differently, the expected proportion of time spent in a state before absorption is invariant to changes of the network type. finally, the several global metrics proposed here, including the probability distribution of global value added/final output, provide guidance for policy makers when estimating the resilience of world trading system and forecasting the macroeconomic developments.", "categories": "econ.gn q-fin.ec", "created": "2020-03-11", "updated": "", "authors": ["olivera kostoska", "viktor stojkoski", "ljupco kocarev"], "url": "https://arxiv.org/abs/2003.05204"}, {"title": "a mixture autoregressive model based on gaussian and student's   $t$-distributions", "id": "2003.05221", "abstract": "we introduce a new mixture autoregressive model which combines gaussian and student's $t$ mixture components. the model has very attractive properties analogous to the gaussian and student's $t$ mixture autoregressive models, but it is more flexible as it enables to model series which consist of both conditionally homoscedastic gaussian regimes and conditionally heteroscedastic student's $t$ regimes. the usefulness of our model is demonstrated in an empirical application to the monthly u.s. interest rate spread between the 3-month treasury bill rate and the effective federal funds rate.", "categories": "econ.em math.st stat.me stat.th", "created": "2020-03-11", "updated": "2020-05-22", "authors": ["savi virolainen"], "url": "https://arxiv.org/abs/2003.05221"}, {"title": "can society function without ethical agents? an informational   perspective", "id": "2003.05441", "abstract": "many facts are learned through the intermediation of individuals with special access to information, such as law enforcement officers, officials with a security clearance, or experts with specific knowledge. this paper considers whether societies can learn about such facts when information is cheap to manipulate, produced sequentially, and these individuals are devoid of ethical motive. the answer depends on an \"information attrition\" condition pertaining to the amount of evidence available which distinguishes, for example, between reproducible scientific evidence and the evidence generated in a crime. applications to institution enforcement, social cohesion, scientific progress, and historical revisionism are discussed.", "categories": "cs.gt econ.th", "created": "2020-03-10", "updated": "", "authors": ["bruno strulovici"], "url": "https://arxiv.org/abs/2003.05441"}, {"title": "electoral systems and international trade policy", "id": "2003.05725", "abstract": "we develop a simple theoretic game a model to analyze the relationship between electoral sys tems and governments' choice in trade policies. we show that existence of international pressure or foreign lobby changes a government's final decision on trade policy, and trade policy in countries with proportional electoral system is more protectionist than in countries with majoritarian electoral system. moreover, lobbies pay more to affect the trade policy outcomes in countries with proportional representation systems.", "categories": "econ.gn econ.th q-fin.ec", "created": "2020-03-12", "updated": "", "authors": ["serkan kucuksenel", "osman gulseven"], "url": "https://arxiv.org/abs/2003.05725"}, {"title": "indemnity payments in agricultural insurance: risk exposure of eu states", "id": "2003.05726", "abstract": "this study estimates the risk contributions of individual european countries regarding the indemnity payments in agricultural insurance. we model the total risk exposure as an insurance portfolio where each country is unique in terms of its risk characteristics. the data has been collected from the recent surveys conducted by the european commission and the world bank. farm accountancy data network is used as well. 22 out of 26 member states are included in the study. the results suggest that the euromediterranean countries are the major risk contributors. these countries not only have the highest expected loss but also high volatility of indemnity payments. nordic countries have the lowest indemnity payments and risk exposure.", "categories": "econ.gn q-fin.ec", "created": "2020-03-12", "updated": "", "authors": ["osman gulseven", "kasirga yildirak"], "url": "https://arxiv.org/abs/2003.05726"}, {"title": "escaping cannibalization? correlation-robust pricing for a unit-demand   buyer", "id": "2003.05913", "abstract": "we consider a robust version of the revenue maximization problem, where a single seller wishes to sell $n$ items to a single unit-demand buyer. in this robust version, the seller knows the buyer's marginal value distribution for each item separately, but not the joint distribution, and prices the items to maximize revenue in the worst case over all compatible correlation structures. we devise a computationally efficient (polynomial in the support size of the marginals) algorithm that computes the worst-case joint distribution for any choice of item prices. and yet, in sharp contrast to the additive buyer case (carroll, 2017), we show that it is np-hard to approximate the optimal choice of prices to within any factor better than $n^{1/2-\\epsilon}$. for the special case of marginal distributions that satisfy the monotone hazard rate property, we show how to guarantee a constant fraction of the optimal worst-case revenue using item pricing; this pricing equates revenue across all possible correlations and can be computed efficiently.", "categories": "cs.gt cs.cc econ.th", "created": "2020-03-12", "updated": "2020-08-25", "authors": ["moshe babaioff", "michal feldman", "yannai a. gonczarowski", "brendan lucier", "inbal talgam-cohen"], "url": "https://arxiv.org/abs/2003.05913"}, {"title": "causal spillover effects using instrumental variables", "id": "2003.06023", "abstract": "i set up a potential-outcomes framework to analyze spillover effects using instrumental variables. i characterize the population compliance types in a setting in which spillovers can occur on both treatment take-up and outcomes, and provide conditions for identification of the marginal distribution of these compliance types. i show that intention-to-treat (itt) parameters aggregate multiple direct and spillover effects for different compliance types, and hence do not have a clear link to causally interpretable parameters. moreover, rescaling itt parameters by first-stage estimands generally recovers a weighted combination of average effects where the sum of weights is larger than one. i then analyze identification of causal direct and spillover effects under one-sided noncompliance, and propose simple estimators that are consistent and asymptotically normal under mild conditions. i use the proposed methods to analyze an experiment on social interactions and voting behavior.", "categories": "econ.em", "created": "2020-03-12", "updated": "2020-09-08", "authors": ["gonzalo vazquez-bare"], "url": "https://arxiv.org/abs/2003.06023"}, {"title": "a risk aware two-stage market mechanism for electricity with renewable   generation", "id": "2003.06119", "abstract": "over the last few decades, electricity markets around the world have adopted multi-settlement structures, allowing for balancing of supply and demand as more accurate forecast information becomes available. given increasing uncertainty due to adoption of renewables, more recent market design work has focused on optimization of expectation of some quantity, e.g. social welfare. however, social planners and policy makers are often risk averse, so that such risk neutral formulations do not adequately reflect prevailing attitudes towards risk, nor explain the decisions that follow. hence we incorporate the commonly used risk measure conditional value at risk (cvar) into the central planning objective, and study how a two-stage market operates when the individual generators are risk neutral. our primary result is to show existence (by construction) of a sequential competitive equilibrium (sceq) in this risk-aware two-stage market. given equilibrium prices, we design a market mechanism which achieves social cost minimization assuming that agents are non strategic.", "categories": "eess.sy cs.sy econ.th math.oc", "created": "2020-03-13", "updated": "", "authors": ["nathan dahlin", "rahul jain"], "url": "https://arxiv.org/abs/2003.06119"}, {"title": "targeting customers under response-dependent costs", "id": "2003.06271", "abstract": "this study provides a formal analysis of the customer targeting decision problem in settings where the cost for marketing action is stochastic and proposes a framework to efficiently estimate the decision variables for campaign profit optimization. targeting a customer is profitable if the positive impact of the marketing treatment on the customer and the associated profit to the company is higher than the cost of the treatment. while there is a growing literature on developing causal or uplift models to identify the customers who are impacted most strongly by the marketing action, no research has investigated optimal targeting when the costs of the action are uncertain at the time of the targeting decision. because marketing incentives are routinely conditioned on a positive response by the customer, e.g. a purchase or contract renewal, stochastic costs are ubiquitous in direct marketing and customer retention campaigns. this study makes two contributions to the literature, which are evaluated on a coupon targeting campaign in an e-commerce setting. first, the authors formally analyze the targeting decision problem under response-dependent costs. profit-optimal targeting requires an estimate of the treatment effect on the customer and an estimate of the customer response probability under treatment. the empirical results demonstrate that the consideration of treatment cost substantially increases campaign profit when used for customer targeting in combination with the estimation of the average or customer-level treatment effect. second, the authors propose a framework to jointly estimate the treatment effect and the response probability combining methods for causal inference with a hurdle mixture model. the proposed causal hurdle model achieves competitive campaign profit while streamlining model building. the code for the empirical analysis is available on github.", "categories": "econ.em stat.ap", "created": "2020-03-13", "updated": "", "authors": ["johannes haupt", "stefan lessmann"], "url": "https://arxiv.org/abs/2003.06271"}, {"title": "a model of justification", "id": "2003.06844", "abstract": "i consider decision-making constrained by considerations of morality, rationality, or other virtues. the decision maker (dm) has a true preference over outcomes, but feels compelled to choose among outcomes that are top-ranked by some preference that he considers \"justifiable.\" this model unites a broad class of empirical work on distributional preferences, charitable donations, prejudice/discrimination, and corruption/bribery. i provide a behavioral characterization of the model. i also show that the set of justifications can be identified from choice behavior when the true preference is known, and that choice behavior substantially restricts both the true preference and justifications when neither is known. i argue that the justifiability model represents an advancement over existing models of rationalization because the structure it places on possible \"rationales\" improves tractability, interpretation and identification.", "categories": "econ.th", "created": "2020-03-15", "updated": "", "authors": ["sarah ridout"], "url": "https://arxiv.org/abs/2003.06844"}, {"title": "degrees of displacement: the impact of household pv battery prosumage on   utility generation and storage", "id": "2003.06987", "abstract": "reductions in the cost of pv and batteries encourage households to invest in pv battery prosumage. we explore the implications for the rest of the power sector by applying two open-source techno-economic models to scenarios in western australia for the year 2030. household pv capacity generally substitutes utility pv, but slightly less so as additional household batteries are installed. wind power is less affected, especially in scenarios with higher shares of renewables. with household batteries operating to maximise self-consumption, utility battery capacities are hardly substituted. wholesale prices to supply households, including those not engaging in prosumage, slightly decrease, while prices for other consumers slightly increase. we conclude that the growth of prosumage has implications on the various elements of the power sector and should be more thoroughly considered by investors, regulators, and power sector planners.", "categories": "econ.gn cs.sy eess.sy physics.soc-ph q-fin.ec", "created": "2020-03-15", "updated": "", "authors": ["kelvin say", "wolf-peter schill", "michele john"], "url": "https://arxiv.org/abs/2003.06987"}, {"title": "uniqueness of dp-nash subgraphs and d-sets in capacitated graphs of   netflix games", "id": "2003.07106", "abstract": "we explore the uniqueness of pure strategy nash equilibria in the netflix games of gerke et al. (arxiv:1905.01693, 2019). let $g=(v,e)$ be a graph and $\\kappa:\\ v\\to \\mathbb{z}_{\\ge 0}$ a function, and call the pair $(g, \\kappa)$ a capacitated graph. a spanning subgraph $h$ of $(g, \\kappa)$ is called a $dp$-nash subgraph if $h$ is bipartite with partite sets $x,y$ called the $d$-set and $p$-set of $h$, respectively, such that no vertex of $p$ is isolated and for every $x\\in x,$ $d_h(x)=\\min\\{d_g(x),\\kappa(x)\\}.$ we prove that whether $(g,\\kappa)$ has a unique $dp$-nash subgraph can be decided in polynomial time. we also show that when $\\kappa(v)=k$ for every $v\\in v$, the problem of deciding whether $(g,\\kappa)$ has a unique $d$-set is polynomial time solvable for $k=0$ and 1, and co-np-complete for $k\\ge2.$", "categories": "math.co econ.th", "created": "2020-03-16", "updated": "2020-03-17", "authors": ["gregory gutin", "philip r neary", "anders yeo"], "url": "https://arxiv.org/abs/2003.07106"}, {"title": "a hedonic metric approach to estimating the demand for differentiated   products: an application to retail milk demand", "id": "2003.07197", "abstract": "this article introduces the hedonic metric (hm) approach as an original method to model the demand for differentiated products. using this approach, initially, we create an n-dimensional hedonic space based on the characteristic information available to consumers. next, we allocate products into this space and estimate the elasticities using distances. our model makes it possible to estimate a large number of differentiated products in a single demand system. we applied our model to estimate the retail demand for fluid milk products.", "categories": "econ.gn q-fin.ec", "created": "2020-03-12", "updated": "", "authors": ["osman gulseven", "michael wohlgenant"], "url": "https://arxiv.org/abs/2003.07197"}, {"title": "game theoretic consequences of resident matching", "id": "2003.07205", "abstract": "the resident matching algorithm, gale-shapley, currently used by sf match and the national residency match program, has been in use for over 50 years without fundamental alteration. the algorithm is a 'stable-marriage' method that favors applicant outcomes. however, in these 50 years, there has been a big shift in the supply and demand of applicants and programs. these changes along with the way the match is implemented have induced a costly race among applicants to apply and interview at as many programs as possible. meanwhile programs also incur high costs as they maximize their probability of matching by interviewing as many candidates as possible.", "categories": "econ.th", "created": "2020-03-12", "updated": "", "authors": ["yue wu"], "url": "https://arxiv.org/abs/2003.07205"}, {"title": "testing many restrictions under heteroskedasticity", "id": "2003.07320", "abstract": "we propose a hypothesis test that allows for many tested restrictions in a heteroskedastic linear regression model. the test compares the conventional f-statistic to a critical value that corrects for many restrictions and conditional heteroskedasticity. the correction utilizes leave-one-out estimation to recenter the conventional critical value and leave-three-out estimation to rescale it. large sample properties of the test are established in an asymptotic framework where the number of tested restrictions may grow in proportion to the number of observations. we show that the test is asymptotically valid and has non-trivial asymptotic power against the same local alternatives as the exact f test when the latter is valid. simulations corroborate the relevance of these theoretical findings and suggest excellent size control in moderately small samples also under strong heteroskedasticity.", "categories": "econ.em stat.me", "created": "2020-03-16", "updated": "", "authors": ["stanislav anatolyev", "mikkel s\u00f8lvsten"], "url": "https://arxiv.org/abs/2003.07320"}, {"title": "keeping the listener engaged: a dynamic model of bayesian persuasion", "id": "2003.07338", "abstract": "we consider a dynamic model of bayesian persuasion. over time, a sender performs a series of experiments to persuade a receiver to take a desired action. due to constraints on the information flow, the sender must take real time to persuade, and the receiver may stop listening and take a final action at any time. in addition, persuasion is costly for both players. to incentivize the receiver to listen, the sender must leave rents that compensate his listening costs, but neither player can commit to her/his future actions. persuasion may totally collapse in markov perfect equilibrium (mpe) of this game. however, for persuasion costs sufficiently small, a version of a folk theorem holds: outcomes that approximate kamenica and gentzkow (2011)'s sender-optimal persuasion as well as full revelation (which is most preferred by the receiver) and everything in between are obtained in mpe, as the cost vanishes.", "categories": "econ.th", "created": "2020-03-16", "updated": "", "authors": ["yeon-koo che", "kyungmin kim", "konrad mierendorff"], "url": "https://arxiv.org/abs/2003.07338"}, {"title": "diagonal preconditioning: theory and algorithms", "id": "2003.07545", "abstract": "diagonal preconditioning has been a staple technique in optimization and machine learning. it often reduces the condition number of the design or hessian matrix it is applied to, thereby speeding up convergence. however, rigorous analyses of how well various diagonal preconditioning procedures improve the condition number of the preconditioned matrix and how that translates into improvements in optimization are rare. in this paper, we first provide an analysis of a popular diagonal preconditioning technique based on column standard deviation and its effect on the condition number using random matrix theory. then we identify a class of design matrices whose condition numbers can be reduced significantly by this procedure. we then study the problem of optimal diagonal preconditioning to improve the condition number of any full-rank matrix and provide a bisection algorithm and a potential reduction algorithm with $o(\\log(\\frac{1}{\\epsilon}))$ iteration complexity, where each iteration consists of an sdp feasibility problem and a newton update using the nesterov-todd direction, respectively. finally, we extend the optimal diagonal preconditioning algorithm to an adaptive setting and compare its empirical performance at reducing the condition number and speeding up convergence for regression and classification problems with that of another adaptive preconditioning technique, namely batch normalization, that is essential in training machine learning models.", "categories": "cs.lg econ.em math.st stat.ml stat.th", "created": "2020-03-17", "updated": "2020-03-24", "authors": ["zhaonan qu", "yinyu ye", "zhengyuan zhou"], "url": "https://arxiv.org/abs/2003.07545"}, {"title": "anomalous supply shortages from dynamic pricing in on-demand mobility", "id": "2003.07736", "abstract": "dynamic pricing schemes are increasingly employed across industries to maintain a self-organized balance of demand and supply. however, throughout complex dynamical systems, unintended collective states exist that may compromise their function. here we reveal how dynamic pricing may induce demand-supply imbalances instead of preventing them. combining game theory and time series analysis of dynamic pricing data from on-demand ride-hailing services, we explain this apparent contradiction. we derive a phase diagram demonstrating how and under which conditions dynamic pricing incentivizes collective action of ride-hailing drivers to induce anomalous supply shortages. by disentangling different timescales in price time series of ride-hailing services at 137 locations across the globe, we identify characteristic patterns in the price dynamics reflecting these anomalous supply shortages. our results provide systemic insights for the regulation of dynamic pricing, in particular in publicly accessible mobility systems, by unraveling under which conditions dynamic pricing schemes promote anomalous supply shortages.", "categories": "physics.soc-ph econ.em", "created": "2020-03-16", "updated": "", "authors": ["malte schr\u00f6der", "david-maximilian storch", "philip marszal", "marc timme"], "url": "https://arxiv.org/abs/2003.07736"}, {"title": "nise estimation of an economic model of crime", "id": "2003.07860", "abstract": "an economic model of crime is used to explore the consistent estimation of a simultaneous linear equation without recourse to instrumental variables. a maximum-likelihood procedure (nise) is introduced, and its results are compared to ordinary least squares and two-stage least squares. the paper is motivated by previous research on the crime model and by the well-known practical problem that valid instruments are frequently unavailable.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2020-03-17", "updated": "", "authors": ["eric blankmeyer"], "url": "https://arxiv.org/abs/2003.07860"}, {"title": "ethnic groups' access to state power and group size", "id": "2003.08064", "abstract": "many countries are ethnically diverse. however, despite the benefits of ethnic heterogeneity, ethnic-based political inequality and discrimination are pervasive. why is this? this study suggests that part of the variation in ethnic-based political inequality depends on the relative size of ethnic groups within each country. using group-level data for 569 ethnic groups in 175 countries from 1946 to 2017, i find evidence of an inverted-u-shaped relationship between an ethnic group's relative size and its access to power. this single-peaked relationship is robust to many alternative specifications, and a battery of robustness checks suggests that relative size influences access to power. through a very simple model, i propose an explanation based on an initial high level of political inequality, and on the incentives that more powerful groups have to continue limiting other groups' access to power. this explanation incorporates essential elements of several existing theories on the relationship between group size and discrimination, and suggests a new empirical prediction: the single-peaked pattern should be weaker in countries where political institutions have historically been less open. this additional prediction is supported by the data.", "categories": "econ.gn q-fin.ec", "created": "2020-03-18", "updated": "", "authors": ["hector galindo-silva"], "url": "https://arxiv.org/abs/2003.08064"}, {"title": "modeling of the greek road transportation network using complex network   analysis", "id": "2003.08091", "abstract": "this article studies the interregional greek road network (grn) by applying complex network analysis (cna) and an empirical approach. the study aims to extract the socioeconomic information immanent to the grn's topology and to interpret the way in which this road network serves and promotes the regional development. the analysis shows that the topology of the grn is submitted to spatial constraints, having lattice-like characteristics. also, the grn's structure is described by a gravity pattern, where places of higher population enjoy greater functionality, and its interpretation in regional terms illustrates the elementary pattern expressed by regional development through road construction. the study also reveals some interesting contradictions between the metropolitan and non-metropolitan (excluding attica and thessaloniki) comparison. overall, the article highlights the effectiveness of using complex network analysis in the modeling of spatial networks and in particular of transportation systems and promotes the use of the network paradigm in the spatial and regional research.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-18", "updated": "", "authors": ["dimitrios tsiotas"], "url": "https://arxiv.org/abs/2003.08091"}, {"title": "transportation networks and their significance to economic development", "id": "2003.08094", "abstract": "this article attempts to highlight the importance that transportation has in the economic development of greece and in particular the importance of the transportation infrastructure and transportation networks, which suggest a fixed structured capital covering the total of the country. for this purpose, longitudinal and cross-sectoral statistical data are examined over a set of fundamental macroeconomic measures and metrics. furthermore, the study attempts to highlight the structural and functional aspects composing the concept of transportation networks and to highlight the necessity of their joint consideration on the relevant research. the transportation networks that are examined in this paper are the greek road (grn), rail (gran), maritime (gmn) and air transport network (gan), which are studied both in terms of their geometry and technical characteristics, as well as of their historical, traffic and political framework. for the empirical assessment of the transportation networks importance in greece an econometric model is constructed, expressing the welfare level of the greek regions as a multivariate function of their transportation infrastructure and of their socioeconomic environment. the further purpose of the article is to highlight, macroscopically, all the aspects related the study of transportation infrastructure and networks.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-18", "updated": "", "authors": ["dimitrios tsiotas", "martha geraki", "spyros niavis"], "url": "https://arxiv.org/abs/2003.08094"}, {"title": "the commuting phenomenon as a complex network: the case of greece", "id": "2003.08096", "abstract": "this article studies the greek interregional commuting network (grn) by using measures and methods of complex network analysis and empirical techniques. the study aims to detect structural characteristics of the commuting phenomenon, which are configured by the functionality of the land transport infrastructures, and to interpret how this network serves and promotes the regional development. in the empirical analysis, a multiple linear regression model for the number of commuters is constructed, which is based on the conceptual framework of the term network, in effort to promote the interdisciplinary dialogue. the analysis highlights the effect of the spatial constraints on the network's structure, provides information on the major road transport infrastructure projects that constructed recently and influenced the country capacity, and outlines a gravity pattern describing the commuting phenomenon, which expresses that cities of high population attract large volumes of commuting activity within their boundaries, a fact that contributes to the reduction of their outgoing commuting and consequently to the increase of their inbound productivity. overall, this paper highlights the effectiveness of complex network analysis in the modeling of spatial and particularly of transportation network and promotes the use of the network paradigm in the spatial and regional research.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-18", "updated": "", "authors": ["dimitrios tsiotas", "konstantinos raptopoulos"], "url": "https://arxiv.org/abs/2003.08096"}, {"title": "experimental design under network interference", "id": "2003.08421", "abstract": "this paper discusses the problem of the design of a two-wave experiment under network interference. we consider (i) a possibly fully connected network, (ii) spillover effects occurring across neighbors, (iii) local dependence of unobservables characteristics. we allow for a class of estimands of interest which includes the average effect of treating the entire network, the average spillover effects, average direct effects, and interactions of the latter two. we propose a design mechanism where the experimenter optimizes over participants and treatment assignments to minimize the variance of the estimators of interest, using the first-wave experiment for estimation of the variance. we characterize conditions on the first and second wave experiments to guarantee unconfounded experimentation, we showcase tradeoffs in the choice of the pilot's size, and we formally characterize the pilot's size relative to the main experiment. we derive asymptotic properties of estimators of interest under the proposed design mechanism and regret guarantees of the proposed method. finally we illustrate the advantage of the method over state-of-art methodologies on simulated and real-world networks.", "categories": "econ.em stat.me", "created": "2020-03-18", "updated": "2020-06-11", "authors": ["davide viviano"], "url": "https://arxiv.org/abs/2003.08421"}, {"title": "causal simulation experiments: lessons from bias amplification", "id": "2003.08449", "abstract": "recent theoretical work in causal inference has explored an important class of variables which, when conditioned on, may further amplify existing unmeasured confounding bias (bias amplification). despite this theoretical work, existing simulations of bias amplification in clinical settings have suggested bias amplification may not be as important in many practical cases as suggested in the theoretical literature.we resolve this tension by using tools from the semi-parametric regression literature leading to a general characterization in terms of the geometry of ols estimators which allows us to extend current results to a larger class of dags, functional forms, and distributional assumptions. we further use these results to understand the limitations of current simulation approaches and to propose a new framework for performing causal simulation experiments to compare estimators. we then evaluate the challenges and benefits of extending this simulation approach to the context of a real clinical data set with a binary treatment, laying the groundwork for a principled approach to sensitivity analysis for bias amplification in the presence of unmeasured confounding.", "categories": "stat.me econ.em math.st stat.ap stat.th", "created": "2020-03-18", "updated": "", "authors": ["tyrel stokes", "russell steele", "ian shrier"], "url": "https://arxiv.org/abs/2003.08449"}, {"title": "gender bias in the erasmus students network", "id": "2003.09167", "abstract": "the erasmus program (european community action scheme for the mobility of university students), the most important student exchange program in the world, financed by the european union and started in 1987, is characterized by a strong gender bias. girls participate to the program more than boys. this work quantifies the gender bias in the erasmus program between 2008 and 2013, using novel data at the university level. it describes the structure of the program in great details, carrying out the analysis across fields of study, and identifies key universities as senders and receivers. in addition, it tests the difference in the degree distribution of the erasmus network along time and between genders, giving evidence of a greater density in the female erasmus network with respect to the one of the male erasmus network.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-03-20", "updated": "", "authors": ["luca de benedictis", "silvia leoni"], "url": "https://arxiv.org/abs/2003.09167"}, {"title": "entropy-norm space for geometric selection of strict nash equilibria in   n-person games", "id": "2003.09225", "abstract": "motivated by empirical evidence that individuals within group decision making simultaneously aspire to maximize utility and avoid inequality we propose a criterion based on the entropy-norm pair for geometric selection of strict nash equilibria in n-person games. for this, we introduce a mapping of an n-person set of nash equilibrium utilities in an entropy-norm space. we suggest that the most suitable group choice is the equilibrium closest to the largest entropy-norm pair of a rescaled entropy-norm space. successive application of this criterion permits an ordering of the possible nash equilibria in an n-person game accounting simultaneously equality and utility of players payoffs. limitations of this approach for certain exceptional cases are discussed. in addition, the criterion proposed is applied and compared with the results of a group decision making experiment.", "categories": "physics.soc-ph econ.th q-fin.gn", "created": "2020-03-20", "updated": "", "authors": ["a. b. leoneti", "g. a. prataviera"], "url": "https://arxiv.org/abs/2003.09225"}, {"title": "kernel density decomposition with an application to the social cost of   carbon", "id": "2003.09276", "abstract": "a kernel density is an aggregate of kernel functions, which are itself densities and could be kernel densities. this is used to decompose a kernel into its constituent parts. pearson's test for equality of proportions is applied to quantiles to test whether the component distributions differ from one another. the proposed methods are illustrated with a meta-analysis of the social cost of carbon. different discount rates lead to significantly different pigou taxes, but not different growth rates. estimates have not varied over time. different authors have contributed different estimates, but these differences are insignificant. kernel decomposition can be applied in many other fields with discrete explanatory variables.", "categories": "stat.me econ.gn q-fin.ec", "created": "2020-03-20", "updated": "", "authors": ["richard s. j. tol"], "url": "https://arxiv.org/abs/2003.09276"}, {"title": "a correlated random coefficient panel model with time-varying   endogeneity", "id": "2003.09367", "abstract": "this paper studies a class of linear panel models with random coefficients. we do not restrict the joint distribution of the time-invariant unobserved heterogeneity and the covariates. we investigate identification of the average partial effect (ape) when fixed-effect techniques cannot be used to control for the correlation between the regressors and the time-varying disturbances. relying on control variables, we develop a constructive two-step identification argument. the first step identifies nonparametrically the conditional expectation of the disturbances given the regressors and the control variables, and the second step uses \"between-group\" variations, correcting for endogeneity, to identify the ape. we propose a natural semiparametric estimator of the ape, show its $\\sqrt{n}$ asymptotic normality and compute its asymptotic variance. the estimator is computationally easy to implement, and monte carlo simulations show favorable finite sample properties. control variables arise in various economic and econometric models, and we provide variations of our argument to obtain identification in some applications. as an empirical illustration, we estimate the average elasticity of intertemporal substitution in a labor supply model with random coefficients.", "categories": "econ.em", "created": "2020-03-20", "updated": "", "authors": ["louise laage"], "url": "https://arxiv.org/abs/2003.09367"}, {"title": "reinforcement learning in economics and finance", "id": "2003.10014", "abstract": "reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. in a given environment, the agent policy provides him some running and terminal rewards. as in online learning, the agent learns sequentially. as in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. in reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. the goal of reinforcement learning is to find an optimal policy -- a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. in this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance.", "categories": "econ.th cs.lg q-fin.cp", "created": "2020-03-22", "updated": "", "authors": ["arthur charpentier", "romuald elie", "carl remlinger"], "url": "https://arxiv.org/abs/2003.10014"}, {"title": "egalitarian solution for games with discrete side payment", "id": "2003.10059", "abstract": "in this paper, we study the egalitarian solution for games with discrete side payment, where the characteristic function is integer-valued and payoffs of players are integral vectors. the egalitarian solution, introduced by dutta and ray in 1989, is a solution concept for transferable utility cooperative games in characteristic form, which combines commitment for egalitarianism and promotion of indivisual interests in a consistent manner. we first point out that the nice properties of the egalitarian solution (in the continuous case) do not extend to games with discrete side payment. then we show that the lorenz stable set, which may be regarded a variant of the egalitarian solution, has nice properties such as the davis and maschler reduced game property and the converse reduced game property. for the proofs we utilize recent results in discrete convex analysis on decreasing minimization on an m-convex set investigated by frank and murota.", "categories": "cs.gt econ.th", "created": "2020-03-22", "updated": "", "authors": ["takafumi otsuka"], "url": "https://arxiv.org/abs/2003.10059"}, {"title": "determining feature importance for actionable climate change mitigation   policies", "id": "2003.10234", "abstract": "given the importance of public support for policy change and implementation, public policymakers and researchers have attempted to understand the factors associated with this support for climate change mitigation policy. in this article, we compare the feasibility of using different supervised learning methods for regression using a novel socio-economic data set which measures public support for potential climate change mitigation policies. following this model selection, we utilize gradient boosting regression, a well-known technique in the machine learning community, but relatively uncommon in public policy and public opinion research, and seek to understand what factors among the several examined in previous studies are most central to shaping public support for mitigation policies in climate change studies. the use of this method provides novel insights into the most important factors for public support for climate change mitigation policies. using national survey data, we find that the perceived risks associated with climate change are more decisive for shaping public support for policy options promoting renewable energy and regulating pollutants. however, we observe a very different behavior related to public support for increasing the use of nuclear energy where climate change risk perception is no longer the sole decisive feature. our findings indicate that public support for renewable energy is inherently different from that for nuclear energy reliance with the risk perception of climate change, dominant for the former, playing a subdued role for the latter.", "categories": "stat.ot econ.gn q-fin.ec", "created": "2020-03-18", "updated": "", "authors": ["romit maulik", "junghwa choi", "wesley wehde", "prasanna balaprakash"], "url": "https://arxiv.org/abs/2003.10234"}, {"title": "modelling network interference with multi-valued treatments: the causal   effect of immigration policy on crime rates", "id": "2003.10525", "abstract": "policy evaluation studies, which intend to assess the effect of an intervention, face some statistical challenges: in real-world settings treatments are not randomly assigned and the analysis might be further complicated by the presence of interference between units. researchers have started to develop novel methods that allow to manage spillover mechanisms in observational studies; recent works focus primarily on binary treatments. however, many policy evaluation studies deal with more complex interventions. for instance, in political science, evaluating the impact of policies implemented by administrative entities often implies a multivariate approach, as a policy towards a specific issue operates at many different levels and can be defined along a number of dimensions. in this work, we extend the statistical framework about causal inference under network interference in observational studies, allowing for a multi-valued individual treatment and an interference structure shaped by a weighted network. the estimation strategy is based on a joint multiple generalized propensity score and allows one to estimate direct effects, controlling for both individual and network covariates. we follow the proposed methodology to analyze the impact of the national immigration policy on the crime rate. we define a multi-valued characterization of political attitudes towards migrants and we assume that the extent to which each country can be influenced by another country is modeled by an appropriate indicator, summarizing their cultural and geographical proximity. results suggest that implementing a highly restrictive immigration policy leads to an increase of the crime rate and the estimated effects is larger if we take into account interference from other countries.", "categories": "stat.ap econ.em stat.me", "created": "2020-02-28", "updated": "2020-06-23", "authors": ["c. tort\u00f9", "i. crimaldi", "f. mealli", "l. forastiere"], "url": "https://arxiv.org/abs/2003.10525"}, {"title": "bemerkungen zum paarweisen vergleich", "id": "2003.10978", "abstract": "the simple pairwise comparison is a method to provide different criteria with weights. we show that the values of those weights (in particular the maximum) depend just on the number of criteria. additionally, it is shown that the distance between the weights is always the same.   -----   der einfache paarweise vergleich ist ein verfahren verschiedene kriterien mit einer gewichtung zu versehen. wir zeigen, dass die werte dieser gewichte (insbesondere auch der maximale wert) ausschlie{\\ss}lich von der anzahl der kriterien abh\\\"angt. dar\\\"uber hinaus wird gezeigt, dass der abstand der gewichtungen stets gleich ist.", "categories": "econ.th", "created": "2020-03-24", "updated": "", "authors": ["stefan l\u00f6rcks"], "url": "https://arxiv.org/abs/2003.10978"}, {"title": "the second worldwide wave of interest in coronavirus since the covid-19   outbreaks in south korea, italy and iran: a google trends study", "id": "2003.10998", "abstract": "the recent emergence of a new coronavirus, covid-19, has gained extensive coverage in public media and global news. as of 24 march 2020, the virus has caused viral pneumonia in tens of thousands of people in wuhan, china, and thousands of cases in 184 other countries and territories. this study explores the potential use of google trends (gt) to monitor worldwide interest in this covid-19 epidemic. gt was chosen as a source of reverse engineering data, given the interest in the topic. current data on covid-19 is retrieved from (gt) using one main search topic: coronavirus. geographical settings for gt are worldwide, china, south korea, italy and iran. the reported period is 15 january 2020 to 24 march 2020. the results show that the highest worldwide peak in the first wave of demand for information was on 31 january 2020. after the first peak, the number of new cases reported daily rose for 6 days. a second wave started on 21 february 2020 after the outbreaks were reported in italy, with the highest peak on 16 march 2020. the second wave is six times as big as the first wave. the number of new cases reported daily is rising day by day. this short communication gives a brief introduction to how the demand for information on coronavirus epidemic is reported through gt.", "categories": "cs.cy cs.ir econ.gn q-fin.ec", "created": "2020-03-24", "updated": "2020-07-28", "authors": ["artur strzelecki"], "url": "https://arxiv.org/abs/2003.10998"}, {"title": "exploring the effects of covid-19 containment policies on crime: an   empirical analysis of the short-term aftermath in los angeles", "id": "2003.11021", "abstract": "this work investigates whether and how covid-19 containment policies had an immediate impact on crime trends in los angeles. the analysis is conducted using bayesian structural time-series and focuses on nine crime categories and on the overall crime count, daily monitored from january 1st 2017 to march 28th 2020. we concentrate on two post-intervention time windows - from march 4th to march 16th and from march 4th to march 28th 2020 - to dynamically assess the short-term effects of mild and strict policies. in los angeles, overall crime has significantly decreased, as well as robbery, shoplifting, theft, and battery. no significant effect has been detected for vehicle theft, burglary, assault with a deadly weapon, intimate partner assault, and homicide. results suggest that, in the first weeks after the interventions are put in place, social distancing impacts more directly on instrumental and less serious crimes. policy implications are also discussed.", "categories": "stat.ot econ.gn q-fin.ec", "created": "2020-03-23", "updated": "2020-10-06", "authors": ["gian maria campedelli", "alberto aziani", "serena favarin"], "url": "https://arxiv.org/abs/2003.11021"}, {"title": "missing at random or not: a semiparametric testing approach", "id": "2003.11181", "abstract": "practical problems with missing data are common, and statistical methods have been developed concerning the validity and/or efficiency of statistical procedures. on a central focus, there have been longstanding interests on the mechanism governing data missingness, and correctly deciding the appropriate mechanism is crucially relevant for conducting proper practical investigations. the conventional notions include the three common potential classes -- missing completely at random, missing at random, and missing not at random. in this paper, we present a new hypothesis testing approach for deciding between missing at random and missing not at random. since the potential alternatives of missing at random are broad, we focus our investigation on a general class of models with instrumental variables for data missing not at random. our setting is broadly applicable, thanks to that the model concerning the missing data is nonparametric, requiring no explicit model specification for the data missingness. the foundational idea is to develop appropriate discrepancy measures between estimators whose properties significantly differ only when missing at random does not hold. we show that our new hypothesis testing approach achieves an objective data oriented choice between missing at random or not. we demonstrate the feasibility, validity, and efficacy of the new test by theoretical analysis, simulation studies, and a real data analysis.", "categories": "stat.me econ.em", "created": "2020-03-24", "updated": "", "authors": ["rui duan", "c. jason liang", "pamela shaw", "cheng yong tang", "yong chen"], "url": "https://arxiv.org/abs/2003.11181"}, {"title": "susceptible-infected-recovered (sir) dynamics of covid-19 and economic   impact", "id": "2003.11221", "abstract": "i estimate the susceptible-infected-recovered (sir) epidemic model for coronavirus disease 2019 (covid-19). the transmission rate is heterogeneous across countries and far exceeds the recovery rate, which enables a fast spread. in the benchmark model, 28% of the population may be simultaneously infected at the peak, potentially overwhelming the healthcare system. the peak reduces to 6.2% under the optimal mitigation policy that controls the timing and intensity of social distancing. a stylized asset pricing model suggests that the stock price temporarily decreases by 50% in the benchmark case but shows a w-shaped, moderate but longer bear market under the optimal policy.", "categories": "q-bio.pe econ.gn q-fin.ec", "created": "2020-03-25", "updated": "2020-03-26", "authors": ["alexis akira toda"], "url": "https://arxiv.org/abs/2003.11221"}, {"title": "determinants of interest rates in the p2p consumer lending market: how   rational are investors?", "id": "2003.11347", "abstract": "in an ideal world, individuals are well informed and make rational choices. regulators can fill in to protect consumers, such as retail investors. online p2p lending is a rather new form of market-based finance where regulation is still in its infancy. we analyze how retail investors price the credit risk of p2p consumer loans in a reverse auction framework where personal interaction is absent. the explained interest rate variance is considerably larger than in comparable studies using bank loan data. our results indicate that retail investors act rational in this weakly regulated environment. this seems surprising when considering the limited set of information provided to the investor. factors representing economic status significantly influence lender evaluations of the borrower's credit risk. the explanatory power of loan-specific factors increase as the market for p2p consumer loans matures. furthermore, we find statistical evidence of some discrimination by the lenders with respect to nationality and gender.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.pr q-fin.rm", "created": "2020-03-25", "updated": "", "authors": ["andreas dietrich", "reto wernli"], "url": "https://arxiv.org/abs/2003.11347"}, {"title": "gender differences in wage expectations", "id": "2003.11496", "abstract": "using a survey on wage expectations among students at two swiss institutions of higher education, we examine the wage expectations of our respondents along two main lines. first, we investigate the rationality of wage expectations by comparing average expected wages from our sample with those of similar graduates; we further examine how our respondents revise their expectations when provided information about actual wages. second, using causal mediation analysis, we test whether the consideration of a rich set of personal and professional controls, namely concerning family formation and children in addition to professional preferences, accounts for the difference in wage expectations across genders. we find that males and females overestimate their wages compared to actual ones, and that males respond in an overconfident manner to information about outside wages. despite the attenuation of the gender difference in wage expectations brought about by the comprehensive set of controls, gender generally retains a significant direct, unexplained effect on wage expectations.", "categories": "econ.gn q-fin.ec", "created": "2020-03-25", "updated": "", "authors": ["ana fernandes", "martin huber", "giannina vaccaro"], "url": "https://arxiv.org/abs/2003.11496"}, {"title": "rationalizing rational expectations: characterization and tests", "id": "2003.11537", "abstract": "in this paper, we build a new test of rational expectations based on the marginal distributions of realizations and subjective beliefs. this test is widely applicable, including in the common situation where realizations and beliefs are observed in two different datasets that cannot be matched. we show that whether one can rationalize rational expectations is equivalent to the distribution of realizations being a mean-preserving spread of the distribution of beliefs. the null hypothesis can then be rewritten as a system of many moment inequality and equality constraints, for which tests have been recently developed in the literature. the test is robust to measurement errors under some restrictions and can be extended to account for aggregate shocks. finally, we apply our methodology to test for rational expectations about future earnings. while individuals tend to be right on average about their future earnings, our test strongly rejects rational expectations.", "categories": "econ.em", "created": "2020-03-25", "updated": "2020-09-17", "authors": ["xavier d'haultfoeuille", "christophe gaillac", "arnaud maurel"], "url": "https://arxiv.org/abs/2003.11537"}, {"title": "the millennial boom, the baby bust, and the housing market", "id": "2003.11565", "abstract": "as baby boomers have begun to downsize and retire, their preferences now overlap with millennials' predilection for urban amenities and smaller living spaces. this confluence in tastes between the two largest age segments of the u.s. population has meaningfully changed the evolution of home prices in the united states. utilizing a bartik shift-share instrument for demography-driven demand shocks, we show that from 2000 to 2018 (i) the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, (ii) within local labor markets, the relative home prices in baby boomer-rich zip codes have declined compared with millennial-rich neighborhoods, and (iii) the zip codes with the largest relative share of smaller homes have grown fastest. these patterns have become more pronounced during the latest economic cycle. we show that the effects are concentrated in areas where housing supply is most inelastic. if this pattern in the housing market persists or expands, the approximately 16.5 trillion in real estate wealth held by households headed by those aged 55 or older will be significantly affected. we find little evidence that these upcoming changes have been incorporated into current prices.", "categories": "econ.gn q-fin.ec", "created": "2020-03-25", "updated": "", "authors": ["marijn a. bolhuis", "judd n. l. cramer"], "url": "https://arxiv.org/abs/2003.11565"}, {"title": "is the juice worth the squeeze? machine learning (ml) in and for   agent-based modelling (abm)", "id": "2003.11985", "abstract": "in recent years, many scholars praised the seemingly endless possibilities of using machine learning (ml) techniques in and for agent-based simulation models (abm). to get a more comprehensive understanding of these possibilities, we conduct a systematic literature review (slr) and classify the literature on the application of ml in and for abm according to a theoretically derived classification scheme. we do so to investigate how exactly machine learning has been utilized in and for agent-based models so far and to critically discuss the combination of these two promising methods. we find that, indeed, there is a broad range of possible applications of ml to support and complement abms in many different ways, already applied in many different disciplines. we see that, so far, ml is mainly used in abm for two broad cases: first, the modelling of adaptive agents equipped with experience learning and, second, the analysis of outcomes produced by a given abm. while these are the most frequent, there also exist a variety of many more interesting applications. this being the case, researchers should dive deeper into the analysis of when and how which kinds of ml techniques can support abm, e.g. by conducting a more in-depth analysis and comparison of different use cases. nonetheless, as the application of ml in and for abm comes at certain costs, researchers should not use ml for abms just for the sake of doing it.", "categories": "econ.th cs.ma", "created": "2020-03-26", "updated": "", "authors": ["johannes dahlke", "kristina bogner", "matthias mueller", "thomas berger", "andreas pyka", "bernd ebersberger"], "url": "https://arxiv.org/abs/2003.11985"}, {"title": "estimating treatment effects with observed confounders and mediators", "id": "2003.11991", "abstract": "given a causal graph, the do-calculus can express treatment effects as functionals of the observational joint distribution that can be estimated empirically. sometimes the do-calculus identifies multiple valid formulae, prompting us to compare the statistical properties of the corresponding estimators. for example, the backdoor formula applies when all confounders are observed and the frontdoor formula applies when an observed mediator transmits the causal effect. in this paper, we investigate the over-identified scenario where both confounders and mediators are observed, rendering both estimators valid. addressing the linear gaussian causal model, we derive the finite-sample variance for both estimators and demonstrate that either estimator can dominate the other by an unbounded constant factor depending on the model parameters. next, we derive an optimal estimator, which leverages all observed variables to strictly outperform the backdoor and frontdoor estimators. we also present a procedure for combining two datasets, with confounders observed in one and mediators in the other. finally, we evaluate our methods on both simulated data and the ihdp and jtpa datasets.", "categories": "stat.me cs.lg econ.em stat.ml", "created": "2020-03-26", "updated": "2020-06-21", "authors": ["shantanu gupta", "zachary c. lipton", "david childers"], "url": "https://arxiv.org/abs/2003.11991"}, {"title": "the network dynamics of social and technological conventions", "id": "2003.12112", "abstract": "when innovations compete for adoption, chance historical events can allow an inferior strategy to spread at the expense of superior alternatives. however, advantage is not always due to chance, and networks have emerged as an important determinant of organizational behavior. to understand what factors can impact the likelihood that the best alternative will be adopted, this paper asks: how does network structure shape the emergence of social and technological conventions? prior research has found that highly influential people, or \"central\" nodes, can be beneficial from the perspective of a single innovation because promotion by central nodes can increase the speed of adoption. in contrast, when considering the competition of multiple strategies, the presence of central nodes may pose a risk, and the resulting \"centralized\" networks are not guaranteed to favor the optimal strategy. this paper uses agent-based simulation to investigate the effect of network structure on a standard model of convention formation, finding that network centralization increases the speed of convention formation but also decreases the likelihood that the best strategy will become widely adopted. surprisingly, this finding does not indicate a speed/optimality trade-off: dense networks are both fast and optimal.", "categories": "physics.soc-ph cs.si econ.gn q-fin.ec", "created": "2020-03-26", "updated": "", "authors": ["joshua becker"], "url": "https://arxiv.org/abs/2003.12112"}, {"title": "sequential monitoring for cointegrating regressions", "id": "2003.12182", "abstract": "we develop monitoring procedures for cointegrating regressions, testing the null of no breaks against the alternatives that there is either a change in the slope, or a change to non-cointegration. after observing the regression for a calibration sample m, we study a cusum-type statistic to detect the presence of change during a monitoring horizon m+1,...,t. our procedures use a class of boundary functions which depend on a parameter whose value affects the delay in detecting the possible break. technically, these procedures are based on almost sure limiting theorems whose derivation is not straightforward. we therefore define a monitoring function which - at every point in time - diverges to infinity under the null, and drifts to zero under alternatives. we cast this sequence in a randomised procedure to construct an i.i.d. sequence, which we then employ to define the detector function. our monitoring procedure rejects the null of no break (when correct) with a small probability, whilst it rejects with probability one over the monitoring horizon in the presence of breaks.", "categories": "econ.em stat.me", "created": "2020-03-26", "updated": "", "authors": ["lorenzo trapani", "emily whitehouse"], "url": "https://arxiv.org/abs/2003.12182"}, {"title": "sorting big data by revealed preference with application to college   ranking", "id": "2003.12198", "abstract": "when ranking big data observations such as colleges in the united states, diverse consumers reveal heterogeneous preferences. the objective of this paper is to sort out a linear ordering for these observations and to recommend strategies to improve their relative positions in the ranking. a properly sorted solution could help consumers make the right choices, and governments make wise policy decisions. previous researchers have applied exogenous weighting or multivariate regression approaches to sort big data objects, ignoring their variety and variability. by recognizing the diversity and heterogeneity among both the observations and the consumers, we instead apply endogenous weighting to these contradictory revealed preferences. the outcome is a consistent steady-state solution to the counterbalance equilibrium within these contradictions. the solution takes into consideration the spillover effects of multiple-step interactions among the observations. when information from data is efficiently revealed in preferences, the revealed preferences greatly reduce the volume of the required data in the sorting process. the employed approach can be applied in many other areas, such as sports team ranking, academic journal ranking, voting, and real effective exchange rates.", "categories": "stat.ml cs.lg econ.gn q-fin.ec", "created": "2020-03-26", "updated": "", "authors": ["xingwei hu"], "url": "https://arxiv.org/abs/2003.12198"}, {"title": "the corisk-index: a data-mining approach to identify industry-specific   risk assessments related to covid-19 in real-time", "id": "2003.12432", "abstract": "while the coronavirus spreads, governments are attempting to reduce contagion rates at the expense of negative economic effects. market expectations plummeted, foreshadowing the risk of a global economic crisis and mass unemployment. governments provide huge financial aid programmes to mitigate the economic shocks. to achieve higher effectiveness with such policy measures, it is key to identify the industries that are most in need of support. in this study, we introduce a data-mining approach to measure industry-specific risks related to covid-19. we examine company risk reports filed to the u.s. securities and exchange commission (sec). this alternative data set can complement more traditional economic indicators in times of the fast-evolving crisis as it allows for a real-time analysis of risk assessments. preliminary findings suggest that the companies' awareness towards corona-related business risks is ahead of the overall stock market developments. our approach allows to distinguish the industries by their risk awareness towards covid-19. based on natural language processing, we identify corona-related risk topics and their perceived relevance for different industries. the preliminary findings are summarised as an up-to-date online index. the corisk-index tracks the industry-specific risk assessments related to the crisis, as it spreads through the economy. the tracking tool is updated weekly. it could provide relevant empirical data to inform models on the economic effects of the crisis. such complementary empirical information could ultimately help policymakers to effectively target financial support in order to mitigate the economic shocks of the crisis.", "categories": "econ.gn q-fin.ec", "created": "2020-03-27", "updated": "2020-04-27", "authors": ["fabian stephany", "niklas stoehr", "philipp darius", "leonie neuh\u00e4user", "ole teutloff", "fabian braesemann"], "url": "https://arxiv.org/abs/2003.12432"}, {"title": "challenge theory: the structure and measurement of risky binary choice   behavior", "id": "2003.12474", "abstract": "challenge theory (shye & haber 2015; 2020) has demonstrated that a newly devised challenge index (ci) attributable to every binary choice problem predicts the popularity of the bold option, the one of lower probability to gain a higher monetary outcome (in a gain problem); and the one of higher probability to lose a lower monetary outcome (in a loss problem). in this paper we show how facet theory structures the choice-behavior concept-space and yields rationalized measurements of gambling behavior. the data of this study consist of responses obtained from 126 student, specifying their preferences in 44 risky decision problems. a faceted smallest space analysis (ssa) of the 44 problems confirmed the hypothesis that the space of binary risky choice problems is partitionable by two binary axial facets: (a) type of problem (gain vs. loss); and (b) ci (low vs. high). four composite variables, representing the validated constructs: gain, loss, high-ci and low-ci, were processed using multiple scaling by partial order scalogram analysis with base coordinates (posac), leading to a meaningful and intuitively appealing interpretation of two necessary and sufficient gambling-behavior measurement scales.", "categories": "econ.gn q-fin.ec", "created": "2020-03-24", "updated": "", "authors": ["samuel shye", "ido haber"], "url": "https://arxiv.org/abs/2003.12474"}, {"title": "scheduling of flexible non-preemptive loads", "id": "2003.13220", "abstract": "a market consisting of a generator with thermal and renewable generation capability, a set of non-preemptive loads (i.e., loads which cannot be interrupted once started), and an independent system operator (iso) is considered. loads are characterized by durations, power demand rates and utility for receiving service, as well as disutility functions giving preferences for time slots in which service is preferred. given this information, along with the generator's thermal generation cost function and forecast renewable generation, the social planner solves a mixed integer program to determine a load activation schedule which maximizes social welfare. assuming price taking behavior, we develop a competitive equilibrium concept based on a relaxed version of the social planner's problem which includes prices for consumption and incentives for flexibility, and allows for probabilistic allocation of power to loads. considering each load as representative of a population of identical loads with scaled characteristics, we demonstrate that the relaxed social planner's problem gives an exact solution to the original mixed integer problem in the large population limit, and give a market mechanism for implementing the competitive equilibrium.", "categories": "eess.sy cs.sy econ.gn q-fin.ec", "created": "2020-03-30", "updated": "", "authors": ["nathan dahlin", "rahul jain"], "url": "https://arxiv.org/abs/2003.13220"}, {"title": "high-dimensional mixed-frequency iv regression", "id": "2003.13478", "abstract": "this paper introduces a high-dimensional linear iv regression for the data sampled at mixed frequencies. we show that the high-dimensional slope parameter of a high-frequency covariate can be identified and accurately estimated leveraging on a low-frequency instrumental variable. the distinguishing feature of the model is that it allows handing high-dimensional datasets without imposing the approximate sparsity restrictions. we propose a tikhonov-regularized estimator and derive the convergence rate of its mean-integrated squared error for time series data. the estimator has a closed-form expression that is easy to compute and demonstrates excellent performance in our monte carlo experiments. we estimate the real-time price elasticity of supply on the australian electricity spot market. our estimates suggest that the supply is relatively inelastic and that its elasticity is heterogeneous throughout the day.", "categories": "econ.em math.st stat.ap stat.ml stat.th", "created": "2020-03-30", "updated": "", "authors": ["andrii babii"], "url": "https://arxiv.org/abs/2003.13478"}, {"title": "by force of habit: self-trapping in a dynamical utility landscape", "id": "2003.13660", "abstract": "historically, rational choice theory has focused on the utility maximization principle to describe how individuals make choices. in reality, there is a computational cost related to exploring the universe of available choices and it is often not clear whether we are truly maximizing an underlying utility function. in particular, memory effects and habit formation may dominate over utility maximisation. we propose a stylized model with a history-dependent utility function where the utility associated to each choice is increased when that choice has been made in the past, with a certain decaying memory kernel. we show that self-reinforcing effects can cause the agent to get stuck with a choice by sheer force of habit. we discuss the special nature of the transition between free exploration of the space of choice and self-trapping. we find in particular that the trapping time distribution is precisely a zipf law at the transition, and that the self-trapped phase exhibits super-aging behaviour.", "categories": "cond-mat.stat-mech econ.gn physics.soc-ph q-fin.ec", "created": "2020-03-30", "updated": "", "authors": ["jos\u00e9 moran", "antoine fosset", "davide luzzati", "jean-philippe bouchaud", "michael benzaquen"], "url": "https://arxiv.org/abs/2003.13660"}, {"title": "specification tests for generalized propensity scores using double   projections", "id": "2003.13803", "abstract": "this paper proposes a new class of nonparametric tests for the correct specification of generalized propensity score models. the test procedure is based on two different projection arguments, which lead to test statistics with several appealing properties. they accommodate high-dimensional covariates; are asymptotically invariant to the estimation method used to estimate the nuisance parameters and do not requite estimators to be root-n asymptotically linear; are fully data-driven and do not require tuning parameters, can be written in closed-form, facilitating the implementation of an easy-to-use multiplier bootstrap procedure. we show that our proposed tests are able to detect a broad class of local alternatives converging to the null at the parametric rate. monte carlo simulation studies indicate that our double projected tests have much higher power than other tests available in the literature, highlighting their practical appeal.", "categories": "econ.em stat.me", "created": "2020-03-30", "updated": "", "authors": ["pedro h. c. sant'anna", "xiaojun song"], "url": "https://arxiv.org/abs/2003.13803"}, {"title": "business disruptions from social distancing", "id": "2003.13983", "abstract": "social distancing interventions can be effective against epidemics but are potentially detrimental for the economy. businesses that rely heavily on face-to-face communication or close physical proximity when producing a product or providing a service are particularly vulnerable. there is, however, no systematic evidence about the role of human interactions across different lines of business and about which will be the most limited by social distancing. here we provide theory-based measures of the reliance of u.s. businesses on human interaction, detailed by industry and geographic location. we find that 49 million workers work in occupations that rely heavily on face-to-face communication or require close physical proximity to other workers. our model suggests that when businesses are forced to reduce worker contacts by half, they need a 12 percent wage subsidy to compensate for the disruption in communication. retail, hotels and restaurants, arts and entertainment and schools are the most affected sectors. our results can help target fiscal assistance to businesses that are most disrupted by social distancing.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-03-31", "updated": "", "authors": ["mikl\u00f3s koren", "rita pet\u0151"], "url": "https://arxiv.org/abs/2003.13983"}, {"title": "the propagation of the economic impact through supply chains: the case   of a mega-city lockdown against the spread of covid-19", "id": "2003.14002", "abstract": "this study quantifies the economic effect of a possible lockdown of tokyo to prevent spread of covid-19. the negative effect of the lockdown may propagate to other regions through supply chains because of shortage of supply and demand. applying an agent-based model to the actual supply chains of nearly 1.6 million firms in japan, we simulate what would happen to production activities outside tokyo when production activities that are not essential to citizens' survival in tokyo were shut down for a certain period. we find that when tokyo is locked down for a month, the indirect effect on other regions would be twice as large as the direct effect on tokyo, leading to a total production loss of 27 trillion yen in japan, or 5.3% of its annual gdp. although the production shut down in tokyo accounts for 21% of the total production in japan, the lockdown would result in a reduction of the daily production in japan by 86% in a month.", "categories": "cs.si econ.gn q-fin.ec", "created": "2020-03-31", "updated": "", "authors": ["hiroyasu inoue", "yasuyuki todo"], "url": "https://arxiv.org/abs/2003.14002"}, {"title": "a wavelet analysis of inter-dependence, contagion and long memory among   global equity markets", "id": "2003.14110", "abstract": "this study attempts to investigate into the structure and features of global equity markets from a time-frequency perspective. an analysis grounded on this framework allows one to capture information from a different dimension, as opposed to the traditional time domain analyses, where multiscale structures of financial markets are clearly extracted. in financial time series, multiscale features manifest themselves due to presence of multiple time horizons. the existence of multiple time horizons necessitates a careful investigation of each time horizon separately as market structures are not homogenous across different time horizons. the presence of multiple time horizons, with varying levels of complexity, requires one to investigate financial time series from a heterogeneous market perspective where market players are said to operate at different investment horizons. this thesis extends the application of time-frequency based wavelet techniques to: i) analyse the interdependence of global equity markets from a heterogeneous investor perspective with a special focus on the indian stock market, ii) investigate the contagion effect, if any, of financial crises on indian stock market, and iii) to study fractality and scaling properties of global equity markets and analyse the efficiency of indian stock markets using wavelet based long memory methods.", "categories": "econ.em nlin.cd stat.ap", "created": "2020-03-31", "updated": "", "authors": ["avishek bhandari"], "url": "https://arxiv.org/abs/2003.14110"}, {"title": "a spatial agent based model for simulating and optimizing networked   eco-industrial systems", "id": "2003.14133", "abstract": "industrial symbiosis involves creating integrated cycles of by-products and waste between networks of industrial actors in order to maximize economic value, while at the same time minimizing environmental strain. in such a network, the global environmental strain is no longer equal to the sum of the environmental strain of the individual actors, but it is dependent on how well the network performs as a whole. the development of methods to understand, manage or optimize such networks remains an open issue. in this paper we put forward a simulation model of by-product flow between industrial actors. the goal is to introduce a method for modelling symbiotic exchanges from a macro perspective. the model takes into account the effect of two main mechanisms on a multi-objective optimization of symbiotic processes. first it allows us to study the effect of geographical properties of the economic system, said differently, where actors are divided in space. second, it allows us to study the effect of clustering complementary actors together as a function of distance, by means of a spatial correlation between the actors' by-products. our simulations unveil patterns that are relevant for macro-level policy. first, our results show that the geographical properties are an important factor for the macro performance of symbiotic processes. second, spatial correlations, which can be interpreted as planned clusters such as eco-industrial parks, can lead to a very effective macro performance, but only if these are strictly implemented. finally, we provide a proof of concept by comparing the model to real world data from the european pollutant release and transfer register database using georeferencing of the companies in the dataset. this work opens up research opportunities in interactive data-driven models and platforms to support real-world implementation of industrial symbiosis.", "categories": "physics.soc-ph cs.ma econ.gn q-fin.ec", "created": "2020-03-31", "updated": "", "authors": ["j. raimbault", "j. broere", "m. somveille", "j. m. serna", "e. strombom", "c. moore", "b. zhu", "l. sugar"], "url": "https://arxiv.org/abs/2003.14133"}, {"title": "optimal combination of arctic sea ice extent measures: a dynamic factor   modeling approach", "id": "2003.14276", "abstract": "the diminishing extent of arctic sea ice is a key indicator of climate change as well as an accelerant for future global warming. since 1978, arctic sea ice has been measured using satellite-based microwave sensing; however, different measures of arctic sea ice extent have been made available based on differing algorithmic transformations of the raw satellite data. we propose and estimate a dynamic factor model that combines four of these measures in an optimal way that accounts for their differing volatility and cross-correlations. we then use the kalman smoother to extract an optimal combined measure of arctic sea ice extent. it turns out that almost all weight is put on the nsidc sea ice index, confirming and enhancing confidence in the sea ice index and the nasa team algorithm on which it is based.", "categories": "stat.ap econ.em", "created": "2020-03-31", "updated": "2020-08-12", "authors": ["francis x. diebold", "maximilian g\u00f6bel", "philippe goulet coulombe", "glenn d. rudebusch", "boyuan zhang"], "url": "https://arxiv.org/abs/2003.14276"}, {"title": "failure of equilibrium selection methods for multiple-principal,   multiple-agent problems with non-rivalrous goods: an analysis of data markets", "id": "2004.00196", "abstract": "the advent of machine learning tools has led to the rise of data markets. these data markets are characterized by multiple data purchasers interacting with a set of data sources. data sources have more information about the quality of data than the data purchasers; additionally, data itself is a non-rivalrous good that can be shared with multiple parties at negligible marginal cost. in this paper, we study the multiple-principal, multiple-agent problem with non-rivalrous goods. under the assumption that the principal's payoff is quasilinear in the payments given to agents, we show that there is a fundamental degeneracy in the market of non-rivalrous goods. specifically, for a general class of payment contracts, there will be an infinite set of generalized nash equilibria. this multiplicity of equilibria also affects common refinements of equilibrium definitions intended to uniquely select an equilibrium: both variational equilibria and normalized equilibria will be non-unique in general. this implies that most existing equilibrium concepts cannot provide predictions on the outcomes of data markets emerging today. the results support the idea that modifications to payment contracts themselves are unlikely to yield a unique equilibrium, and either changes to the models of study or new equilibrium concepts will be required to determine unique equilibria in settings with multiple principals and a non-rivalrous good.", "categories": "cs.gt econ.th", "created": "2020-03-31", "updated": "", "authors": ["samir wadhwa", "roy dong"], "url": "https://arxiv.org/abs/2004.00196"}, {"title": "containment efficiency and control strategies for the corona pandemic   costs", "id": "2004.00493", "abstract": "the rapid spread of the coronavirus (covid-19) confronts policy makers with the problem of measuring the effectiveness of containment strategies and the need to balance public health considerations with the economic costs of a persistent lockdown. we introduce a modified epidemic model, the controlled-sir model, in which the disease reproduction rate evolves dynamically in response to political and societal reactions. an analytic solution is presented. the model reproduces official covid-19 cases counts of a large number of regions and countries that surpassed the peak of the outbreak. a single unbiased feedback parameter is extracted from field data and used to formulate an index that measures the efficiency of containment policies (the cei index). cei values for a range of countries are given. for two variants of the controlled-sir model, detailed estimates of the total medical and socio-economic costs are evaluated over the entire course of the epidemic. costs comprise medical care cost, the economic cost of social distancing, as well as the economic value of lives saved. under plausible parameters, strict measures fare better than a hands-off policy. strategies based on actual case numbers lead to substantially higher total costs than strategies based on the overall history of the epidemic.", "categories": "physics.soc-ph econ.gn q-bio.pe q-fin.ec", "created": "2020-04-01", "updated": "2020-04-13", "authors": ["claudius gros", "roser valenti", "lukas schneider", "kilian valenti", "daniel gros"], "url": "https://arxiv.org/abs/2004.00493"}, {"title": "a note on the provision of a public service of different quality", "id": "2004.00669", "abstract": "we study how the quality dimension affects the social optimum in a model of spatial differentiation where two facilities provide a public service. if quality enters linearly in the individuals' utility function, a symmetric configuration, in which both facilities have the same quality and serve groups of individuals of the same size, does not maximize the social welfare. this is a surprising result as all individuals are symmetrically identical having the same quality valuation. we also show that a symmetric configuration of facilities may maximize the social welfare if the individuals' marginal utility of quality is decreasing.", "categories": "econ.gn cs.gt q-fin.ec", "created": "2020-03-31", "updated": "", "authors": ["monica anna giovanniello", "simone tonin"], "url": "https://arxiv.org/abs/2004.00669"}, {"title": "status hierarchy and group cooperation: a generalized model", "id": "2004.00944", "abstract": "in a refreshing mathematical investigation, mark (2018) shows that status hierarchy may facilitate the emergence of cooperation in groups. despite the contribution, the present paper notes that there are limitations in mark's model that makes it less realistic than it could in explaining real-world experiences. consequently, we present a more generalized modified framework in which his model is a special case, by developing and introducing a new hierarchy measure into the model to estimate the cooperation level in a set of hierarchical structures omitted in mark's work yet common in everyday life--those with multiple leaders. we derived the conditions under which cooperation can emerge in these groups, and verified our analytical predictions in agent-based computer simulations. in so doing, not only does our model elaborate on its predecessor and support mark's general prediction. for theory, our work further reveals two novel phenomena of group cooperation: both the relative number of cooperators to defectors in groups and the assortativity among these different roles can backfire; they are not always the higher, the better for cooperation to thrive. for methodology, the hierarchy measure developed and our model using the measure may also be applied in future research on a wide range of related topics.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-04-02", "updated": "2020-08-17", "authors": ["hsuan-wei lee", "yen-ping chang", "yen-sheng chiang"], "url": "https://arxiv.org/abs/2004.00944"}, {"title": "pruned wasserstein index generation model and wigpy package", "id": "2004.00999", "abstract": "recent proposal of wasserstein index generation model (wig) has shown a new direction for automatically generating indices. however, it is challenging in practice to fit large datasets for two reasons. first, the sinkhorn distance is notoriously expensive to compute and suffers from dimensionality severely. second, it requires to compute a full $n\\times n$ matrix to be fit into memory, where $n$ is the dimension of vocabulary. when the dimensionality is too large, it is even impossible to compute at all. i hereby propose a lasso-based shrinkage method to reduce dimensionality for the vocabulary as a pre-processing step prior to fitting the wig model. after we get the word embedding from word2vec model, we could cluster these high-dimensional vectors by $k$-means clustering, and pick most frequent tokens within each cluster to form the \"base vocabulary\". non-base tokens are then regressed on the vectors of base token to get a transformation weight and we could thus represent the whole vocabulary by only the \"base tokens\". this variant, called pruned wig (pwig), will enable us to shrink vocabulary dimension at will but could still achieve high accuracy. i also provide a \\textit{wigpy} module in python to carry out computation in both flavor. application to economic policy uncertainty (epu) index is showcased as comparison with existing methods of generating time-series sentiment indices.", "categories": "cs.lg cs.cl econ.gn q-fin.ec", "created": "2020-03-30", "updated": "2020-07-09", "authors": ["fangzhou xie"], "url": "https://arxiv.org/abs/2004.00999"}, {"title": "greater search cost reduces prices", "id": "2004.01238", "abstract": "the optimal price of each firm falls in the search cost of consumers, in the limit to the monopoly price, despite the exit of lower-value consumers in response to costlier search. exit means that fewer inframarginal consumers remain. the decrease in marginal buyers is smaller, because part of demand is composed of customers coming from rival firms. these buyers can be held up and are not marginal. higher search cost reduces the fraction of incoming switchers among buyers, which decreases the hold-up motive, thus the price.", "categories": "econ.th cs.gt", "created": "2020-04-02", "updated": "", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/2004.01238"}, {"title": "predicting skill shortages in labor markets: a machine learning approach", "id": "2004.01311", "abstract": "skill shortages are a drain on society. they hamper economic opportunities for individuals, slow growth for firms, and impede labor productivity in aggregate. therefore, the ability to understand and predict skill shortages in advance is critical for policy-makers and educators to help alleviate their adverse effects. this research implements a high-performing machine learning approach to predict occupational skill shortages. in addition, we demonstrate methods to analyze the underlying skill demands of occupations in shortage and the most important features for predicting skill shortages. for this work, we compile a unique dataset of both labor demand and labor supply occupational data in australia from 2012 to 2018. this includes data from 7.7 million job advertisements (ads) and 20 official labor force measures. we use these data as explanatory variables and leverage the xgboost classifier to predict yearly skills shortage classifications for 132 standardized occupations. the models we construct achieve macro-f1 average performance scores of up to 83 per cent. our results show that job ads data and employment statistics were the highest performing feature sets for predicting year-to-year skills shortage changes for occupations. we also find that features such as 'hours worked', years of 'education', years of 'experience', and median 'salary' are highly important features for predicting occupational skill shortages. this research provides a robust data-driven approach for predicting and analyzing skill shortages, which can assist policy-makers, educators, and businesses to prepare for the future of work.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-04-02", "updated": "2020-08-26", "authors": ["nik dawson", "marian-andrei rizoiu", "benjamin johnston", "mary-anne williams"], "url": "https://arxiv.org/abs/2004.01311"}, {"title": "computational complexity of the hylland-zeckhauser scheme for one-sided   matching markets", "id": "2004.01348", "abstract": "in 1979, hylland and zeckhauser \\cite{hylland} gave a simple and general scheme for implementing a one-sided matching market using the power of a pricing mechanism. their method has nice properties -- it is incentive compatible in the large and produces an allocation that is pareto optimal -- and hence it provides an attractive, off-the-shelf method for running an application involving such a market. with matching markets becoming ever more prevalant and impactful, it is imperative to finally settle the computational complexity of this scheme.   we present the following partial resolution:   1. a combinatorial, strongly polynomial time algorithm for the special case of $0/1$ utilities.   2. an example that has only irrational equilibria, hence proving that this problem is not in ppad. furthermore, its equilibria are disconnected, hence showing that the problem does not admit a convex programming formulation.   3. a proof of membership of the problem in the class fixp.   we leave open the (difficult) question of determining if the problem is fixp-hard. settling the status of the special case when utilities are in the set $\\{0, {\\frac 1 2}, 1 \\}$ appears to be even more difficult.", "categories": "cs.gt cs.cc econ.th math.co", "created": "2020-04-02", "updated": "2020-04-22", "authors": ["vijay v. vazirani", "mihalis yannakakis"], "url": "https://arxiv.org/abs/2004.01348"}, {"title": "targeting predictors in random forest regression", "id": "2004.01411", "abstract": "random forest regression (rf) is an extremely popular tool for the analysis of high-dimensional data. nonetheless, its benefits may be lessened in sparse settings, due to weak predictors, and a pre-estimation dimension reduction (targeting) step is required. we show that proper targeting controls the probability of placing splits along strong predictors, thus providing an important complement to rf's feature sampling. this is supported by simulations using representative finite samples. moreover, we quantify the immediate gain from targeting in terms of increased strength of individual trees. macroeconomic and financial applications show that the bias-variance tradeoff implied by targeting, due to increased correlation among trees in the forest, is balanced at a medium degree of targeting, selecting the best 10-30\\% of commonly applied predictors. improvements in predictive accuracy of targeted rf relative to ordinary rf are considerable, up to 12-13\\%, occurring both in recessions and expansions, particularly at long horizons.", "categories": "econ.em", "created": "2020-04-03", "updated": "2020-05-20", "authors": ["daniel borup", "bent jesper christensen", "nicolaj n\u00f8rgaard m\u00fchlbach", "mikkel slot nielsen"], "url": "https://arxiv.org/abs/2004.01411"}, {"title": "machine learning algorithms for financial asset price forecasting", "id": "2004.01504", "abstract": "this research paper explores the performance of machine learning (ml) algorithms and techniques that can be used for financial asset price forecasting. the prediction and forecasting of asset prices and returns remains one of the most challenging and exciting problems for quantitative finance and practitioners alike. the massive increase in data generated and captured in recent years presents an opportunity to leverage machine learning algorithms. this study directly compares and contrasts state-of-the-art implementations of modern machine learning algorithms on high performance computing (hpc) infrastructures versus the traditional and highly popular capital asset pricing model (capm) on u.s equities data. the implemented machine learning models - trained on time series data for an entire stock universe (in addition to exogenous macroeconomic variables) significantly outperform the capm on out-of-sample (oos) test data.", "categories": "q-fin.st cs.lg econ.em stat.ml", "created": "2020-03-31", "updated": "", "authors": ["philip ndikum"], "url": "https://arxiv.org/abs/2004.01504"}, {"title": "comprehensive review of deep reinforcement learning methods and   applications in economics", "id": "2004.01509", "abstract": "the popularity of deep reinforcement learning (drl) methods in economics have been exponentially increased. drl through a wide range of capabilities from reinforcement learning (rl) and deep learning (dl) for handling sophisticated dynamic business environments offers vast opportunities. drl is characterized by scalability with the potential to be applied to high-dimensional problems in conjunction with noisy and nonlinear patterns of economic data. in this work, we first consider a brief review of dl, rl, and deep rl methods in diverse applications in economics providing an in-depth insight into the state of the art. furthermore, the architecture of drl applied to economic applications is investigated in order to highlight the complexity, robustness, accuracy, performance, computational tasks, risk constraints, and profitability. the survey results indicate that drl can provide better performance and higher accuracy as compared to the traditional algorithms while facing real economic problems at the presence of risk parameters and the ever-increasing uncertainties.", "categories": "q-fin.st cs.lg econ.gn q-fin.ec stat.ml", "created": "2020-03-21", "updated": "", "authors": ["amir mosavi", "pedram ghamisi", "yaser faghan", "puhong duan"], "url": "https://arxiv.org/abs/2004.01509"}, {"title": "credible, truthful, and two-round (optimal) auctions via cryptographic   commitments", "id": "2004.01598", "abstract": "we consider the sale of a single item to multiple buyers by a revenue-maximizing seller. recent work of akbarpour and li formalizes \\emph{credibility} as an auction desideratum, and prove that the only optimal, credible, strategyproof auction is the ascending price auction with reserves (akbarpour and li, 2019).   in contrast, when buyers' valuations are mhr, we show that the mild additional assumption of a cryptographically secure commitment scheme suffices for a simple \\emph{two-round} auction which is optimal, strategyproof, and credible (even when the number of bidders is only known by the auctioneer).   we extend our analysis to the case when buyer valuations are $\\alpha$-strongly regular for any $\\alpha > 0$, up to arbitrary $\\varepsilon$ in credibility. interestingly, we also prove that this construction cannot be extended to regular distributions, nor can the $\\varepsilon$ be removed with multiple bidders.", "categories": "cs.gt cs.cr econ.th", "created": "2020-04-03", "updated": "2020-05-25", "authors": ["matheus v. x. ferreira", "s. matthew weinberg"], "url": "https://arxiv.org/abs/2004.01598"}, {"title": "uniform inference in high-dimensional generalized additive models", "id": "2004.01623", "abstract": "we develop a method for uniform valid confidence bands of a nonparametric component $f_1$ in the general additive model $y=f_1(x_1)+\\ldots + f_p(x_p) + \\varepsilon$ in a high-dimensional setting. we employ sieve estimation and embed it in a high-dimensional z-estimation framework allowing us to construct uniformly valid confidence bands for the first component $f_1$. as usual in high-dimensional settings where the number of regressors $p$ may increase with sample, a sparsity assumption is critical for the analysis. we also run simulations studies which show that our proposed method gives reliable results concerning the estimation properties and coverage properties even in small samples. finally, we illustrate our procedure with an empirical application demonstrating the implementation and the use of the proposed method in practice.", "categories": "stat.me econ.em stat.ml", "created": "2020-04-03", "updated": "", "authors": ["philipp bach", "sven klaassen", "jannis kueck", "martin spindler"], "url": "https://arxiv.org/abs/2004.01623"}, {"title": "on the structure of stable tournament solutions", "id": "2004.01651", "abstract": "a fundamental property of choice functions is stability, which, loosely speaking, prescribes that choice sets are invariant under adding and removing unchosen alternatives. we provide several structural insights that improve our understanding of stable choice functions. in particular, (i) we show that every stable choice function is generated by a unique simple choice function, which never excludes more than one alternative, (ii) we completely characterize which simple choice functions give rise to stable choice functions, and (iii) we prove a strong relationship between stability and a new property of tournament solutions called local reversal symmetry. based on these findings, we provide the first concrete tournament---consisting of 24 alternatives---in which the tournament equilibrium set fails to be stable. furthermore, we prove that there is no more discriminating stable tournament solution than the bipartisan set and that the bipartisan set is the unique most discriminating tournament solution which satisfies standard properties proposed in the literature.", "categories": "econ.th cs.gt math.co", "created": "2020-04-03", "updated": "", "authors": ["felix brandt", "markus brill", "hans georg seedig", "warut suksompong"], "url": "https://arxiv.org/abs/2004.01651"}, {"title": "reselling information", "id": "2004.01788", "abstract": "information is replicable in that it can be simultaneously consumed and sold to others. we study how resale affects a decentralized market for information. we show that even if the initial seller is an informational monopolist, she captures non-trivial rents from at most a single buyer: her payoffs converge to 0 as soon as a single buyer has bought information. by contrast, if the seller can also sell valueless tokens, there exists a ``prepay equilibrium'' where payment is extracted from all buyers before the information good is released. by exploiting resale possibilities, this prepay equilibrium gives the seller as high a payoff as she would achieve if resale were prohibited.", "categories": "cs.gt econ.th", "created": "2020-04-03", "updated": "", "authors": ["s. nageeb ali", "ayal chen-zion", "erik lillethun"], "url": "https://arxiv.org/abs/2004.01788"}, {"title": "inside the mind of a stock market crash", "id": "2004.01831", "abstract": "we analyze how investor expectations about economic growth and stock returns changed during the february-march 2020 stock market crash induced by the covid-19 pandemic, as well as during the subsequent partial stock market recovery. we surveyed retail investors who are clients of vanguard at three points in time: (i) on february 11-12, around the all-time stock market high, (ii) on march 11-12, after the stock market had collapsed by over 20\\%, and (iii) on april 16-17, after the market had rallied 25\\% from its lowest point. following the crash, the average investor turned more pessimistic about the short-run performance of both the stock market and the real economy. investors also perceived higher probabilities of both further extreme stock market declines and large declines in short-run real economic activity. in contrast, investor expectations about long-run (10-year) economic and stock market outcomes remained largely unchanged, and, if anything, improved. disagreement among investors about economic and stock market outcomes also increased substantially following the stock market crash, with the disagreement persisting through the partial market recovery. those respondents who were the most optimistic in february saw the largest decline in expectations, and sold the most equity. those respondents who were the most pessimistic in february largely left their portfolios unchanged during and after the crash.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2020-04-03", "updated": "2020-05-21", "authors": ["stefano giglio", "matteo maggiori", "johannes stroebel", "stephen utkus"], "url": "https://arxiv.org/abs/2004.01831"}, {"title": "kernel estimation of spot volatility with microstructure noise using   pre-averaging", "id": "2004.01865", "abstract": "we first revisit the problem of kernel estimation of spot volatility in a general continuous it\\^o semimartingale model in the absence of microstructure noise, and prove a central limit theorem with optimal convergence rate, which is an extension of figueroa and li (2020) as we allow for a general two-sided kernel function. next, to handle the microstructure noise of ultra high-frequency observations, we present a new type of pre-averaging/kernel estimator for spot volatility under the presence of additive microstructure noise. we prove central limit theorems for the estimation error with an optimal rate and study the problems of optimal bandwidth and kernel selection. as in the case of a simple kernel estimator of spot volatility in the absence of microstructure noise, we show that the asymptotic variance of the pre-averaging/kernel estimator is minimal for exponential or laplace kernels, hence, justifying the need of working with unbounded kernels as proposed in this work. feasible implementation of the proposed estimators with optimal bandwidth is also developed. monte carlo experiments confirm the superior performance of the devised method.", "categories": "econ.em math.st q-fin.st stat.th", "created": "2020-04-04", "updated": "", "authors": ["jos\u00e9 e. figueroa-l\u00f3pez", "bei wu"], "url": "https://arxiv.org/abs/2004.01865"}, {"title": "incentive compatibility in sender-receiver stopping games", "id": "2004.01910", "abstract": "we introduce a model of sender-receiver stopping games, where the state of the world follows an iid--process throughout the game. at each period, the sender observes the current state, and sends a message to the receiver, suggesting either to stop or to continue. the receiver, only seeing the message but not the state, decides either to stop the game, or to continue which takes the game to the next period. the payoff to each player is a function of the state when the receiver quits, with higher states leading to better payoffs. the horizon of the game can be finite or infinite.   we prove existence and uniqueness of responsive (i.e. non-babbling) perfect bayesian equilibrium (pbe) under mild conditions on the game primitives in the case where the players are sufficiently patient. the responsive pbe has a remarkably simple structure, which builds on the identification of an easy-to-implement and compute class of threshold strategies for the sender. with the help of these threshold strategies, we derive simple expressions describing this pbe. it turns out that in this pbe the receiver obediently follows the recommendations of the sender. hence, surprisingly, the sender alone plays the decisive role, and regardless of the payoff function of the receiver the sender always obtains the best possible payoff for himself.", "categories": "cs.gt econ.th", "created": "2020-04-04", "updated": "", "authors": ["aditya aradhye", "j\u00e1nos flesch", "mathias staudigl", "dries vermeulen"], "url": "https://arxiv.org/abs/2004.01910"}, {"title": "effects of the affordable care act dependent coverage mandate on health   insurance coverage for individuals in same-sex couples", "id": "2004.02296", "abstract": "a large body of research documents that the 2010 dependent coverage mandate of the affordable care act was responsible for significantly increasing health insurance coverage among young adults. no prior research has examined whether sexual minority young adults also benefitted from the dependent coverage mandate, despite previous studies showing lower health insurance coverage among sexual minorities and the fact that their higher likelihood of strained relationships with their parents might predict a lower ability to use parental coverage. our estimates from the american community surveys using difference-in-differences and event study models show that men in same-sex couples age 21-25 were significantly more likely to have any health insurance after 2010 compared to the associated change for slightly older 27 to 31-year-old men in same-sex couples. this increase is concentrated among employer-sponsored insurance, and it is robust to permutations of time periods and age groups. effects for women in same-sex couples and men in different-sex couples are smaller than the associated effects for men in same-sex couples. these findings confirm the broad effects of expanded dependent coverage and suggest that eliminating the federal dependent mandate could reduce health insurance coverage among young adult sexual minorities in same-sex couples.", "categories": "econ.gn q-fin.ec", "created": "2020-04-05", "updated": "", "authors": ["christopher s. carpenter", "gilbert gonzales", "tara mckay", "dario sansone"], "url": "https://arxiv.org/abs/2004.02296"}, {"title": "split cycle: a new condorcet consistent voting method independent of   clones and immune to spoilers", "id": "2004.02350", "abstract": "we propose a condorcet consistent voting method that we call split cycle. split cycle belongs to the small family of known voting methods satisfying independence of clones and the pareto principle. unlike other methods in this family, split cycle satisfies a new criterion we call immunity to spoilers, which concerns adding candidates to elections, as well as the known criteria of positive involvement and negative involvement, which concern adding voters to elections. thus, relative to other clone-independent paretian methods, split cycle mitigates \"spoiler effects\" and \"strong no show paradoxes.\"", "categories": "cs.gt cs.ma econ.th", "created": "2020-04-05", "updated": "2020-09-10", "authors": ["wesley h. holliday", "eric pacuit"], "url": "https://arxiv.org/abs/2004.02350"}, {"title": "final topology for preference spaces", "id": "2004.02357", "abstract": "most decision problems can be understood as a mapping from a preference space into a set of outcomes. when preferences are representable via utility functions, this generates a mapping from a space of utility functions into outcomes. we say a model is continuous in utilities (resp., preferences) if small perturbations of utility functions (resp., preferences) generate small changes in outcomes. while similar, these two concepts are equivalent only when the topology satisfies the following universal property: for each continuous mapping from preferences to outcomes there is a unique mapping from utilities to outcomes that is faithful to the preference map and is continuous. the topologies that satisfy such a universal property are called final topologies. in this paper, we analyze the properties of the final topology for preference sets. this is of practical importance since most of the analysis on continuity is done via utility functions and not the primitive preference space. our results allow the researcher to extrapolate continuity in utility to continuity in the underlying preferences.", "categories": "econ.th", "created": "2020-04-05", "updated": "", "authors": ["pablo schenone"], "url": "https://arxiv.org/abs/2004.02357"}, {"title": "spanning analysis of stock market anomalies under prospect stochastic   dominance", "id": "2004.02670", "abstract": "we develop and implement methods for determining whether introducing new securities or relaxing investment constraints improves the investment opportunity set for prospect investors. we formulate a new testing procedure for prospect spanning for two nested portfolio sets based on subsampling and linear programming. in an application, we use the prospect spanning framework to evaluate whether well-known anomalies are spanned by standard factors. we find that of the strategies considered, many expand the opportunity set of the prospect type investors, thus have real economic value for them. in-sample and out-of-sample results prove remarkably consistent in identifying genuine anomalies for prospect investors.", "categories": "q-fin.pm econ.em q-fin.st stat.ap stat.me", "created": "2020-04-06", "updated": "", "authors": ["stelios arvanitis", "olivier scaillet", "nikolas topaloglou"], "url": "https://arxiv.org/abs/2004.02670"}, {"title": "what do online listings tell us about the housing market?", "id": "2004.02706", "abstract": "traditional data sources for the analysis of housing markets show several limitations, that recently started to be overcome using data coming from housing sales advertisements (ads) websites. in this paper, using a large dataset of ads in italy, we provide the first comprehensive analysis of the problems and potential of these data. the main problem is that multiple ads (\"duplicates\") can correspond to the same housing unit. we show that this issue is mainly caused by sellers' attempt to increase visibility of their listings. duplicates lead to misrepresentation of the volume and composition of housing supply, but this bias can be corrected by identifying duplicates with machine learning tools. we then focus on the potential of these data. we show that the timeliness, granularity, and online nature of these data allow monitoring of housing demand, supply and liquidity, and that the (asking) prices posted on the website can be more informative than transaction prices.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-04-06", "updated": "", "authors": ["michele loberto", "andrea luciani", "marco pangallo"], "url": "https://arxiv.org/abs/2004.02706"}, {"title": "double debiased machine learning nonparametric inference with continuous   treatments", "id": "2004.03036", "abstract": "we propose a nonparametric inference method for causal effects of continuous treatment variables, under unconfoundedness and in the presence of high-dimensional or nonparametric nuisance parameters. our double debiased machine learning (dml) estimators for the average dose-response function (or the average structural function) and the partial effects are asymptotically normal with nonparametric convergence rates. the nuisance estimators for the conditional expectation function and the conditional density can be nonparametric kernel or series estimators or ml methods. using a kernel-based doubly robust influence function and cross-fitting, we give tractable primitive conditions under which the nuisance estimators do not affect the first-order large sample distribution of the dml estimators. we justify the use of kernel to localize the continuous treatment at a given value by the gateaux derivative. we implement various ml methods in monte carlo simulations and an empirical application on a job training program evaluation.", "categories": "econ.em", "created": "2020-04-06", "updated": "", "authors": ["kyle colangelo", "ying-ying lee"], "url": "https://arxiv.org/abs/2004.03036"}, {"title": "the economics of social data", "id": "2004.03107", "abstract": "a data intermediary pays consumers for information about their preferences and sells the information so acquired to firms that use it to tailor their products and prices. the social dimension of the individual data---whereby an individual's data are predictive of the behavior of others---generates a data externality that reduces the intermediary's cost of acquiring information. we derive the intermediary's optimal data policy and show that it preserves the privacy of the consumers' identities while providing precise information about market demand to the firms. this enables the intermediary to capture the entire value of information as the number of consumers grows large.", "categories": "cs.gt econ.gn q-fin.ec", "created": "2020-04-06", "updated": "", "authors": ["dirk bergemann", "alessandro bonatti", "tan gan"], "url": "https://arxiv.org/abs/2004.03107"}, {"title": "visualising the evolution of english covid-19 cases with topological   data analysis ball mapper", "id": "2004.03282", "abstract": "understanding disease spread through data visualisation has concentrated on trends and maps. whilst these are helpful, they neglect important multi-dimensional interactions between characteristics of communities. using the topological data analysis ball mapper algorithm we construct an abstract representation of nuts3 level economic data, overlaying onto it the confirmed cases of covid-19 in england. in so doing we may understand how the disease spreads on different socio-economical dimensions. it is observed that some areas of the characteristic space have quickly raced to the highest levels of infection, while others close by in the characteristic space, do not show large infection growth. likewise, we see patterns emerging in very different areas that command more monitoring. a strong contribution for topological data analysis, and the ball mapper algorithm especially, in comprehending dynamic epidemic data is signposted.", "categories": "physics.soc-ph cs.hc econ.em q-bio.pe", "created": "2020-04-07", "updated": "2020-04-18", "authors": ["pawel dlotko", "simon rudkin"], "url": "https://arxiv.org/abs/2004.03282"}, {"title": "inference in unbalanced panel data models with interactive fixed effects", "id": "2004.03414", "abstract": "in this article, we study the limiting behavior of bai (2009)'s interactive fixed effects estimator in the presence of randomly missing data. in extensive simulation experiments, we show that the inferential theory derived by bai (2009) and moon and weidner (2017) approximates the behavior of the estimator fairly well. however, we find that the fraction and pattern of randomly missing data affect the performance of the estimator. additionally, we use the interactive fixed effects estimator to reassess the baseline analysis of acemoglu et al. (2019). allowing for a more general form of unobserved heterogeneity as the authors, we confirm significant effects of democratization on growth.", "categories": "econ.em", "created": "2020-04-07", "updated": "", "authors": ["daniel czarnowske", "amrei stammann"], "url": "https://arxiv.org/abs/2004.03414"}, {"title": "robust empirical bayes confidence intervals", "id": "2004.03448", "abstract": "we construct robust empirical bayes confidence intervals (ebcis) in a normal means problem. the intervals are centered at the usual linear empirical bayes estimator, but use a critical value accounting for shrinkage. parametric ebcis that assume a normal distribution for the means (morris, 1983) may substantially undercover when this assumption is violated, and we derive a simple rule of thumb for gauging the potential coverage distortion. in contrast, our ebcis control coverage regardless of the means distribution, while remaining close in length to the parametric ebcis when the means are indeed gaussian. if the means are treated as fixed, our ebcis have an average coverage guarantee: the coverage probability is at least $1 - \\alpha$ on average across the $n$ ebcis for each of the means. our empirical applications consider effects of u.s. neighborhoods on intergenerational mobility, and structural changes in a large dynamic factor model for the eurozone.", "categories": "econ.em stat.me", "created": "2020-04-07", "updated": "2020-06-15", "authors": ["timothy b. armstrong", "michal koles\u00e1r", "mikkel plagborg-m\u00f8ller"], "url": "https://arxiv.org/abs/2004.03448"}, {"title": "crisis-critical intellectual property: findings from the covid-19   pandemic", "id": "2004.03715", "abstract": "within national and international innovation systems a pandemic calls for large-scale action by many actors across sectors, to mobilise resources, developing and manufacturing crisis-critical products (cc-products) efficiently and in the huge quantities needed. nowadays, this also includes digital innovations from complex epidemiological models, ai, to open data platforms for prevention, diagnostic and treatment. amongst the many challenges during a pandemic, innovation and manufacturing stakeholders find themselves engaged in new relationships, and are likely to face intellectual property (ip) related challenges. this paper adopts an ip perspective on the covid-19 pandemic to identify pandemic related ip considerations and ip challenges. the focus is on challenges related to research, development and urgent upscaling of capacity to manufacture cc-products in the huge volumes suddenly in demand. its purpose is to provide a structure for steering clear of ip challenges to avoid delays in fighting a pandemic. we identify 4 stakeholder groups concerned with ip challenges: (i) governments, (ii) organisations owning existing crisis-critical ip, described as incumbents in crisis-critical sectors (cc-sectors), (iii) manufacturing firms from other sectors normally not producing cc-products suddenly rushing into cc-sectors to support the manufacturing of cc-products (new entrants), and (iv) voluntary grassroot initiatives that are formed during a pandemic. this paper discusses ip challenges related to the development and manufacturing of technologies and products for (i) prevention (of spread), (ii) diagnosis of infected patients and (iii) the development of treatments. we offer an initial discussion of potential response measures to reduce ip associated risks among industrial stakeholders during a pandemic.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-04-06", "updated": "2020-05-01", "authors": ["frank tietze", "pratheeba vimalnath", "leonidas aristodemou", "jenny molloy"], "url": "https://arxiv.org/abs/2004.03715"}, {"title": "manipulation-proof machine learning", "id": "2004.03865", "abstract": "an increasing number of decisions are guided by machine learning algorithms. in many settings, from consumer credit to criminal justice, those decisions are made by applying an estimator to data on an individual's observed behavior. but when consequential decisions are encoded in rules, individuals may strategically alter their behavior to achieve desired outcomes. this paper develops a new class of estimator that is stable under manipulation, even when the decision rule is fully transparent. we explicitly model the costs of manipulating different behaviors, and identify decision rules that are stable in equilibrium. through a large field experiment in kenya, we show that decision rules estimated with our strategy-robust method outperform those based on standard supervised learning approaches.", "categories": "econ.th cs.lg econ.em", "created": "2020-04-08", "updated": "", "authors": ["daniel bj\u00f6rkegren", "joshua e. blumenstock", "samsun knight"], "url": "https://arxiv.org/abs/2004.03865"}, {"title": "bias optimal vol-of-vol estimation: the role of window overlapping", "id": "2004.04013", "abstract": "we derive a feasible criterion for the bias-optimal selection of the tuning parameters involved in estimating the integrated volatility of the spot volatility via the simple realized estimator by barndorff-nielsen and veraart (2009). our analytic results are obtained assuming that the spot volatility is a continuous mean-reverting process and that consecutive local windows for estimating the spot volatility are allowed to overlap in a finite sample setting. moreover, our analytic results support some optimal selections of tuning parameters prescribed in the literature, based on numerical evidence. interestingly, it emerges that the window-overlapping is crucial for optimizing the finite-sample bias of volatility-of-volatility estimates.", "categories": "econ.em", "created": "2020-04-08", "updated": "", "authors": ["giacomo toscano", "maria cristina recchioni"], "url": "https://arxiv.org/abs/2004.04013"}, {"title": "applications of the coase theorem", "id": "2004.04247", "abstract": "the coase theorem has a central place in the theory of environmental economics and regulation. but its applicability for solving real-world externality problems remains debated. in this paper, we first place this seminal contribution in its historical context. we then survey the experimental literature that has tested the importance of the many, often tacit assumptions in the coase theorem in the laboratory. we discuss a selection of applications of the coase theorem to actual environmental problems, distinguishing between situations in which the polluter or the pollutee pays. while limited in scope, coasian bargaining over externalities offers a pragmatic solution to problems that are difficult to solve in any other way.", "categories": "econ.gn q-fin.ec", "created": "2020-04-08", "updated": "", "authors": ["tatyana deryugina", "frances moore", "richard s. j. tol"], "url": "https://arxiv.org/abs/2004.04247"}, {"title": "classifying economics for the common good: connecting sustainable   development goals to jel codes", "id": "2004.04384", "abstract": "how does economics research help in solving societal challenges? this brief note sheds additional light on this question by providing ways to connect journal of economic literature (jel) codes and sustainable development goals (sdgs) of the united nations. these simple linkages illustrate that the themes of sdgs have corresponding jel classification codes. as the mappings presented here are necessarily imperfect and incomplete, there is plenty of room for improvements. in an ideal world, there would be a jel classification system for sdgs, a separate jel code for each of the 17 sdgs.", "categories": "econ.gn q-fin.ec", "created": "2020-04-09", "updated": "", "authors": ["jussi t. s. heikkil\u00e4"], "url": "https://arxiv.org/abs/2004.04384"}, {"title": "on the factors influencing the choices of weekly telecommuting   frequencies of post-secondary students in toronto", "id": "2004.04683", "abstract": "the paper presents an empirical investigation of telecommuting frequency choices by post-secondary students in toronto. it uses a dataset collected through a large-scale travel survey conducted on post-secondary students of four major universities in toronto and it employs multiple alternative econometric modelling techniques for the empirical investigation. results contribute on two fronts. firstly, it presents empirical investigations of factors affecting telecommuting frequency choices of post-secondary students that are rare in literature. secondly, it identifies better a performing econometric modelling technique for modelling telecommuting frequency choices. empirical investigation clearly reveals that telecommuting for school related activities is prevalent among post-secondary students in toronto. around 80 percent of 0.18 million of the post-secondary students of the region, who make roughly 36,000 trips per day, also telecommute at least once a week. considering that large numbers of students need to spend a long time travelling from home to campus with around 33 percent spending more than two hours a day on travelling, telecommuting has potential to enhance their quality of life. empirical investigations reveal that car ownership and living farther from the campus have similar positive effects on the choice of higher frequency of telecommuting. students who use a bicycle for regular travel are least likely to telecommute, compared to those using transit or a private car.", "categories": "econ.em", "created": "2020-04-09", "updated": "", "authors": ["khandker nurul habib", "ph. d.", "n/a peng"], "url": "https://arxiv.org/abs/2004.04683"}, {"title": "the benefits and costs of social distancing in rich and poor countries", "id": "2004.04867", "abstract": "social distancing is the primary policy prescription for combating the covid-19 pandemic, and has been widely adopted in europe and north america. we estimate the value of disease avoidance using an epidemiological model that projects the spread of covid-19 across rich and poor countries. social distancing measures that \"flatten the curve\" of the disease to bring demand within the capacity of healthcare systems are predicted to save many lives in high-income countries, such that practically any economic cost is worth bearing. these social distancing policies are estimated to be less effective in poor countries with younger populations less susceptible to covid-19, and more limited healthcare systems, which were overwhelmed before the pandemic. moreover, social distancing lowers disease risk by limiting people's economic opportunities. poorer people are less willing to make those economic sacrifices. they place relatively greater value on their livelihood concerns compared to contracting covid-19. not only are the epidemiological and economic benefits of social distancing much smaller in poorer countries, such policies may exact a heavy toll on the poorest and most vulnerable. workers in the informal sector lack the resources and social protections to isolate themselves and sacrifice economic opportunities until the virus passes. by limiting their ability to earn a living, social distancing can lead to an increase in hunger, deprivation, and related mortality and morbidity. rather than a blanket adoption of social distancing measures, we advocate for the exploration of alternative harm-reduction strategies, including universal mask adoption and increased hygiene measures.", "categories": "econ.gn q-fin.ec", "created": "2020-04-09", "updated": "", "authors": ["zachary barnett-howell", "ahmed mushfiq mobarak"], "url": "https://arxiv.org/abs/2004.04867"}, {"title": "forecasts with bayesian vector autoregressions under real time   conditions", "id": "2004.04984", "abstract": "this paper investigates the sensitivity of forecast performance measures to taking a real time versus pseudo out-of-sample perspective. we use monthly vintages for the united states (us) and the euro area (ea) and estimate a set of vector autoregressive (var) models of different sizes with constant and time-varying parameters (tvps) and stochastic volatility (sv). our results suggest differences in the relative ordering of model performance for point and density forecasts depending on whether real time data or truncated final vintages in pseudo out-of-sample simulations are used for evaluating forecasts. no clearly superior specification for the us or the ea across variable types and forecast horizons can be identified, although larger models featuring tvps appear to be affected the least by missing values and data revisions. we identify substantial differences in performance metrics with respect to whether forecasts are produced for the us or the ea.", "categories": "econ.em stat.ap", "created": "2020-04-10", "updated": "", "authors": ["michael pfarrhofer"], "url": "https://arxiv.org/abs/2004.04984"}, {"title": "synthetic control group methods in the presence of interference: the   direct and spillover effects of light rail on neighborhood retail activity", "id": "2004.05027", "abstract": "in recent years, synthetic control group (scg) methods have received great attention from scholars and have been subject to extensions and comparisons with alternative approaches for program evaluation. however, the existing methodological literature mainly relies on the assumption of non-interference. we propose to generalize the scg method to studies where interference between the treated and the untreated units is plausible. we frame our discussion in the potential outcomes approach. under a partial interference assumption, we formally define relevant direct and spillover effects. we also consider the \"unrealized\" spillover effect on the treated unit in the hypothetical scenario that another unit in the treated unit's neighborhood had been assigned to the intervention. then we investigate the assumptions under which we can identify and estimate the causal effects of interest, and show how they can be estimated using the scg method. we apply our approach to the analysis of an observational study, where the focus is on assessing direct and spillover causal effects of a new light rail line recently built in florence (italy) on the commercial vitality of the street where it was built and of the streets in the treated street's neighborhood.", "categories": "stat.ap econ.em", "created": "2020-04-10", "updated": "2020-06-18", "authors": ["giulio grossi", "patrizia lattarulo", "marco mariani", "alessandra mattei", "\u00f6zge \u00f6ner"], "url": "https://arxiv.org/abs/2004.05027"}, {"title": "wild bootstrap inference for penalized quantile regression for   longitudinal data", "id": "2004.05127", "abstract": "existing work on penalized quantile regression for longitudinal data has been focused almost exclusively on point estimation. in this work, we investigate statistical inference. we first show that the pairs bootstrap that samples cross-sectional units with replacement does not approximate well the limiting distribution of the penalized estimator. we then propose a wild residual bootstrap procedure and show that it is asymptotically valid for approximating the distribution of the penalized estimator. the new method is easy to implement and uses weight distributions that are standard in the literature. simulation studies are carried out to investigate the small sample behavior of the proposed approach in comparison with existing procedures. finally, we illustrate the new approach using u.s. census data to estimate a high-dimensional model that includes more than eighty thousand parameters.", "categories": "econ.em", "created": "2020-04-10", "updated": "", "authors": ["carlos lamarche", "thomas parker"], "url": "https://arxiv.org/abs/2004.05127"}, {"title": "effective alleviation of rural poverty depends on the interplay between   productivity, nutrients, water and soil quality", "id": "2004.05229", "abstract": "most of the world poorest people come from rural areas and depend on their local ecosystems for food production. recent research has highlighted the importance of self-reinforcing dynamics between low soil quality and persistent poverty but little is known on how they affect poverty alleviation. we investigate how the intertwined dynamics of household assets, nutrients (especially phosphorus), water and soil quality influence food production and determine the conditions for escape from poverty for the rural poor. we have developed a suite of dynamic, multidimensional poverty trap models of households that combine economic aspects of growth with ecological dynamics of soil quality, water and nutrient flows to analyze the effectiveness of common poverty alleviation strategies such as intensification through agrochemical inputs, diversification of energy sources and conservation tillage. our results show that (i) agrochemical inputs can reinforce poverty by degrading soil quality, (ii) diversification of household energy sources can create possibilities for effective application of other strategies, and (iii) sequencing of interventions can improve effectiveness of conservation tillage. our model-based approach demonstrates the interdependence of economic and ecological dynamics which preclude blanket solution for poverty alleviation. stylized models as developed here can be used for testing effectiveness of different strategies given biophysical and economic settings in the target region.", "categories": "econ.gn q-fin.ec", "created": "2020-04-08", "updated": "", "authors": ["sonja radosavljevic", "l. jamila haider", "steven j. lade", "maja schluter"], "url": "https://arxiv.org/abs/2004.05229"}, {"title": "closing gaps in asymptotic fair division", "id": "2004.05563", "abstract": "we study a resource allocation setting where $m$ discrete items are to be divided among $n$ agents with additive utilities, and the agents' utilities for individual items are drawn at random from a probability distribution. since common fairness notions like envy-freeness and proportionality cannot always be satisfied in this setting, an important question is when allocations satisfying these notions exist. in this paper, we close several gaps in the line of work on asymptotic fair division. first, we prove that the classical round-robin algorithm is likely to produce an envy-free allocation provided that $m=\\omega(n\\log n/\\log\\log n)$, matching the lower bound from prior work. we then show that a proportional allocation exists with high probability as long as $m\\geq n$, while an allocation satisfying envy-freeness up to any item (efx) is likely to be present for any relation between $m$ and $n$. finally, we consider a related setting where each agent is assigned exactly one item and the remaining items are left unassigned, and show that the transition from non-existence to existence with respect to envy-free assignments occurs at $m=en$.", "categories": "cs.gt econ.th math.pr", "created": "2020-04-12", "updated": "", "authors": ["pasin manurangsi", "warut suksompong"], "url": "https://arxiv.org/abs/2004.05563"}, {"title": "a machine learning approach for flagging incomplete bid-rigging cartels", "id": "2004.05629", "abstract": "we propose a new method for flagging bid rigging, which is particularly useful for detecting incomplete bid-rigging cartels. our approach combines screens, i.e. statistics derived from the distribution of bids in a tender, with machine learning to predict the probability of collusion. as a methodological innovation, we calculate such screens for all possible subgroups of three or four bids within a tender and use summary statistics like the mean, median, maximum, and minimum of each screen as predictors in the machine learning algorithm. this approach tackles the issue that competitive bids in incomplete cartels distort the statistical signals produced by bid rigging. we demonstrate that our algorithm outperforms previously suggested methods in applications to incomplete cartels based on empirical data from switzerland.", "categories": "econ.em cs.lg stat.ml", "created": "2020-04-12", "updated": "", "authors": ["hannes wallimann", "david imhof", "martin huber"], "url": "https://arxiv.org/abs/2004.05629"}, {"title": "mflica: an r package for inferring leadership of coordination from time   series", "id": "2004.06092", "abstract": "leadership is a process that leaders influence followers to achieve collective goals. one of special cases of leadership is the coordinated pattern initiation. in this context, leaders are initiators who initiate coordinated patterns that everyone follows. given a set of individual-multivariate time series of real numbers, the mflica package provides a framework for r users to infer coordination events within time series, initiators and followers of these coordination events, as well as dynamics of group merging and splitting. the mflica package also has a visualization function to make results of leadership inference more understandable. the package is available on comprehensive r archive network (cran) at https://cran.r-project.org/package=mflica.", "categories": "cs.si econ.em stat.co stat.me", "created": "2020-04-10", "updated": "", "authors": ["chainarong amornbunchornvej"], "url": "https://arxiv.org/abs/2004.06092"}, {"title": "the effect of stay-at-home orders on covid-19 cases and fatalities in   the united states", "id": "2004.06098", "abstract": "governments issue \"stay at home\" orders to reduce the spread of contagious diseases, but the magnitude of such orders' effectiveness is uncertain. in the united states these orders were not coordinated at the national level during the coronavirus disease 2019 (covid-19) pandemic, which creates an opportunity to use spatial and temporal variation to measure the policies' effect with greater accuracy. here, we combine data on the timing of stay-at-home orders with daily confirmed covid-19 cases and fatalities at the county level in the united states. we estimate the effect of stay-at-home orders using a difference-in-differences design that accounts for unmeasured local variation in factors like health systems and demographics and for unmeasured temporal variation in factors like national mitigation actions and access to tests. compared to counties that did not implement stay-at-home orders, the results show that the orders are associated with a 30.2 percent (11.0 to 45.2) reduction in weekly cases after one week, a 40.0 percent (23.4 to 53.0) reduction after two weeks, and a 48.6 percent (31.1 to 61.7) reduction after three weeks. stay-at-home orders are also associated with a 59.8 percent (18.3 to 80.2) reduction in weekly fatalities after three weeks. these results suggest that stay-at-home orders reduced confirmed cases by 390,000 (170,000 to 680,000) and fatalities by 41,000 (27,000 to 59,000) within the first three weeks in localities where they were implemented.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2020-04-13", "updated": "2020-05-07", "authors": ["james h. fowler", "seth j. hill", "remy levin", "nick obradovich"], "url": "https://arxiv.org/abs/2004.06098"}, {"title": "estimating the covid-19 infection rate: anatomy of an inference problem", "id": "2004.06178", "abstract": "as a consequence of missing data on tests for infection and imperfect accuracy of tests, reported rates of population infection by the sars cov-2 virus are lower than actual rates of infection. hence, reported rates of severe illness conditional on infection are higher than actual rates. understanding the time path of the covid-19 pandemic has been hampered by the absence of bounds on infection rates that are credible and informative. this paper explains the logical problem of bounding these rates and reports illustrative findings, using data from illinois, new york, and italy. we combine the data with assumptions on the infection rate in the untested population and on the accuracy of the tests that appear credible in the current context. we find that the infection rate might be substantially higher than reported. we also find that the infection fatality rate in italy is substantially lower than reported.", "categories": "econ.em stat.ap stat.ot", "created": "2020-04-13", "updated": "", "authors": ["charles f. manski", "francesca molinari"], "url": "https://arxiv.org/abs/2004.06178"}, {"title": "on vickrey's income averaging", "id": "2004.06289", "abstract": "we consider a small set of axioms for income averaging -- recursivity, continuity, and the boundary condition for the present. these properties yield a unique averaging function that is the density of the reflected brownian motion with a drift started at the current income and moving over the past incomes. when averaging is done over the short past, the weighting function is asymptotically converging to a gaussian. when averaging is done over the long horizon, the weighing function converges to the exponential distribution. for all intermediate averaging scales, we derive an explicit solution that interpolates between the two.", "categories": "econ.th econ.em", "created": "2020-04-13", "updated": "", "authors": ["stefan steinerberger", "aleh tsyvinski"], "url": "https://arxiv.org/abs/2004.06289"}, {"title": "abrupt declines in tropospheric nitrogen dioxide over china after the   outbreak of covid-19", "id": "2004.06542", "abstract": "china's policy interventions to reduce the spread of the coronavirus disease 2019 have environmental and economic impacts. tropospheric nitrogen dioxide indicates economic activities, as nitrogen dioxide is primarily emitted from fossil fuel consumption. satellite measurements show a 48% drop in tropospheric nitrogen dioxide vertical column densities from the 20 days averaged before the 2020 lunar new year to the 20 days averaged after. this is 20% larger than that from recent years. we relate to this reduction to two of the government's actions: the announcement of the first report in each province and the date of a province's lockdown. both actions are associated with nearly the same magnitude of reductions. our analysis offers insights into the unintended environmental and economic consequences through reduced economic activities.", "categories": "physics.ao-ph econ.gn q-fin.ec", "created": "2020-04-14", "updated": "", "authors": ["fei liu", "aaron page", "sarah a. strode", "yasuko yoshida", "sungyeon choi", "bo zheng", "lok n. lamsal", "can li", "nickolay a. krotkov", "henk eskes", "ronald van der a", "pepijn veefkind", "pieternel levelt", "joanna joiner", "oliver p. hauser"], "url": "https://arxiv.org/abs/2004.06542"}, {"title": "a search model of statistical discrimination", "id": "2004.06645", "abstract": "we offer a search-theoretic model of statistical discrimination, in which firms treat identical groups unequally based on their occupational choices. the model admits symmetric equilibria in which the group characteristic is ignored, but also asymmetric equilibria in which a group is statistically discriminated against, even when symmetric equilibria are unique. moreover, a robust possibility is that symmetric equilibria become unstable when the group characteristic is introduced. unlike most previous literature, our model can justify affirmative action since it eliminates asymmetric equilibria without distorting incentives.", "categories": "econ.th", "created": "2020-04-14", "updated": "2020-04-24", "authors": ["jiadong gu", "peter norman"], "url": "https://arxiv.org/abs/2004.06645"}, {"title": "schr\\\"odinger's ants: a continuous description of kirman's recruitment   model", "id": "2004.06667", "abstract": "we show how the approach to equilibrium in kirman's ants model can be fully characterized in terms of the spectrum of a schr\\\"odinger equation with a p\\\"oschl-teller ($\\tan^2$) potential. among other interesting properties, we have found that in the bimodal phase where ants visit mostly one food site at a time, the switch time between the two sources only depends on the ``spontaneous conversion\" rate and not on the recruitment rate. more complicated correlation functions can be computed exactly, and involve higher and higher eigenvalues and eigenfunctions of the schr\\\"odinger operator, which can be expressed in terms of hypergeometric functions.", "categories": "physics.soc-ph cond-mat.stat-mech econ.gn q-fin.ec", "created": "2020-04-14", "updated": "", "authors": ["jos\u00e9 moran", "antoine fosset", "michael benzaquen", "jean-philippe bouchaud"], "url": "https://arxiv.org/abs/2004.06667"}, {"title": "epidemic control via stochastic optimal control", "id": "2004.06680", "abstract": "we study the problem of optimal control of the stochastic sir model. models of this type are used in mathematical epidemiology to capture the time evolution of highly infectious diseases such as covid-19. our approach relies on reformulating the hamilton-jacobi-bellman equation as a stochastic minimum principle. this results in a system of forward backward stochastic differential equations, which is amenable to numerical solution via monte carlo simulations. we present a number of numerical solutions of the system under a variety of scenarios.", "categories": "q-bio.pe econ.em math.oc q-fin.cp", "created": "2020-04-14", "updated": "2020-05-01", "authors": ["andrew lesniewski"], "url": "https://arxiv.org/abs/2004.06680"}, {"title": "supply and demand shocks in the covid-19 pandemic: an industry and   occupation perspective", "id": "2004.06759", "abstract": "we provide quantitative predictions of first order supply and demand shocks for the u.s. economy associated with the covid-19 pandemic at the level of individual occupations and industries. to analyze the supply shock, we classify industries as essential or non-essential and construct a remote labor index, which measures the ability of different occupations to work from home. demand shocks are based on a study of the likely effect of a severe influenza epidemic developed by the us congressional budget office. compared to the pre-covid period, these shocks would threaten around 22% of the us economy's gdp, jeopardise 24% of jobs and reduce total wage income by 17%. at the industry level, sectors such as transport are likely to have output constrained by demand shocks, while sectors relating to manufacturing, mining and services are more likely to be constrained by supply shocks. entertainment, restaurants and tourism face large supply and demand shocks. at the occupation level, we show that high-wage occupations are relatively immune from adverse supply and demand-side shocks, while low-wage occupations are much more vulnerable. we should emphasize that our results are only first-order shocks -- we expect them to be substantially amplified by feedback effects in the production network.", "categories": "econ.gn q-fin.ec", "created": "2020-04-14", "updated": "", "authors": ["r. maria del rio-chanona", "penny mealy", "anton pichler", "francois lafond", "doyne farmer"], "url": "https://arxiv.org/abs/2004.06759"}, {"title": "economic conditions for innovation: private vs. public sector", "id": "2004.07814", "abstract": "the hicks induced innovation hypothesis states that a price increase of a production factor is a spur to invention. we propose an alternative hypothesis restating that a spur to invention require not only an increase of one factor but also a decrease of at least one other factor to offset the companies' cost. we illustrate the need for our alternative hypothesis in a historical example of the industrial revolution in the united kingdom. furthermore, we econometrically evaluate both hypotheses in a case study of research and development (r&d) in 29 oecd countries from 2003 to 2017. specifically, we investigate dependence of investments to r&d on economic environment represented by average wages and oil prices using panel regression. we find that our alternative hypothesis is supported for r&d funded and/or performed by business enterprises while the original hicks hypothesis holds for r&d funded by the government and r&d performed by universities. our results reflect that business sector is significantly influenced by market conditions, unlike the government and higher education sectors.", "categories": "econ.gn q-fin.ec", "created": "2020-04-15", "updated": "2020-09-03", "authors": ["tom\u00e1\u0161 evan", "vladim\u00edr hol\u00fd"], "url": "https://arxiv.org/abs/2004.07814"}, {"title": "covid-19: $r_0$ is lower where outbreak is larger", "id": "2004.07827", "abstract": "we use daily data from lombardy, the italian region most affected by the covid-19 outbreak, to calibrate a sir model individually on each municipality. these are all covered by the same health system and, in the post-lockdown phase we focus on, all subject to the same social distancing regulations. we find that municipalities with a higher number of cases at the beginning of the period analyzed have a lower rate of diffusion, which cannot be imputed to herd immunity. in particular, there is a robust and strongly significant negative correlation between the estimated basic reproduction number ($r_0$) and the initial outbreak size, in contrast with the role of $r_0$ as a \\emph{predictor} of outbreak size. we explore different possible explanations for this phenomenon and conclude that a higher number of cases causes changes of behavior, such as a more strict adoption of social distancing measures among the population, that reduce the spread. this result calls for a transparent, real-time distribution of detailed epidemiological data, as such data affects the behavior of populations in areas affected by the outbreak.", "categories": "q-bio.pe econ.gn physics.soc-ph q-fin.ec", "created": "2020-04-16", "updated": "", "authors": ["pietro battiston", "simona gamba"], "url": "https://arxiv.org/abs/2004.07827"}, {"title": "identification of a class of index models: a topological approach", "id": "2004.07900", "abstract": "we establish nonparametric identification in a class of so-called index models using a novel approach that relies on general topological results. our proof strategy requires substantially weaker conditions on the functions and distributions characterizing the model compared to existing strategies; in particular, it does not require any large support conditions on the regressors of our model. we apply the general identification result to additive random utility and competing risk models.", "categories": "econ.em math.st stat.th", "created": "2020-04-16", "updated": "", "authors": ["mogens fosgerau", "dennis kristensen"], "url": "https://arxiv.org/abs/2004.07900"}, {"title": "the socio-economic determinants of the coronavirus disease (covid-19)   pandemic", "id": "2004.07947", "abstract": "the magnitude of the coronavirus disease (covid-19) pandemic has an enormous impact on the social life and the economic activities in almost every country in the world. besides the biological and epidemiological factors, a multitude of social and economic criteria also govern the extent of the coronavirus disease spread in the population. consequently, there is an active debate regarding the critical socio-economic determinants that contribute to the resulting pandemic. in this paper, we contribute towards the resolution of the debate by leveraging bayesian model averaging techniques and country level data to investigate the potential of 29 determinants, describing a diverse set of socio-economic characteristics, in explaining the coronavirus pandemic outcome. we show that the true empirical model behind the coronavirus outcome is constituted only of few determinants, but the extent to which each determinant is able to provide a credible explanation varies between countries due to their heterogeneous socio-economic characteristics. to understand the relationship between the potential determinants in the specification of the true model, we develop the coronavirus determinants jointness space. in this space, two determinants are connected with each other if they are able to jointly explain the coronavirus outcome. as constructed, the obtained map acts as a bridge between theoretical investigations and empirical observations, and offers an alternate view for the joint importance of the socio-economic determinants when used for developing policies aimed at preventing future epidemic crises.", "categories": "physics.soc-ph econ.gn q-fin.ec stat.ap", "created": "2020-04-14", "updated": "2020-05-16", "authors": ["viktor stojkoski", "zoran utkovski", "petar jolakoski", "dragan tevdovski", "ljupco kocarev"], "url": "https://arxiv.org/abs/2004.07947"}, {"title": "the direct and spillover effects of a nationwide socio-emotional   learning program for disruptive students", "id": "2004.08126", "abstract": "social and emotional learning (sel) programs teach disruptive students to improve their classroom behavior. small-scale programs in high-income countries have been shown to improve treated students' behavior and academic outcomes. using a randomized experiment, we show that a nationwide sel program in chile has no effect on eligible students. we find evidence that very disruptive students may hamper the program's effectiveness. adhd, a disorder correlated with disruptiveness, is much more prevalent in chile than in high-income countries, so very disruptive students may be more present in chile than in the contexts where sel programs have been shown to work.", "categories": "econ.em", "created": "2020-04-17", "updated": "", "authors": ["cl\u00e9ment de chaisemartin", "nicol\u00e1s navarrete h."], "url": "https://arxiv.org/abs/2004.08126"}, {"title": "mean field game approach to bitcoin mining", "id": "2004.08167", "abstract": "we present an analysis of the proof-of-work consensus algorithm, used on the bitcoin blockchain, using a mean field game framework. using a master equation, we provide an equilibrium characterization of the total computational power devoted to mining the blockchain (hashrate). from a simple setting we show how the master equation approach allows us to enrich the model by relaxing most of the simplifying assumptions. the essential structure of the game is preserved across all the enrichments. in deterministic settings, the hashrate ultimately reaches a steady state in which it increases at the rate of technological progress. in stochastic settings, there exists a target for the hashrate for every possible random state. as a consequence, we show that in equilibrium the security of the underlying blockchain is either $i)$ constant, or $ii)$ increases with the demand for the underlying cryptocurrency.", "categories": "econ.th math.ap q-fin.gn", "created": "2020-04-17", "updated": "", "authors": ["charles bertucci", "louis bertucci", "jean-michel lasry", "pierre-louis lions"], "url": "https://arxiv.org/abs/2004.08167"}, {"title": "causal inference in case-control studies", "id": "2004.08318", "abstract": "we investigate identification of causal parameters in case-control and related studies. the odds ratio in the sample is our main estimand of interest and we articulate its relationship with causal parameters under various scenarios. it turns out that the odds ratio is generally a sharp upper bound for counterfactual relative risk under some monotonicity assumptions, without resorting to strong ignorability, nor to the rare-disease assumption. further, we propose semparametrically efficient, easy-to-implement, machine-learning-friendly estimators of the aggregated (log) odds ratio by exploiting an explicit form of the efficient influence function. using our new estimators, we develop methods for causal inference and illustrate the usefulness of our methods by a real-data example.", "categories": "econ.em stat.me stat.ml", "created": "2020-04-17", "updated": "", "authors": ["sung jae jun", "sokbae lee"], "url": "https://arxiv.org/abs/2004.08318"}, {"title": "estimating and projecting air passenger traffic during the covid-19   coronavirus outbreak and its socio-economic impact", "id": "2004.08460", "abstract": "the main focus of this study is to collect and prepare data on air passengers traffic worldwide with the scope of analyze the impact of travel ban on the aviation sector. based on historical data from january 2010 till october 2019, a forecasting model is implemented in order to set a reference baseline. making use of airplane movements extracted from online flight tracking platforms and on-line booking systems, this study presents also a first assessment of recent changes in flight activity around the world as a result of the covid-19 pandemic. to study the effects of air travel ban on aviation and in turn its socio-economic, several scenarios are constructed based on past pandemic crisis and the observed flight volumes. it turns out that, according to this hypothetical scenarios, in the first quarter of 2020 the impact of aviation losses could have negatively reduced world gdp by 0.02% to 0.12% according to the observed data and, in the worst case scenarios, at the end of 2020 the loss could be as high as 1.41-1.67% and job losses may reach the value of 25-30 millions. focusing on eu27, the gdp loss may amount to 1.66-1.98% by the end of 2020 and the number of job losses from 4.2 to 5 millions in the worst case scenarios. some countries will be more affected than others in the short run and most european airlines companies will suffer from the travel ban.", "categories": "stat.ap econ.em physics.soc-ph", "created": "2020-04-17", "updated": "2020-04-21", "authors": ["stefano maria iacus", "fabrizio natale", "carlos satamaria", "spyridon spyratos", "michele vespe"], "url": "https://arxiv.org/abs/2004.08460"}, {"title": "loss aversion and the welfare ranking of policy interventions", "id": "2004.08468", "abstract": "in this paper we develop theoretical criteria and econometric methods to rank policy interventions in terms of welfare when individuals are loss-averse. the new criterion for \"loss aversion-sensitive dominance\" defines a weak partial ordering of the distributions of policy-induced gains and losses. it applies to the class of welfare functions which model individual preferences with non-decreasing and loss-averse attitudes towards changes in outcomes. we also develop new statistical methods to test loss aversion-sensitive dominance in practice, using nonparametric plug-in estimates. we establish the limiting distributions of uniform test statistics by showing that they are directionally differentiable. this implies that inference can be conducted by a special resampling procedure. since point-identification of the distribution of policy-induced gains and losses may require very strong assumptions, we also extend comparison criteria, test statistics, and resampling procedures to the partially-identified case. finally, we illustrate our methods with an empirical application to welfare comparison of two income support programs.", "categories": "econ.em", "created": "2020-04-17", "updated": "", "authors": ["sergio firpo", "antonio f. galvao", "martyna kobus", "thomas parker", "pedro rosa-dias"], "url": "https://arxiv.org/abs/2004.08468"}, {"title": "transitioning out of the coronavirus lockdown: a framework for   zone-based social distancing", "id": "2004.08504", "abstract": "in the face of elevated pandemic risk, is it necessary to completely lock down the population, imposing extreme social distancing? canonical epidemiological models suggest this may be unavoidable for months at a time, despite the heavy social and human cost of physically isolating people. alternatively, people could retreat into socially or economically defined defensive zones, with more interactions inside their zone than across zones. starting from a complete lockdown, zones could facilitate responsible reopening of education, government, and firms, as a well-implemented structure can dramatically slow the diffusion of the disease. this paper provides a framework for understanding and evaluating the effectiveness of zones for social distancing.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-04-17", "updated": "", "authors": ["eric friedman", "john friedman", "simon johnson", "adam landsberg"], "url": "https://arxiv.org/abs/2004.08504"}, {"title": "determination of bayesian optimal warranty length under type-ii unified   hybrid censoring scheme", "id": "2004.08533", "abstract": "determination of an appropriate warranty length for the lifetime of the product is an important issue to the manufacturer. in this article, optimal warranty length of the product for the combined free replacement and the pro-rata warranty policy is computed based on the type-ii unified hybrid censored data. a non-linear pro-rata warranty policy is proposed in this context. the optimal warranty length is obtained by maximizing an expected utility function. the expectation is taken with respect to the posterior predictive model for the time-to-failure data. it is observed that the non-linear pro-rata warranty policy gives a larger warranty length with maximum profit as compared to linear warranty policy. finally, a real-data set is analyzed in order to illustrate the advantage of using non-linear pro-rata warranty policy.", "categories": "econ.gn q-fin.ec stat.co", "created": "2020-04-18", "updated": "", "authors": ["tanmay sen", "ritwik bhattacharya", "biswabrata pradhan", "yogesh mani tripathi"], "url": "https://arxiv.org/abs/2004.08533"}, {"title": "infection arbitrage", "id": "2004.08701", "abstract": "increasing the infection risk early in an epidemic is individually and socially optimal under some parameter values. the reason is that the early patients recover or die before the peak of the epidemic, which flattens the peak. this improves welfare if the peak exceeds the capacity of the healthcare system and the social loss rises rapidly enough in the number infected. the individual incentive to get infected early comes from the greater likelihood of receiving treatment than at the peak when the disease has overwhelmed healthcare capacity. calibration to the covid-19 pandemic data suggests that catching the infection at the start was individually optimal and for some loss functions would have reduced the aggregate loss.", "categories": "q-bio.pe econ.th", "created": "2020-04-18", "updated": "2020-04-26", "authors": ["sander heinsalu"], "url": "https://arxiv.org/abs/2004.08701"}, {"title": "estimating high-dimensional discrete choice model of differentiated   products with random coefficients", "id": "2004.08791", "abstract": "we propose an estimation procedure for discrete choice models of differentiated products with possibly high-dimensional product attributes. in our model, high-dimensional attributes can be determinants of both mean and variance of the indirect utility of a product. the key restriction in our model is that the high-dimensional attributes affect the variance of indirect utilities only through finitely many indices. in a framework of the random-coefficients logit model, we show a bound on the error rate of a $l_1$-regularized minimum distance estimator and prove the asymptotic linearity of the de-biased estimator.", "categories": "econ.em", "created": "2020-04-19", "updated": "", "authors": ["masayuki sawada", "kohei kawaguchi"], "url": "https://arxiv.org/abs/2004.08791"}, {"title": "the moral burden of ambiguity aversion", "id": "2004.08892", "abstract": "in their article, \"egalitarianism under severe uncertainty\", philosophy and public affairs, 46:3, 2018, thomas rowe and alex voorhoeve develop an original moral decision theory for cases under uncertainty, called \"pluralist egalitarianism under uncertainty\". in this paper, i firstly sketch their views and arguments. i then elaborate on their moral decision theory by discussing how it applies to choice scenarios in health ethics. finally, i suggest a new two-stage ellsberg thought experiment challenging the core of the principle of their theory. in such an experiment pluralist egalitarianism seems to suggest the wrong, morally and rationally speaking, course of action -- no matter whether i consider my thought experiment in a simultaneous or a sequential setting.", "categories": "econ.th cs.ai", "created": "2020-04-19", "updated": "2020-04-26", "authors": ["brian jabarian"], "url": "https://arxiv.org/abs/2004.08892"}, {"title": "on the dynamics emerging from pandemics and infodemics", "id": "2004.08917", "abstract": "this position paper discusses emerging behavioral, social, and economic dynamics related to the covid-19 pandemic and puts particular emphasis on two emerging issues: first, delayed effects (or second strikes) of pandemics caused by dread risk effects are discussed whereby two factors which might influence the existence of such effects are identified, namely the accessibility of (mis-)information and the effects of policy decisions on adaptive behavior. second, the issue of individual preparedness to hazardous events is discussed. as events such as the covid-19 pandemic unfolds complex behavioral patterns which are hard to predict, sophisticated models which account for behavioral, social, and economic dynamics are required to assess the effectivity and efficiency of decision-making.", "categories": "physics.soc-ph cs.si econ.gn math.ds q-fin.ec", "created": "2020-04-19", "updated": "", "authors": ["stephan leitner"], "url": "https://arxiv.org/abs/2004.08917"}, {"title": "consistent calibration of economic scenario generators: the case for   conditional simulation", "id": "2004.09042", "abstract": "economic scenario generators (esgs) simulate economic and financial variables forward in time for risk management and asset allocation purposes. it is often not feasible to calibrate the dynamics of all variables within the esg to historical data alone. calibration to forward-information such as future scenarios and return expectations is needed for stress testing and portfolio optimization, but no generally accepted methodology is available. this paper introduces the conditional scenario simulator, which is a framework for consistently calibrating simulations and projections of economic and financial variables both to historical data and forward-looking information. the framework can be viewed as a multi-period, multi-factor generalization of the black-litterman model, and can embed a wide array of financial and macroeconomic models. two practical examples demonstrate this in a frequentist and bayesian setting.", "categories": "econ.em q-fin.pm q-fin.rm", "created": "2020-04-19", "updated": "", "authors": ["misha van beek"], "url": "https://arxiv.org/abs/2004.09042"}, {"title": "effects of the covid-19 pandemic on population mobility under mild   policies: causal evidence from sweden", "id": "2004.09087", "abstract": "sweden has adopted far less restrictive social distancing policies than most countries following the covid-19 pandemic. this paper uses data on all mobile phone users, from one major swedish mobile phone network, to examine the impact of the coronavirus outbreak under the swedish mild recommendations and restrictions regime on individual mobility and if changes in geographical mobility vary over different socio-economic strata. having access to data for january-march in both 2019 and 2020 enables the estimation of causal effects of the covid-19 outbreak by adopting a difference-in-differences research design. the paper reaches four main conclusions: (i) the daytime population in residential areas increased significantly (64 percent average increase); (ii) the daytime presence in industrial and commercial areas decreased significantly (33 percent average decrease); (iii) the distance individuals move from their homes during a day was substantially reduced (38 percent decrease in the maximum distance moved and 36 percent increase in share of individuals who move less than one kilometer from home); (iv) similar reductions in mobility were found for residents in areas with different socioeconomic and demographic characteristics. these results show that mild government policies can compel people to adopt social distancing behavior.", "categories": "econ.gn q-bio.pe q-fin.ec", "created": "2020-04-20", "updated": "", "authors": ["matz dahlberg", "per-anders edin", "erik gr\u00f6nqvist", "johan lyhagen", "john \u00f6sth", "alexey siretskiy", "marina toger"], "url": "https://arxiv.org/abs/2004.09087"}, {"title": "multi-frequency-band tests for white noise under heteroskedasticity", "id": "2004.09161", "abstract": "this paper proposes a new family of multi-frequency-band (mfb) tests for the white noise hypothesis by using the maximum overlap discrete wavelet packet transform (modwpt). the modwpt allows the variance of a process to be decomposed into the variance of its components on different equal-length frequency sub-bands, and the mfb tests then measure the distance between the modwpt-based variance ratio and its theoretical null value jointly over several frequency sub-bands. the resulting mfb tests have the chi-squared asymptotic null distributions under mild conditions, which allow the data to be heteroskedastic. the mfb tests are shown to have the desirable size and power performance by simulation studies, and their usefulness is further illustrated by two applications.", "categories": "econ.em stat.me", "created": "2020-04-20", "updated": "", "authors": ["mengya liu", "fukan zhu", "ke zhu"], "url": "https://arxiv.org/abs/2004.09161"}, {"title": "a system dynamics model of bitcoin: mining as an efficient market and   the possibility of \"peak hash\"", "id": "2004.09212", "abstract": "the mining of bitcoin is modeled using system dynamics, showing that the past evolution of the network hash rate can be explained to a large extent by an efficient market hypothesis applied to the mining of blocks. the possibility of a decrease in the network hash rate from the next halving event (may 2020) is exposed, implying that the network may be close to 'peak hash', if the price of bitcoin and the revenues from transaction fees will remain at approximately the present level.", "categories": "cs.cr econ.gn q-fin.ec", "created": "2020-04-20", "updated": "", "authors": ["davide lasi", "lukas saul"], "url": "https://arxiv.org/abs/2004.09212"}, {"title": "a comprehensive analysis of soccer penalty shootout designs", "id": "2004.09225", "abstract": "the standard rule of soccer penalty shootouts has received serious criticism due to its bias towards the team kicking the first penalty in each round. the rule-making body of the sport has decided in 2017 to try alternative designs. this paper offers an extensive overview of eight penalty shootout mechanisms, one of them first introduced here. their fairness is analysed under three possible mathematical models of psychological pressure. we also consider the probability of reaching the sudden death stage, as well as the complexity and strategy-proofness of the rules. some designs are found to be inferior because they do not lead to a substantial gain in fairness compared to simpler mechanisms. changing the standard rule by reversing the shooting order in the sudden death stage improves fairness, while it remains less complicated than the regulation of field hockey shootouts. our work has the potential to impact decision-makers who can save resources by choosing only theoretically competitive policy options for field experiments.", "categories": "cs.gt econ.gn q-fin.ec", "created": "2020-04-20", "updated": "2020-08-27", "authors": ["l\u00e1szl\u00f3 csat\u00f3", "d\u00f3ra gr\u00e9ta petr\u00f3czy"], "url": "https://arxiv.org/abs/2004.09225"}, {"title": "a social network analysis of occupational segregation", "id": "2004.09293", "abstract": "we develop a network model of occupational segregation between social groups divided along gender or racial dimensions, generated by the existence of positive assortative matching among individuals from the same group. if referrals are important for job search, then expected homophily in the structure of job contact networks induces different career choices for individuals from different social groups. this further translates into stable occupational segregation equilibria in the labor market. we derive conditions for wage and unemployment inequality in the segregation equilibria and characterize both the first and the second best social welfare optima. we find that utilitarian socially optimal policies always involve segregation, but that integration policies are justifiable by additional distributional concerns. our analysis suggests that social interaction through homophilous job referral networks is an important channel for the propagation and persistence of gender and racial inequalities in the labour market, complementary to classical theories such as taste or statistical discrimination.", "categories": "econ.th cs.si", "created": "2020-04-20", "updated": "", "authors": ["i. s. buhai", "m. j. van der leij"], "url": "https://arxiv.org/abs/2004.09293"}, {"title": "non-linear interlinkages and key objectives amongst the paris agreement   and the sustainable development goals", "id": "2004.09318", "abstract": "the united nations' ambitions to combat climate change and prosper human development are manifested in the paris agreement and the sustainable development goals (sdgs), respectively. these are inherently inter-linked as progress towards some of these objectives may accelerate or hinder progress towards others. we investigate how these two agendas influence each other by defining networks of 18 nodes, consisting of the 17 sdgs and climate change, for various groupings of countries. we compute a non-linear measure of conditional dependence, the partial distance correlation, given any subset of the remaining 16 variables. these correlations are treated as weights on edges, and weighted eigenvector centralities are calculated to determine the most important nodes. we find that sdg 6, clean water and sanitation, and sdg 4, quality education, are most central across nearly all groupings of countries. in developing regions, sdg 17, partnerships for the goals, is strongly connected to the progress of other objectives in the two agendas whilst, somewhat surprisingly, sdg 8, decent work and economic growth, is not as important in terms of eigenvector centrality.", "categories": "econ.em stat.ap", "created": "2020-04-16", "updated": "", "authors": ["felix laumann", "julius von k\u00fcgelgen", "mauricio barahona"], "url": "https://arxiv.org/abs/2004.09318"}, {"title": "awareness of crash risk improves kelly strategies in simulated financial   time series", "id": "2004.09368", "abstract": "we simulate a simplified version of the price process including bubbles and crashes proposed in kreuser and sornette (2018). the price process is defined as a geometric random walk combined with jumps modelled by separate, discrete distributions associated with positive (and negative) bubbles. the key ingredient of the model is to assume that the sizes of the jumps are proportional to the bubble size. thus, the jumps tend to efficiently bring back excess bubble prices close to a normal or fundamental value (efficient crashes). this is different from existing processes studied that assume jumps that are independent of the mispricing. the present model is simplified compared to kreuser and sornette (2018) in that we ignore the possibility of a change of the probability of a crash as the price accelerates above the normal price. we study the behaviour of investment strategies that maximize the expected log of wealth (kelly criterion) for the risky asset and a risk-free asset. we show that the method behaves similarly to kelly on geometric brownian motion in that it outperforms other methods in the long-term and it beats classical kelly. as a primary source of outperformance, we determine knowledge about the presence of crashes, but interestingly find that knowledge of only the size, and not the time of occurrence, already provides a significant and robust edge. we then perform an error analysis to show that the method is robust with respect to variations in the parameters. the method is most sensitive to errors in the expected return.", "categories": "econ.em", "created": "2020-04-20", "updated": "", "authors": ["jan-christian gerlach", "jerome kreuser", "didier sornette"], "url": "https://arxiv.org/abs/2004.09368"}, {"title": "real implications of quantitative easing in the euro area: a   complex-network perspective", "id": "2004.09418", "abstract": "the long-lasting socio-economic impact of the global financial crisis has questioned the adequacy of traditional tools in explaining periods of financial distress, as well as the adequacy of the existing policy response. in particular, the effect of complex interconnections among financial institutions on financial stability has been widely recognized. a recent debate focused on the effects of unconventional policies aimed at achieving both price and financial stability. in particular, quantitative easing (qe, i.e., the large-scale asset purchase programme conducted by a central bank upon the creation of new money) has been recently implemented by the european central bank (ecb). in this context, two questions deserve more attention in the literature. first, to what extent, by injecting liquidity, the qe may alter the bank-firm lending level and stimulate the real economy. second, to what extent the qe may also alter the pattern of intra-financial exposures among financial actors (including banks, investment funds, insurance corporations, and pension funds) and what are the implications in terms of financial stability. here, we address these two questions by developing a methodology to map the macro-network of financial exposures among institutional sectors across financial instruments (e.g., equity, bonds, and loans) and we illustrate our approach on recently available data (i.e., data on loans and private and public securities purchased within the qe). we then test the effect of the implementation of ecb's qe on the time evolution of the financial linkages in the macro-network of the euro area, as well as the effect on macroeconomic variables, such as output and prices.", "categories": "econ.gn q-fin.ec", "created": "2020-04-02", "updated": "", "authors": ["chiara perillo", "stefano battiston"], "url": "https://arxiv.org/abs/2004.09418"}, {"title": "the impact of birth order on behavior in contact team sports: the   evidence of rugby teams in argentina", "id": "2004.09421", "abstract": "several studies have shown that birth order and the sex of siblings may have an influence on individual behavioral traits. in particular, it has been found that second brothers (of older male siblings) tend to have more disciplinary problems. if this is the case, this should also be shown in contact sports. to assess this hypothesis we use a data set from the south rugby union (urs) from bah\\'ia blanca, argentina, and information obtained by surveying more than four hundred players of that league. we find a statistically significant positive relation between being a second-born male rugby player with an older male brother and the number of yellow cards received.   \\textbf{keywords:} birth order; behavior; contact sports; rugby.", "categories": "econ.gn q-fin.ec", "created": "2020-04-16", "updated": "", "authors": ["fernando delbianco", "federico fioravanti", "fernando tohm\u00e9"], "url": "https://arxiv.org/abs/2004.09421"}, {"title": "noise-induced randomization in regression discontinuity designs", "id": "2004.09458", "abstract": "regression discontinuity designs are used to estimate causal effects in settings where treatment is determined by whether an observed running variable crosses a pre-specified threshold. while the resulting sampling design is sometimes described as akin to a locally randomized experiment in a neighborhood of the threshold, standard formal analyses do not make reference to probabilistic treatment assignment and instead identify treatment effects via continuity arguments. here we propose a new approach to identification, estimation, and inference in regression discontinuity designs that exploits measurement error in the running variable. under an assumption that the measurement error is exogenous, we show how to consistently estimate causal effects using a class of linear estimators that weight treated and control units so as to balance a latent variable of which the running variable is a noisy measure. we find this approach to facilitate identification of both familiar estimands from the literature, as well as policy-relevant estimands that correspond to the effects of realistic changes to the existing treatment assignment rule. we demonstrate the method with a study of retention of hiv patients and evaluate its performance using simulated data and a regression discontinuity design artificially constructed from test scores in early childhood.", "categories": "stat.me econ.em", "created": "2020-04-20", "updated": "2020-09-29", "authors": ["dean eckles", "nikolaos ignatiadis", "stefan wager", "han wu"], "url": "https://arxiv.org/abs/2004.09458"}, {"title": "black-box strategies and equilibrium for games with cumulative prospect   theoretic players", "id": "2004.09592", "abstract": "the betweenness property of preference relations states that a probability mixture of two lotteries should lie between them in preference. it is a weakened form of the independence property and hence satisfied in expected utility theory (eut). experimental violations of betweenness are well-documented and several preference theories, notably cumulative prospect theory (cpt), do not satisfy betweenness. we prove that cpt preferences satisfy betweenness if and only if they conform with eut preferences. in game theory, lack of betweenness in the players' preference relations makes it essential to distinguish between the two interpretations of a mixed action by a player - conscious randomizations by the player and the uncertainty in the beliefs of the opponents. we elaborate on this distinction and study its implication for the definition of nash equilibrium. this results in four different notions of equilibrium, with pure and mixed action nash equilibrium being two of them. we dub the other two pure and mixed black-box strategy nash equilibrium respectively. we resolve the issue of existence of such equilibria and examine how these different notions of equilibrium compare with each other.", "categories": "econ.th cs.gt", "created": "2020-04-20", "updated": "", "authors": ["soham r. phade", "venkat anantharam"], "url": "https://arxiv.org/abs/2004.09592"}, {"title": "a geometric characterization of ves and kadiyala-type production   functions", "id": "2004.09617", "abstract": "the basic concepts of the differential geometry are shortly reviewed and applied to the study of ves production function in the spirit of the works of v\\^ilcu and collaborators. a similar characterization is given for a more general production function, namely the kadiyala production function, in the case of developable surfaces.", "categories": "econ.th", "created": "2020-04-20", "updated": "", "authors": ["nicol\u00f2 cangiotti", "mattia sensi"], "url": "https://arxiv.org/abs/2004.09617"}, {"title": "inference by stochastic optimization: a free-lunch bootstrap", "id": "2004.09627", "abstract": "assessing sampling uncertainty in extremum estimation can be challenging when the asymptotic variance is not analytically tractable. bootstrap inference offers a feasible solution but can be computationally costly especially when the model is complex. this paper uses iterates of a specially designed stochastic optimization algorithm as draws from which both point estimates and bootstrap standard errors can be computed in a single run. the draws are generated by the gradient and hessian computed from batches of data that are resampled at each iteration. we show that these draws yield consistent estimates and asymptotically valid frequentist inference for a large class of regular problems. the algorithm provides accurate standard errors in simulation examples and empirical applications at low computational costs. the draws from the algorithm also provide a convenient way to detect data irregularities.", "categories": "econ.em math.st stat.th", "created": "2020-04-20", "updated": "2020-09-14", "authors": ["jean-jacques forneron", "serena ng"], "url": "https://arxiv.org/abs/2004.09627"}, {"title": "revealing cluster structures based on mixed sampling frequencies", "id": "2004.09770", "abstract": "this paper proposes a new nonparametric mixed data sampling (midas) model and develops a framework to infer clusters in a panel dataset of mixed sampling frequencies. the nonparametric midas estimation method is more flexible but substantially less costly to estimate than existing approaches. the proposed clustering algorithm successfully recovers true membership in the cross-section both in theory and in simulations without requiring prior knowledge such as the number of clusters. this methodology is applied to estimate a mixed-frequency okun's law model for the state-level data in the u.s. and uncovers four clusters based on the dynamic features of labor markets.", "categories": "econ.em stat.me", "created": "2020-04-21", "updated": "", "authors": ["yeonwoo rho", "yun liu", "hie joo ahn"], "url": "https://arxiv.org/abs/2004.09770"}, {"title": "how much income inequality is too much?", "id": "2004.09835", "abstract": "we propose a highly schematic economic model in which, in some cases, wage inequalities lead to higher overall social welfare. this is due to the fact that high earners can consume low productivity, non essential products, which allows everybody to remain employed even when the productivity of essential goods is high and producing them does not require everybody to work. we derive a relation between heterogeneities in technologies and the minimum gini coefficient required to maximize global welfare. stronger inequalities appear to be economically unjustified. our model may shed light on the role of non-essential goods in the economy, a topical issue when thinking about the post-covid-19 world.", "categories": "econ.gn q-fin.ec", "created": "2020-04-21", "updated": "", "authors": ["jean-philippe bouchaud"], "url": "https://arxiv.org/abs/2004.09835"}, {"title": "the rise of science in low-carbon energy technologies", "id": "2004.09959", "abstract": "successfully combating climate change will require substantial technological improvements in low-carbon energy technologies (lcets), but designing efficient allocation of r\\&d budgets requires a better understanding of how lcets rely on scientific knowledge. using data covering almost all us patents and scientific articles that are cited by them over the past two centuries, we describe the evolution of knowledge bases of ten key lcets and show how technological interdependencies have changed over time. the composition of low-carbon energy innovations shifted over time, from hydro and wind energy in the 19th and early 20th century, to nuclear fission after world war ii, and more recently to solar pv and back to wind. in recent years, solar pv, nuclear fusion and biofuels (including energy from waste) have 35-65\\% of their citations directed toward scientific papers, while this ratio is less than 10\\% for wind, solar thermal, hydro, geothermal, and nuclear fission. over time, the share of patents citing science and the share of citations that are to scientific papers has been increasing for all technology types. the analysis of the scientific knowledge base of each lcet reveals three fairly separate clusters, with nuclear energy technologies, biofuels and waste, and all the other lcets. our detailed description of knowledge requirements for each lcet helps to design of targeted innovation policies.", "categories": "cs.dl econ.gn q-fin.ec", "created": "2020-04-21", "updated": "2020-09-04", "authors": ["kerstin h\u00f6tte", "anton pichler", "fran\u00e7ois lafond"], "url": "https://arxiv.org/abs/2004.09959"}, {"title": "bayesian optimization of hyperparameters when the marginal likelihood is   estimated by mcmc", "id": "2004.10092", "abstract": "bayesian models often involve a small set of hyperparameters determined by maximizing the marginal likelihood. bayesian optimization is a popular iterative method where a gaussian process posterior of the underlying function is sequentially updated by new function evaluations. an acquisition strategy uses this posterior distribution to decide where to place the next function evaluation. we propose a novel bayesian optimization framework for situations where the user controls the computational effort, and therefore the precision of the function evaluations. this is a common situation in econometrics where the marginal likelihood is often computed by markov chain monte carlo (mcmc) methods, with the precision of the marginal likelihood estimate determined by the number of mcmc draws. the proposed acquisition strategy gives the optimizer the option to explore the function with cheap noisy evaluations and therefore finds the optimum faster. prior hyperparameter estimation in the steady-state bayesian vector autoregressive (bvar) model on us macroeconomic time series data is used for illustration. the proposed method is shown to find the optimum much quicker than traditional bayesian optimization or grid search.", "categories": "stat.co econ.em", "created": "2020-04-21", "updated": "", "authors": ["oskar gustafsson", "mattias villani", "p\u00e4r stockhammar"], "url": "https://arxiv.org/abs/2004.10092"}, {"title": "an information-theoretic approach to the analysis of location and   co-location patterns", "id": "2004.10548", "abstract": "we propose a statistical framework to quantify location and co-location associations of economic activities using information-theoretic measures. we relate the resulting measures to existing measures of revealed comparative advantage, localization and specialization and show that they can all be seen as part of the same framework. using a bayesian approach, we provide measures of uncertainty of the estimated quantities. furthermore, the information-theoretic approach can be readily extended to move beyond pairwise co-locations and instead capture multivariate associations. to illustrate the framework, we apply our measures to the co-location of occupations in us cities, showing the associations between different groups of occupations.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2020-04-22", "updated": "", "authors": ["alje van dam", "andres gomez-lievano", "frank neffke", "koen frenken"], "url": "https://arxiv.org/abs/2004.10548"}, {"title": "venturing the definition of green energy transition: a systematic   literature review", "id": "2004.10562", "abstract": "the issue of climate change has become increasingly noteworthy in the past years, the transition towards a renewable energy system is a priority in the transition to a sustainable society. in this document, we explore the definition of green energy transition, how it is reached, and what are the driven factors to achieve it. to answer that firstly, we have conducted a literature review discovering definitions from different disciplines, secondly, gathering the key factors that are drivers for energy transition, finally, an analysis of the factors is conducted within the context of european union data. preliminary results have shown that household net income and governmental legal actions related to environmental issues are potential candidates to predict energy transition within countries. with this research, we intend to spark new research directions in order to get a common social and scientific understanding of green energy transition.", "categories": "econ.gn q-fin.ec", "created": "2020-04-22", "updated": "2020-04-23", "authors": ["pedro v hernandez serrano", "amrapali zaveri"], "url": "https://arxiv.org/abs/2004.10562"}, {"title": "cost estimation for alternative aviation plans against potential   radiation exposure associated with solar proton events for the airline   industry", "id": "2004.10869", "abstract": "we present a systematic approach to effectively evaluate potential risk cost caused by exposure to solar proton events (spes) from solar flares for the airline industry. we also evaluate associated health risks from radiation, to provide relevant alternative ways to minimize economic loss and opportunity. the estimated radiation dose induced by each spe for the passengers of each flight is calculated using exokyoto and phits. we determine a few scenarios for the estimated dose limit at 1 and 20msv, corresponding to the effective dose limit for the general public and occupational exposure, respectively, as well as a higher dose induced an extreme superflare. we set a hypothetical airline shutdown scenario at 1msv for a single flight per passenger, due to legal restrictions under the potential radiation dose. in such a scenario, we calculate the potential loss in direct and opportunity cost under the cancelation of the flight. at the same time, we considered that, even under such a scenario, if the airplane flies at a slightly lower altitude (from 12 to 9.5km: atmospheric depth from 234 to 365g/cm$^{2}$), the total loss becomes much smaller than flight cancelation, and the estimated total dose goes down from 1.2 to 0.45msv, which is below the effective dose limit for the general public. in case of flying at an even lower altitude (7km: atmospheric depth 484g/cm$^{2}$), the estimated total dose becomes much smaller, 0.12 msv. if we assume the increase of fuel cost is proportional to the increase in atmospheric depth, the increase in cost becomes 1.56 and 2.07 for the case of flying at 9.5 km and at 7 km, respectively. lower altitude flights provide more safety for the potential risk of radiation doses induced by severe spes. at the same time, since there is total loss caused by flight cancelation, we propose that considering lower flight altitude is the best protection against solar flares.", "categories": "econ.gn astro-ph.im astro-ph.sr physics.space-ph q-fin.ec", "created": "2020-04-22", "updated": "", "authors": ["yosuke a. yamashiki", "moe fujita", "tatsuhiko sato", "hiroyuki maehara", "yuta notsu", "kazunari shibata"], "url": "https://arxiv.org/abs/2004.10869"}, {"title": "the category of node-and-choice extensive-form games", "id": "2004.11196", "abstract": "this paper develops the category $\\mathbf{ncg}$. its objects are node-and-choice games, which include essentially all extensive-form games. its morphisms allow arbitrary transformations of a game's nodes, choices, and players, as well as monotonic transformations of the utility functions of the game's players. among the morphisms are subgame inclusions. several characterizations and numerous properties of the isomorphisms are derived. for example, it is shown that isomorphisms preserve the game-theoretic concepts of no-absentmindedness, perfect-information, and (pure-strategy) nash-equilibrium. finally, full subcategories are defined for choice-sequence games and choice-set games, and relationships among these two subcategories and $\\mathbf{ncg}$ itself are expressed and derived via isomorphic inclusions and equivalences.", "categories": "econ.th cs.lo math.ct", "created": "2020-04-23", "updated": "2020-07-28", "authors": ["peter a. streufert"], "url": "https://arxiv.org/abs/2004.11196"}, {"title": "the divergence between industrial infrastructure and research output   among the gcc member states", "id": "2004.11235", "abstract": "in this article, we provide a comparative analysis of the industry, communication, and research infrastructure among the gcc member states as measured by the united nations sustainable development goal 9. sdg 9 provides a clear framework for measuring the performance of nations in achieving sustainable industrialization. three pillars of this goal are defined as quality logistics and efficient transportation, availability of mobile-cellular network with high-speed internet access, and quality research output. based on the data from both the united nations' sdg database and the bertelsmann stiftung sdg-index, our results suggest that while most of the sub-goals in sdg 9 are achieved, significant challenges remain ahead. notably, the research output of the gcc member states is not in par with that of the developed world. we suggest the gcc decisionmakers initiate national and supranational research schemes in order to boost research and development in the region.", "categories": "econ.gn q-fin.ec", "created": "2020-04-22", "updated": "", "authors": ["osman gulseven", "abdulrahman elmi", "odai bataineh"], "url": "https://arxiv.org/abs/2004.11235"}, {"title": "does subjective well-being contribute to our understanding of mexican   well-being?", "id": "2004.11420", "abstract": "the article reviews the history of well-being to gauge how subjective question surveys can improve our understanding of well-being in mexico. the research uses data at the level of the 32 federal entities or states, taking advantage of the heterogeneity in development indicator readings between and within geographical areas, the product of socioeconomic inequality. the data come principally from two innovative subjective questionnaires, biare and envipe, which intersect in their fully representative state-wide applications in 2014, but also from conventional objective indicator sources such as the hdi and conventional surveys. this study uses two approaches, a descriptive analysis of a state-by-state landscape of indicators, both subjective and objective, in an initial search for stand-out well-being patterns, and an econometric study of a large selection of mainly subjective indicators inspired by theory and the findings of previous mexican research. descriptive analysis confirms that subjective well-being correlates strongly with and complements objective data, providing interesting directions for analysis. the econometrics literature indicates that happiness increases with income and satisfying of material needs as theory suggests, but also that mexicans are relatively happy considering their mediocre incomes and high levels of insecurity, the last of which, by categorizing according to satisfaction with life, can be shown to impact poorer people disproportionately. the article suggests that well-being is a complex, multidimensional construct which can be revealed by using exploratory multi-regression and partial correlations models which juxtapose subjective and objective indicators.", "categories": "econ.em", "created": "2020-04-23", "updated": "", "authors": ["jeremy heald", "erick trevi\u00f1o aguilar"], "url": "https://arxiv.org/abs/2004.11420"}, {"title": "high-dimensional macroeconomic forecasting using message passing   algorithms", "id": "2004.11485", "abstract": "this paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. first, it treats a regression model with time-varying coefficients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. inference in this specification proceeds using bayesian hierarchical priors that shrink the high-dimensional vector of coefficients either towards zero or time-invariance. second, it introduces the frameworks of factor graphs and message passing as a means of designing efficient bayesian estimation algorithms. in particular, a generalized approximate message passing (gamp) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. the result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. in a forecasting exercise for u.s. price inflation this methodology is shown to work very well.", "categories": "stat.me econ.em q-fin.st stat.ml", "created": "2020-04-23", "updated": "", "authors": ["dimitris korobilis"], "url": "https://arxiv.org/abs/2004.11485"}, {"title": "machine learning econometrics: bayesian algorithms and methods", "id": "2004.11486", "abstract": "as the amount of economic and other data generated worldwide increases vastly, a challenge for future generations of econometricians will be to master efficient algorithms for inference in empirical models with large information sets. this chapter provides a review of popular estimation algorithms for bayesian inference in econometrics and surveys alternative algorithms developed in machine learning and computing science that allow for efficient computation in high-dimensional settings. the focus is on scalability and parallelizability of each algorithm, as well as their ability to be adopted in various empirical settings in economics and finance.", "categories": "stat.co econ.em", "created": "2020-04-23", "updated": "", "authors": ["dimitris korobilis", "davide pettenuzzo"], "url": "https://arxiv.org/abs/2004.11486"}, {"title": "statistical discrimination in ratings-guided markets", "id": "2004.11531", "abstract": "we study statistical discrimination of individuals based on payoff-irrelevant social identities in markets where ratings/recommendations facilitate social learning among users. despite the potential promise and guarantee for the ratings/recommendation algorithms to be fair and free of human bias and prejudice, we identify the possible vulnerability of the ratings-based social learning to discriminatory inferences on social groups. in our model, users' equilibrium attention decisions may lead data to be sampled differentially across different groups so that differential inferences on individuals may emerge based on their group identities. we explore policy implications in terms of regulating trading relationships as well as algorithm design.", "categories": "cs.gt econ.th", "created": "2020-04-24", "updated": "", "authors": ["yeon-koo che", "kyungmin kim", "weijie zhong"], "url": "https://arxiv.org/abs/2004.11531"}, {"title": "a comparison of methods for treatment assignment with an application to   playlist generation", "id": "2004.11532", "abstract": "this study presents a systematic comparison of methods for individual treatment assignment, a general problem that arises in many applications and has received significant attention from economists, computer scientists, and social scientists. we characterize the various methods proposed in the literature into three general approaches: learning models to predict outcomes, learning models to predict causal effects, and learning models to predict optimal treatment assignments. we show analytically that optimizing for outcome or causal-effect prediction is not the same as optimizing for treatment assignments, and thus we should prefer learning models that optimize for treatment assignments. we then compare and contrast the three approaches empirically in the context of choosing, for each user, the best algorithm for playlist generation in order to optimize engagement. this is the first comparison of the different treatment assignment approaches on a real-world application at scale (based on more than half a billion individual treatment assignments). our results show (i) that applying different algorithms to different users can improve streams substantially compared to deploying the same algorithm for everyone, (ii) that personalized assignments improve substantially with larger data sets, and (iii) that learning models by optimizing treatment assignments rather than outcome or causal-effect predictions can improve treatment assignment performance by more than 28%.", "categories": "econ.em cs.lg stat.me stat.ml", "created": "2020-04-24", "updated": "2020-07-31", "authors": ["carlos fern\u00e1ndez-lor\u00eda", "foster provost", "jesse anderton", "benjamin carterette", "praveen chandar"], "url": "https://arxiv.org/abs/2004.11532"}, {"title": "inside the mind of investors during the covid-19 pandemic: evidence from   the stocktwits data", "id": "2004.11686", "abstract": "we study the investor beliefs, sentiment and disagreement, about stock market returns during the covid-19 pandemic using a large number of messages of investors on a social media investing platform, \\textit{stocktwits}. the rich and multimodal features of stocktwits data allow us to explore the evolution of sentiment and disagreement within and across investors, sectors, and even industries. we find that the sentiment (disagreement) has a sharp decrease (increase) across all investors with any investment philosophy, horizon, and experience between february 19, 2020, and march 23, 2020, where a historical market high followed by a record drop. surprisingly, these measures have a sharp reverse toward the end of march. however, the performance of these measures across various sectors is heterogeneous. financial and healthcare sectors are the most pessimistic and optimistic divisions, respectively.", "categories": "q-fin.st econ.gn q-fin.ec q-fin.tr", "created": "2020-04-24", "updated": "2020-05-08", "authors": ["hasan fallahgoul"], "url": "https://arxiv.org/abs/2004.11686"}, {"title": "microeconometrics with partial identification", "id": "2004.11751", "abstract": "this chapter reviews the microeconometrics literature on partial identification, focusing on the developments of the last thirty years. the topics presented illustrate that the available data combined with credible maintained assumptions may yield much information about a parameter of interest, even if they do not reveal it exactly. special attention is devoted to discussing the challenges associated with, and some of the solutions put forward to, (1) obtain a tractable characterization of the values for the parameters of interest which are observationally equivalent, given the available data and maintained assumptions; (2) estimate this set of values; (3) conduct test of hypotheses and make confidence statements. the chapter reviews advances in partial identification analysis both as applied to learning (functionals of) probability distributions that are well-defined in the absence of models, as well as to learning parameters that are well-defined only in the context of particular models. a simple organizing principle is highlighted: the source of the identification problem can often be traced to a collection of random variables that are consistent with the available data and maintained assumptions. this collection may be part of the observed data or be a model implication. in either case, it can be formalized as a random set. random set theory is then used as a mathematical framework to unify a number of special results and produce a general methodology to carry out partial identification analysis.", "categories": "econ.em", "created": "2020-04-24", "updated": "", "authors": ["francesca molinari"], "url": "https://arxiv.org/abs/2004.11751"}, {"title": "environmental economics and uncertainty: review and a machine learning   outlook", "id": "2004.11780", "abstract": "economic assessment in environmental science concerns the measurement or valuation of environmental impacts, adaptation, and vulnerability. integrated assessment modeling is a unifying framework of environmental economics, which attempts to combine key elements of physical, ecological, and socioeconomic systems. uncertainty characterization in integrated assessment varies by component models: uncertainties associated with mechanistic physical models are often assessed with an ensemble of simulations or monte carlo sampling, while uncertainties associated with impact models are evaluated by conjecture or econometric analysis. manifold sampling is a machine learning technique that constructs a joint probability model of all relevant variables which may be concentrated on a low-dimensional geometric structure. compared with traditional density estimation methods, manifold sampling is more efficient especially when the data is generated by a few latent variables. the manifold-constrained joint probability model helps answer policy-making questions from prediction, to response, and prevention. manifold sampling is applied to assess risk of offshore drilling in the gulf of mexico.", "categories": "econ.gn physics.ao-ph q-fin.ec stat.ap", "created": "2020-04-24", "updated": "", "authors": ["ruda zhang", "patrick wingo", "rodrigo duran", "kelly rose", "jennifer bauer", "roger ghanem"], "url": "https://arxiv.org/abs/2004.11780"}, {"title": "the what, when and where of limit order books", "id": "2004.11953", "abstract": "we model the limit order book (lob) as a continuous markov process and develop an algebra to describe its dynamics based on the fundamental events of the book: order arrivals and cancellations. we show how all observables (prices, returns, and liquidity measures) are governed by the same variables which also drive arrival and cancellation rates. the sensitivity of our model is evaluated in a simulation study and an empirical analysis. we estimate several linearized model specifications based on the theoretical description of the lob and conduct in- and out-of-sample forecasts on several frequencies. the in-sample results based on contemporaneous information suggest that our model describes up to 90% of the variation of close-to-close returns, the adjusted $r^2$ still ranges at around 80%. in the more realistic setting where only past information enters the model, we still observe an adjusted $r^2$ in the range of 15%. the direction of the next return can be predicted, out-of-sample, with an accuracy of over 75% for short time horizons below 10 minutes. out-of-sample, on average, we obtain $r^2$ values for the mincer-zarnowitz regression of around 2-3% and an $rmspe$ that is 10 times lower than values documented in the literature.", "categories": "q-fin.tr econ.em q-fin.mf q-fin.st stat.ap", "created": "2020-04-24", "updated": "", "authors": ["johannes bleher", "michael bleher", "thomas dimpfl"], "url": "https://arxiv.org/abs/2004.11953"}, {"title": "bayesian clustered coefficients regression with auxiliary covariates   assistant random effects", "id": "2004.12022", "abstract": "in regional economics research, a problem of interest is to detect similarities between regions, and estimate their shared coefficients in economics models. in this article, we propose a mixture of finite mixtures (mfm) clustered regression model with auxiliary covariates that account for similarities in demographic or economic characteristics over a spatial domain. our bayesian construction provides both inference for number of clusters and clustering configurations, and estimation for parameters for each cluster. empirical performance of the proposed model is illustrated through simulation experiments, and further applied to a study of influential factors for monthly housing cost in georgia.", "categories": "stat.me econ.em stat.ap", "created": "2020-04-24", "updated": "", "authors": ["guanyu hu", "yishu xue", "zhihua ma"], "url": "https://arxiv.org/abs/2004.12022"}, {"title": "sensitivity to calibrated parameters", "id": "2004.12100", "abstract": "across many fields in economics, a common approach to estimation of economic models is to calibrate a sub-set of model parameters and keep them fixed when estimating the remaining parameters. calibrated parameters likely affect conclusions based on the model but estimation time often makes a systematic investigation of the sensitivity to calibrated parameters infeasible. i propose a simple and computationally low-cost measure of the sensitivity of parameters and other objects of interest to the calibrated parameters. in the main empirical application, i revisit the analysis of life-cycle savings motives in gourinchas and parker (2002) and show that some estimates are sensitive to calibrations.", "categories": "econ.em", "created": "2020-04-25", "updated": "", "authors": ["thomas h. j\u00f8rgensen"], "url": "https://arxiv.org/abs/2004.12100"}, {"title": "limiting bias from test-control interference in online marketplace   experiments", "id": "2004.12162", "abstract": "in an a/b test, the typical objective is to measure the total average treatment effect (tate), which measures the difference between the average outcome if all users were treated and the average outcome if all users were untreated. however, a simple difference-in-means estimator will give a biased estimate of the tate when outcomes of control units depend on the outcomes of treatment units, an issue we refer to as test-control interference. using a simulation built on top of data from airbnb, this paper considers the use of methods from the network interference literature for online marketplace experimentation. we model the marketplace as a network in which an edge exists between two sellers if their goods substitute for one another. we then simulate seller outcomes, specifically considering a \"status quo\" context and \"treatment\" context that forces all sellers to lower their prices. we use the same simulation framework to approximate tate distributions produced by using blocked graph cluster randomization, exposure modeling, and the hajek estimator for the difference in means. we find that while blocked graph cluster randomization reduces the bias of the naive difference-in-means estimator by as much as 62%, it also significantly increases the variance of the estimator. on the other hand, the use of more sophisticated estimators produces mixed results. while some provide (small) additional reductions in bias and small reductions in variance, others lead to increased bias and variance. overall, our results suggest that experiment design and analysis techniques from the network experimentation literature are promising tools for reducing bias due to test-control interference in marketplace experiments.", "categories": "stat.ap econ.em stat.me", "created": "2020-04-25", "updated": "", "authors": ["david holtz", "sinan aral"], "url": "https://arxiv.org/abs/2004.12162"}, {"title": "dynamically consistent objective and subjective rationality", "id": "2004.12347", "abstract": "a group of experts, for instance climate scientists, is to choose among two policies $f$ and $g$. consider the following decision rule. if all experts agree that the expected utility of $f$ is higher than the expected utility of $g$, the unanimity rule applies, and $f$ is chosen. otherwise the precautionary principle is implemented and the policy yielding the highest minimal expected utility is chosen.   this decision rule may lead to time inconsistencies when an intermediate period of partial resolution of uncertainty is added. we provide axioms that enlarge the initial group of experts with veto power, which leads to a set of probabilistic beliefs that is \"rectangular\" in a minimal sense. this makes this decision rule dynamically consistent and provides, as a byproduct, a novel behavioral characterization of rectangularity.", "categories": "econ.th", "created": "2020-04-26", "updated": "", "authors": ["lorenzo bastianello", "jos\u00e9 heleno faro", "ana santos"], "url": "https://arxiv.org/abs/2004.12347"}, {"title": "maximum likelihood estimation of stochastic frontier models with   endogeneity", "id": "2004.12369", "abstract": "we study a closed-form maximum likelihood estimator of stochastic frontier models with endogeneity in cross-section data when both error components may be correlated with inputs and environmental variables. we achieve identification using a control function assumption. we show that the conditional distribution of the stochastic inefficiency term given the control functions is a folded normal distribution, which reduces to the half-normal distribution when both inputs and environmental variables are independent of the stochastic inefficiency term. hence, our framework is a natural generalization of the normal half-normal stochastic frontier model with endogeneity. we further provide a battese-coelli estimator of technical efficiency in this context. our estimator is computationally fast and easy to implement. we showcase its finite sample properties in monte-carlo simulations and an empirical application to farmers in nepal.", "categories": "econ.em stat.ap", "created": "2020-04-26", "updated": "2020-04-29", "authors": ["samuele centorrino", "mar\u00eda p\u00e9rez-urdiales"], "url": "https://arxiv.org/abs/2004.12369"}, {"title": "inference with many weak instruments", "id": "2004.12445", "abstract": "we develop a concept of weak identification in linear iv models in which the number of instruments can grow at the same rate or slower than the sample size. we propose a jackknifed version of the classical weak identification-robust anderson-rubin (ar) test statistic. large-sample inference based on the jackknifed ar is valid under heteroscedasticity and weak identification. the feasible version of this statistic uses a novel variance estimator. the test has uniformly correct size and good power properties. we also develop a pre-test for weak identification that is related to the size property of a wald test based on the jackknife instrumental variable estimator (jive). this new pre-test is valid under heteroscedasticity and with many instruments.", "categories": "econ.em", "created": "2020-04-26", "updated": "", "authors": ["anna mikusheva", "liyang sun"], "url": "https://arxiv.org/abs/2004.12445"}, {"title": "reducing interference bias in online marketplace pricing experiments", "id": "2004.12489", "abstract": "online marketplace designers frequently run a/b tests to measure the impact of proposed product changes. however, given that marketplaces are inherently connected, total average treatment effect estimates obtained through bernoulli randomized experiments are often biased due to violations of the stable unit treatment value assumption. this can be particularly problematic for experiments that impact sellers' strategic choices, affect buyers' preferences over items in their consideration set, or change buyers' consideration sets altogether. in this work, we measure and reduce bias due to interference in online marketplace experiments by using observational data to create clusters of similar listings, and then using those clusters to conduct cluster-randomized field experiments. we provide a lower bound on the magnitude of bias due to interference by conducting a meta-experiment that randomizes over two experiment designs: one bernoulli randomized, one cluster randomized. in both meta-experiment arms, treatment sellers are subject to a different platform fee policy than control sellers, resulting in different prices for buyers. by conducting a joint analysis of the two meta-experiment arms, we find a large and statistically significant difference between the total average treatment effect estimates obtained with the two designs, and estimate that 32.60% of the bernoulli-randomized treatment effect estimate is due to interference bias. we also find weak evidence that the magnitude and/or direction of interference bias depends on extent to which a marketplace is supply- or demand-constrained, and analyze a second meta-experiment to highlight the difficulty of detecting interference bias when treatment interventions require intention-to-treat analysis.", "categories": "stat.me econ.em stat.ap", "created": "2020-04-26", "updated": "", "authors": ["david holtz", "ruben lobel", "inessa liskovich", "sinan aral"], "url": "https://arxiv.org/abs/2004.12489"}, {"title": "structural regularization", "id": "2004.12601", "abstract": "we propose a novel method for modeling data by using structural models based on economic theory as regularizers for statistical models. we show that even if a structural model is misspecified, as long as it is informative about the data-generating mechanism, our method can outperform both the (misspecified) structural model and un-structural-regularized statistical models. our method permits a bayesian interpretation of theory as prior knowledge and can be used both for statistical prediction and causal inference. it contributes to transfer learning by showing how incorporating theory into statistical modeling can significantly improve out-of-domain predictions and offers a way to synthesize reduced-form and structural approaches for causal effect estimation. simulation experiments demonstrate the potential of our method in various settings, including first-price auctions, dynamic models of entry and exit, and demand estimation with instrumental variables. our method has potential applications not only in economics, but in other scientific disciplines whose theoretical models offer important insight but are subject to significant misspecification concerns.", "categories": "econ.em cs.lg", "created": "2020-04-27", "updated": "2020-06-12", "authors": ["jiaming mao", "zhesheng zheng"], "url": "https://arxiv.org/abs/2004.12601"}, {"title": "state dependence and unobserved heterogeneity in the extensive margin of   trade", "id": "2004.12655", "abstract": "we study the role and drivers of persistence in the extensive margin of bilateral trade. motivated by a stylized heterogeneous firms model of international trade with market entry costs, we propose new bias-corrected dynamic binary choice estimators with three sets of high-dimensional fixed effects. monte carlo simulations confirm their desirable statistical properties. a reassessment of the most commonly studied determinants of the extensive margin of trade demonstrates that both true state dependence and unobserved heterogeneity contribute strongly to trade persistence and that taking this persistence into account matters significantly in identifying the effects of trade policies on the extensive margin.", "categories": "econ.em", "created": "2020-04-27", "updated": "", "authors": ["julian hinz", "amrei stammann", "joschka wanner"], "url": "https://arxiv.org/abs/2004.12655"}, {"title": "measuring wage inequality under right censoring", "id": "2004.12856", "abstract": "in this paper we investigate potential changes which may have occurred over the last two decades in the probability mass of the right tail of the wage distribution, through the analysis of the corresponding tail index. in specific, a conditional tail index estimator is introduced which explicitly allows for right tail censoring (top-coding), which is a feature of the widely used current population survey (cps), as well as of other surveys. ignoring the top-coding may lead to inconsistent estimates of the tail index and to under or over statements of inequality and of its evolution over time. thus, having a tail index estimator that explicitly accounts for this sample characteristic is of importance to better understand and compute the tail index dynamics in the censored right tail of the wage distribution. the contribution of this paper is threefold: i) we introduce a conditional tail index estimator that explicitly handles the top-coding problem, and evaluate its finite sample performance and compare it with competing methods; ii) we highlight that the factor values used to adjust the top-coded wage have changed over time and depend on the characteristics of individuals, occupations and industries, and propose suitable values; and iii) we provide an in-depth empirical analysis of the dynamics of the us wage distribution's right tail using the public-use cps database from 1992 to 2017.", "categories": "econ.em", "created": "2020-04-27", "updated": "", "authors": ["jo\u00e3o nicolau", "pedro raposo", "paulo m. m. rodrigues"], "url": "https://arxiv.org/abs/2004.12856"}, {"title": "integrated design of unmanned aerial mobility network: a data-driven   risk-averse approach", "id": "2004.13000", "abstract": "the real challenge in drone-logistics is to develop an economically-feasible unmanned aerial mobility network (uamn). in this paper, we propose an integrated airport location (strategic decision) and routes planning (operational decision) optimization framework to minimize the total cost of the network, while guaranteeing flow constraints, capacity constraints, and electricity constraints. to facility expensive long-term infrastructure planning facing demand uncertainty, we develop a data-driven risk-averse two-stage stochastic optimization model based on the wasserstein distance. we develop a reformulation technique which simplifies the worst-case expectation term in the original model, and obtain a fractable min-max solution procedure correspondingly. using lagrange multipliers, we successfully decompose decision variables and reduce the complexity of computation. to provide managerial insights, we design specific numerical examples. for example, we find that the optimal network configuration is affected by the \"pooling effects\" in channel capacities. a nice feature of our dro framework is that the optimal network design is relatively robust under demand uncertainty. interestingly, a candidate node without historical demand records can be chosen to locate an airport. we demonstrate the application of our model for a real medical resources transportation problem with our industry partner, collecting donated blood to a blood bank in hangzhou, china.", "categories": "math.oc econ.gn q-fin.ec", "created": "2020-04-27", "updated": "", "authors": ["wenjuan hou", "tao fang", "zhi pei", "qiao-chu he"], "url": "https://arxiv.org/abs/2004.13000"}, {"title": "matching with generalized lexicographic choice rules", "id": "2004.13261", "abstract": "motivated by the need for real-world matching problems, this paper formulates a large class of practical choice rules, generalized lexicographic choice rules (glcr), for institutions that consist of multiple divisions. institutions fill their divisions sequentially, and each division is endowed with a sub-choice rule that satisfies classical substitutability and size monotonicity in conjunction with a new property that we introduce, quota monotonicity. we allow rich interactions between divisions in the form of capacity transfers. the overall choice rule of an institution is defined as the union of the sub-choices of its divisions. the cumulative offer mechanism (com) with respect to glcr is the unique stable and strategy-proof mechanism. we define a choice-based improvement notion and show that the com respects improvements. we employ the theory developed in this paper in our companion paper, ayg\\\"un and turhan (2020), to design satisfactory matching mechanisms for india with comprehensive affirmative action constraints.", "categories": "econ.th", "created": "2020-04-27", "updated": "2020-07-28", "authors": ["orhan ayg\u00fcn", "bertan turhan"], "url": "https://arxiv.org/abs/2004.13261"}, {"title": "designing direct matching mechanism for india with comprehensive   affirmative action", "id": "2004.13264", "abstract": "since 1950, india has been implementing the most comprehensive affirmative action program in the world. vertical reservations are provided to members of historically discriminated scheduled castes (sc), scheduled tribes (st), and other backward classes (obc). horizontal reservations are provided for other disadvantaged groups, such as women and disabled people, within each vertical category. there is no well-defined procedure to implement horizontal reservations jointly with vertical reservation and obc de-reservations. sequential processes currently in use for obc de-reservations and meritorious reserve candidates lead to severe shortcomings. most importantly, indirect mechanisms currently used in practice do not allow reserve category applicants to fully express their preferences. to overcome these and other related issues, we design several different choice rules for institutions that take meritocracy, vertical and horizontal reservations, and obc de-reservations into account. we propose a centralized mechanism to satisfactorily clear matching markets in india.", "categories": "econ.th", "created": "2020-04-27", "updated": "2020-07-04", "authors": ["orhan ayg\u00fcn", "bertan turhan"], "url": "https://arxiv.org/abs/2004.13264"}, {"title": "slot-specific priorities with capacity transfers", "id": "2004.13265", "abstract": "in many real-world matching applications, there are restrictions for institutions either on priorities of their slots or on the transferability of unfilled slots over others (or both). motivated by the need in such real-life matching problems, this paper formulates a family of practical choice rules, slot-specific priorities with capacity transfers (sspwct). these practical rules invoke both slot-specific priorities structure and transferability of vacant slots. we show that the cumulative offer mechanism (com) is stable, strategy-proof and respects improvements with regards to sspwct choice rules. transferring the capacity of one more unfilled slot, while all else is constant, leads to strategy-proof pareto improvement of the com. following kominer's (2020) formulation, we also provide comparative static results for expansion of branch capacity and addition of new contracts in the sspwct framework. our results have implications for resource allocation problems with diversity considerations.", "categories": "econ.th", "created": "2020-04-27", "updated": "2020-09-21", "authors": ["michelle avataneo", "bertan turhan"], "url": "https://arxiv.org/abs/2004.13265"}, {"title": "the ai economist: improving equality and productivity with ai-driven tax   policies", "id": "2004.13332", "abstract": "tackling real-world socio-economic challenges requires designing and testing economic policies. however, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. in this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. we propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. we make four main contributions. first, we present an economic simulation environment that features competitive pressures and market dynamics. we validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. second, we show that ai-driven tax policies improve the trade-off between equality and productivity by 16% over baseline policies, including the prominent saez tax framework. third, we showcase several emergent features: ai-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. moreover, ai-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by ai agents. lastly, ai-driven tax policies are also effective when used in experiments with human participants. in experiments conducted on mturk, an ai tax policy provides an equality-productivity trade-off that is similar to that provided by the saez framework along with higher inverse-income weighted social welfare.", "categories": "econ.gn cs.lg q-fin.ec stat.ml", "created": "2020-04-28", "updated": "", "authors": ["stephan zheng", "alexander trott", "sunil srinivasa", "nikhil naik", "melvin gruesbeck", "david c. parkes", "richard socher"], "url": "https://arxiv.org/abs/2004.13332"}, {"title": "multinomial logit processes and preference discovery: inside and outside   the black box", "id": "2004.13376", "abstract": "we provide two characterizations, one axiomatic and the other neuro-computational, of the dependence of choice probabilities on deadlines, within the widely used softmax representation \\[ p_{t}\\left( a,a\\right) =\\dfrac{e^{\\frac{u\\left( a\\right) }{\\lambda \\left( t\\right) }+\\alpha \\left( a\\right) }}{\\sum_{b\\in a}e^{\\frac{u\\left( b\\right) }{\\lambda \\left( t\\right) }+\\alpha \\left( b\\right) }}% \\] where $p_{t}\\left( a,a\\right) $ is the probability that alternative $a$ is selected from the set $a$ of feasible alternatives if $t$ is the time available to decide, $\\lambda$ is a time dependent noise parameter measuring the unit cost of information, $u$ is a time independent utility function, and $\\alpha$ is an alternative-specific bias that determines the initial choice probabilities and possibly reflects prior information.   our axiomatic analysis provides a behavioral foundation of softmax (also known as multinomial logit model when $\\alpha$ is constant). our neuro-computational derivation provides a biologically inspired algorithm that may explain the emergence of softmax in choice behavior. jointly, the two approaches provide a thorough understanding of soft-maximization in terms of internal causes (neurophysiological mechanisms) and external effects (testable implications).", "categories": "econ.th q-bio.nc", "created": "2020-04-28", "updated": "2020-09-09", "authors": ["simone cerreia-vioglio", "fabio maccheroni", "massimo marinacci", "aldo rustichini"], "url": "https://arxiv.org/abs/2004.13376"}, {"title": "causal inference on networks under continuous treatment interference", "id": "2004.13459", "abstract": "this paper presents a methodology to draw causal inference in a non-experimental setting subject to network interference. specifically, we develop a generalized propensity score-based estimator that allows us to estimate both direct and spillover effects of a continuous treatment, which spreads through weighted and directed edges of a network. to showcase this methodology, we investigate whether and how spillover effects shape the optimal level of policy interventions in agricultural markets. our results show that, in this context, neglecting interference may lead to a downward bias when assessing policy effectiveness.", "categories": "stat.me econ.em stat.ap", "created": "2020-04-28", "updated": "", "authors": ["davide del prete", "laura forastiere", "valerio leone sciabolazza"], "url": "https://arxiv.org/abs/2004.13459"}, {"title": "how do online consumers review negatively?", "id": "2004.13463", "abstract": "negative reviews on e-commerce platforms, mainly in the form of texts, are posted by online consumers to express complaints about unsatisfactory experiences, providing a proxy of big data for sellers to consider improvements. however, the exact knowledge that lies beyond the negative reviewing still remains unknown. aimed at a systemic understanding of how online consumers post negative reviews, using 1, 450, 000 negative reviews from jd.com, the largest b2c platform in china, the behavioral patterns from temporal, perceptional and emotional perspectives are comprehensively explored in the present study. massive consumers behind these reviews across four sectors in the most recent 10 years are further split into five levels to reveal group discriminations at a fine resolution. circadian rhythms of negative reviewing after making purchases were found, and the periodic intervals suggest stable habits in online consumption and that consumers tend to negatively review at the same hour of the purchase. consumers from lower levels express more intensive negative feelings, especially on product pricing and seller attitudes, while those from upper levels demonstrate a stronger momentum of negative emotion. the value of negative reviews from higher-level consumers is thus unexpectedly highlighted because of less emotionalization and less biased narration, while the longer-lasting characteristic of these consumers' negative responses also stresses the need for more attention from sellers. our results shed light on implementing distinguished proactive strategies in different buyer groups to help mitigate the negative impact due to negative reviews.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-04-28", "updated": "", "authors": ["menghan sun", "jichang zhao"], "url": "https://arxiv.org/abs/2004.13463"}, {"title": "covid-19 causes record decline in global co2 emissions", "id": "2004.13614", "abstract": "the considerable cessation of human activities during the covid-19 pandemic has affected global energy use and co2 emissions. here we show the unprecedented decrease in global fossil co2 emissions from january to april 2020 was of 7.8% (938 mt co2 with a +6.8% of 2-{\\sigma} uncertainty) when compared with the period last year. in addition other emerging estimates of covid impacts based on monthly energy supply or estimated parameters, this study contributes to another step that constructed the near-real-time daily co2 emission inventories based on activity from power generation (for 29 countries), industry (for 73 countries), road transportation (for 406 cities), aviation and maritime transportation and commercial and residential sectors emissions (for 206 countries). the estimates distinguished the decline of co2 due to covid-19 from the daily, weekly and seasonal variations as well as the holiday events. the covid-related decreases in co2 emissions in road transportation (340.4 mt co2, -15.5%), power (292.5 mt co2, -6.4% compared to 2019), industry (136.2 mt co2, -4.4%), aviation (92.8 mt co2, -28.9%), residential (43.4 mt co2, -2.7%), and international shipping (35.9mt co2, -15%). regionally, decreases in china were the largest and earliest (234.5 mt co2,-6.9%), followed by europe (eu-27 & uk) (138.3 mt co2, -12.0%) and the u.s. (162.4 mt co2, -9.5%). the declines of co2 are consistent with regional nitrogen oxides concentrations observed by satellites and ground-based networks, but the calculated signal of emissions decreases (about 1gt co2) will have little impacts (less than 0.13ppm by april 30, 2020) on the overserved global co2 concertation. however, with observed fast co2 recovery in china and partial re-opening globally, our findings suggest the longer-term effects on co2 emissions are unknown and should be carefully monitored using multiple measures.", "categories": "econ.gn physics.geo-ph physics.soc-ph q-fin.ec", "created": "2020-04-28", "updated": "2020-06-14", "authors": ["zhu liu", "philippe ciais", "zhu deng", "ruixue lei", "steven j. davis", "sha feng", "bo zheng", "duo cui", "xinyu dou", "pan he", "biqing zhu", "chenxi lu", "piyu ke", "taochun sun", "yuan wang", "xu yue", "yilong wang", "yadong lei", "hao zhou", "zhaonan cai", "yuhui wu", "runtao guo", "tingxuan han", "jinjun xue", "olivier boucher", "eulalie boucher", "frederic chevallier", "yimin wei", "haiwang zhong", "chongqing kang", "ning zhang", "bin chen", "fengming xi", "fran\u00e7ois marie", "qiang zhang", "dabo guan", "peng gong", "daniel m. kammen", "kebin he", "hans joachim schellnhuber"], "url": "https://arxiv.org/abs/2004.13614"}, {"title": "wealth distribution under the spread of infectious diseases", "id": "2004.13620", "abstract": "we develop a mathematical framework to study the economic impact of infectious diseases by integrating epidemiological dynamics with a kinetic model of wealth exchange. the multi-agent description leads to study the evolution over time of a system of kinetic equations for the wealth densities of susceptible, infectious and recovered individuals, whose proportions are driven by a classical compartmental model in epidemiology. explicit calculations show that the spread of the disease seriously affects the distribution of wealth, which, unlike the situation in the absence of epidemics, can converge towards a stationary state with a bimodal form. furthermore, simulations confirm the ability of the model to describe different phenomena characteristics of economic trends in situations compromised by the rapid spread of an epidemic, such as the unequal impact on the various wealth classes and the risk of a shrinking middle class.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-04-28", "updated": "", "authors": ["g. dimarco", "l. pareschi", "g. toscani", "m. zanella"], "url": "https://arxiv.org/abs/2004.13620"}, {"title": "engineering economics in the conflux network", "id": "2004.13696", "abstract": "proof-of-work blockchains need to be carefully designed so as to create the proper incentives for miners to faithfully maintain the network in a sustainable way. this paper describes how the economic engineering of the conflux network, a high throughput proof-of-work blockchain, leads to sound economic incentives that support desirable and sustainable mining behavior. in detail, this paper parameterizes the level of income, and thus network security, that conflux can generate, and it describes how this depends on user behavior and \"policy variables'' such as block and interest inflation. it also discusses how the underlying economic engineering design makes the conflux network resilient against double spending and selfish mining attacks.", "categories": "econ.gn q-fin.ec", "created": "2020-04-28", "updated": "", "authors": ["yuxi cai", "fan long", "andreas park", "andreas veneris"], "url": "https://arxiv.org/abs/2004.13696"}, {"title": "technological improvement rate estimates for all technologies: use of   patent data and an extended domain description", "id": "2004.13919", "abstract": "in this work, we attempt to provide a comprehensive granular account of the pace of technological change. more specifically, we survey estimated yearly performance improvement rates for nearly all definable technologies for the first time. we do this by creating a correspondence of all patents within the us patent system to a set of technology domains. a technology domain is a body of patented inventions achieving the same technological function using the same knowledge and scientific principles. we obtain a set of 1757 domains using an extension of the previously defined classification overlap method (com). these domains contain 97.14% of all patents within the entire us patent system. from the identified patent sets, we calculated the average centrality of the patents in each domain to estimate their improvement rates, following a methodology tested in prior work. the estimated improvement rates vary from a low of 1.9% per year for the mechanical skin treatment - hair removal and wrinkles domain to a high of 228.8% per year for the network management - client-server applications domain. we developed a one-line descriptor identifying the technological function achieved and the underlying knowledge base for the largest 50, fastest 20 as well as slowest 20 of these domains, which cover more than forty percent of the patent system. in general, the rates of improvement were not a strong function of the patent set size and the fastest improving domains are predominantly software-based. we make available an online system that allows for automated searching for domains and improvement rates corresponding to any technology of interest to researchers, strategists and policy formulators.", "categories": "econ.gn cs.cy physics.soc-ph q-fin.ec q-fin.gn q-fin.pm", "created": "2020-04-28", "updated": "", "authors": ["anuraag singh", "giorgio triulzi", "christopher l. magee"], "url": "https://arxiv.org/abs/2004.13919"}, {"title": "distress propagation on production networks: coarse-graining and   modularity of linkages", "id": "2004.14485", "abstract": "distress propagation occurs in connected networks, its rate and extent being dependent on network topology. to study this, we choose economic production networks as a paradigm. an economic network can be examined at many levels: linkages among individual agents (microscopic), among firms/sectors (mesoscopic) or among countries (macroscopic). new emergent dynamical properties appear at every level, so the granularity matters. for viral epidemics, even an individual node may act as an epicenter of distress and potentially affect the entire network. economic networks, however, are known to be immune at the micro-levels and more prone to failure in the meso/macro-levels. we propose a dynamical interaction model to characterize the mechanism of distress propagation, across different modules of a network, initiated at different epicenters. vulnerable modules often lead to large degrees of destabilization. we demonstrate our methodology using a unique empirical data-set of input-output linkages across 0.14 million firms in one administrative state of india, a developing economy. the network has multiple hub-and-spoke structures that exhibits moderate disassortativity, which varies with the level of coarse-graining. the novelty lies in characterizing the production network at different levels of granularity or modularity, and finding `too-big-to-fail' modules supersede `too-central-to-fail' modules in distress propagation.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-04-29", "updated": "", "authors": ["ashish kumar", "anindya s. chakrabarti", "anirban chakraborti", "tushar nandi"], "url": "https://arxiv.org/abs/2004.14485"}, {"title": "virus dynamics with behavioral responses", "id": "2004.14533", "abstract": "motivated by epidemics such as covid-19, we study the spread of a contagious disease when behavior responds to the disease's prevalence. we extend the sir epidemiological model to include endogenous meeting rates. individuals benefit from economic activity, but activity involves interactions with potentially infected individuals. the main focus is a theoretical analysis of contagion dynamics and behavioral responses to changes in risk. we obtain a simple condition for when public-health interventions or changes in disease prevalence will paradoxically increase infection rates due to risk compensation. behavioral responses are most likely to undermine public-health interventions near the peak of severe diseases.", "categories": "q-bio.pe econ.th", "created": "2020-04-29", "updated": "2020-06-19", "authors": ["krishna dasaratha"], "url": "https://arxiv.org/abs/2004.14533"}, {"title": "stable roommate problem with diversity preferences", "id": "2004.14640", "abstract": "in the multidimensional stable roommate problem, agents have to be allocated to rooms and have preferences over sets of potential roommates. we study the complexity of finding good allocations of agents to rooms under the assumption that agents have diversity preferences [bredereck et al., 2019]: each agent belongs to one of the two types (e.g., juniors and seniors, artists and engineers), and agents' preferences over rooms depend solely on the fraction of agents of their own type among their potential roommates. we consider various solution concepts for this setting, such as core and exchange stability, pareto optimality and envy-freeness. on the negative side, we prove that envy-free, core stable or (strongly) exchange stable outcomes may fail to exist and that the associated decision problems are np-complete. on the positive side, we show that these problems are in fpt with respect to the room size, which is not the case for the general stable roommate problem. moreover, for the classic setting with rooms of size two, we present a linear-time algorithm that computes an outcome that is core and exchange stable as well as pareto optimal. many of our results for the stable roommate problem extend to the stable marriage problem.", "categories": "cs.gt econ.th", "created": "2020-04-30", "updated": "", "authors": ["niclas boehmer", "edith elkind"], "url": "https://arxiv.org/abs/2004.14640"}, {"title": "the interaction between credit constraints and uncertainty shocks", "id": "2004.14719", "abstract": "can uncertainty about credit availability trigger a slowdown in real activity? this question is answered by using a novel method to identify shocks to uncertainty in access to credit. time-variation in uncertainty about credit availability is estimated using particle markov chain monte carlo. we extract shocks to time-varying credit uncertainty and decompose it into two parts: the first captures the \"pure\" effect of a shock to the second moment; the second captures total effects of uncertainty including effects on the first moment. using state-dependent local projections, we find that the \"pure\" effect by itself generates a sharp slowdown in real activity and the effects are largely countercyclical. we feed the estimated shocks into a flexible price real business cycle model with a collateral constraint and show that when the collateral constraint binds, an uncertainty shock about credit access is recessionary leading to a simultaneous decline in consumption, investment, and output.", "categories": "econ.em", "created": "2020-04-30", "updated": "", "authors": ["pratiti chatterjee", "david gunawan", "robert kohn"], "url": "https://arxiv.org/abs/2004.14719"}, {"title": "spruce budworm and oil price: a biophysical analogy", "id": "2004.14898", "abstract": "the behavior of complex systems is one of the most intriguing phenomena investigated by recent science; natural and artificial systems offer a wide opportunity for this kind of analysis. the energy conversion is both a process based on important physical laws and one of the most important economic sectors; the interaction between these two aspects of energy production suggests the possibility to apply some of the approaches of the dynamic systems' analysis. in particular, a phase plot, which is one of the methods to detect a correlation between quantities in a complex system, provides a good way to establish qualitative analogies between the ecological systems and the economic ones and may shed light on the processes governing the evolution of the system. the aim of this paper is to highlight the analogies between some peculiar characteristics of the oil production vs. price and show in which way such characteristics are similar to some behavioral mechanisms found in nature.", "categories": "econ.th physics.soc-ph", "created": "2020-04-30", "updated": "", "authors": ["luciano celi", "claudio della volpe", "luca pardi", "stefano siboni"], "url": "https://arxiv.org/abs/2004.14898"}, {"title": "soft affirmative action and minority recruitment", "id": "2004.14953", "abstract": "we study search, evaluation, and selection of candidates of unknown quality for a position. we examine the effects of \"soft\" affirmative action policies increasing the relative percentage of minority candidates in the candidate pool. we show that, while meant to encourage minority hiring, such policies may backfire if the evaluation of minority candidates is noisier than that of non-minorities. this may occur even if minorities are at least as qualified and as valuable as non-minorities. the results provide a possible explanation for why certain soft affirmative action policies have proved counterproductive, even in the absence of (implicit) biases.", "categories": "econ.th econ.gn q-fin.ec", "created": "2020-04-30", "updated": "", "authors": ["daniel fershtman", "alessandro pavan"], "url": "https://arxiv.org/abs/2004.14953"}, {"title": "what are we weighting for? a mechanistic model for probability weighting", "id": "2005.00056", "abstract": "behavioural economics provides labels for patterns in human economic behaviour. probability weighting is one such label. it expresses a mismatch between probabilities used in a formal model of a decision (i.e. model parameters) and probabilities inferred from real people's decisions (the same parameters estimated empirically). the inferred probabilities are called \"decision weights.\" it is considered a robust experimental finding that decision weights are higher than probabilities for rare events, and (necessarily, through normalisation) lower than probabilities for common events. typically this is presented as a cognitive bias, i.e. an error of judgement by the person. here we point out that the same observation can be described differently: broadly speaking, probability weighting means that a decision maker has greater uncertainty about the world than the observer. we offer a plausible mechanism whereby such differences in uncertainty arise naturally: when a decision maker must estimate probabilities as frequencies in a time series while the observer knows them a priori. this suggests an alternative presentation of probability weighting as a principled response by a decision maker to uncertainties unaccounted for in an observer's model.", "categories": "econ.th", "created": "2020-04-30", "updated": "", "authors": ["ole peters", "alexander adamou", "mark kirstein", "yonatan berman"], "url": "https://arxiv.org/abs/2005.00056"}, {"title": "two burning questions on covid-19: did shutting down the economy help?   can we (partially) reopen the economy without risking the second wave?", "id": "2005.00072", "abstract": "as we reach the apex of the covid-19 pandemic, the most pressing question facing us is: can we even partially reopen the economy without risking a second wave? we first need to understand if shutting down the economy helped. and if it did, is it possible to achieve similar gains in the war against the pandemic while partially opening up the economy? to do so, it is critical to understand the effects of the various interventions that can be put into place and their corresponding health and economic implications. since many interventions exist, the key challenge facing policy makers is understanding the potential trade-offs between them, and choosing the particular set of interventions that works best for their circumstance. in this memo, we provide an overview of synthetic interventions (a natural generalization of synthetic control), a data-driven and statistically principled method to perform what-if scenario planning, i.e., for policy makers to understand the trade-offs between different interventions before having to actually enact them. in essence, the method leverages information from different interventions that have already been enacted across the world and fits it to a policy maker's setting of interest, e.g., to estimate the effect of mobility-restricting interventions on the u.s., we use daily death data from countries that enforced severe mobility restrictions to create a \"synthetic low mobility u.s.\" and predict the counterfactual trajectory of the u.s. if it had indeed applied a similar intervention. using synthetic interventions, we find that lifting severe mobility restrictions and only retaining moderate mobility restrictions (at retail and transit locations), seems to effectively flatten the curve. we hope this provides guidance on weighing the trade-offs between the safety of the population, strain on the healthcare system, and impact on the economy.", "categories": "econ.em cs.lg stat.ap", "created": "2020-04-30", "updated": "2020-05-10", "authors": ["anish agarwal", "abdullah alomar", "arnab sarker", "devavrat shah", "dennis shen", "cindy yang"], "url": "https://arxiv.org/abs/2005.00072"}, {"title": "how average is average? temporal patterns in human behaviour as measured   by mobile phone data -- or why chose thursdays", "id": "2005.00137", "abstract": "mobile phone data -- with file sizes scaling into terabytes -- easily overwhelm the computational capacity available to some researchers. moreover, for ethical reasons, data access is often granted only to particular subsets, restricting analyses to cover single days, weeks, or geographical areas. consequently, it is frequently impossible to set a particular analysis or event in its context and know how typical it is, compared to other days, weeks or months. this is important for academic referees questioning research on mobile phone data and for the analysts in deciding how to sample, how much data to process, and which events are anomalous. all these issues require an understanding of variability in big data to answer the question of how average is average? this paper provides a method, using a large mobile phone dataset, to answer these basic but necessary questions. we show that file size is a robust proxy for the activity level of phone users by profiling the temporal variability of the data at an hourly, daily and monthly level. we then apply time-series analysis to isolate temporal periodicity. finally, we discuss confidence limits to anomalous events in the data. we recommend an analytical approach to mobile phone data selection which suggests that ideally data should be sampled across days, across working weeks, and across the year, to obtain a representative average. however, where this is impossible, the temporal variability is such that specific weekdays' data can provide a fair picture of other days in their general structure.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2020-04-30", "updated": "", "authors": ["marina toger", "ian shuttleworth", "john \u00f6sth"], "url": "https://arxiv.org/abs/2005.00137"}, {"title": "on the equivalence of neural and production networks", "id": "2005.00510", "abstract": "this paper identifies for the first time the mathematical equivalence between economic networks of cobb-douglas agents and artificial neural networks. it explores two implications of this equivalence under general conditions. first, a burgeoning literature has established that network propagation can transform microeconomic perturbations into large aggregate shocks. neural network equivalence amplifies the magnitude and complexity of this phenomenon. second, if economic agents adjust their production and utility functions in optimal response to local conditions, market pricing is a sufficient and robust channel for information feedback leading to global, macro-scale learning at the level of the economy as a whole.", "categories": "econ.th", "created": "2020-05-01", "updated": "", "authors": ["roy gernhardt", "bjorn persson"], "url": "https://arxiv.org/abs/2005.00510"}, {"title": "multialternative neural decision processes", "id": "2005.01081", "abstract": "we introduce an algorithmic decision process for multialternative choice that combines binary comparisons and markovian exploration. we show that a functional property, transitivity, makes it testable.", "categories": "cs.ai econ.th q-bio.nc", "created": "2020-05-03", "updated": "2020-05-16", "authors": ["carlo baldassi", "simone cerreia-vioglio", "fabio maccheroni", "massimo marinacci", "marco pirazzini"], "url": "https://arxiv.org/abs/2005.01081"}, {"title": "dynamic reserves in matching markets", "id": "2005.01103", "abstract": "we study a school choice problem under affirmative action policies where authorities reserve a certain fraction of the slots at each school for specific student groups, and where students have preferences not only over the schools they are matched to but also the type of slots they receive. such reservation policies might cause waste in instances of low demand from some student groups. to propose a solution to this issue, we construct a family of choice functions, dynamic reserves choice functions, for schools that respect within-group fairness and allow the transfer of otherwise vacant slots from low-demand groups to high-demand groups. we propose the cumulative offer mechanism (com) as an allocation rule where each school uses a dynamic reserves choice function and show that it is stable with respect to schools' choice functions, is strategy-proof, and respects improvements. furthermore, we show that transferring more of the otherwise vacant slots leads to strategy-proof pareto improvement under the com.", "categories": "econ.th", "created": "2020-05-03", "updated": "", "authors": ["orhan ayg\u00fcn", "bertan turhan"], "url": "https://arxiv.org/abs/2005.01103"}, {"title": "exponential-growth prediction bias and compliance with safety measures   in the times of covid-19", "id": "2005.01273", "abstract": "we conduct a unique, amazon mturk-based global experiment to investigate the importance of an exponential-growth prediction bias (egpb) in understanding why the covid-19 outbreak has exploded. the scientific basis for our inquiry is the well-established fact that disease spread, especially in the initial stages, follows an exponential function meaning few positive cases can explode into a widespread pandemic if the disease is sufficiently transmittable. we define prediction bias as the systematic error arising from faulty prediction of the number of cases x-weeks hence when presented with y-weeks of prior, actual data on the same. our design permits us to identify the root of this under-prediction as an egpb arising from the general tendency to underestimate the speed at which exponential processes unfold. our data reveals that the \"degree of convexity\" reflected in the predicted path of the disease is significantly and substantially lower than the actual path. the bias is significantly higher for respondents from countries at a later stage relative to those at an early stage of disease progression. we find that individuals who exhibit egpb are also more likely to reveal markedly reduced compliance with the who-recommended safety measures, find general violations of safety protocols less alarming, and show greater faith in their government's actions. a simple behavioral nudge which shows prior data in terms of raw numbers, as opposed to a graph, causally reduces egpb. clear communication of risk via raw numbers could increase accuracy of risk perception, in turn facilitating compliance with suggested protective behaviors.", "categories": "econ.gn q-fin.ec", "created": "2020-04-29", "updated": "", "authors": ["ritwik banerjee", "joydeep bhattacharya", "priyama majumdar"], "url": "https://arxiv.org/abs/2005.01273"}, {"title": "optimal epidemic suppression under an icu constraint", "id": "2005.01327", "abstract": "how much and when should we limit economic and social activity to ensure that the health-care system is not overwhelmed during an epidemic? we study a setting where icu resources are constrained while suppression is costly (e.g., limiting economic interaction). providing a fully analytical solution we show that the common wisdom of \"flattening the curve\", where suppression measures are continuously taken to hold down the spread throughout the epidemic, is suboptimal. instead, the optimal suppression is discontinuous. the epidemic should be left unregulated in a first phase and when the icu constraint is approaching society should quickly lock down (a discontinuity). after the lockdown regulation should gradually be lifted, holding the rate of infected constant thus respecting the icu resources while not unnecessarily limiting economic activity. in a final phase, regulation is lifted. we call this strategy \"filling the box\".", "categories": "econ.th math.oc", "created": "2020-05-04", "updated": "", "authors": ["laurent miclo", "daniel spiro", "j\u00f6rgen weibull"], "url": "https://arxiv.org/abs/2005.01327"}, {"title": "ensemble forecasting for intraday electricity prices: simulating   trajectories", "id": "2005.01365", "abstract": "recent studies concerning the point electricity price forecasting have shown evidence that the hourly german intraday continuous market is weak-form efficient. therefore, we take a novel, advanced approach to the problem. a probabilistic forecasting of the hourly intraday electricity prices is performed by simulating trajectories in every trading window to receive a realistic ensemble to allow for more efficient intraday trading and redispatch. a generalized additive model is fitted to the price differences with the assumption that they follow a zero-inflated distribution, precisely a mixture of the dirac and the student's t-distributions. moreover, the mixing term is estimated using a high-dimensional logistic regression with lasso penalty. we model the expected value and volatility of the series using i.a. autoregressive and no-trade effects or load, wind and solar generation forecasts and accounting for the non-linearities in e.g. time to maturity. both the in-sample characteristics and forecasting performance are analysed using a rolling window forecasting study. multiple versions of the model are compared to several benchmark models and evaluated using probabilistic forecasting measures and significance tests. the study aims to forecast the price distribution in the german intraday continuous market in the last 3 hours of trading, but the approach allows for application to other continuous markets, especially in europe. the results prove superiority of the mixture model over the benchmarks gaining the most from the modelling of the volatility. they also indicate that the introduction of xbid reduced the market volatility.", "categories": "q-fin.st econ.em stat.ap", "created": "2020-05-04", "updated": "2020-08-29", "authors": ["micha\u0142 narajewski", "florian ziel"], "url": "https://arxiv.org/abs/2005.01365"}, {"title": "neural networks and value at risk", "id": "2005.01686", "abstract": "utilizing a generative regime switching framework, we perform monte-carlo simulations of asset returns for value at risk threshold estimation. using equity markets and long term bonds as test assets in the global, us, euro area and uk setting over an up to 1,250 weeks sample horizon ending in august 2018, we investigate neural networks along three design steps relating (i) to the initialization of the neural network, (ii) its incentive function according to which it has been trained and (iii) the amount of data we feed. first, we compare neural networks with random seeding with networks that are initialized via estimations from the best-established model (i.e. the hidden markov). we find latter to outperform in terms of the frequency of var breaches (i.e. the realized return falling short of the estimated var threshold). second, we balance the incentive structure of the loss function of our networks by adding a second objective to the training instructions so that the neural networks optimize for accuracy while also aiming to stay in empirically realistic regime distributions (i.e. bull vs. bear market frequencies). in particular this design feature enables the balanced incentive recurrent neural network (rnn) to outperform the single incentive rnn as well as any other neural network or established approach by statistically and economically significant levels. third, we half our training data set of 2,000 days. we find our networks when fed with substantially less data (i.e. 1,000 days) to perform significantly worse which highlights a crucial weakness of neural networks in their dependence on very large data sets ...", "categories": "q-fin.rm cs.lg econ.em", "created": "2020-05-04", "updated": "2020-05-06", "authors": ["alexander arimond", "damian borth", "andreas hoepner", "michael klawunn", "stefan weisheit"], "url": "https://arxiv.org/abs/2005.01686"}, {"title": "on track for retirement?", "id": "2005.01692", "abstract": "over sixty percent of employees at a large south african company contribute the minimum rate of 7.5 percent to a retirement fund, far below the rate of 15 percent recommended by financial advisers. i use a field experiment to investigate whether providing employees with a retirement calculator, which shows projections of retirement income, leads to increases in contributions. the impact is negligible. the lack of response to the calculator suggests many employees may wish to save less than the minimum. i use a model of asymmetric information to explain why the employer sets a binding minimum.", "categories": "econ.gn q-fin.ec", "created": "2020-05-04", "updated": "2020-09-28", "authors": ["matthew olckers"], "url": "https://arxiv.org/abs/2005.01692"}, {"title": "the murphy decomposition and the calibration-resolution principle: a new   perspective on forecast evaluation", "id": "2005.01835", "abstract": "i provide a unifying perspective on forecast evaluation, characterizing accurate forecasts of all types, from simple point to complete probabilistic forecasts, in terms of two fundamental underlying properties, autocalibration and resolution, which can be interpreted as describing a lack of systematic mistakes and a high information content. this \"calibration-resolution principle\" gives a new insight into the nature of forecasting and generalizes the famous sharpness principle by gneiting et al. (2007) from probabilistic to all types of forecasts. it amongst others exposes the shortcomings of several widely used forecast evaluation methods. the principle is based on a fully general version of the murphy decomposition of loss functions, which i provide. special cases of this decomposition are well-known and widely used in meteorology.   besides using the decomposition in this new theoretical way, after having introduced it and the underlying properties in a proper theoretical framework, accompanied by an illustrative example, i also employ it in its classical sense as a forecast evaluation method as the meteorologists do: as such, it unveils the driving forces behind forecast errors and complements classical forecast evaluation methods. i discuss estimation of the decomposition via kernel regression and then apply it to popular economic forecasts. analysis of mean forecasts from the us survey of professional forecasters and quantile forecasts derived from bank of england fan charts indeed yield interesting new insights and highlight the potential of the method.", "categories": "stat.me econ.em", "created": "2020-05-04", "updated": "", "authors": ["marc-oliver pohle"], "url": "https://arxiv.org/abs/2005.01835"}, {"title": "equilibria of nonatomic anonymous games", "id": "2005.01839", "abstract": "we add here another layer to the literature on nonatomic anonymous games started with the 1973 paper by schmeidler. more specifically, we define a new notion of equilibrium which we call $\\varepsilon$-estimated equilibrium and prove its existence for any positive $\\varepsilon$. this notion encompasses and brings to nonatomic games recent concepts of equilibrium such as self-confirming, peer-confirming, and berk--nash. this augmented scope is our main motivation. at the same time, our approach also resolves some conceptual problems present in schmeidler (1973), pointed out by shapley. in that paper\\ the existence of pure-strategy nash equilibria has been proved for any nonatomic game with a continuum of players, endowed with an atomless countably additive probability. but, requiring borel measurability of strategy profiles may impose some limitation on players' choices and introduce an exogenous dependence among\\ players' actions, which clashes with the nature of noncooperative game theory. our suggested solution is to consider every subset of players as measurable. this leads to a nontrivial purely finitely additive component which might prevent the existence of equilibria and requires a novel mathematical approach to prove the existence of $\\varepsilon$-equilibria.", "categories": "econ.th cs.gt", "created": "2020-05-04", "updated": "", "authors": ["simone cerreia-vioglio", "fabio maccheroni", "david schmeidler"], "url": "https://arxiv.org/abs/2005.01839"}, {"title": "levels of structural change: an analysis of china's development push   1998-2014", "id": "2005.01882", "abstract": "we investigate structural change in the pr china during a period of particularly rapid growth 1998-2014. for this, we utilize sectoral data from the world input-output database and firm-level data from the chinese industrial enterprise database. starting with correlation laws known from the literature (fabricant's laws), we investigate which empirical regularities hold at the sectoral level and show that many of these correlations cannot be recovered at the firm level. for a more detailed analysis, we propose a multi-level framework, which is validated with empirically. for this, we perform a robust regression, since various input variables at the firm-level as well as the residuals of exploratory ols regressions are found to be heavy-tailed. we conclude that fabricant's laws and other regularities are primarily characteristics of the sectoral level which rely on aspects like infrastructure, technology level, innovation capabilities, and the knowledge base of the relevant labor force. we illustrate our analysis by showing the development of some of the larger sectors in detail and offer some policy implications in the context of development economics, evolutionary economics, and industrial organization.", "categories": "econ.gn q-fin.ec", "created": "2020-05-04", "updated": "2020-09-30", "authors": ["torsten heinrich", "jangho yang", "shuanping dai"], "url": "https://arxiv.org/abs/2005.01882"}, {"title": "stochastic stackelberg games", "id": "2005.01997", "abstract": "in this paper, we consider a discrete-time stochastic stackelberg game where there is a defender (also called leader) who has to defend a target and an attacker (also called follower). both attacker and defender have conditionally independent private types, conditioned on action and previous state, that evolve as controlled markov processes. the objective is to compute the stochastic stackelberg equilibrium of the game where defender commits to a strategy. the attacker's strategy is the best response to the defender strategy and defender's strategy is optimum given the attacker plays the best response. in general, computing such equilibrium involves solving a fixed-point equation for the whole game. in this paper, we present an algorithm that computes such strategies by solving smaller fixed-point equations for each time $t$. this reduces the computational complexity of the problem from double exponential in time to linear in time. based on this algorithm, we compute stochastic stackelberg equilibrium of a security example.", "categories": "math.oc cs.gt econ.th", "created": "2020-05-05", "updated": "", "authors": ["deepanshu vasal"], "url": "https://arxiv.org/abs/2005.01997"}, {"title": "identification in economies with frictions", "id": "2005.02010", "abstract": "identification in heterogeneous agent economies can be obtained by utilizing restrictions which are consistent with alternative mechanisms. survey data that are informative about the proportion of agents whose behavior is distorted due to financial frictions provides additional information. the latter can shrink the set of admissible models and preference parameter values. this paper provides identification analysis, simulation evidence and an application using spanish data where the borrowing constrained are identified by combining information from different surveys. results suggest that incomplete markets can potentially rationalize the gap between micro and macro elasticities for aggregate consumption and labor supply.", "categories": "econ.em", "created": "2020-05-05", "updated": "2020-07-13", "authors": ["andreas tryphonides"], "url": "https://arxiv.org/abs/2005.02010"}, {"title": "stocks vote with their feet: can a piece of paper document fights the   covid-19 pandemic?", "id": "2005.02034", "abstract": "assessing the trend of the covid-19 pandemic and policy effectiveness is essential for both policymakers and stock investors, but challenging because the crisis has unfolded with extreme speed and the previous index was not suitable for measuring policy effectiveness for covid-19. this paper builds an index of policy effectiveness on fighting covid-19 pandemic, whose building method is similar to the index of policy uncertainty, based on province-level paper documents released in china from jan.1st to apr.16th of 2020. this paper also studies the relationships among covid-19 daily confirmed cases, stock market volatility, and document-based policy effectiveness in china. this paper uses the dcc-garch model to fit conditional covariance's change rule of multi-series. this paper finally tests four hypotheses, about the time-space difference of policy effectiveness and its overflow effect both on the covid-19 pandemic and stock market. through the inner interaction of this triad structure, we can bring forward more specific and scientific suggestions to maintain stability in the stock market at such exceptional times.", "categories": "econ.em", "created": "2020-05-05", "updated": "", "authors": ["j. su", "q. zhong"], "url": "https://arxiv.org/abs/2005.02034"}, {"title": "how to manage the post pandemic opening? a pontryagin maximum principle   approach", "id": "2005.02283", "abstract": "the covid-19 pandemic has completely disrupted the operation of our societies. its elusive transmission process, characterized by an unusually long incubation period, as well as a high contagion capacity, has forced many countries to take quarantine and social isolation measures that conspire against the performance of national economies. this situation confronts decision makers in different countries with the alternative of reopening the economies, thus facing the unpredictable cost of a rebound of the infection. this work tries to offer an initial theoretical framework to handle this alternative.", "categories": "econ.gn nlin.ao physics.soc-ph q-fin.ec", "created": "2020-05-05", "updated": "2020-06-24", "authors": ["r. mansilla"], "url": "https://arxiv.org/abs/2005.02283"}, {"title": "a theory of the saving rate of the rich", "id": "2005.02379", "abstract": "empirical evidence suggests that the rich have higher propensity to save than do the poor. while this observation may appear to contradict the homotheticity of preferences, we theoretically show that that is not the case. specifically, we consider an income fluctuation problem with homothetic preferences and general shocks and prove that consumption functions are asymptotically linear, with an exact analytical characterization of asymptotic marginal propensities to consume (mpc). we provide necessary and sufficient conditions for the asymptotic mpcs to be zero. we solve a calibrated model with standard constant relative risk aversion utility and show that asymptotic mpcs can be zero in empirically plausible settings, implying an increasing and large saving rate of the rich.", "categories": "econ.th", "created": "2020-05-04", "updated": "2020-07-13", "authors": ["qingyin ma", "alexis akira toda"], "url": "https://arxiv.org/abs/2005.02379"}, {"title": "the information content of taster's valuation in tea auctions of india", "id": "2005.02814", "abstract": "tea auctions across india occur as an ascending open auction, conducted online. before the auction, a sample of the tea lot is sent to potential bidders and a group of tea tasters. the seller's reserve price is a confidential function of the tea taster's valuation, which also possibly acts as a signal to the bidders.   in this paper, we work with the dataset from a single tea auction house, j thomas, of tea dust category, on 49 weeks in the time span of 2018-2019, with the following objectives in mind:   $\\bullet$ objective classification of the various categories of tea dust (25) into a more manageable, and robust classification of the tea dust, based on source and grades.   $\\bullet$ predict which tea lots would be sold in the auction market, and a model for the final price conditioned on sale.   $\\bullet$ to study the distribution of price and ratio of the sold tea auction lots.   $\\bullet$ make a detailed analysis of the information obtained from the tea taster's valuation and its impact on the final auction price.   the model used has shown various promising results on cross-validation. the importance of valuation is firmly established through analysis of causal relationship between the valuation and the actual price. the authors hope that this study of the properties and the detailed analysis of the role played by the various factors, would be significant in the decision making process for the players of the auction game, pave the way to remove the manual interference in an attempt to automate the auction procedure, and improve tea quality in markets.", "categories": "stat.ap econ.em", "created": "2020-05-04", "updated": "", "authors": ["abhinandan dalal", "diganta mukherjee", "subhrajyoty roy"], "url": "https://arxiv.org/abs/2005.02814"}, {"title": "spatial dependence in the rank-size distribution of cities", "id": "2005.02836", "abstract": "power law distributions characterise several natural and social phenomena. the zipf law for cities is one of those. the study views the question of whether that global regularity is independent of different spatial distributions of cities. for that purpose, a typical zipfian rank-size distribution of cities is generated with random numbers. this distribution is then cast into different settings of spatial coordinates. for the estimation, the variables rank and size are supplemented by spatial spillover effects in a standard spatial econometric approach. results suggest that distance and contiguity effects matter. this finding is further corroborated by three country analyses.", "categories": "physics.soc-ph econ.em", "created": "2020-05-06", "updated": "", "authors": ["rolf bergs"], "url": "https://arxiv.org/abs/2005.02836"}, {"title": "quantifying the economic impact of covid-19 in mainland china using   human mobility data", "id": "2005.03010", "abstract": "to contain the pandemic of coronavirus (covid-19) in mainland china, the authorities have put in place a series of measures, including quarantines, social distancing, and travel restrictions. while these strategies have effectively dealt with the critical situations of outbreaks, the combination of the pandemic and mobility controls has slowed china's economic growth, resulting in the first quarterly decline of gross domestic product (gdp) since gdp began to be calculated, in 1992. to characterize the potential shrinkage of the domestic economy, from the perspective of mobility, we propose two new economic indicators: the new venues created (nvc) and the volumes of visits to venue (v^3), as the complementary measures to domestic investments and consumption activities, using the data of baidu maps. the historical records of these two indicators demonstrated strong correlations with the past figures of chinese gdp, while the status quo has dramatically changed this year, due to the pandemic. we hereby presented a quantitative analysis to project the impact of the pandemic on economies, using the recent trends of nvc and v^3. we found that the most affected sectors would be travel-dependent businesses, such as hotels, educational institutes, and public transportation, while the sectors that are mandatory to human life, such as workplaces, residential areas, restaurants, and shopping sites, have been recovering rapidly. analysis at the provincial level showed that the self-sufficient and self-sustainable economic regions, with internal supplies, production, and consumption, have recovered faster than those regions relying on global supply chains.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-06", "updated": "", "authors": ["jizhou huang", "haifeng wang", "haoyi xiong", "miao fan", "an zhuo", "ying li", "dejing dou"], "url": "https://arxiv.org/abs/2005.03010"}, {"title": "detecting latent communities in network formation models", "id": "2005.03226", "abstract": "this paper proposes a logistic undirected network formation model which allows for assortative matching on observed individual characteristics and the presence of edge-wise fixed effects. we model the coefficients of observed characteristics to have a latent community structure and the edge-wise fixed effects to be of low rank. we propose a multi-step estimation procedure involving nuclear norm regularization, sample splitting, iterative logistic regression and spectral clustering to detect the latent communities. we show that the latent communities can be exactly recovered when the expected degree of the network is of order log n or higher, where n is the number of nodes in the network. the finite sample performance of the new estimation and inference methods is illustrated through both simulated and real datasets.", "categories": "econ.em stat.me", "created": "2020-05-06", "updated": "2020-05-14", "authors": ["shujie ma", "liangjun su", "yichong zhang"], "url": "https://arxiv.org/abs/2005.03226"}, {"title": "distributional robustness of k-class estimators and the pulse", "id": "2005.03353", "abstract": "recently, in causal discovery, invariance properties such as the moment criterion which two-stage least square estimator leverage have been exploited for causal structure learning: e.g., in cases, where the causal parameter is not identifiable, some structure of the non-zero components may be identified, and coverage guarantees are available. subsequently, anchor regression has been proposed to trade-off invariance and predictability. the resulting estimator is shown to have optimal predictive performance under bounded shift interventions. in this paper, we show that the concepts of anchor regression and k-class estimators are closely related. establishing this connection comes with two benefits: (1) it enables us to prove robustness properties for existing k-class estimators when considering distributional shifts. and, (2), we propose a novel estimator in instrumental variable settings by minimizing the mean squared prediction error subject to the constraint that the estimator lies in an asymptotically valid confidence region of the causal parameter. we call this estimator pulse (p-uncorrelated least squares estimator) and show that it can be computed efficiently, even though the underlying optimization problem is non-convex. we further prove that it is consistent. we perform simulation experiments illustrating that there are several settings including weak instrument settings, where pulse outperforms other estimators and suffers from less variability.", "categories": "econ.em cs.lg stat.ml", "created": "2020-05-07", "updated": "2020-07-21", "authors": ["martin emil jakobsen", "jonas peters"], "url": "https://arxiv.org/abs/2005.03353"}, {"title": "green hydrogen: optimal supply chains and power sector benefits", "id": "2005.03464", "abstract": "green hydrogen can help to decarbonize transportation, but its power sector interactions are not well understood. it may contribute to integrating variable renewable energy sources if production is sufficiently flexible in time. using an open-source co-optimization model of the power sector and four options for supplying hydrogen at german filling stations, we find a trade-off between energy efficiency and temporal flexibility: for lower shares of renewables and hydrogen, more energy-efficient and less flexible small-scale on-site electrolysis is optimal. for higher shares of renewables and/or hydrogen, more flexible but less energy-efficient large-scale hydrogen supply chains gain importance as they allow disentangling hydrogen production from demand via storage. liquid hydrogen emerges as particularly beneficial, followed by liquid organic hydrogen carriers and gaseous hydrogen. large-scale hydrogen supply chains can deliver substantial power sector benefits, mainly through reduced renewable surplus generation. energy modelers and system planners should consider the distinct flexibility characteristics of hydrogen supply chains in more detail when assessing the role of green hydrogen in future energy transition scenarios.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-04-09", "updated": "2020-07-20", "authors": ["fabian stockl", "wolf-peter schill", "alexander zerrahn"], "url": "https://arxiv.org/abs/2005.03464"}, {"title": "are the covid19 restrictions really worth the cost? a comparison of   estimated mortality in australia from covid19 and economic recession", "id": "2005.03491", "abstract": "there has been considerable public debate about whether the economic impact of the current covid19 restrictions are worth the costs. although the potential impact of covid19 has been modelled extensively, very few numbers have been presented in the discussions about potential economic impacts. for a good answer to the question - will the restrictions cause as much harm as covid19? - credible evidence-based estimates are required, rather than simply rhetoric. here we provide some preliminary estimates to compare the impact of the current restrictions against the direct impact of the virus. since most countries are currently taking an approach that reduces the number of covid19 deaths, the estimates we provide for deaths from covid19 are deliberately taken from the low end of the estimates of the infection fatality rate, while estimates for deaths from an economic recession are deliberately computed from double the high end of confidence interval for severe economic recessions. this ensures that an adequate challenge to the status quo of the current restrictions is provided. our analysis shows that strict restrictions to eradicate the virus are likely to lead to at least eight times fewer total deaths than an immediate return to work scenario.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-05-05", "updated": "", "authors": ["neil w bailey", "daniel west"], "url": "https://arxiv.org/abs/2005.03491"}, {"title": "modeling high-dimensional unit-root time series", "id": "2005.03496", "abstract": "this paper proposes a new procedure to build factor models for high-dimensional unit-root time series by postulating that a $p$-dimensional unit-root process is a nonsingular linear transformation of a set of unit-root processes, a set of stationary common factors, which are dynamically dependent, and some idiosyncratic white noise components. for the stationary components, we assume that the factor process captures the temporal-dependence and the idiosyncratic white noise series explains, jointly with the factors, the cross-sectional dependence. the estimation of nonsingular linear loading spaces is carried out in two steps. first, we use an eigenanalysis of a nonnegative definite matrix of the data to separate the unit-root processes from the stationary ones and a modified method to specify the number of unit roots. we then employ another eigenanalysis and a projected principal component analysis to identify the stationary common factors and the white noise series. we propose a new procedure to specify the number of white noise series and, hence, the number of stationary common factors, establish asymptotic properties of the proposed method for both fixed and diverging $p$ as the sample size $n$ increases, and use simulation and a real example to demonstrate the performance of the proposed method in finite samples. we also compare our method with some commonly used ones in the literature regarding the forecast ability of the extracted factors and find that the proposed method performs well in out-of-sample forecasting of a 508-dimensional pm$_{2.5}$ series in taiwan.", "categories": "stat.me econ.em", "created": "2020-05-05", "updated": "2020-08-11", "authors": ["zhaoxing gao", "ruey s. tsay"], "url": "https://arxiv.org/abs/2005.03496"}, {"title": "diffusion copulas: identification and estimation", "id": "2005.03513", "abstract": "we propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (upd). this modelling strategy yields a general class of semiparametric markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. we provide primitive conditions for the identification of the upd parameters together with the unknown transformations from discrete samples. likelihood-based estimators of both parametric and nonparametric components are developed and we analyze the asymptotic properties of these. kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. a simulation study investigates the finite sample performance of our estimators in the context of modelling us short-term interest rates. we also present a simple application of the proposed method for modelling the cboe volatility index data.", "categories": "econ.em stat.me", "created": "2020-05-07", "updated": "", "authors": ["ruijun bu", "kaddour hadri", "dennis kristensen"], "url": "https://arxiv.org/abs/2005.03513"}, {"title": "know your clients' behaviours: a cluster analysis of financial   transactions", "id": "2005.03625", "abstract": "in canada, financial advisors and dealers are required by provincial securities commissions and self-regulatory organizations--charged with direct regulation over investment dealers and mutual fund dealers--to respectively collect and maintain know your client (kyc) information, such as their age or risk tolerance, for investor accounts. with this information, investors, under their advisor's guidance, make decisions on their investments which are presumed to be beneficial to their investment goals. our unique dataset is provided by a financial investment dealer with over 50,000 accounts for over 23,000 clients. we use a modified behavioural finance recency, frequency, monetary model for engineering features that quantify investor behaviours, and machine learning clustering algorithms to find groups of investors that behave similarly. we show that the kyc information collected does not explain client behaviours, whereas trade and transaction frequency and volume are most informative. we believe the results shown herein encourage financial regulators and advisors to use more advanced metrics to better understand and predict investor behaviours.", "categories": "econ.em stat.ap stat.ml", "created": "2020-05-07", "updated": "2020-05-14", "authors": ["john r. j. thompson", "longlong feng", "r. mark reesor", "chuck grace"], "url": "https://arxiv.org/abs/2005.03625"}, {"title": "belief-averaged relative utilitarianism", "id": "2005.03693", "abstract": "we study preference aggregation under uncertainty when individual and collective preferences are based on subjective expected utility. a natural procedure for determining the collective preferences of a group then is to average its members' beliefs and add up their $(0,1)$-normalized utility functions. this procedure extends the well-known relative utilitarianism to decision making under uncertainty. we show that it is the only aggregation function that gives tie-breaking rights to agents who join a group and satisfies an independence condition in the spirit of arrow's independence of irrelevant alternatives as well as four undiscriminating axioms.", "categories": "econ.th cs.gt", "created": "2020-05-07", "updated": "", "authors": ["florian brandl"], "url": "https://arxiv.org/abs/2005.03693"}, {"title": "an emissions trading system to reach ndc targets in the chilean electric   sector", "id": "2005.03843", "abstract": "in the context of the paris agreement, chile has pledged to reduce greenhouse gases (ghg) intensity by at least 30% below 2007 levels by 2030, and to phase out coal as a energy source by 2040, among other strategies. in pursue of these goals, chile has implemented a $5 per tonne of co2 emission tax, first of its kind in latin america. however, such a low price has proven to be insufficient. in our work, we study an alternative approach for capping and pricing carbon emissions in the chilean electric sector; the cap and trade paradigm. we model the chilean electric market (generators and emissions auctioneer) as a two stage capacity expansion equilibrium problem, where we allow future investment and trading of emission permits among generator agents. the model studies generation and future investments in the chilean electric sector in two regimes of demand: deterministic and stochastic. we show that the current chilean greenhouse gases (ghg) intensity pledge does not drive an important shift in the future chilean electric matrix. to encourage a shift to greener technologies, a more stringent carbon budget must be considered, resulting in a carbon price approximately ten times higher than the present one. we also show that achieving the emissions reduction goal does not necessarily results in further reductions of carbon generation, or phasing out coal in the longer term. finally, we demonstrate that under technology change costs reductions, higher demand scenarios will relax the need for stringent carbon budgets to achieve new renewable energy investments and hence meet the chilean pledges. these results suggest that some aspects of the chilean pledge require further analysis, of the economic impact, particularly with the recent announcement of achieving carbon neutrality towards 2050.", "categories": "econ.gn math.oc q-fin.ec", "created": "2020-05-08", "updated": "", "authors": ["p\u00eda amigo", "sebasti\u00e1n cea-echenique", "felipe feijoo"], "url": "https://arxiv.org/abs/2005.03843"}, {"title": "dynamic shrinkage priors for large time-varying parameter regressions   using scalable markov chain monte carlo methods", "id": "2005.03906", "abstract": "time-varying parameter (tvp) regression models can involve a huge number of coefficients. careful prior elicitation is required to yield sensible posterior and predictive inferences. in addition, the computational demands of markov chain monte carlo (mcmc) methods mean their use is limited to the case where the number of predictors is not too large. in light of these two concerns, this paper proposes a new dynamic shrinkage prior which reflects the empirical regularity that tvps are typically sparse (i.e. time variation may occur only episodically and only for some of the coefficients). a scalable mcmc algorithm is developed which is capable of handling very high dimensional tvp regressions or tvp vector autoregressions. in an exercise using artificial data we demonstrate the accuracy and computational efficiency of our methods. in an application involving the term structure of interest rates in the eurozone, we find our dynamic shrinkage prior to effectively pick out small amounts of parameter change and our methods to forecast well.", "categories": "econ.em stat.me", "created": "2020-05-08", "updated": "", "authors": ["niko hauzenberger", "florian huber", "gary koop"], "url": "https://arxiv.org/abs/2005.03906"}, {"title": "fractional trends in unobserved components models", "id": "2005.03988", "abstract": "we develop a generalization of unobserved components models that allows for a wide range of long-run dynamics by modelling the permanent component as a fractionally integrated process. the model does not require stationarity and can be cast in state space form. in a multivariate setup, fractional trends may yield a cointegrated system. we derive the kalman filter estimator for the common fractionally integrated component and establish consistency and asymptotic (mixed) normality of the maximum likelihood estimator. we apply the model to extract a common long-run component of three us inflation measures, where we show that the $i(1)$ assumption is likely to be violated for the common trend.", "categories": "econ.em", "created": "2020-05-08", "updated": "2020-05-20", "authors": ["tobias hartl", "rolf tschernig", "enzo weber"], "url": "https://arxiv.org/abs/2005.03988"}, {"title": "how reliable are bootstrap-based heteroskedasticity robust tests?", "id": "2005.04089", "abstract": "we develop theoretical finite-sample results concerning the size of wild bootstrap-based heteroskedasticity robust tests in linear regression models. in particular, these results provide an efficient diagnostic check, which can be used to weed out tests that are unreliable for a given testing problem in the sense that they overreject substantially. this allows us to assess the reliability of a large variety of wild bootstrap-based tests in an extensive numerical study.", "categories": "math.st econ.em stat.me stat.th", "created": "2020-05-08", "updated": "", "authors": ["benedikt m. p\u00f6tscher", "david preinerstorfer"], "url": "https://arxiv.org/abs/2005.04089"}, {"title": "incentive-compatible critical values", "id": "2005.04141", "abstract": "statistical hypothesis tests are a cornerstone of scientific research. the tests are informative when their size is properly controlled, so the frequency of rejecting true null hypotheses (type i error) stays below a prespecified nominal level. publication bias exaggerates test sizes, however. since scientists can typically only publish results that reject the null hypothesis, they have the incentive to continue conducting studies until attaining rejection. such $p$-hacking takes many forms: from collecting additional data to examining multiple regression specifications, all in the search of statistical significance. the process inflates test sizes above their nominal levels because the critical values used to determine rejection assume that test statistics are constructed from a single study---abstracting from $p$-hacking. this paper addresses the problem by constructing critical values that are compatible with scientists' behavior given their incentives. we assume that researchers conduct studies until finding a test statistic that exceeds the critical value, or until the benefit from conducting an extra study falls below the cost. we then solve for the incentive-compatible critical value (iccv). when the iccv is used to determine rejection, readers can be confident that size is controlled at the desired significance level, and that the researcher's response to the incentives delineated by the critical value is accounted for. since they allow researchers to search for significance among multiple studies, iccvs are larger than classical critical values. yet, for a broad range of researcher behaviors and beliefs, iccvs lie in a fairly narrow range.", "categories": "econ.em stat.me", "created": "2020-05-08", "updated": "", "authors": ["adam mccloskey", "pascal michaillat"], "url": "https://arxiv.org/abs/2005.04141"}, {"title": "probabilistic multi-step-ahead short-term water demand forecasting with   lasso", "id": "2005.04522", "abstract": "water demand is a highly important variable for operational control and decision making. hence, the development of accurate forecasts is a valuable field of research to further improve the efficiency of water utilities. focusing on probabilistic multi-step-ahead forecasting, a time series model is introduced, to capture typical autoregressive, calendar and seasonal effects, to account for time-varying variance, and to quantify the uncertainty and path-dependency of the water demand process. to deal with the high complexity of the water demand process a high-dimensional feature space is applied, which is efficiently tuned by an automatic shrinkage and selection operator (lasso). it allows to obtain an accurate, simple interpretable and fast computable forecasting model, which is well suited for real-time applications. the complete probabilistic forecasting framework allows not only for simulating the mean and the marginal properties, but also the correlation structure between hours within the forecasting horizon. for practitioners, complete probabilistic multi-step-ahead forecasts are of considerable relevance as they provide additional information about the expected aggregated or cumulative water demand, so that a statement can be made about the probability with which a water storage capacity can guarantee the supply over a certain period of time. this information allows to better control storage capacities and to better ensure the smooth operation of pumps. to appropriately evaluate the forecasting performance of the considered models, the energy score (es) as a strictly proper multidimensional evaluation criterion, is introduced. the methodology is applied to the hourly water demand data of a german water supplier.", "categories": "stat.ap econ.em stat.ml", "created": "2020-05-09", "updated": "", "authors": ["jens kley-holsteg", "florian ziel"], "url": "https://arxiv.org/abs/2005.04522"}, {"title": "pandemic, shutdown and consumer spending: lessons from scandinavian   policy responses to covid-19", "id": "2005.04630", "abstract": "this paper uses transaction data from a large bank in scandinavia to estimate the effect of social distancing laws on consumer spending in the covid-19 pandemic. the analysis exploits a natural experiment to disentangle the effects of the virus and the laws aiming to contain it: denmark and sweden were similarly exposed to the pandemic but only denmark imposed significant restrictions on social and economic activities. we estimate that aggregate spending dropped by around 25 percent in sweden and, as a result of the shutdown, by 4 additional percentage points in denmark. this implies that most of the economic contraction is caused by the virus itself and occurs regardless of social distancing laws. the age gradient in the estimates suggest that social distancing reinforces the virus-induced drop in spending for low health-risk individuals but attenuates it for high-risk individuals by lowering the overall prevalence of the virus in the society.", "categories": "econ.gn q-fin.ec", "created": "2020-05-10", "updated": "", "authors": ["asger lau andersen", "emil toft hansen", "niels johannesen", "adam sheridan"], "url": "https://arxiv.org/abs/2005.04630"}, {"title": "posterior probabilities for lorenz and stochastic dominance of   australian income distributions", "id": "2005.04870", "abstract": "using hilda data for the years 2001, 2006, 2010, 2014 and 2017, we compute posterior probabilities for dominance for all pairwise comparisons of income distributions in these years. the dominance criteria considered are lorenz dominance and first and second order stochastic dominance. the income distributions are estimated using an infinite mixture of gamma density functions, with posterior probabilities computed as the proportion of markov chain monte carlo draws that satisfy the inequalities that define the dominance criteria. we find welfare improvements from 2001 to 2006 and qualified improvements from 2006 to the later three years. evidence of an ordering between 2010, 2014 and 2017 cannot be established.", "categories": "econ.em", "created": "2020-05-11", "updated": "", "authors": ["david gunawan", "william e. griffiths", "duangkamon chotikapanich"], "url": "https://arxiv.org/abs/2005.04870"}, {"title": "macroeconomic forecasting with fractional factor models", "id": "2005.04897", "abstract": "we combine high-dimensional factor models with fractional integration methods and derive models where nonstationary, potentially cointegrated data of different persistence is modelled as a function of common fractionally integrated factors. a two-stage estimator, that combines principal components and the kalman filter, is proposed. the forecast performance is studied for a high-dimensional us macroeconomic data set, where we find that benefits from the fractional factor models can be substantial, as they outperform univariate autoregressions, principal components, and the factor-augmented error-correction model.", "categories": "econ.em", "created": "2020-05-11", "updated": "", "authors": ["tobias hartl"], "url": "https://arxiv.org/abs/2005.04897"}, {"title": "functional decision theory in an evolutionary environment", "id": "2005.05154", "abstract": "functional decision theory (fdt) is a fairly new mode of decision theory and a normative viewpoint on how an agent should maximize expected utility. the current standard in decision theory and computer science is causal decision theory (cdt), largely seen as superior to the main alternative evidential decision theory (edt). these theories prescribe three distinct methods for maximizing utility. we explore how fdt differs from cdt and edt, and what implications it has on the behavior of fdt agents and humans. it has been shown in previous research how fdt can outperform cdt and edt. we additionally show fdt performing well on more classical game theory problems and argue for its extension to human problems to show that its potential for superiority is robust. we also make fdt more concrete by displaying it in an evolutionary environment, competing directly against other theories.", "categories": "econ.th cs.ai", "created": "2020-05-06", "updated": "", "authors": ["noah topper"], "url": "https://arxiv.org/abs/2005.05154"}, {"title": "choice with endogenous categorization", "id": "2005.05196", "abstract": "we propose a novel categorical thinking model (ctm) where the framing of the decision problem affects how the agent categorizes each product, and the product's category affects her evaluation of the product. we show that a number of prominent models of salience, status quo bias, loss-aversion, inequality aversion, and present bias all fit under the umbrella of ctm. this suggests categorization as an underlying mechanism for key departures from the neoclassical model of choice and an account for diverse sets of evidence that are anomalous from its perspective. we specialize ctm to provide a behavioral foundation for the salient thinking model of bordalo et al. (2013), highlighting its strong predictions and distinctions from other existing models.", "categories": "econ.th", "created": "2020-05-11", "updated": "", "authors": ["andrew ellis", "yusufcan masatlioglu"], "url": "https://arxiv.org/abs/2005.05196"}, {"title": "bounded topologies on banach spaces and some of their uses in economic   theory: a review", "id": "2005.05202", "abstract": "known results are reviewed about the bounded and convex bounded variants, bt and cbt, of a topology t on a real banach space. the focus is on the cases of t = w(p*, p) and of t = m(p*, p), which are the weak* and the mackey topologies on a dual banach space p*. the convex bounded mackey topology, cbm(p*, p), is known to be identical to m(p*, p). as for bm(p*, p), it is conjectured to be strictly stronger than m(p*, p) or, equivalently, not to be a vector topology (except when p is reflexive). some uses of the bounded mackey and the bounded weak* topologies in economic theory and its applications are pointed to. also reviewed are the bounded weak and the compact weak topologies, bw(y, y*) and kw(y, y*), on a general banach space y, as well as their convex variants (cbw and ckw).", "categories": "math.fa econ.th", "created": "2020-05-11", "updated": "2020-09-13", "authors": ["andrew j. wrobel"], "url": "https://arxiv.org/abs/2005.05202"}, {"title": "energy limits to the gross domestic product on earth", "id": "2005.05244", "abstract": "once carbon emission neutrality and other sustainability goals have been achieved, a widespread assumption is that economic growth at current rates can be sustained beyond the 21st century. however, even if we achieve these goals, this article shows that the overall size of earth's global economy is facing an upper limit purely due to energy and thermodynamic factors. for that, we break down global warming into two components: the greenhouse gas effect and heat dissipation from energy consumption related to economic activities. for the temperature increase due to greenhouse gas emissions, we take 2 {\\deg}c and 5 {\\deg}c as our lower and upper bounds. for the warming effect of heat dissipation related to energy consumption, we use a simplified model for global warming and an extrapolation of the historical correlation between global gross domestic product (gdp) and primary energy production. combining the two effects, we set the acceptable global warming temperature limit to 7 {\\deg}c above pre-industrial levels. we develop four scenarios, based on the viability of large-scale deployment of carbon-neutral energy sources. our results indicate that for a 2% annual gdp growth, the upper limit will be reached at best within a few centuries, even in favorable scenarios where new energy sources such as fusion power are deployed on a massive scale. we conclude that unless gdp can be largely decoupled from energy consumption, thermodynamics will put a hard cap on the size of earth's economy. further economic growth would necessarily require expanding economic activities into space.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-05-11", "updated": "", "authors": ["andreas m. hein", "jean-baptiste rudelle"], "url": "https://arxiv.org/abs/2005.05244"}, {"title": "fractional trends and cycles in macroeconomic time series", "id": "2005.05266", "abstract": "we develop a generalization of correlated trend-cycle decompositions that avoids prior assumptions about the long-run dynamic characteristics by modelling the permanent component as a fractionally integrated process and incorporating a fractional lag operator into the autoregressive polynomial of the cyclical component. the model allows for an endogenous estimation of the integration order jointly with the other model parameters and, therefore, no prior specification tests with respect to persistence are required. we relate the model to the beveridge-nelson decomposition and derive a modified kalman filter estimator for the fractional components. identification, consistency, and asymptotic normality of the maximum likelihood estimator are shown. for us macroeconomic data we demonstrate that, unlike $i(1)$ correlated unobserved components models, the new model estimates a smooth trend together with a cycle hitting all nber recessions. while $i(1)$ unobserved components models yield an upward-biased signal-to-noise ratio whenever the integration order of the data-generating mechanism is greater than one, the fractionally integrated model attributes less variation to the long-run shocks due to the fractional trend specification and a higher variation to the cycle shocks due to the fractional lag operator, leading to more persistent cycles and smooth trend estimates that reflect macroeconomic common sense.", "categories": "econ.em", "created": "2020-05-11", "updated": "2020-05-23", "authors": ["tobias hartl", "rolf tschernig", "enzo weber"], "url": "https://arxiv.org/abs/2005.05266"}, {"title": "causal estimation of stay-at-home orders on sars-cov-2 transmission", "id": "2005.05469", "abstract": "accurately estimating the effectiveness of stay-at-home orders (shos) on reducing social contact and disease spread is crucial for mitigating pandemics. leveraging individual-level location data for 10 million smartphones, we observe that by april 30th---when nine in ten americans were under a sho---daily movement had fallen 70% from pre-covid levels. one-quarter of this decline is causally attributable to shos, with wide demographic differences in compliance, most notably by political affiliation. likely trump voters reduce movement by 9% following a local sho, compared to a 21% reduction among their clinton-voting neighbors, who face similar exposure risks and identical government orders. linking social distancing behavior with an epidemic model, we estimate that reductions in movement have causally reduced sars-cov-2 transmission rates by 49%.", "categories": "physics.soc-ph cs.si econ.gn q-bio.pe q-fin.ec", "created": "2020-05-11", "updated": "", "authors": ["m. keith chen", "yilin zhuo", "malena de la fuente", "ryne rohla", "elisa f. long"], "url": "https://arxiv.org/abs/2005.05469"}, {"title": "staggered release policies for covid-19 control: costs and benefits of   sequentially relaxing restrictions by age", "id": "2005.05549", "abstract": "strong social distancing restrictions have been crucial to controlling the covid-19 outbreak thus far, and the next question is when and how to relax these restrictions. a sequential timing of relaxing restrictions across groups is explored in order to identify policies that simultaneously reduce health risks and economic stagnation relative to current policies. the goal will be to mitigate health risks, particularly among the most fragile sub-populations, while also managing the deleterious effect of restrictions on economic activity. the results of this paper show that a properly constructed sequential release of age-defined subgroups from strict social distancing protocols can lead to lower overall fatality rates than the simultaneous release of all individuals after a lockdown. the optimal release policy, in terms of minimizing overall death rate, must be sequential in nature, and it is important to properly time each step of the staggered release. this model allows for testing of various timing choices for staggered release policies, which can provide insights that may be helpful in the design, testing, and planning of disease management policies for the ongoing covid-19 pandemic and future outbreaks.", "categories": "q-bio.pe econ.gn math.ds q-fin.ec", "created": "2020-05-12", "updated": "", "authors": ["henry zhao", "zhilan feng", "carlos castillo-chavez", "simon a. levin"], "url": "https://arxiv.org/abs/2005.05549"}, {"title": "existence of structured perfect bayesian equilibrium in dynamic games of   asymmetric information", "id": "2005.05586", "abstract": "in~[1],authors considered a general finite horizon model of dynamic game of asymmetric information, where n players have types evolving as independent markovian process, where each player observes its own type perfectly and actions of all players. the authors present a sequential decomposition algorithm to find all structured perfect bayesian equilibria of the game. the algorithm consists of solving a class of fixed-point of equations for each time $t,\\pi_t$, whose existence was left as an open question. in this paper, we prove existence of these fixed-point equations for compact metric spaces.", "categories": "cs.gt cs.sy econ.th eess.sy", "created": "2020-05-12", "updated": "2020-05-29", "authors": ["deepanshu vasal"], "url": "https://arxiv.org/abs/2005.05586"}, {"title": "communication, renegotiation and coordination with private values", "id": "2005.05713", "abstract": "an equilibrium is communication-proof if it is unaffected by new opportunities to communicate and renegotiate. we characterize the set of equilibria of coordination games with pre-play communication in which players have private preferences over the feasible coordinated outcomes. communication-proof equilibria provide a narrow selection from the large set of qualitatively diverse bayesian nash equilibria in such games. under a communication-proof equilibrium, players never miscoordinate, play their jointly preferred outcome whenever there is one, and communicate only the ordinal part of their preferences. moreover, such equilibria are robust to changes in players' beliefs, interim pareto efficient, and evolutionarily stable.", "categories": "econ.th", "created": "2020-05-12", "updated": "2020-09-14", "authors": ["yuval heller", "christoph kuzmics"], "url": "https://arxiv.org/abs/2005.05713"}, {"title": "information validates the prior: a theorem on bayesian updating and   applications", "id": "2005.05714", "abstract": "we develop a result on expected posteriors for bayesians with heterogenous priors, dubbed information validates the prior (ivp). under familiar ordering requirements, anne expects a (blackwell) more informative experiment to bring bob's posterior mean closer to anne's prior mean. we apply the result in two contexts of games of asymmetric information: voluntary testing or certification, and costly signaling or falsification. ivp can be used to determine how an agent's behavior responds to additional exogenous or endogenous information. we discuss economic implications.", "categories": "econ.th", "created": "2020-05-12", "updated": "2020-07-22", "authors": ["navin kartik", "frances lee", "wing suen"], "url": "https://arxiv.org/abs/2005.05714"}, {"title": "evolution, heritable risk, and skewness loving", "id": "2005.05772", "abstract": "our understanding of risk preferences can be sharpened by considering their evolutionary basis. the existing literature has focused on two sources of risk: idiosyncratic risk and aggregate risk. we introduce a new source of risk, heritable risk, in which there is a positive correlation between the fitness of a newborn agent and the fitness of her parent. heritable risk was plausibly common in our evolutionary past and it leads to a strictly higher growth rate than the other sources of risk. we show that the presence of heritable risk in the evolutionary past may explain the tendency of people to exhibit skewness loving today.", "categories": "econ.th q-bio.pe", "created": "2020-05-12", "updated": "2020-08-06", "authors": ["yuval heller", "arthur robson"], "url": "https://arxiv.org/abs/2005.05772"}, {"title": "instability of defection in the prisoner's dilemma: analysis of best   experienced payoff dynamics", "id": "2005.05779", "abstract": "we study population dynamics under which each revising agent tests each strategy k times, with each trial being against a newly drawn opponent, and chooses the strategy whose mean payoff was highest. when k = 1, defection is globally stable in the prisoner`s dilemma. by contrast, when k > 1 we show that there exists a globally stable state in which agents cooperate with probability between 28% and 50%. next, we characterize stability of strict equilibria in general games. our results demonstrate that the empirically plausible case of k > 1 can yield qualitatively different predictions than the case of k = 1 that is commonly studied in the literature.", "categories": "econ.th", "created": "2020-05-12", "updated": "2020-09-13", "authors": ["srinivas arigapudi", "yuval heller", "igal milchtaich"], "url": "https://arxiv.org/abs/2005.05779"}, {"title": "moment conditions for dynamic panel logit models with fixed effects", "id": "2005.05942", "abstract": "this paper builds on bonhomme (2012) to develop a method to systematically construct moment conditions for dynamic panel data logit models with fixed effects. after introducing the moment conditions obtained in this way, we explore their implications for identification and estimation of the model parameters that are common to all individuals, and we find that those common model parameters are estimable at root-$n$ rate for many more dynamic panel logit models than has been appreciated by the existing literature. in the case where the model contains one lagged variable, the moment conditions in kitazawa (2013, 2016) are transformations of a subset of ours. a gmm estimator that is based on the moment conditions is shown to perform well in monte carlo simulations and in an empirical illustration to labor force participation.", "categories": "econ.em", "created": "2020-05-12", "updated": "2020-06-21", "authors": ["bo e. honor\u00e9", "martin weidner"], "url": "https://arxiv.org/abs/2005.05942"}, {"title": "socio-economic impacts of covid-19 on household consumption and poverty", "id": "2005.05945", "abstract": "the covid-19 pandemic has caused a massive economic shock across the world due to business interruptions and shutdowns from social-distancing measures. to evaluate the socio-economic impact of covid-19 on individuals, a micro-economic model is developed to estimate the direct impact of distancing on household income, savings, consumption, and poverty. the model assumes two periods: a crisis period during which some individuals experience a drop in income and can use their precautionary savings to maintain consumption; and a recovery period, when households save to replenish their depleted savings to pre-crisis level. the san francisco bay area is used as a case study, and the impacts of a lockdown are quantified, accounting for the effects of unemployment insurance (ui) and the cares act federal stimulus. assuming a shelter-in-place period of three months, the poverty rate would temporarily increase from 17.1% to 25.9% in the bay area in the absence of social protection, and the lowest income earners would suffer the most in relative terms. if fully implemented, the combination of ui and cares could keep the increase in poverty close to zero, and reduce the average recovery time, for individuals who suffer an income loss, from 11.8 to 6.7 months. however, the severity of the economic impact is spatially heterogeneous, and certain communities are more affected than the average and could take more than a year to recover. overall, this model is a first step in quantifying the household-level impacts of covid-19 at a regional scale. this study can be extended to explore the impact of indirect macroeconomic effects, the role of uncertainty in households' decision-making and the potential effect of simultaneous exogenous shocks (e.g., natural disasters).", "categories": "econ.gn q-fin.ec", "created": "2020-05-12", "updated": "", "authors": ["amory martin", "maryia markhvida", "st\u00e9phane hallegatte", "brian walsh"], "url": "https://arxiv.org/abs/2005.05945"}, {"title": "stabilizing congestion in decentralized record-keepers", "id": "2005.06093", "abstract": "we argue that recent developments in proof-of-work consensus mechanisms can be used in accordance with advancements in formal verification techniques to build a distributed payment protocol that addresses important economic drawbacks from cost efficiency, scalability and adaptablity common to current decentralized record-keeping systems. we enable the protocol to autonomously adjust system throughput according to a feasibly computable statistic - system difficulty. we then provide a formal economic analysis of a decentralized market place for record-keeping that is consistent with our protocol design and show that, when block rewards are zero, the system admits stable, self-regulating levels of transaction fees and wait-times across varying levels of demand. we also provide an analysis of the various technological requirements needed to instantiate such a system in a commercially viable setting, and identify relevant research directions.", "categories": "cs.cr cs.gt econ.gn q-fin.ec", "created": "2020-05-12", "updated": "", "authors": ["assimakis kattis", "fabian trottner"], "url": "https://arxiv.org/abs/2005.06093"}, {"title": "which bills are lobbied? predicting and interpreting lobbying activity   in the us", "id": "2005.06386", "abstract": "using lobbying data from opensecrets.org, we offer several experiments applying machine learning techniques to predict if a piece of legislation (us bill) has been subjected to lobbying activities or not. we also investigate the influence of the intensity of the lobbying activity on how discernible a lobbied bill is from one that was not subject to lobbying. we compare the performance of a number of different models (logistic regression, random forest, cnn and lstm) and text embedding representations (bow, tf-idf, glove, law2vec). we report results of above 0.85% roc auc scores, and 78% accuracy. model performance significantly improves (95% roc auc, and 88% accuracy) when bills with higher lobbying intensity are looked at. we also propose a method that could be used for unlabelled data. through this we show that there is a considerably large number of previously unlabelled us bills where our predictions suggest that some lobbying activity took place. we believe our method could potentially contribute to the enforcement of the us lobbying disclosure act (lda) by indicating the bills that were likely to have been affected by lobbying but were not filed as such.", "categories": "econ.gn cs.cl cs.lg q-fin.ec", "created": "2020-04-29", "updated": "", "authors": ["ivan slobozhan", "peter ormosi", "rajesh sharma"], "url": "https://arxiv.org/abs/2005.06386"}, {"title": "short-term investments and indices of risk", "id": "2005.06576", "abstract": "we study various decision problems regarding short-term investments in risky assets whose returns evolve continuously in time. we show that in each problem, all risk-averse decision makers have the same (problem-dependent) ranking over short-term risky assets. moreover, in each problem, the ranking is represented by the same risk index as in the case of cara utility agents and normally distributed risky assets.", "categories": "q-fin.pm econ.th", "created": "2020-05-13", "updated": "", "authors": ["yuval heller", "amnon schreiber"], "url": "https://arxiv.org/abs/2005.06576"}, {"title": "infinite-duration all-pay bidding games", "id": "2005.06636", "abstract": "a graph game is a two-player zero-sum game in which the players move a token throughout a graph to produce an infinite path, which determines the winner or payoff of the game. in \"bidding games\", in each turn, we hold an 'auction' (bidding) to determine which player moves the token. the players simultaneously submit bids and the higher bidder moves the token. several different payment schemes have been considered. in \"first-price\" bidding, only the higher bidder pays his bid, while in \"all-pay\" bidding, both players pay their bids. bidding games were largely studied with variants of first-price bidding. in this work, we study, for the first time, infinite-duration all-pay bidding games, and show that they exhibit the elegant mathematical properties of their first-price counterparts. this is in stark contrast with reachability games, which are known to be much more complicated under all-pay bidding than first-price bidding. another orthogonal distinction between the bidding rules is in the recipient of the payments: in \"richman\" bidding, the bids are paid to the other player, and in \"poorman\" bidding, the bids are paid to the 'bank'. we focus on strongly-connected games with \"mean-payoff\" and \"parity\" objectives. we completely solve all-pay richman games: a simple argument shows that deterministic strategies cannot guarantee anything in this model, and it is technically much more challenging to find optimal probabilistic strategies that achieve the same expected guarantees in a game as can be obtained with deterministic strategies under first-price bidding. under poorman all-pay bidding, in contrast to richman bidding, deterministic strategies are useful and guarantee a payoff that is only slightly lower than the optimal payoff under first-price poorman bidding. our proofs are constructive and based on new and significantly simpler constructions for first-price bidding.", "categories": "econ.th cs.gt cs.lo", "created": "2020-05-12", "updated": "", "authors": ["guy avni", "isma\u00ebl jecker", "\u0111or\u0111e \u017eikeli\u0107"], "url": "https://arxiv.org/abs/2005.06636"}, {"title": "turing's children: representation of sexual minorities in stem", "id": "2005.06664", "abstract": "we provide the first nationally representative estimates of sexual minority representation in stem fields by studying 142,641 men and women in same-sex couples from the 2009-2018 american community surveys. these data indicate that men in same-sex couples are 12 percentage points less likely to have completed a bachelor's degree in a stem field compared to men in different-sex couples; there is no gap observed for women in same-sex couples compared to women in different-sex couples. the stem gap between men in same-sex and different-sex couples is larger than the stem gap between white and black men but is smaller than the gender stem gap. we also document a gap in stem occupations between men in same-sex and different-sex couples, and we replicate this finding using independently drawn data from the 2013-2018 national health interview surveys. these differences persist after controlling for demographic characteristics, location, and fertility. our findings further the call for interventions designed at increasing representation of sexual minorities in stem.", "categories": "econ.gn q-fin.ec", "created": "2020-05-13", "updated": "", "authors": ["dario sansone", "christopher s. carpenter"], "url": "https://arxiv.org/abs/2005.06664"}, {"title": "combining population and study data for inference on event rates", "id": "2005.06769", "abstract": "this note considers the problem of conducting statistical inference on the share of individuals in some subgroup of a population that experience some event. the specific complication is that the size of the subgroup needs to be estimated, whereas the number of individuals that experience the event is known. the problem is motivated by the recent study of streeck et al. (2020), who estimate the infection fatality rate (ifr) of sars-cov-2 infection in a german town that experienced a super-spreading event in mid-february 2020. in their case the subgroup of interest is comprised of all infected individuals, and the event is death caused by the infection. we clarify issues with the precise definition of the target parameter in this context, and propose confidence intervals (cis) based on classical statistical principles that result in good coverage properties.", "categories": "stat.ap econ.em", "created": "2020-05-14", "updated": "", "authors": ["christoph rothe"], "url": "https://arxiv.org/abs/2005.06769"}, {"title": "patterns of social mobility across social groups in india", "id": "2005.06771", "abstract": "social mobility captures the extent to which socio-economic status of children, is independent of status of their respective parents. in order to measure social mobility, most widely used indicators of socio-economic status are income, education and occupation. while social mobility measurement based on income is less contested, data availability in indian context limits us to observing mobility patterns along the dimensions of either education or occupation. in this study we observe social mobility patterns for different social groups along these two main dimensions, and find that while upward and downward mobility prospects in education for scs/sts is somewhat improving in the recent times, occupational mobility patterns are rather worrisome. these results motivate the need for reconciling disparate trends along education and occupation, in order to get a more comprehensive picture of social mobility in the country.", "categories": "econ.gn q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["vinay reddy venumuddala"], "url": "https://arxiv.org/abs/2005.06771"}, {"title": "informal labour in india", "id": "2005.06795", "abstract": "india like many other developing countries is characterized by huge proportion of informal labour in its total workforce. the percentage of informal workforce is close to 92% of total as computed from nsso 68th round on employment and unemployment, 2011-12. there are many traditional and geographical factors which might have been responsible for this staggering proportion of informality in our country. as a part of this study, we focus mainly on finding out how informality varies with region, sector, gender, social group, and working age groups. further we look at how total inequality is contributed by formal and informal labour, and how much do occupations/industries contribute to inequality within each of formal and informal labour groups separately. for the purposes of our study we use nsso rounds 61 (2004-05) and 68 (2011-12) on employment and unemployment. the study intends to look at an overall picture of informality, and based on the data highlight any inferences which are visible from the data.", "categories": "econ.gn q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["vinay reddy venumuddala"], "url": "https://arxiv.org/abs/2005.06795"}, {"title": "determinants of occupational mobility within the social stratification   structure in india", "id": "2005.06802", "abstract": "in this study, we make use of empirically observed occupational stratification patterns, in order to identify the relationship between education and social mobility of individuals - the latter is approximated by the social distance of an individual's occupation from his/her household's traditional niche occupation. our study draws upon a novel occupational network construction proposed in lambert et.al (2018), with slight adjustments, to empirically identify social stratification patterns using cross sectional household surveys available in the indian context. we use ihds-2 data-set for the purpose of our study.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["vinay reddy venumuddala"], "url": "https://arxiv.org/abs/2005.06802"}, {"title": "$alpha-$ robust equilibrium in anonymous games", "id": "2005.06812", "abstract": "in this paper, we consider the notion of $\\alpha-$ robust equilibrium for finite strategic players in anonymous games, where utility of a player depends on other players' actions only through the resulting distribution of actions played. this equilibrium is defined as the set of strategies of the players such that no user wants to deviate as long as $n-\\alpha-1$ number are playing the equilibrium strategies. we provide sufficient conditions for the existence of this equilibrium. in addition, we prove a part of berge's maximal theorem for correspondences.", "categories": "econ.th cs.gt", "created": "2020-05-14", "updated": "", "authors": ["deepanshu vasal", "randall berry"], "url": "https://arxiv.org/abs/2005.06812"}, {"title": "do ads harm news consumption?", "id": "2005.06840", "abstract": "many online news publishers finance their websites by displaying ads alongside content. yet, remarkably little is known about how exposure to such ads impacts users' news consumption. we examine this question using 3.1 million anonymized browsing sessions from 79,856 users on a news website and the quasi-random variation created by ad blocker adoption. we find that seeing ads has a robust negative effect on the quantity and variety of news consumption: users who adopt ad blockers subsequently consume 20% more news articles corresponding to 10% more categories. the effect persists over time and is largely driven by consumption of \"hard\" news. the effect is primarily attributable to a learning mechanism, wherein users gain positive experience with the ad-free site; a cognitive mechanism, wherein ads impede processing of content, also plays a role. our findings open an important discussion on the suitability of advertising as a monetization model for valuable digital content.", "categories": "econ.gn q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["shunyao yan", "klaus m. miller", "bernd skiera"], "url": "https://arxiv.org/abs/2005.06840"}, {"title": "dynamic shrinkage in time-varying parameter stochastic volatility in   mean models", "id": "2005.06851", "abstract": "successful forecasting models strike a balance between parsimony and flexibility. this is often achieved by employing suitable shrinkage priors that penalize model complexity but also reward model fit. in this note, we modify the stochastic volatility in mean (svm) model proposed in chan (2017) by introducing state-of-the-art shrinkage techniques that allow for time-variation in the degree of shrinkage. using a real-time inflation forecast exercise, we show that employing more flexible prior distributions on several key parameters slightly improves forecast performance for the united states (us), the united kingdom (uk) and the euro area (ea). comparing in-sample results reveals that our proposed model yields qualitatively similar insights to the original version of the model.", "categories": "econ.em stat.ap", "created": "2020-05-14", "updated": "", "authors": ["florian huber", "michael pfarrhofer"], "url": "https://arxiv.org/abs/2005.06851"}, {"title": "fractional top trading cycle", "id": "2005.06878", "abstract": "efficiency and fairness are two desiderata in market design. fairness requires randomization in many environments. as one of the few successful matching mechanisms, top trading cycle (ttc) is well known for being efficient to solve deterministic allocation problems. but it is inadequate to incorporate randomization efficiently and fairly. we propose a class of fractional ttc mechanisms to solve random allocation problems efficiently and fairly. dropping the graph-based definition of ttc, we use parameterized linear equations to describe how agents trade endowments or priorities at each step. our mechanisms are ex-ante efficient. they satisfy various fairness axioms when parameters are properly chosen. we apply them to a couple of market design problems and obtain efficient and fair assignments in all of them.", "categories": "econ.th", "created": "2020-05-14", "updated": "2020-05-22", "authors": ["jingsheng yu", "jun zhang"], "url": "https://arxiv.org/abs/2005.06878"}, {"title": "existence and uniqueness of recursive utility models in $l_p$", "id": "2005.07067", "abstract": "recursive preferences, of the sort developed by epstein and zin (1989), play an integral role in modern macroeconomics and asset pricing theory. unfortunately, it is non-trivial to establish the unique existence of a solution to recursive utility models. we show that the tightest known existence and uniqueness conditions can be extended to (i) schorfheide, song and yaron (2018) recursive utilities and (ii) recursive utilities with `narrow framing'. further, we sharpen the solution space of borovicka and stachurski (2019) from $l_1$ to $l_p$ so that the results apply to a broader class of modern asset pricing models. for example, using $l_2$ hilbert space theory, we find the class of parameters which generate a unique $l_2$ solution to the bansal and yaron (2004) and schorfheide, song and yaron (2018) models.", "categories": "econ.th math.pr", "created": "2020-05-13", "updated": "", "authors": ["flint o'neil"], "url": "https://arxiv.org/abs/2005.07067"}, {"title": "approval-based shortlisting", "id": "2005.07094", "abstract": "shortlisting is the task of reducing a long list of alternatives to a (smaller) set of best or most suitable alternatives from which a final winner will be chosen. shortlisting is often used in the nomination process of awards or in recommender systems to display featured objects. in this paper, we analyze shortlisting methods that are based on approval data, a common type of preferences. furthermore, we assume that the size of the shortlist, i.e., the number of best or most suitable alternatives, is not fixed but determined by the shortlisting method. we axiomatically analyze established and new shortlisting methods and complement this analysis with an experimental evaluation based on biased voters and noisy quality estimates. our results lead to recommendations which shortlisting methods to use, depending on the desired properties.", "categories": "cs.gt econ.th", "created": "2020-05-14", "updated": "", "authors": ["martin lackner", "jan maly"], "url": "https://arxiv.org/abs/2005.07094"}, {"title": "information design for congested social services: optimal need-based   persuasion", "id": "2005.07253", "abstract": "we study the effectiveness of information design in reducing congestion in social services catering to users with varied levels of need. in the absence of price discrimination and centralized admission, the provider relies on sharing information about wait times to improve welfare. we consider a stylized model with heterogeneous users who differ in their private outside options: low-need users have an acceptable outside option to the social service, whereas high-need users have no viable outside option. upon arrival, a user decides to wait for the service by joining an unobservable first-come-first-serve queue, or leave and seek her outside option. to reduce congestion and improve social outcomes, the service provider seeks to persuade more low-need users to avail their outside option, and thus better serve high-need users. we characterize the pareto-optimal signaling mechanisms and compare their welfare outcomes against several benchmarks. we show that if either type is the overwhelming majority of the population, information design does not provide improvement over sharing full information or no information. on the other hand, when the population is a mixture of the two types, information design not only pareto dominates full-information and no-information mechanisms, in some regimes it also achieves the same welfare as the \"first-best\", i.e., the pareto-optimal centralized admission policy with knowledge of users' types.", "categories": "cs.gt econ.th", "created": "2020-05-14", "updated": "2020-08-12", "authors": ["jerry anunrojwong", "krishnamurthy iyer", "vahideh manshadi"], "url": "https://arxiv.org/abs/2005.07253"}, {"title": "dynamic information design", "id": "2005.07267", "abstract": "we consider the problem of dynamic information design with one sender and one receiver where the sender observers a private state of the system and takes an action to send a signal based on its observation to a receiver. based on this signal, the receiver takes an action that determines rewards for both the sender and the receiver and controls the state of the system. in this technical note, we show that this problem can be considered as a problem of dynamic game of asymmetric information and its perfect bayesian equilibrium (pbe) and stackelberg equilibrium (se) can be analyzed using the algorithms presented in [1], [2] by the same author (among others). we then extend this model when there is one sender and multiple receivers and provide algorithms to compute a class of equilibria of this game.", "categories": "econ.th cs.gt cs.sy eess.sy", "created": "2020-05-13", "updated": "", "authors": ["deepanshu vasal"], "url": "https://arxiv.org/abs/2005.07267"}, {"title": "mercury-related health benefits from retrofitting coal-fired power   plants in china", "id": "2005.07346", "abstract": "china has implemented retrofitting measures in coal-fired power plants (cfpps) to reduce air pollution through small unit shutdown (sus), the installation of air pollution control devices (apcds) and power generation efficiency (pge) improvement. the reductions in highly toxic hg emissions and their related health impacts by these measures have not been well studied. to refine mitigation options, we evaluated the health benefits of reduced hg emissions via retrofitting measures during china's 12th five-year plan by combining plant-level hg emission inventories with the china hg risk source-tracking model. we found that the measures reduced hg emissions by 23.5 tons (approximately 1/5 of that from cfpps in 2010), preventing 0.0021 points of per-foetus intelligence quotient (iq) decrements and 114 deaths from fatal heart attacks. these benefits were dominated by cfpp shutdowns and apcd installations. provincial health benefits were largely attributable to hg reductions in other regions. we also demonstrated the necessity of considering human health impacts, rather than just hg emission reductions, in selecting hg control devices. this study also suggests that hg control strategies should consider various factors, such as cfpp locations, population densities and trade-offs between reductions of total hg (thg) and hg2+.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["jiashuo li", "sili zhou", "wendong wei", "jianchuan qi", "yumeng li", "bin chen", "ning zhang", "dabo guan", "haoqi qian", "xiaohui wu", "jiawen miao", "long chen", "sai liang", "kuishuang feng"], "url": "https://arxiv.org/abs/2005.07346"}, {"title": "fast and accurate variational inference for models with many latent   variables", "id": "2005.07430", "abstract": "models with a large number of latent variables are often used to fully utilize the information in big or complex data. however, they can be difficult to estimate using standard approaches, and variational inference methods are a popular alternative. key to the success of these is the selection of an approximation to the target density that is accurate, tractable and fast to calibrate using optimization methods. mean field or structured gaussian approximations are common, but these can be inaccurate and slow to calibrate when there are many latent variables. instead, we propose a family of tractable variational approximations that are more accurate and faster to calibrate for this case. the approximation is a parsimonious copula model for the parameter posterior, combined with the exact conditional posterior of the latent variables. we derive a simplified expression for the re-parameterization gradient of the variational lower bound, which is the main ingredient of efficient optimization algorithms used to implement variational estimation. we illustrate using two substantive econometric examples. the first is a nonlinear state space model for u.s. inflation. the second is a random coefficients tobit model applied to a rich marketing dataset with one million sales observations from a panel of 10,000 individuals. in both cases, we show that our approximating family is faster to calibrate than either mean field or structured gaussian approximations, and that the gains in posterior estimation accuracy are considerable.", "categories": "stat.me econ.em", "created": "2020-05-15", "updated": "", "authors": ["rub\u00e9n loaiza-maya", "michael stanley smith", "david j. nott", "peter j. danaher"], "url": "https://arxiv.org/abs/2005.07430"}, {"title": "exploring weak strategy-proofness in voting theory", "id": "2005.07521", "abstract": "voting is the aggregation of individual preferences in order to select a winning alternative. selection of a winner is accomplished via a voting rule, e.g., rank-order voting, majority rule, plurality rule, approval voting. which voting rule should be used? in social choice theory, desirable properties of voting rules are expressed as axioms to be satisfied. this thesis focuses on axioms concerning strategic manipulation by voters. sometimes, voters may intentionally misstate their true preferences in order to alter the outcome for their own advantage. for example, in plurality rule, if a voter knows that their top-choice candidate will lose, then they might instead vote for their second-choice candidate just to avoid an even less desirable result. when no coalition of voters can strategically manipulate, then the voting rule is said to satisfy the axiom of strategy-proofness. a less restrictive axiom is weak strategy-proofness (as defined by dasgupta and maskin (2019)), which allows for strategic manipulation by all but the smallest coalitions. under certain intuitive conditions, dasgupta and maskin (2019) proved that the only voting rules satisfying strategy-proofness are rank-order voting and majority rule. in my thesis, i generalize their result, by proving that rank-order voting and majority rule are surprisingly still the only voting rules satisfying weak strategy-proofness.", "categories": "econ.th", "created": "2020-05-13", "updated": "", "authors": ["anne carlstein"], "url": "https://arxiv.org/abs/2005.07521"}, {"title": "farmers' situation in agriculture markets and role of public   interventions in india", "id": "2005.07538", "abstract": "in our country, majority of agricultural workers (who may include farmers working within a cooperative framework, or those who work individually either as owners or tenants) are shown to be reaping the least amount of profits in the agriculture value chain when compared to the effort they put in. there is a good amount of literature which broadly substantiates this situation in our country. main objective of this study is to have a broad understanding of the role played by public systems in this value chain, particularly in the segment that interacts with farmers. as a starting point, we first try to get a better understanding of how farmers are placed in a typical agriculture value chain. for this we take the help of recent seminal works on this topic that captured the situation of farmers' within certain types of value chains. then, we isolate the segment which interacts with farmers and deep-dive into data to understand the role played by public interventions in determining farmers' income from agriculture. nsso 70th round on situation assessment survey of farmers has data pertaining to the choices of farmers and the type of their interaction with different players in the value chain. using this data we tried to get a econometric picture of the role played by government interventions and the extent to which they determine the incomes that a typical farming household derives out of agriculture.", "categories": "econ.gn q-fin.ec", "created": "2020-05-14", "updated": "", "authors": ["vinay reddy venumuddala"], "url": "https://arxiv.org/abs/2005.07538"}, {"title": "optimal trade-off between economic activity and health during an   epidemic", "id": "2005.07590", "abstract": "this paper considers a simple model where a social planner can influence the spread-intensity of an infection wave, and, consequently, also the economic activity and population health, through a single parameter. population health is assumed to only be negatively affected when the number of simultaneously infected exceeds health care capacity. the main finding is that if (i) the planner attaches a positive weight on economic activity and (ii) it is more harmful for the economy to be locked down for longer than shorter time periods, then the optimal policy is to (weakly) exceed health care capacity at some time.", "categories": "econ.th", "created": "2020-05-15", "updated": "", "authors": ["tommy andersson", "albin erlanson", "daniel spiro", "robert \u00f6stling"], "url": "https://arxiv.org/abs/2005.07590"}, {"title": "conformal prediction: a unified review of theory and new challenges", "id": "2005.07972", "abstract": "in this work we provide a review of basic ideas and novel developments about conformal prediction -- an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions -- that is able to yield in a very straightforward way predictions sets that are valid in a statistical sense also in in the finite sample case. the in-depth discussion provided in the paper covers the theoretical underpinnings of conformal prediction, and then proceeds to list the more advanced developments and adaptations of the original idea.", "categories": "cs.lg econ.em stat.me stat.ml", "created": "2020-05-16", "updated": "", "authors": ["gianluca zeni", "matteo fontana", "simone vantini"], "url": "https://arxiv.org/abs/2005.07972"}, {"title": "funding public projects: a case for the nash product rule", "id": "2005.07997", "abstract": "we study a mechanism design problem where a community of agents wishes to fund public projects via voluntary monetary contributions by the community members. this serves as a model for participatory budgeting without an exogenously available budget, as well as donor coordination when interpreting charities as public projects and donations as contributions. our aim is to identify a mutually beneficial distribution of the individual contributions. in the preference aggregation problem that we study, agents report linear utility functions over projects together with the amount of their contributions, and the mechanism determines a socially optimal distribution of the money. we identify a specific mechanism---the nash product rule---which picks the distribution that maximizes the product of the agents' utilities. this rule is pareto efficient, and we prove that it satisfies attractive incentive properties: the nash rule spends an agent's contribution only on projects the agent finds acceptable, and it provides strong participation incentives. we also discuss issues of strategyproofness and monotonicity.", "categories": "cs.gt econ.th", "created": "2020-05-16", "updated": "", "authors": ["florian brandl", "felix brandt", "dominik peters", "christian stricker", "warut suksompong"], "url": "https://arxiv.org/abs/2005.07997"}, {"title": "nested model averaging on solution path for high-dimensional linear   regression", "id": "2005.08057", "abstract": "we study the nested model averaging method on the solution path for a high-dimensional linear regression problem. in particular, we propose to combine model averaging with regularized estimators (e.g., lasso and slope) on the solution path for high-dimensional linear regression. in simulation studies, we first conduct a systematic investigation on the impact of predictor ordering on the behavior of nested model averaging, then show that nested model averaging with lasso and slope compares favorably with other competing methods, including the infeasible lasso and slope with the tuning parameter optimally selected. a real data analysis on predicting the per capita violent crime in the united states shows an outstanding performance of the nested model averaging with lasso.", "categories": "stat.me cs.lg econ.em stat.co stat.ml", "created": "2020-05-16", "updated": "", "authors": ["yang feng", "qingfeng liu"], "url": "https://arxiv.org/abs/2005.08057"}, {"title": "how sustainable environments have reduced the diffusion of coronavirus   disease 2019: the interaction between spread of covid-19 infection, polluting   industrialization, wind (renewable) energy", "id": "2005.08293", "abstract": "this study endeavors to explain the relation between air pollution and particulate compounds emissions, wind resources and energy, and the diffusion of covid-19 infection to provide insights of sustainable policy to prevent future epidemics. the statistical analysis here focuses on case study of italy, one of the countries to experience a rapid increase in confirmed cases and deaths. results reveal two main findings: 1) cities in regions with high wind speed and a high wind energy production in mw have a lower number of infected individuals of covid-19 infection and total deaths; 2) cities located in hinterland zones (mostly those bordering large urban conurbations) with high polluting industrialization, low wind speed and less cleaner production have a greater number of infected individuals and total deaths. hence, cities with pollution industrialization and low renewable energy have also to consider low wind speed and other climatological factors that can increase stagnation of the air in the atmosphere with potential problems for public health in the presence of viral agents. results here suggest that current pandemic of coronavirus disease and future epidemics similar to covid-19 infection cannot be solved only with research and practice of medicine, immunology and microbiology but also with a proactive strategy directed to interventions for a sustainable development. overall, then, this study has to conclude that a strategy to prevent future epidemics similar to covid-19 infection must also be based on sustainability science to support a higher level of renewable energy and cleaner production to reduce polluting industrialization and, as result, the factors determining the spread of coronavirus disease and other infections in society.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-05-17", "updated": "", "authors": ["mario coccia"], "url": "https://arxiv.org/abs/2005.08293"}, {"title": "the natural capital indicator framework (ncif): a framework of   indicators for national natural capital reporting", "id": "2005.08568", "abstract": "it is now widely recognised that components of the environment play the role of economic assets, termed natural capital, that are a foundation of social and economic development. national governments monitor the state and trends of natural capital through a range of activities including natural capital accounting, national ecosystem assessments, ecosystem service valuation, and economic and environmental analyses. indicators play an integral role in these activities as they facilitate the reporting of complex natural capital information. one factor that hinders the success of these activities and their comparability across countries is the absence of a coherent framework of indicators concerning natural capital (and its benefits) that can aid decision-making. here we present an integrated natural capital indicator framework (ncif) alongside example indicators, which provides an illustrative structure for countries to select and organise indicators to assess their use of and dependence on natural capital. the ncif sits within a wider context of indicators related to natural, human, social and manufactured capital, and associated flows of benefits. the framework provides decision-makers with a structured approach to selecting natural capital indicators with which to make decisions about economic development that take into account national natural capital and associated flows of benefits.", "categories": "econ.gn q-fin.ec", "created": "2020-05-18", "updated": "", "authors": ["alison fairbrass", "georgina mace", "paul ekins", "ben milligan"], "url": "https://arxiv.org/abs/2005.08568"}, {"title": "two-sided random matching markets: ex-ante equivalence of the deferred   acceptance procedures", "id": "2005.08584", "abstract": "stable matching in a community consisting of $n$ men and $n$ women is a classical combinatorial problem that has been the subject of intense theoretical and empirical study since its introduction in 1962 in a seminal paper by gale and shapley.   when the input preference profile is generated from a distribution, we study the output distribution of two stable matching procedures: women-proposing-deferred-acceptance and men-proposing-deferred-acceptance. we show that the two procedures are ex-ante equivalent: that is, under certain conditions on the input distribution, their output distributions are identical.   in terms of technical contributions, we generalize (to the non-uniform case) an integral formula, due to knuth and pittel, which gives the probability that a fixed matching is stable. using an inclusion-exclusion principle on the set of rotations, we give a new formula which gives the probability that a fixed matching is the women/men-optimal stable matching. we show that those two probabilities are equal with an integration by substitution.", "categories": "cs.gt cs.dm cs.ds econ.th", "created": "2020-05-18", "updated": "", "authors": ["simon mauras"], "url": "https://arxiv.org/abs/2005.08584"}, {"title": "irregular identification of structural models with nonparametric   unobserved heterogeneity", "id": "2005.08611", "abstract": "one of the most important empirical findings in microeconometrics is the pervasiveness of heterogeneity in economic behaviour (cf. heckman 2001). this paper shows that cumulative distribution functions and quantiles of the nonparametric unobserved heterogeneity have an infinite efficiency bound in many structural economic models of interest. the paper presents a relatively simple check of this fact. the usefulness of the theory is demonstrated with several relevant examples in economics, including, among others, the proportion of individuals with severe long term unemployment duration, the average marginal effect and the proportion of individuals with a positive marginal effect in a correlated random coefficient model with heterogenous first-stage effects, and the distribution and quantiles of random coefficients in linear, binary and the mixed logit models. monte carlo simulations illustrate the finite sample implications of our findings for the distribution and quantiles of the random coefficients in the mixed logit model.", "categories": "econ.em", "created": "2020-05-18", "updated": "", "authors": ["juan carlos escanciano"], "url": "https://arxiv.org/abs/2005.08611"}, {"title": "application of nonlinear autoregressive with exogenous input (narx)   neural network in macroeconomic forecasting, national goal setting and global   competitiveness assessment", "id": "2005.08735", "abstract": "this paper selects the narx neural network as the method through literature review, and constructs specific narx neural networks under application scenarios involving macroeconomic forecasting, national goal setting and global competitiveness assessment. through case studies on china, us and eurozone, this study explores how those limited & partial exogenous inputs or abundant & comprehensive exogenous inputs, a small set of most relevant exogenous inputs or a large set of exogenous inputs covering all major aspects of the macro economy, whole area related exogenous inputs or both whole area and subdivision area related exogenous inputs specifically affect the forecasting performance of narx neural networks for specific macroeconomic indicators or indices. through the case study on russia this paper explores how the limited & most relevant exogenous inputs set or the abundant & comprehensive exogenous inputs set specifically influences the prediction performance of those specific narx neural networks for national goal setting. finally, comparative studies on the application of narx neural networks for the forecasts of global competitiveness indices (gcis) of various economies are conducted, in order to explore whether the specific narx neural network trained on the basis of the gci related data of some economies can make sufficiently accurate predictions about gcis of other economies, and whether the specific narx neural network trained on the basis of the data of some type of economies can give more accurate predictions about gcis of the same type of economies than those of different type of economies. based on all of the above successful application, this paper provides policy recommendations on applying fully trained narx neural networks that are assessed as qualified to assist or even replace the deductive and inductive abilities of the human brain in a variety of appropriate tasks.", "categories": "physics.soc-ph econ.gn q-fin.ec q-fin.gn", "created": "2020-05-15", "updated": "", "authors": ["liyang tang"], "url": "https://arxiv.org/abs/2005.08735"}, {"title": "social inequality measures: the kolkata index in comparison with other   measures", "id": "2005.08762", "abstract": "we provide a survey of the kolkata index of social inequality, focusing in particular on income inequality. we look at both continuous and discrete income distributions. we also compare the kolkata index to some other measures like the gini coefficient, hirsch index and the pietra index. lastly, we provide some empirical studies which illustrate the differences between the kolkata index and the gini coefficient.", "categories": "econ.gn q-fin.ec", "created": "2020-05-15", "updated": "", "authors": ["suchismita banerjee", "bikas k. chakrabarti", "manipushpak mitra", "suresh mutuswami"], "url": "https://arxiv.org/abs/2005.08762"}, {"title": "the distributional short-term impact of the covid-19 crisis on wages in   the united states", "id": "2005.08763", "abstract": "this paper uses bureau of labor statistics employment and wage data to study the distributional impact of the covid-19 crisis on wages in the united states by mid-april. it answers whether wages of lower-wage workers decreased more than others', and to what extent. we find that the covid-19 outbreak exacerbates existing inequalities. workers at the bottom quintile in mid-march were three times more likely to be laid off by mid-april compared to higher-wage workers. weekly wages of workers at the bottom quintile decreased by 6% on average between mid-february and mid-march and by 26% between mid-march and mid-april. the average decrease for higher quintiles was less than 1% between mid-february and mid-march and about 10% between mid-march and mid-april. we also find that workers aged 16-24 were hit much harder than older workers. hispanic workers were also hurt more than other racial groups. their wages decreased by 2-3 percentage points more than other workers' between mid-march and mid-april.", "categories": "econ.gn q-fin.ec", "created": "2020-05-18", "updated": "", "authors": ["yonatan berman"], "url": "https://arxiv.org/abs/2005.08763"}, {"title": "disaster resilience and asset prices", "id": "2005.08929", "abstract": "this paper investigates whether security markets price the effect of social distancing on firms' operations. we document that firms that are more resilient to social distancing significantly outperformed those with lower resilience during the covid-19 outbreak, even after controlling for the standard risk factors. similar cross-sectional return differentials already emerged before the covid-19 crisis: the 2014-19 cumulative return differential between more and less resilient firms is of similar size as during the outbreak, suggesting growing awareness of pandemic risk well in advance of its materialization. finally, we use stock option prices to infer the market's return expectations after the onset of the pandemic: even at a two-year horizon, stocks of more pandemic-resilient firms are expected to yield significantly lower returns than less resilient ones, reflecting their lower exposure to disaster risk. hence, going forward, markets appear to price exposure to a new risk factor, namely, pandemic risk.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.rm", "created": "2020-05-18", "updated": "2020-05-19", "authors": ["marco pagano", "christian wagner", "josef zechner"], "url": "https://arxiv.org/abs/2005.08929"}, {"title": "patterns in demand side financial inclusion in india -- an inquiry using   ihds panel data", "id": "2005.08961", "abstract": "in the following study, we inquire into the financial inclusion from a demand side perspective. utilizing ihds round-1 (2004-05) and round-2 (2011-12), starting from a broad picture of demand side access to finance at the country level, we venture into analysing the patterns at state level, and then lastly at district level. particularly at district level, we focus on agriculture households in rural areas to identify if there is a shift in the demand side financial access towards non-agriculture households in certain parts of the country. in order to do this, we use district level 'basic statistical returns of scheduled commercial banks' for the years 2004 and 2011, made available by rbi, to first construct supply side financial inclusion indices, and then infer about a relative shift in access to formal finance away from agriculture households, using a logistic regression framework.", "categories": "econ.gn q-fin.ec", "created": "2020-05-17", "updated": "", "authors": ["vinay reddy venumuddala"], "url": "https://arxiv.org/abs/2005.08961"}, {"title": "revealing gender-specific costs of stem in an extended roy model of   major choice", "id": "2005.09095", "abstract": "we derive sharp bounds on the non consumption utility component in an extended roy model of sector selection. we interpret this non consumption utility component as a compensating wage differential. the bounds are derived under the assumption that potential wages in each sector are (jointly) stochastically monotone with respect to an observed selection shifter. the lower bound can also be interpreted as the minimum cost subsidy necessary to change sector choices and make them observationally indistinguishable from choices made under the classical roy model of sorting on potential wages only. the research is motivated by the analysis of women's choice of university major and their underrepresentation in mathematics intensive fields. with data from a german graduate survey, and using the proportion of women on the stem faculty at the time of major choice as our selection shifter, we find high costs of choosing the stem sector for women from the former west germany, especially for low realized incomes and low proportion of women on the stem faculty, interpreted as a scarce presence of role models.", "categories": "econ.em", "created": "2020-05-18", "updated": "2020-06-02", "authors": ["marc henry", "romuald meango", "ismael mourifie"], "url": "https://arxiv.org/abs/2005.09095"}, {"title": "the effects of smartphones on well-being: theoretical integration and   research agenda", "id": "2005.09100", "abstract": "as smartphones become ever more integrated in peoples lives, a burgeoning new area of research has emerged on their well-being effects. we propose that disparate strands of research and apparently contradictory findings can be integrated under three basic hypotheses, positing that smartphones influence well-being by (1) replacing other activities (displacement hypothesis), (2) interfering with concurrent activities (interference hypothesis), and (3) affording access to information and activities that would otherwise be unavailable (complementarity hypothesis). using this framework, we highlight methodological issues and go beyond net effects to examine how and when phones boost versus hurt well-being. we examine both psychological and contextual mediators and moderators of the effects, thus outlining an agenda for future research.", "categories": "cs.hc cs.cy cs.it cs.mm econ.gn math.it q-fin.ec", "created": "2020-05-18", "updated": "", "authors": ["kostadin kushlev", "matthew r leitao"], "url": "https://arxiv.org/abs/2005.09100"}, {"title": "is being an only child harmful to psychological health?: evidence from   an instrumental variable analysis of china's one-child policy", "id": "2005.09130", "abstract": "this paper evaluates the effects of being an only child in a family on psychological health, leveraging data on the one-child policy in china. we use an instrumental variable approach to address the potential unmeasured confounding between the fertility decision and psychological health, where the instrumental variable is an index on the intensity of the implementation of the one-child policy. we establish an analytical link between the local instrumental variable approach and principal stratification to accommodate the continuous instrumental variable. within the principal stratification framework, we postulate a bayesian hierarchical model to infer various causal estimands of policy interest while adjusting for the clustering data structure. we apply the method to the data from the china family panel studies and find small but statistically significant negative effects of being an only child on self-reported psychological health for some subpopulations. our analysis reveals treatment effect heterogeneity with respect to both observed and unobserved characteristics. in particular, urban males suffer the most from being only children, and the negative effect has larger magnitude if the families were more resistant to the one-child policy. we also conduct sensitivity analysis to assess the key instrumental variable assumption.", "categories": "stat.ap econ.em", "created": "2020-05-18", "updated": "2020-06-11", "authors": ["shuxi zeng", "fan li", "peng ding"], "url": "https://arxiv.org/abs/2005.09130"}, {"title": "a flexible stochastic conditional duration model", "id": "2005.09166", "abstract": "we introduce a new stochastic duration model for transaction times in asset markets. we argue that widely accepted rules for aggregating seemingly related trades mislead inference pertaining to durations between unrelated trades: while any two trades executed in the same second are probably related, it is extremely unlikely that all such pairs of trades are, in a typical sample. by placing uncertainty about which trades are related within our model, we improve inference for the distribution of durations between unrelated trades, especially near zero. we introduce a normalized conditional distribution for durations between unrelated trades that is both flexible and amenable to shrinkage towards an exponential distribution, which we argue is an appropriate first-order model. thanks to highly efficient draws of state variables, numerical efficiency of posterior simulation is much higher than in previous studies. in an empirical application, we find that the conditional hazard function for durations between unrelated trades varies much less than what most studies find. we claim that this is because we avoid statistical artifacts that arise from deterministic trade-aggregation rules and unsuitable parametric distributions.", "categories": "econ.em q-fin.st stat.me", "created": "2020-05-18", "updated": "", "authors": ["samuel gingras", "william j. mccausland"], "url": "https://arxiv.org/abs/2005.09166"}, {"title": "fractional top trading cycle on the full preference domain", "id": "2005.09340", "abstract": "efficiency and fairness are two desiderata in market design. fairness requires randomization in many environments. observing the inadequacy of top trading cycle (ttc) to incorporate randomization, yu and zhang (2020) propose the class of fractional ttc mechanisms to solve random allocation problems efficiently and fairly. the assumption of strict preferences in the paper restricts the application scope. this paper extends fractional ttc to the full preference domain in which agents can be indifferent between objects. efficiency and fairness of fractional ttc are preserved. as a corollary, we obtain an extension of the probabilistic serial mechanism in the house allocation model to the full preference domain. our extension does not require any knowledge beyond elementary computation.", "categories": "econ.th", "created": "2020-05-19", "updated": "", "authors": ["jingsheng yu", "jun zhang"], "url": "https://arxiv.org/abs/2005.09340"}, {"title": "coalition and core in resource allocation and exchange", "id": "2005.09351", "abstract": "in discrete exchange economies with possibly redundant and joint ownership, the conventional strong core may be empty, while the weak core may include unintuitive outcomes. we propose new core notions in the conventional flavor by regarding endowments as rights to consume or trade with others. our key idea is to identify self-enforcing coalitions and to redistribute their redundant property rights. our first notion lies between the strong core and the weak core and is independent of balbuzanov and kotowski's (2019) exclusion core. our second notion refines the first and the exclusion core by combining their different merits. we generalize the you request my house - i get your turn mechanism to find our core allocations.", "categories": "econ.th cs.gt", "created": "2020-05-19", "updated": "", "authors": ["jun zhang"], "url": "https://arxiv.org/abs/2005.09351"}, {"title": "instrumental variables with treatment-induced selection: exact bias   results", "id": "2005.09583", "abstract": "instrumental variables (iv) estimation suffers selection bias when the analysis conditions on the treatment. judea pearl's early graphical definition of instrumental variables explicitly prohibited conditioning on the treatment. nonetheless, the practice remains common. in this paper, we derive exact analytic expressions for iv selection bias across a range of data-generating models, and for various selection-inducing procedures. we present four sets of results for linear models. first, iv selection bias depends on the conditioning procedure (covariate adjustment vs. sample truncation). second, iv selection bias due to covariate adjustment is the limiting case of iv selection bias due to sample truncation. third, in certain models, the iv and ols estimators under selection bound the true causal effect in large samples. fourth, we characterize situations where iv remains preferred to ols despite selection on the treatment. these results broaden the notion of iv selection bias beyond sample truncation, replace prior simulation findings with exact analytic formulas, and enable formal sensitivity analyses.", "categories": "econ.em stat.me", "created": "2020-05-19", "updated": "", "authors": ["felix elwert", "elan segarra"], "url": "https://arxiv.org/abs/2005.09583"}, {"title": "understanding the effects of tennessee's open covid-19 testing policy:   bounding policy effects with nonrandomly missing data", "id": "2005.09605", "abstract": "increased testing for covid-19 is seen as one of the most important steps to re-open the economy. the current paper considers tennessee's \"open-testing\" policy where the state substantially increased testing while removing symptom requirements for individuals to be tested. to understand the effect of the policy, we combine standard identifying assumptions with additional weak assumptions to deal with non-random testing that lead to bounds on policy effects of interest. our results suggest that tennessee's open-testing policy has reduced total cases (which are not fully observed), confirmed cases, and trips to work among counties with a fast-growing number of confirmed cases.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-05-19", "updated": "2020-06-08", "authors": ["brantly callaway", "tong li"], "url": "https://arxiv.org/abs/2005.09605"}, {"title": "treatment recommendation with distributional targets", "id": "2005.09717", "abstract": "we study the problem of a decision maker who must provide the best possible treatment recommendation based on an experiment. the desirability of the outcome distribution resulting from the policy recommendation is measured through a functional capturing the distributional characteristic that the decision maker is interested in optimizing. this could be, e.g., its inherent inequality, welfare, level of poverty or its distance to a desired outcome distribution. if the functional of interest is not quasi-convex or if there are constraints, the optimal recommendation may be a mixture of treatments. this vastly expands the set of recommendations that must be considered. we characterize the difficulty of the problem by obtaining maximal expected regret lower bounds. furthermore, we propose two regret-optimal policies. the first policy is static and thus applicable irrespectively of the subjects arriving sequentially or not in the course of the experimental phase. the second policy can utilize that subjects arrive sequentially by successively eliminating inferior treatments and thus spends the sampling effort where it is most needed.", "categories": "econ.em math.st stat.ml stat.th", "created": "2020-05-19", "updated": "2020-05-27", "authors": ["anders bredahl kock", "david preinerstorfer", "bezirgen veliyev"], "url": "https://arxiv.org/abs/2005.09717"}, {"title": "computations and complexities of tarski's fixed points and supermodular   games", "id": "2005.09836", "abstract": "we consider two models of computation for tarski's order preserving function f related to fixed points in a complete lattice: the oracle function model and the polynomial function model. in both models, we find the first polynomial time algorithm for finding a tarski's fixed point. in addition, we provide a matching oracle bound for determining the uniqueness in the oracle function model and prove it is co-np hard in the polynomial function model. the existence of the pure nash equilibrium in supermodular games is proved by tarski's fixed point theorem. exploring the difference between supermodular games and tarski's fixed point, we also develop the computational results for finding one pure nash equilibrium and determining the uniqueness of the equilibrium in supermodular games.", "categories": "cs.gt cs.cc econ.th", "created": "2020-05-19", "updated": "", "authors": ["chuangyin dang", "qi qi", "yinyu ye"], "url": "https://arxiv.org/abs/2005.09836"}, {"title": "communication and cooperation in markets", "id": "2005.09839", "abstract": "many markets rely on traders truthfully communicating who has cheated in the past and ostracizing those traders from future trade. this paper investigates when truthful communication is incentive compatible. we find that if each side has a myopic incentive to deviate, then communication incentives are satisfied only when the volume of trade is low. by contrast, if only one side has a myopic incentive to deviate, then communication incentives do not constrain the volume of supportable trade. accordingly, there are strong gains from structuring trade so that one side either moves first or has its cooperation guaranteed by external enforcement.", "categories": "econ.th", "created": "2020-05-20", "updated": "", "authors": ["s. nageeb ali", "david a. miller"], "url": "https://arxiv.org/abs/2005.09839"}, {"title": "uniform rates for kernel estimators of weakly dependent data", "id": "2005.09951", "abstract": "this paper provides new uniform rate results for kernel estimators of absolutely regular stationary processes that are uniform in the bandwidth and in infinite-dimensional classes of dependent variables and regressors. our results are useful for establishing asymptotic theory for two-step semiparametric estimators in time series models. we apply our results to obtain nonparametric estimates and their rates for expected shortfall processes.", "categories": "econ.em", "created": "2020-05-20", "updated": "", "authors": ["juan carlos escanciano"], "url": "https://arxiv.org/abs/2005.09951"}, {"title": "stochastic modeling of assets and liabilities with mortality risk", "id": "2005.09974", "abstract": "this paper describes a general approach for stochastic modeling of assets returns and liability cash-flows of a typical pensions insurer. on the asset side, we model the investment returns on equities and various classes of fixed-income instruments including short- and long-maturity fixed-rate bonds as well as index-linked and corporate bonds. on the liability side, the risks are driven by future mortality developments as well as price and wage inflation. all the risk factors are modeled as a multivariate stochastic process that captures the dynamics and the dependencies across different risk factors. the model is easy to interpret and to calibrate to both historical data and to forecasts or expert views concerning the future. the simple structure of the model allows for efficient computations. the construction of a million scenarios takes only a few minutes on a personal computer. the approach is illustrated with an asset-liability analysis of a defined benefit pension fund.", "categories": "q-fin.rm econ.em math.oc", "created": "2020-05-20", "updated": "", "authors": ["sergio alvares maffra", "john armstrong", "teemu pennanen"], "url": "https://arxiv.org/abs/2005.09974"}, {"title": "artificial intelligence versus maya angelou: experimental evidence that   people cannot differentiate ai-generated from human-written poetry", "id": "2005.09980", "abstract": "the release of openly available, robust natural language generation algorithms (nlg) has spurred much public attention and debate. one reason lies in the algorithms' purported ability to generate human-like text across various domains. empirical evidence using incentivized tasks to assess whether people (a) can distinguish and (b) prefer algorithm-generated versus human-written text is lacking. we conducted two experiments assessing behavioral reactions to the state-of-the-art natural language generation algorithm gpt-2 (ntotal = 830). using the identical starting lines of human poems, gpt-2 produced samples of poems. from these samples, either a random poem was chosen (human-out-of-the-loop) or the best one was selected (human-in-the-loop) and in turn matched with a human-written poem. in a new incentivized version of the turing test, participants failed to reliably detect the algorithmically-generated poems in the human-in-the-loop treatment, yet succeeded in the human-out-of-the-loop treatment. further, people reveal a slight aversion to algorithm-generated poetry, independent on whether participants were informed about the algorithmic origin of the poem (transparency) or not (opacity). we discuss what these results convey about the performance of nlg algorithms to produce human-like text and propose methodologies to study such learning algorithms in human-agent experimental settings.", "categories": "cs.ai cs.cl econ.gn q-fin.ec", "created": "2020-05-20", "updated": "2020-09-08", "authors": ["nils k\u00f6bis", "luca mossink"], "url": "https://arxiv.org/abs/2005.09980"}, {"title": "coopetition against an amazon", "id": "2005.10038", "abstract": "this paper studies cooperative data-sharing between competitors vying to predict a consumer's tastes. we design optimal data-sharing schemes both for when they compete only with each other, and for when they additionally compete with an amazon---a company with more, better data. in both cases we show that participants benefit from such coopetition. we then apply the insights from our optimal schemes to more general settings.", "categories": "cs.gt econ.th", "created": "2020-05-20", "updated": "", "authors": ["ronen gradwohl", "moshe tennenholtz"], "url": "https://arxiv.org/abs/2005.10038"}, {"title": "on the nuisance of control variables in regression analysis", "id": "2005.10314", "abstract": "control variables are included in regression analyses to estimate the causal effect of a treatment on an outcome. in this note we argue that the estimated effect sizes of control variables are unlikely to have a causal interpretation themselves though. we therefore recommend to refrain from reporting marginal effects of controls in regression tables and instead to focus exclusively on the variables of interest in the results sections of empirical research papers.", "categories": "econ.em", "created": "2020-05-20", "updated": "2020-10-01", "authors": ["paul h\u00fcnermund", "beyers louw"], "url": "https://arxiv.org/abs/2005.10314"}, {"title": "production networks and epidemic spreading: how to restart the uk   economy?", "id": "2005.10585", "abstract": "we analyse the economics and epidemiology of different scenarios for a phased restart of the uk economy. our economic model is designed to address the unique features of the covid-19 pandemic. social distancing measures affect both supply and demand, and input-output constraints play a key role in restricting economic output. standard models for production functions are not adequate to model the short-term effects of lockdown. a survey of industry analysts conducted by ihs markit allows us to evaluate which inputs for each industry are absolutely necessary for production over a two month period. our model also includes inventory dynamics and feedback between unemployment and consumption. we demonstrate that economic outcomes are very sensitive to the choice of production function, show how supply constraints cause strong network effects, and find some counter-intuitive effects, such as that reopening only a few industries can actually lower aggregate output. occupation-specific data and contact surveys allow us to estimate how different industries affect the transmission rate of the disease. we investigate six different re-opening scenarios, presenting our best estimates for the increase in r0 and the increase in gdp. our results suggest that there is a reasonable compromise that yields a relatively small increase in r0 and delivers a substantial boost in economic output. this corresponds to a situation in which all non-consumer facing industries reopen, schools are open only for workers who need childcare, and everyone who can work from home continues to work from home.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-21", "updated": "", "authors": ["anton pichler", "marco pangallo", "r. maria del rio-chanona", "fran\u00e7ois lafond", "j. doyne farmer"], "url": "https://arxiv.org/abs/2005.10585"}, {"title": "sequential fundraising and social insurance", "id": "2005.10711", "abstract": "seed fundraising for ventures often takes place by sequentially approaching potential contributors, whose decisions are observed by other contributors. the fundraising succeeds when a target number of investments is reached. when a single investment suffices, this setting resembles the classic information cascades model. however, when more than one investment is needed, the solution is radically different and exhibits surprising complexities. we analyze a setting where contributors' levels of information are i.i.d. draws from a known distribution, and find strategies in equilibrium for all. we show that participants rely on {\\em social insurance}, i.e., invest despite having unfavorable private information, relying on future player strategies to protect them from loss. {\\em delegation} is an extreme form of social insurance where a contributor will unconditionally invest, effectively delegating the decision to future players. in a typical fundraising, early contributors will invest unconditionally, stopping when the target is \"close enough\", thus {\\em de facto} delegating the business of determining fundraising success or failure to the last contributors.", "categories": "cs.gt econ.th", "created": "2020-05-21", "updated": "", "authors": ["amir ban", "moran koren"], "url": "https://arxiv.org/abs/2005.10711"}, {"title": "commitment on volunteer crowdsourcing platforms: implications for growth   and engagement", "id": "2005.10731", "abstract": "volunteer crowdsourcing platforms, such as food recovery organizations, match volunteers with tasks which are often recurring. to ensure completion of such tasks, platforms frequently use a commitment lever known as \"adoption.\" despite being effective in reducing match uncertainty, high levels of adoption reduce match availability for volunteers which in turn can suppress future engagement. we study how platforms should balance these two opposing factors. our research is motivated by a collaboration with food rescue u.s. (frus), a volunteer-based food recovery organization active in over 33 locations across the u.s. for platforms such as frus, success crucially depends on efficient volunteer utilization and engagement. consequently, effectively utilizing non-monetary levers, such as adoption, is critical. based on our analysis of fine-grained frus data, we develop a model for a repeated two-sided matching market consisting of tasks (prearranged donations) and volunteers. our model incorporates the uncertainty in match compatibility as well as the negative impact of failing to match on future engagement. we study the platform's optimal policy for setting the adoption level to maximize the total discounted number of matches. our analysis reveals that the optimal myopic policy is either full or no adoption. for sufficiently thick markets, we show that such a myopic policy is also optimal in the long run. in thinner markets, even though a static policy of full or no adoption can be suboptimal, we show it achieves a constant-factor approximation where the factor improves with market thickness. using our analytical and empirical results, we revisit the current design of the frus platform and make location-specific policy recommendations. more broadly, our work sheds light on how other two-sided platforms can control the double-edged impacts that commitment levers have on growth and engagement.", "categories": "cs.gt econ.th", "created": "2020-05-21", "updated": "2020-09-18", "authors": ["irene lo", "vahideh manshadi", "scott rodilitz", "ali shameli"], "url": "https://arxiv.org/abs/2005.10731"}, {"title": "scanner data in inflation measurement: from raw data to price indices", "id": "2005.11233", "abstract": "scanner data offer new opportunities for cpi or hicp calculation. they can be obtained from a~wide variety of~retailers (supermarkets, home electronics, internet shops, etc.) and provide information at the level of~the barcode. one of~advantages of~using scanner data is the fact that they contain complete transaction information, i.e. prices and quantities for every sold item. to use scanner data, it must be carefully processed. after clearing data and unifying product names, products should be carefully classified (e.g. into coicop 5 or below), matched, filtered and aggregated. these procedures often require creating new it or writing custom scripts (r, python, mathematica, sas, others). one of~new challenges connected with scanner data is the appropriate choice of~the index formula. in this article we present a~proposal for the implementation of~individual stages of~handling scanner data. we also point out potential problems during scanner data processing and their solutions. finally, we compare a~large number of~price index methods based on real scanner datasets and we verify their sensitivity on adopted data filtering and aggregating methods.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2020-05-22", "updated": "", "authors": ["jacek bia\u0142ek", "maciej ber\u0119sewicz"], "url": "https://arxiv.org/abs/2005.11233"}, {"title": "cooperation in small groups -- an optimal transport approach", "id": "2005.11244", "abstract": "if agents cooperate only within small groups of some bounded sizes, is there a way to partition the population into small groups such that no collection of agents can do better by forming a new group? this paper revisited f-core in a transferable utility setting. by providing a new formulation to the problem, we built up a link between f-core and the transportation theory. such a link helps us to establish an exact existence result, and a characterization result of f-core for a general class of agents, as well as some improvements in computing the f-core in the finite type case.", "categories": "econ.th cs.gt math.oc", "created": "2020-05-22", "updated": "", "authors": ["xinyang wang"], "url": "https://arxiv.org/abs/2005.11244"}, {"title": "identifying key sectors in the regional economy: a network analysis   approach using input-output data", "id": "2005.11285", "abstract": "by applying network analysis techniques to large input-output system, we identify key sectors in the local/regional economy. we overcome the limitations of traditional measures of centrality by using random-walk based measures, as an extension of blochl et al. (2011). these are more appropriate to analyze very dense networks, i.e. those in which most nodes are connected to all other nodes. these measures also allow for the presence of recursive ties (loops), since these are common in economic systems (depending to the level of aggregation, most firms buy from and sell to other firms in the same industrial sector). the centrality measures we present are well suited for capturing sectoral effects missing from the usual output and employment multipliers. we also develop an r package (xtranat) for the processing of data from implan(r) models and for computing the newly developed measures.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-22", "updated": "", "authors": ["fernando depaolis", "phil murphy", "m. clara depaolis kaluza"], "url": "https://arxiv.org/abs/2005.11285"}, {"title": "a de-biased direct question approach to measuring consumers' willingness   to pay", "id": "2005.11318", "abstract": "knowledge of consumers' willingness to pay (wtp) is a prerequisite to profitable price-setting. to gauge consumers' wtp, practitioners often rely on a direct single question approach in which consumers are asked to explicitly state their wtp for a product. despite its popularity among practitioners, this approach has been found to suffer from hypothetical bias. in this paper, we propose a rigorous method that improves the accuracy of the direct single question approach. specifically, we systematically assess the hypothetical biases associated with the direct single question approach and explore ways to de-bias it. our results show that by using the de-biasing procedures we propose, we can generate a de-biased direct single question approach that is accu-rate enough to be useful for managerial decision-making. we validate this approach with two studies in this paper.", "categories": "econ.gn q-fin.ec", "created": "2020-05-22", "updated": "", "authors": ["reto hofstetter", "klaus m. miller", "harley krohmer", "z. john zhang"], "url": "https://arxiv.org/abs/2005.11318"}, {"title": "macroeconomic factors for inflation in argentine 2013-2019", "id": "2005.11455", "abstract": "the aim of this paper is to investigate the use of the factor analysis in order to identify the role of the relevant macroeconomic variables in driving the inflation. the macroeconomic predictors that usually affect the inflation are summarized using a small number of factors constructed by the principal components. this allows us to identify the crucial role of money growth, inflation expectation and exchange rate in driving the inflation. then we use this factors to build econometric models to forecast inflation. specifically, we use univariate and multivariate models such as classical autoregressive, factor models and favar models. results of forecasting suggest that models which incorporate more economic information outperform the benchmark. furthermore, causality test and impulse response are performed in order to examine the short-run dynamics of inflation to shocks in the principal factors.", "categories": "econ.em", "created": "2020-05-22", "updated": "", "authors": ["manuel lopez galvan"], "url": "https://arxiv.org/abs/2005.11455"}, {"title": "scenes from a monopoly: quickest detection of ecological regimes", "id": "2005.11500", "abstract": "we study the stochastic dynamics of a renewable resource harvested by a monopolist facing a downward sloping demand curve. we introduce a framework where harvesting sequentially affects the resource's potential to regenerate, resulting in an endogenous ecological regime shift. in a multi-period setting, the firm's objective is to find the profit-maximizing harvesting policy while simultaneously detecting in the quickest time possible the change in regime. encapsulating the idea of environmental surveillance, the use of quickest detection method allows us to easily translate our framework to real-time detection. solving analytically, we show that a negative regime shift induces an aggressive extraction behaviour due to a combination of faster detection, a sense of urgency, and higher markups. precautionary behaviour can result due to increasing resource rent. we study the probability of extinction and show the emergence of catastrophe risk which can be both reversible and irreversible.", "categories": "q-fin.ec econ.gn", "created": "2020-05-23", "updated": "2020-09-18", "authors": ["neha deopa", "daniele rinaldo"], "url": "https://arxiv.org/abs/2005.11500"}, {"title": "evaluation of banking sectors development in bangladesh in light of   financial reform", "id": "2005.11669", "abstract": "historically, the performance of the banking sector has been weak, characterized by weak asset quality, inadequate provisioning, and negative capitalization of state-owned banks. to overcome these problems, the initial phase of banking reform (1980-1990) focused on the promotion of private ownership and denationalization of nationalized commercial banks (scbs). during the second phase of reform, financial sector reform project (fsrp) of world bank was launched in 1990 with the focus on gradual deregulations of the interest rate structure, providing market-oriented incentives for priority sector lending and improvement in the debt recovery environment. moreover, a large number of private commercial banks were granted licenses during the second phase of reforms. bangladesh bank adopted basel-i norms in 1996 and basel-ii during 2010. moreover, the central bank strengthening project initiated in 2003 focused on effective regulatory and supervisory system, particularly strengthening the legal framework of banking sector. this study evaluates how successfully the banking sector of bangladesh has evolved over the past decades in light of financial reform measures undertaken to strengthen this sector.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-05-24", "updated": "", "authors": ["nusrat jahan", "k. m. golam muhiuddin"], "url": "https://arxiv.org/abs/2005.11669"}, {"title": "a constraint-satisfaction agent-based model for the macroeconomy", "id": "2005.11748", "abstract": "we introduce a prototype agent-based model of the macroeconomy, with a budgetary constraint at its core. the model is related to a class of constraint satisfaction problems, which has been thoroughly investigated in computer science. we identify three different regimes of our toy economy upon varying the amount of debt that each agent can accumulate before defaulting. in presence of a very loose constraint on debt, endogenous crises leading to waves of synchronized bankruptcies are present. in the opposite regime of very tight debt constraining, the bankruptcy rate is extremely high and the economy remains structure-less. in an intermediate regime, the economy is stable with very low bankruptcy rate and no aggregate-level crises. this third regime displays a rich phenomenology: the system spontaneously and dynamically self-organizes in a set of cheap and expensive goods (i.e. some kind of \"speciation\"), with switches triggered by random fluctuations and feedback loops. our analysis confirms the central role that debt levels play in the stability of the economy.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-24", "updated": "", "authors": ["dhruv sharma", "jean-philippe bouchaud", "marco tarzia", "francesco zamponi"], "url": "https://arxiv.org/abs/2005.11748"}, {"title": "bootstrap inference for quantile treatment effects in randomized   experiments with matched pairs", "id": "2005.11967", "abstract": "this paper examines methods of inference concerning quantile treatment effects (qtes) in randomized experiments with matched-pairs designs (mpds). we derive the limit distribution of the qte estimator under mpds, highlighting the difficulties that arise in analytical inference due to parameter tuning. we show that the naive weighted bootstrap fails to approximate the limit distribution of the qte estimator under mpds because it ignores the dependence structure within the matched pairs. to address this difficulty we propose two bootstrap methods that can consistently approximate the limit distribution: the gradient bootstrap and the weighted bootstrap of the inverse propensity score weighted (ipw) estimator. the gradient bootstrap is free of tuning parameters but requires knowledge of the pair identities. the weighted bootstrap of the ipw estimator does not require such knowledge but involves one tuning parameter. both methods are straightforward to implement and able to provide pointwise confidence intervals and uniform confidence bands that achieve exact limiting coverage rates. we demonstrate their finite sample performance using simulations and provide an empirical application to a well-known dataset in microfinance.", "categories": "econ.em", "created": "2020-05-25", "updated": "2020-08-02", "authors": ["liang jiang", "xiaobin liu", "peter c. b. phillips", "yichong zhang"], "url": "https://arxiv.org/abs/2005.11967"}, {"title": "on evaluation of risky investment projects. investment certainty   equivalence", "id": "2005.12173", "abstract": "the purpose of the study is to propose a methodology for evaluation and ranking of risky investment projects.an investment certainty equivalence approach dual to the conventional separation of riskless and risky contributions based on cash flow certainty equivalence is introduced. proposed ranking of investment projects is based on gauging them with the omega measure, which is defined as the ratio of chances to obtain profit/return greater than some critical (minimal acceptable) profitability over the chances to obtain the profit/return less than the critical one.detailed consideration of alternative riskless investment is presented. various performance measures characterizing investment projects with a special focus on the role of reinvestment are discussed. relation between the proposed methodology and the conventional approach based on utilization of risk-adjusted discount rate (radr) is discussed. findings are supported with an illustrative example.the methodology proposed can be used to rank projects of different nature, scale and lifespan. in contrast to the conventional radr approach for investment project evaluation, in the proposed method a risk profile of a specific project is explicitly analyzed in terms of appropriate performance measure distribution. no ad-hoc assumption about suitable risk-premium is made.", "categories": "q-fin.rm econ.gn q-fin.ec", "created": "2020-05-25", "updated": "", "authors": ["andrey leonidov", "ilya tipunin", "ekaterina serebryannikova"], "url": "https://arxiv.org/abs/2005.12173"}, {"title": "an alternative to synthetic control for models with many covariates   under sparsity", "id": "2005.12225", "abstract": "the synthetic control method is a an econometric tool to evaluate causal effects when only one unit is treated. while initially aimed at evaluating the effect of large-scale macroeconomic changes with very few available control units, it has increasingly been used in place of more well-known microeconometric tools in a broad range of applications, but its properties in this context are unknown. this paper introduces an alternative to the synthetic control method, which is developed both in the usual asymptotic framework and in the high-dimensional scenario. we propose an estimator of average treatment effect that is doubly robust, consistent and asymptotically normal. it is also immunized against first-step selection mistakes. we illustrate these properties using monte carlo simulations and applications to both standard and potentially high-dimensional settings, and offer a comparison with the synthetic control method.", "categories": "econ.em stat.me", "created": "2020-05-25", "updated": "", "authors": ["marianne bl\u00e9haut", "xavier d'haultfoeuille", "j\u00e9r\u00e9my l'hour", "alexandre b. tsybakov"], "url": "https://arxiv.org/abs/2005.12225"}, {"title": "fair policy targeting", "id": "2005.12395", "abstract": "one of the major concerns of targeting interventions on individuals in social welfare programs is discrimination: individualized treatments may induce disparities on sensitive attributes such as age, gender, or race. this paper addresses the question of the design of fair and efficient treatment allocation rules. we adopt the non-maleficence perspective of \"first do no harm\": we propose to select the fairest allocation within the pareto frontier. we provide envy-freeness justifications to novel counterfactual notions of fairness. we discuss easy-to-implement estimators of the policy function, by casting the optimization into a mixed-integer linear program formulation. we derive regret bounds on the unfairness of the estimated policy function, and small sample guarantees on the pareto frontier. finally, we illustrate our method using an application from education economics.", "categories": "econ.em stat.me stat.ml", "created": "2020-05-25", "updated": "", "authors": ["davide viviano", "jelena bradic"], "url": "https://arxiv.org/abs/2005.12395"}, {"title": "the race between technological progress and female advancement: changes   in gender and skill premia in oecd countries", "id": "2005.12600", "abstract": "in recent decades, the male-female wage gap has fallen, while the skilled-unskilled wage gap has risen in advanced countries. the rate of decline in the gender wage gap has tended to be greater for unskilled than skilled workers, while the rate of increase in the skill wage gap has tended to be greater for male than female workers. to account for these trends, we develop an aggregate production function extended to allow for gender-specific capital-skill complementarity, and estimate it using shift-hare instruments and cross-country panel data from oecd countries. we confirm that information and communication technology (ict) equipment is not only more complementary to skilled than unskilled workers but also more complementary to female than male workers. our results show that changes in gender and skill premia are the outcome of the race between progress in ict and advances in female educational attainment and employment.", "categories": "econ.gn q-fin.ec", "created": "2020-05-26", "updated": "", "authors": ["hiroya taniguchi", "ken yamada"], "url": "https://arxiv.org/abs/2005.12600"}, {"title": "on the causes and consequences of deviations from rational behavior", "id": "2005.12638", "abstract": "this paper presents novel evidence for the prevalence of deviations from rational behavior in human decision making - and for the corresponding causes and consequences. the analysis is based on move-by-move data from chess tournaments and an identification strategy that compares behavior of professional chess players to a rational behavioral benchmark that is constructed using modern chess engines. the evidence documents the existence of several distinct dimensions in which human players deviate from a rational benchmark. in particular, the results show deviations related to loss aversion, time pressure, fatigue, and cognitive limitations. the results also demonstrate that deviations do not necessarily lead to worse performance. consistent with an important influence of intuition and experience, faster decisions are associated with more frequent deviations from the rational benchmark, yet they are also associated with better performance.", "categories": "econ.gn cs.ai q-fin.ec", "created": "2020-05-26", "updated": "", "authors": ["dainis zegners", "uwe sunde", "anthony strittmatter"], "url": "https://arxiv.org/abs/2005.12638"}, {"title": "the probability of a robust inference for internal validity and its   applications in regression models", "id": "2005.12784", "abstract": "the internal validity of observational study is often subject to debate. in this study, we define the unobserved sample based on the counterfactuals and formalize its relationship with the null hypothesis statistical testing (nhst) for regression models. the probability of a robust inference for internal validity, i.e., the piv, is the probability of rejecting the null hypothesis again based on the ideal sample which is defined as the combination of the observed and unobserved samples, provided the same null hypothesis has already been rejected for the observed sample. when the unconfoundedness assumption is dubious, one can bound the piv of an inference based on bounded belief about the mean counterfactual outcomes, which is often needed in this case. essentially, the piv is statistical power of the nhst that is thought to be built on the ideal sample. we summarize the process of evaluating internal validity with the piv into a six-step procedure and illustrate it with an empirical example (i.e., hong and raudenbush (2005)).", "categories": "stat.me econ.em stat.ap", "created": "2020-05-21", "updated": "", "authors": ["tenglong li", "kenneth a. frank"], "url": "https://arxiv.org/abs/2005.12784"}, {"title": "periodic strategies ii: generalizations and extensions", "id": "2005.12832", "abstract": "at a mixed nash equilibrium, the payoff of a player does not depend on her own action, as long as her opponent sticks to his. in a periodic strategy, a concept developed in a previous paper (arxiv:1307.2035v4), in contrast, the own payoff does not depend on the opponent's action. here, we generalize this to multi-player simultaneous perfect information strategic form games. we show that also in this class of games, there always exists at least one periodic strategy, and we investigate the mathematical properties of such periodic strategies. in addition, we demonstrate that periodic strategies may exist in games with incomplete information; we shall focus on bayesian games. moreover we discuss the differences between the periodic strategies formalism and cooperative game theory. in fact, the periodic strategies are obtained in a purely non-cooperative way, and periodic strategies are as cooperative as the nash equilibria are. finally, we incorporate the periodic strategies in an epistemic game theory framework, and discuss several features of this approach.", "categories": "cs.gt econ.th", "created": "2020-05-26", "updated": "", "authors": ["v. k. oikonomou", "j. jost"], "url": "https://arxiv.org/abs/2005.12832"}, {"title": "from tether to libra: stablecoins, digital currency and the future of   money", "id": "2005.12949", "abstract": "this paper provides an overview on stablecoins and introduces a novel terminology to help better identify stablecoins with truly disruptive potential. it provides a compact definition for stablecoins, identifying the unique features that make them distinct from previously known payment systems. furthermore, it surveys the different use cases for stablecoins as well as the underlying economic incentives for creating them. finally, it outlines critical regulatory considerations that constrain stablecoins and summarizes key factors that are driving their rapid development.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-05-26", "updated": "", "authors": ["alexander lipton", "aetienne sardon", "fabian sch\u00e4r", "christian sch\u00fcpbach"], "url": "https://arxiv.org/abs/2005.12949"}, {"title": "impact of the state of emergency declaration for covid-19 on preventive   behaviors and mental conditions in japan: difference in difference analysis   using panel data", "id": "2005.13008", "abstract": "during the covid-19 epidemic in japan between march and april 2020, internet surveys were conducted to construct panel data to investigate changes at the individual level regarding preventive behaviors and mental conditions by surveying the same respondents at different times. specifically, the difference-in-difference (did) method was used to explore the impact of the covid-19 state of emergency declared by the government. key findings were: (1) the declaration led people to stay home, while also generating anger, fear, and anxiety. (2) the effect of the declaration on the promotion of preventive behaviors was larger than the detrimental effect on mental conditions. (3) overall, the effect on women was larger than that on men.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-05-23", "updated": "", "authors": ["eiji yamamura.", "yoshiro tsutsui"], "url": "https://arxiv.org/abs/2005.13008"}, {"title": "strengthening science, technology, and innovation-based incubators to   help achieve sustainable development goals: lessons from india", "id": "2005.13138", "abstract": "policymakers in developing countries increasingly see science, technology, and innovation (sti) as an avenue for meeting sustainable development goals (sdgs), with sti-based startups as a key part of these efforts. market failures call for government interventions in supporting sti for sdgs and publicly-funded incubators can potentially fulfil this role. using the specific case of india, we examine how publicly-funded incubators could contribute to strengthening sti-based entrepreneurship. india's sti policy and its links to societal goals span multiple decades -- but since 2015 these goals became formally organized around the sdgs. we examine why sti-based incubators were created under different policy priorities before 2015, the role of public agencies in implementing these policies, and how some incubators were particularly effective in addressing the societal challenges that can now be mapped to sdgs. we find that effective incubation for supporting sti-based entrepreneurship to meet societal goals extended beyond traditional incubation activities. for sti-based incubators to be effective, policymakers must strengthen the 'incubation system'. this involves incorporating targeted sdgs in specific incubator goals, promoting coordination between existing incubator programs, developing a performance monitoring system, and finally, extending extensive capacity building at multiple levels including for incubator managers and for broader sti in the country.", "categories": "econ.gn q-fin.ec", "created": "2020-05-26", "updated": "2020-06-02", "authors": ["kavita surana", "anuraag singh", "ambuj d sagar"], "url": "https://arxiv.org/abs/2005.13138"}, {"title": "oligopoly dynamics", "id": "2005.13228", "abstract": "the present notes summarise the oligopoly dynamics lectures professor lu\\'is cabral gave at the bank of portugal in september and october 2017. the lectures discuss a set industrial organisation problems in a dynamic environment, namely learning by doing, switching costs, price wars, networks and platforms, and ladder models of innovation. methodologically, the materials cover analytical solutions of known points (e.g., $\\delta = 0$), the discussion of firms' strategies based on intuitions derived directly from their value functions with no model solving, and the combination of analytical and numerical procedures to reach model solutions. state space analysis is done for both continuous and discrete cases. all errors are my own.", "categories": "cs.gt econ.th", "created": "2020-05-27", "updated": "", "authors": ["bernardo melo pimentel"], "url": "https://arxiv.org/abs/2005.13228"}, {"title": "bibliometric indices as a measure of long-term competitive balance in   knockout tournaments", "id": "2005.13416", "abstract": "we argue for the application of bibliometric indices to quantify long-term uncertainty of outcome in sports. the euclidean index is proposed to reward quality over quantity, while the rectangle index can be an appropriate measure of core performance. their differences are highlighted through an axiomatic analysis and several examples. our approach also requires a weighting scheme to compare different achievements. the methodology is illustrated by studying the knockout stage of the uefa champions league in the 16 seasons played between 2003 and 2019: club and country performances as well as three types of competitive balance are considered. measuring competition at the level of national associations is a novelty. all results are remarkably robust concerning the bibliometric index and the assigned weights. inequality has not increased among the elite clubs and between the national associations, however, it has changed within some countries. since the performances of national associations are more stable than the results of individual clubs, it would be better to build the seeding in the uefa champions league group stage upon association coefficients adjusted for league finishing positions rather than club coefficients.", "categories": "stat.ap cs.gt econ.gn q-fin.ec", "created": "2020-05-27", "updated": "2020-09-11", "authors": ["l\u00e1szl\u00f3 csat\u00f3", "d\u00f3ra gr\u00e9ta petr\u00f3czy"], "url": "https://arxiv.org/abs/2005.13416"}, {"title": "probabilistic multivariate electricity price forecasting using implicit   generative ensemble post-processing", "id": "2005.13417", "abstract": "the reliable estimation of forecast uncertainties is crucial for risk-sensitive optimal decision making. in this paper, we propose implicit generative ensemble post-processing, a novel framework for multivariate probabilistic electricity price forecasting. we use a likelihood-free implicit generative model based on an ensemble of point forecasting models to generate multivariate electricity price scenarios with a coherent dependency structure as a representation of the joint predictive distribution. our ensemble post-processing method outperforms well-established model combination benchmarks. this is demonstrated on a data set from the german day-ahead market. as our method works on top of an ensemble of domain-specific expert models, it can readily be deployed to other forecasting tasks.", "categories": "stat.ap econ.em q-fin.rm q-fin.st stat.ml", "created": "2020-05-27", "updated": "", "authors": ["tim janke", "florian steinke"], "url": "https://arxiv.org/abs/2005.13417"}, {"title": "breiman's \"two cultures\" revisited and reconciled", "id": "2005.13596", "abstract": "in a landmark paper published in 2001, leo breiman described the tense standoff between two cultures of data modeling: parametric statistical and algorithmic machine learning. the cultural division between these two statistical learning frameworks has been growing at a steady pace in recent years. what is the way forward? it has become blatantly obvious that this widening gap between \"the two cultures\" cannot be averted unless we find a way to blend them into a coherent whole. this article presents a solution by establishing a link between the two cultures. through examples, we describe the challenges and potential gains of this new integrated statistical thinking.", "categories": "stat.ml cs.ai cs.lg econ.em stat.me", "created": "2020-05-27", "updated": "", "authors": ["n/a subhadeep", "n/a mukhopadhyay", "kaijun wang"], "url": "https://arxiv.org/abs/2005.13596"}, {"title": "covid-19 and global economic growth: policy simulations with a   pandemic-enabled neoclassical growth model", "id": "2005.13722", "abstract": "during the covid-19 pandemic of 2019/2020, authorities have used temporary ad-hoc policy measures, such as lockdowns and mass quarantines, to slow its transmission. however, the consequences of widespread use of these unprecedented measures are poorly understood. to contribute to the understanding of the economic and human consequences of such policy measures, we therefore construct a mathematical model of an economy under the impact of a pandemic, select parameter values to represent the global economy under the impact of covid-19, and perform numerical experiments by simulating a large number of possible policy responses. by varying the starting date of the policy intervention in the simulated scenarios, we find that the most effective policy intervention occurs around the time when the number of active infections is growing at its highest rate -- that is, the results suggest that the most severe measures should only be implemented when the disease is sufficiently spread. the intensity of the intervention, above a certain threshold, does not appear to have a great impact on the outcomes in our simulations, due to the strongly concave relationship that we identify between production shortfall and infection rate reductions. our experiments further suggest that the intervention should last until after the peak established by the reduced infection rate, which implies that stricter policies should last longer. the model and its implementation, along with the general insights from our policy experiments, may help policymakers design effective emergency policy responses in the face of a serious pandemic, and contribute to our understanding of the relationship between the economic growth and the spread of infectious diseases.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-27", "updated": "2020-06-15", "authors": ["ian m. trotter", "lu\u00eds a. c. schmidt", "bruno c. m. pinto", "andrezza l. batista", "j\u00e9ssica pellenz", "maritza isidro", "aline rodrigues", "attawan g. s. suela", "loredany rodrigues"], "url": "https://arxiv.org/abs/2005.13722"}, {"title": "a complete characterization of infinitely repeated two-player games   having computable strategies with no computable best response under   limit-of-means payoff", "id": "2005.13921", "abstract": "it is well-known that for infinitely repeated games, there are computable strategies that have best responses, but no computable best responses. these results were originally proved for either specific games (e.g., prisoner's dilemma), or for classes of games satisfying certain conditions not known to be both necessary and sufficient.   we derive a complete characterization in the form of simple necessary and sufficient conditions for the existence of a computable strategy without a computable best response under limit-of-means payoff. we further refine the characterization by requiring the strategy profiles to be nash equilibria or subgame-perfect equilibria, and we show how the characterizations entail that it is efficiently decidable whether an infinitely repeated game has a computable strategy without a computable best response.", "categories": "cs.gt econ.th", "created": "2020-05-28", "updated": "2020-06-10", "authors": ["jakub dargaj", "jakob grue simonsen"], "url": "https://arxiv.org/abs/2005.13921"}, {"title": "machine learning time series regressions with an application to   nowcasting", "id": "2005.14057", "abstract": "this paper introduces structured machine learning regressions for high-dimensional time series data potentially sampled at different frequencies. the sparse-group lasso estimator can take advantage of such time series data structures and outperforms the unstructured lasso. we establish oracle inequalities for the sparse-group lasso estimator within a framework that allows for the mixing processes and recognizes that the financial and the macroeconomic data may have heavier than exponential tails. an empirical application to nowcasting us gdp growth indicates that the estimator performs favorably compared to other alternatives and that the text data can be a useful addition to more traditional numerical data.", "categories": "econ.em stat.ap stat.me stat.ml", "created": "2020-05-28", "updated": "2020-05-28", "authors": ["andrii babii", "eric ghysels", "jonas striaukas"], "url": "https://arxiv.org/abs/2005.14057"}, {"title": "on sustainable equilibria", "id": "2005.14094", "abstract": "following the ideas laid out in myerson (1996), hofbauer (2000) defined an equilibrium of a game as sustainable if it can be made the unique equilibrium of a game obtained by deleting a subset of the strategies that are inferior replies to it, and then adding others. hofbauer also formalized myerson's conjecture about the relationship between the sustainability of an equilibrium and its index: for a generic class of games, an equilibrium is sustainable iff its index is +1. von schemde and von stengel (2008) proved this conjecture for bimatrix games. this paper shows that the conjecture is true for all finite games. more precisely, we prove that an isolated equilibrium of a given game has index +1 if and only if it can be made unique in a larger game obtained by adding finitely many inferior reply strategies.", "categories": "econ.th", "created": "2020-05-28", "updated": "", "authors": ["srihari govindan", "rida laraki", "lucas pahl"], "url": "https://arxiv.org/abs/2005.14094"}, {"title": "causal impact of masks, policies, behavior on early covid-19 pandemic in   the u.s", "id": "2005.14168", "abstract": "this paper evaluates the dynamic impact of various policies adopted by us states on the growth rates of confirmed covid-19 cases and deaths as well as social distancing behavior measured by google mobility reports, where we take into consideration people's voluntarily behavioral response to new information of transmission risks. our analysis finds that both policies and information on transmission risks are important determinants of covid-19 cases and deaths and shows that a change in policies explains a large fraction of observed changes in social distancing behavior. our counterfactual experiments suggest that nationally mandating face masks for employees on april 1st could have reduced the growth rate of cases and deaths by more than 10 percentage points in late april, and could have led to as much as 17 to 55 percent less deaths nationally by the end of may, which roughly translates into 17 to 55 thousand saved lives. our estimates imply that removing non-essential business closures (while maintaining school closures, restrictions on movie theaters and restaurants) could have led to -20 to 60 percent more cases and deaths by the end of may. we also find that, without stay-at-home orders, cases would have been larger by 25 to 170 percent, which implies that 0.5 to 3.4 million more americans could have been infected if stay-at-home orders had not been implemented. finally, not having implemented any policies could have led to at least a 7 fold increase with an uninformative upper bound in cases (and deaths) by the end of may in the us, with considerable uncertainty over the effects of school closures, which had little cross-sectional variation.", "categories": "econ.em stat.ap", "created": "2020-05-28", "updated": "2020-07-07", "authors": ["victor chernozhukov", "hiroyuki kasaha", "paul schrimpf"], "url": "https://arxiv.org/abs/2005.14168"}, {"title": "competition among large and heterogeneous small firms", "id": "2005.14442", "abstract": "we extend the model of parenti (2018) on large and small firms by introducing cost heterogeneity among small firms. we propose a novel necessary and sufficient condition for the existence of such a mixed market structure. furthermore, in contrast to parenti (2018), we show that in the presence of cost heterogeneity among small firms, trade liberalization may raise or reduce the mass of small firms in operation.", "categories": "econ.gn q-fin.ec", "created": "2020-05-29", "updated": "", "authors": ["lijun pan", "yongjin wang"], "url": "https://arxiv.org/abs/2005.14442"}, {"title": "mortality containment vs. economics opening: optimal policies in a   seiard model", "id": "2006.00085", "abstract": "we adapt a seird differential model with asymptomatic population and covid deaths, which we call seaird, to simulate the evolution of covid-19, and add a control function affecting both the diffusion of the virus and gdp, featuring all direct and indirect containment policies; to model feasibility, the control is assumed to be a piece-wise linear function satisfying additional constraints. we describe the joint dynamics of infection and the economy and discuss the trade-off between production and fatalities. in particular, we carefully study the conditions for the existence of the optimal policy response and its uniqueness. uniqueness crucially depends on the marginal rate of substitution between the statistical value of a human life and gdp; we show an example with a phase transition: above a certain threshold, there is a unique optimal containment policy; below the threshold, it is optimal to abstain from any containment; and at the threshold itself there are two optimal policies. we then explore and evaluate various profiles of various control policies dependent on a small number of parameters.", "categories": "physics.soc-ph econ.gn math.oc q-bio.pe q-fin.ec", "created": "2020-05-29", "updated": "", "authors": ["andrea aspri", "elena beretta", "alberto gandolfi", "etienne wasmer"], "url": "https://arxiv.org/abs/2006.00085"}, {"title": "the impacts of asymmetry on modeling and forecasting realized volatility   in japanese stock markets", "id": "2006.00158", "abstract": "this study investigates the impacts of asymmetry on the modeling and forecasting of realized volatility in the japanese futures and spot stock markets. we employ heterogeneous autoregressive (har) models allowing for three types of asymmetry: positive and negative realized semivariance (rsv), asymmetric jumps, and leverage effects. the estimation results show that leverage effects clearly influence the modeling of realized volatility models. leverage effects exist for both the spot and futures markets in the nikkei 225. although realized semivariance aids better modeling, the estimations of rsv models depend on whether these models have leverage effects. asymmetric jump components do not have a clear influence on realized volatility models. while leverage effects and realized semivariance also improve the out-of-sample forecast performance of volatility models, asymmetric jumps are not useful for predictive ability. the empirical results of this study indicate that asymmetric information, in particular, leverage effects and realized semivariance, yield better modeling and more accurate forecast performance. accordingly, asymmetric information should be included when we model and forecast the realized volatility of japanese stock markets.", "categories": "q-fin.st econ.em", "created": "2020-05-29", "updated": "", "authors": ["daiki maki", "yasushi ota"], "url": "https://arxiv.org/abs/2006.00158"}, {"title": "parametric modeling of quantile regression coefficient functions with   longitudinal data", "id": "2006.00160", "abstract": "in ordinary quantile regression, quantiles of different order are estimated one at a time. an alternative approach, which is referred to as quantile regression coefficients modeling (qrcm), is to model quantile regression coefficients as parametric functions of the order of the quantile. in this paper, we describe how the qrcm paradigm can be applied to longitudinal data. we introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. we propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on $\\ell_1$ and $\\ell_2$ penalization. we provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. the proposed method has been implemented in the r package qrcm.", "categories": "stat.me econ.em", "created": "2020-05-29", "updated": "", "authors": ["paolo frumento", "matteo bottai", "iv\u00e1n fern\u00e1ndez-val"], "url": "https://arxiv.org/abs/2006.00160"}, {"title": "measuring and visualizing place-based space-time job accessibility", "id": "2006.00268", "abstract": "place-based accessibility measures, such as the gravity-based model, are widely applied to study the spatial accessibility of workers to job opportunities in cities. however, gravity-based measures often suffer from three main limitations: (1) they are sensitive to the spatial configuration and scale of the units of analysis, which are not specifically designed for capturing job accessibility patterns and are often too coarse; (2) they omit the temporal dynamics of job opportunities and workers in the calculation, instead assuming that they remain stable over time; and (3) they do not lend themselves to dynamic geovisualization techniques. in this paper, a new methodological framework for measuring and visualizing place-based job accessibility in space and time is presented that overcomes these three limitations. first, discretization and dasymetric mapping approaches are used to disaggregate counts of jobs and workers over specific time intervals to a fine-scale grid. second, shen (1998) gravity-based accessibility measure is modified to account for temporal fluctuations in the spatial distributions of the supply of jobs and the demand of workers and is used to estimate hourly job accessibility at each cell. third, a four-dimensional volumetric rendering approach is employed to integrate the hourly job access estimates into a space-time cube environment, which enables the users to interactively visualize the space-time job accessibility patterns. the integrated framework is demonstrated in the context of a case study of the tampa bay region of florida. the findings demonstrate the value of the proposed methodology in job accessibility analysis and the policy-making process.", "categories": "cs.cy econ.gn q-fin.ec stat.ap", "created": "2020-05-30", "updated": "", "authors": ["yujie hu", "joni downs"], "url": "https://arxiv.org/abs/2006.00268"}, {"title": "the impact of covid-19 on the uk fresh food supply chain", "id": "2006.00279", "abstract": "the resilience of the food supply chain is a matter of critical importance, both for national security and broader societal well bring. covid19 has presented a test to the current system, as well as means by which to explore whether the uk's food supply chain will be resilient to future disruptions. in the face of a growing need to ensure that food supply is more environmentally sustainable and socially just, covod19 also represents an opportunity to consider the ability of the system to innovative, and its capacity for change. the purpose of this case based study is to explore the response and resilience of the uk fruit and vegetable food supply chain to covid19, and to assess this empirical evidence in the context of a resilience framework based on the adaptive cycle. to achieve this we reviewed secondary data associated with changes to retail demand, conducted interviews with 23 organisations associated with supply to this market, and conducted four video workshops with 80 organisations representing half of the uk fresh produce community. the results highlight that, despite significant disruption, the retail dominated fresh food supply chain has demonstrated a high degree of resilience. in the context of the adaptive cycle, the system has shown signs of being stuck in a rigidity trap, as yet unable to exploit more radical innovations that may also assist in addressing other drivers for change. this has highlighted the significant role that innovation and r&d communities will need to play in enabling the supply chain to imagine and implement alternative future states post covid.", "categories": "econ.gn q-fin.ec", "created": "2020-05-30", "updated": "", "authors": ["rebecca mitchell", "roger maull", "simon pearson", "steve brewer", "martin collison"], "url": "https://arxiv.org/abs/2006.00279"}, {"title": "statistical decision properties of imprecise trials assessing covid-19   drugs", "id": "2006.00343", "abstract": "as the covid-19 pandemic progresses, researchers are reporting findings of randomized trials comparing standard care with care augmented by experimental drugs. the trials have small sample sizes, so estimates of treatment effects are imprecise. seeing imprecision, clinicians reading research articles may find it difficult to decide when to treat patients with experimental drugs. whatever decision criterion one uses, there is always some probability that random variation in trial outcomes will lead to prescribing sub-optimal treatments. a conventional practice when comparing standard care and an innovation is to choose the innovation only if the estimated treatment effect is positive and statistically significant. this practice defers to standard care as the status quo. to evaluate decision criteria, we use the concept of near-optimality, which jointly considers the probability and magnitude of decision errors. an appealing decision criterion from this perspective is the empirical success rule, which chooses the treatment with the highest observed average patient outcome in the trial. considering the design of recent and ongoing covid-19 trials, we show that the empirical success rule yields treatment results that are much closer to optimal than those generated by prevailing decision criteria based on hypothesis tests.", "categories": "econ.em q-bio.qm stat.me", "created": "2020-05-30", "updated": "", "authors": ["charles f. manski", "aleksey tetenov"], "url": "https://arxiv.org/abs/2006.00343"}, {"title": "evaluating the properties of a first choice weighted approval voting   system", "id": "2006.00368", "abstract": "plurality and approval voting are two well-known voting systems with different strengths and weaknesses. in this paper we consider a new voting system we call beta(k) which allows voters to select a single first-choice candidate and approve of any other number of candidates, where k denotes the relative weight given to a first choice; this system is essentially a hybrid of plurality and approval. our primary goal is to characterize the behavior of beta(k) for any value of k. under certain reasonable assumptions, beta(k) can be made to mimic plurality or approval voting in the event of a single winner while potentially breaking ties otherwise. under the assumption that voters are honest, we show that it is possible to find the values of k for which a given candidate will win the election if the respective approval and plurality votes are known. finally, we show how some of the commonly used voting system criteria are satisfied by beta(k).", "categories": "econ.th", "created": "2020-05-30", "updated": "", "authors": ["peter butler", "jerry lin"], "url": "https://arxiv.org/abs/2006.00368"}, {"title": "lockdown strategies, mobility patterns and covid-19", "id": "2006.00531", "abstract": "we develop a multiple-events model and exploit within and between country variation in the timing, type and level of intensity of various public policies to study their dynamic effects on the daily incidence of covid-19 and on population mobility patterns across 135 countries. we remove concurrent policy bias by taking into account the contemporaneous presence of multiple interventions. the main result of the paper is that cancelling public events and imposing restrictions on private gatherings followed by school closures have quantitatively the most pronounced effects on reducing the daily incidence of covid-19. they are followed by workplace as well as stay-at-home requirements, whose statistical significance and levels of effect are not as pronounced. instead, we find no effects for international travel controls, public transport closures and restrictions on movements across cities and regions. we establish that these findings are mediated by their effect on population mobility patterns in a manner consistent with time-use and epidemiological factors.", "categories": "econ.em physics.soc-ph q-bio.pe", "created": "2020-05-31", "updated": "", "authors": ["nikos askitas", "konstantinos tatsiramos", "bertrand verheyden"], "url": "https://arxiv.org/abs/2006.00531"}, {"title": "shared value economics: an axiomatic approach", "id": "2006.00581", "abstract": "the concept of shared value was introduced by porter and kramer as a new conception of capitalism. shared value describes the strategy of organizations that simultaneously enhance their competitiveness and the social conditions of related stakeholders such as employees, suppliers and the natural environment. the idea has generated strong interest, but also some controversy due to a lack of a precise definition, measurement techniques and difficulties to connect theory to practice. we overcome these drawbacks by proposing an economic framework based on three key aspects: coalition formation, sustainability and consistency, meaning that conclusions can be tested by means of logical deductions and empirical applications. the presence of multiple agents to create shared value and the optimization of both social and economic criteria in decision making represent the core of our quantitative definition of shared value. we also show how economic models can be characterized as shared value models by means of logical deductions. summarizing, our proposal builds on the foundations of shared value to improve its understanding and to facilitate the suggestion of economic hypotheses, hence accommodating the concept of shared value within modern economic theory.", "categories": "cs.gt econ.th", "created": "2020-05-31", "updated": "", "authors": ["francisco salas-molina", "juan antonio rodr\u00edguez aguilar", "filippo bistaffa"], "url": "https://arxiv.org/abs/2006.00581"}, {"title": "influence via ethos: on the persuasive power of reputation in   deliberation online", "id": "2006.00707", "abstract": "deliberation among individuals online plays a key role in shaping the opinions that drive votes, purchases, donations and other critical offline behavior. yet, the determinants of opinion-change via persuasion in deliberation online remain largely unexplored. our research examines the persuasive power of $\\textit{ethos}$ -- an individual's \"reputation\" -- using a 7-year panel of over a million debates from an argumentation platform containing explicit indicators of successful persuasion. we identify the causal effect of reputation on persuasion by constructing an instrument for reputation from a measure of past debate competition, and by controlling for unstructured argument text using neural models of language in the double machine-learning framework. we find that an individual's reputation significantly impacts their persuasion rate above and beyond the validity, strength and presentation of their arguments. in our setting, we find that having 10 additional reputation points causes a 31% increase in the probability of successful persuasion over the platform average. we also find that the impact of reputation is moderated by characteristics of the argument content, in a manner consistent with a theoretical model that attributes the persuasive power of reputation to heuristic information-processing under cognitive overload. we discuss managerial implications for platforms that facilitate deliberative decision-making for public and private organizations online.", "categories": "econ.em cs.cl stat.ap", "created": "2020-06-01", "updated": "", "authors": ["emaad manzoor", "george h. chen", "dokyun lee", "michael d. smith"], "url": "https://arxiv.org/abs/2006.00707"}, {"title": "do public program benefits crowd out private transfers in developing   countries? a critical review of recent evidence", "id": "2006.00737", "abstract": "precipitated by rapid globalization, rising inequality, population growth, and longevity gains, social protection programs have been on the rise in low- and middle-income countries (lmics) in the last three decades. however, the introduction of public benefits could displace informal mechanisms for risk-protection, which are especially prevalent in lmics. if the displacement of private transfers is considerably large, the expansion of social protection programs could even lead to social welfare loss. in this paper, we critically survey the recent empirical literature on crowd-out effects in response to public policies, specifically in the context of lmics. we review and synthesize patterns from the behavioral response to various types of social protection programs. furthermore, we specifically examine for heterogeneous treatment effects by important socioeconomic characteristics. we conclude by drawing on lessons from our synthesis of studies. if poverty reduction objectives are considered, along with careful program targeting that accounts for potential crowd-out effects, there may well be a net social gain.", "categories": "econ.em", "created": "2020-06-01", "updated": "2020-06-05", "authors": ["plamen nikolov", "matthew bonci"], "url": "https://arxiv.org/abs/2006.00737"}, {"title": "the importance of cognitive domains and the returns to schooling in   south africa: evidence from two labor surveys", "id": "2006.00739", "abstract": "numerous studies have considered the important role of cognition in estimating the returns to schooling. how cognitive abilities affect schooling may have important policy implications, especially in developing countries during periods of increasing educational attainment. using two longitudinal labor surveys that collect direct proxy measures of cognitive skills, we study the importance of specific cognitive domains for the returns to schooling in two samples. we instrument for schooling levels and we find that each additional year of schooling leads to an increase in earnings by approximately 18-20 percent. the estimated effect sizes-based on the two-stage least squares estimates-are above the corresponding ordinary least squares estimates. furthermore, we estimate and demonstrate the importance of specific cognitive domains in the classical mincer equation. we find that executive functioning skills (i.e., memory and orientation) are important drivers of earnings in the rural sample, whereas higher-order cognitive skills (i.e., numeracy) are more important for determining earnings in the urban sample. although numeracy is tested in both samples, it is only a statistically significant predictor of earnings in the urban sample.", "categories": "econ.gn q-fin.ec", "created": "2020-06-01", "updated": "2020-06-05", "authors": ["plamen nikolov", "nusrat jimi"], "url": "https://arxiv.org/abs/2006.00739"}, {"title": "power trades and network congestion externalities", "id": "2006.00916", "abstract": "as power generation by renewable sources increases, power transmission patterns over the electric grid change. we show that due to physical laws, these new transmission patterns lead to non-intuitive grid congestion externalities. we derive the conditions under which network externalities due to power trades occur. calibration shows that each additional unit of power traded between northern and western europe reduces transmission capacity for the southern and eastern regions by 27% per unit traded. given such externalities, new investments in the electric grid infrastructure cannot be made piecemeal. power transit fares can help finance investment in regions facing network congestion externalities.", "categories": "eess.sy cs.sy econ.gn physics.soc-ph q-fin.ec", "created": "2020-05-28", "updated": "", "authors": ["nayara aguiar", "indraneel chakraborty", "vijay gupta"], "url": "https://arxiv.org/abs/2006.00916"}, {"title": "variational inequality type formulations of general market equilibrium   problems with local information", "id": "2006.01178", "abstract": "we suggest a new approach to creation of general market equilibrium models involving economic agents with local and partial knowledge about the system and under different restrictions. the market equilibrium problem is then formulated as a quasi-variational inequality that enables us to establish existence results for the model in different settings. we also describe dynamic processes, which fall into information exchange schemes of the proposed market model. in particular, we propose an iterative solution method for quasi-variational inequalities, which is based on evaluations of the proper market information only in a neighborhood of the current market state without knowledge of the whole feasible set and prove its convergence.", "categories": "math.oc econ.th", "created": "2020-06-01", "updated": "2020-06-13", "authors": ["igor konnov"], "url": "https://arxiv.org/abs/2006.01178"}, {"title": "do private household transfers to the elderly respond to public pension   benefits? evidence from rural china", "id": "2006.01185", "abstract": "ageing populations in developing countries have spurred the introduction of public pension programs to preserve the standard of living for the elderly. the often-overlooked mechanism of intergenerational transfers, however, can dampen these intended policy effects as adult children who make income contributions to their parents could adjust their behavior to changes in their parents' income. exploiting a unique policy intervention in china, we examine using a difference-in-difference-in-differences (ddd) approach how a new pension program impacts inter vivos transfers. we show that pension benefits lower the propensity of receiving transfers from adult children in the context of a large middle-income country and we also estimate a small crowd-out effect. taken together, these estimates fit the pattern of previous research in high-income countries, although our estimates of the crowd-out effect are significantly smaller than previous studies in both high-income and middle-income countries.", "categories": "econ.gn q-fin.ec", "created": "2020-06-01", "updated": "2020-06-05", "authors": ["plamen nikolov", "alan adelman"], "url": "https://arxiv.org/abs/2006.01185"}, {"title": "new robust inference for predictive regressions", "id": "2006.01191", "abstract": "we propose two robust methods for testing hypotheses on unknown parameters of predictive regression models under heterogeneous and persistent volatility as well as endogenous, persistent and/or fat-tailed regressors and errors. the proposed robust testing approaches are applicable both in the case of discrete and continuous time models. both of the methods use the cauchy estimator to effectively handle the problems of endogeneity, persistence and/or fat-tailedness in regressors and errors. the difference between our two methods is how the heterogeneous volatility is controlled. the first method relies on robust t-statistic inference using group estimators of a regression parameter of interest proposed in ibragimov and muller, 2010. it is simple to implement, but requires the exogenous volatility assumption. to relax the exogenous volatility assumption, we propose another method which relies on the nonparametric correction of volatility. the proposed methods perform well compared with widely used alternative inference procedures in terms of their finite sample properties.", "categories": "econ.em math.st stat.th", "created": "2020-06-01", "updated": "2020-08-10", "authors": ["rustam ibragimov", "jihyun kim", "anton skrobotov"], "url": "https://arxiv.org/abs/2006.01191"}, {"title": "new approaches to robust inference on market (non-)efficiency,   volatility clustering and nonlinear dependence", "id": "2006.01212", "abstract": "many key variables in finance, economics and risk management, including financial returns and foreign exchange rates, exhibit nonlinear dependence, heterogeneity and heavy-tailedness of some usually largely unknown type.   the presence of non-linear dependence (usually modelled using garch-type dynamics) and heavy-tailedness may make problematic the analysis of (non-)efficiency, volatility clustering and predictive regressions in economic and financial markets using traditional approaches that appeal to asymptotic normality of sample autocorrelation functions (acfs) of returns and their squares.   the paper presents several new approaches to deal with the above problems. we provide the results that motivate the use of measures of market (non-)efficiency, volatility clustering and nonlinear dependence based on (small) powers of absolute returns and their signed versions. the paper provides asymptotic theory for sample analogues of the above measures in the case of general time series, including garch-type processes. it further develops new approaches to robust inference on them in the case of general garch-type processes exhibiting heavy-tailedness properties. the approaches are based on robust inference methods exploiting conservativeness properties of t-statistics ibragimov and muller (2010,2016) and several new results on their applicability in the settings considered. in the approaches, estimates of parameters of interest are computed for groups of data and the inference is based on t-statistics in resulting group estimates. this results in valid robust inference under a wide range of heterogeneity and dependence assumptions satisfied in financial and economic markets. numerical results and empirical applications confirm advantages of the new approaches over existing ones and their wide applicability.", "categories": "econ.em math.st stat.th", "created": "2020-06-01", "updated": "", "authors": ["rustam ibragimov", "rasmus pedersen", "anton skrobotov"], "url": "https://arxiv.org/abs/2006.01212"}, {"title": "revisiting money and labor for valuing environmental goods and services   in developing countries", "id": "2006.01290", "abstract": "many stated preference studies conducted in developing countries provide a low willingness to pay (wtp) for a wide range of goods and services. however, recent studies in these countries indicate that this may partly be a result of the choice of payment vehicle, not the preference for the good. thus, low wtp may not indicate a low welfare effect for public projects in developing countries. we argue that in a setting where 1) there is imperfect substitutability between money and other measures of wealth (e.g. labor), and 2) institutions are perceived to be corrupt, including payment vehicles that are currently available to the individual and less pron to corruption may be needed to obtain valid welfare estimates. otherwise, we risk underestimating the welfare benefit of projects. we demonstrate this through a rural household contingent valuation (cv) survey designed to elicit the value of access to reliable irrigation water in ethiopia. of the total average annual wtp for access to reliable irrigation service, cash contribution comprises only 24.41 %. the implication is that socially desirable projects might be rejected based on cost-benefit analysis as a result of welfare gain underestimation due to mismatch of payment vehicles choice in valuation study.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-01", "updated": "2020-08-26", "authors": ["habtamu tilahun kassahun", "jette bredahl jacobsen", "charles f. nicholson"], "url": "https://arxiv.org/abs/2006.01290"}, {"title": "estimates of derivatives of (log) densities and related objects", "id": "2006.01328", "abstract": "we estimate the density and its derivatives using a local polynomial approximation to the logarithm of an unknown density $f$. the estimator is guaranteed to be nonnegative and achieves the same optimal rate of convergence in the interior as well as the boundary of the support of $f$. the estimator is therefore well-suited to applications in which nonnegative density estimates are required, such as in semiparametric maximum likelihood estimation. in addition, we show that our estimator compares favorably with other kernel-based methods, both in terms of asymptotic performance and computational ease. simulation results confirm that our method can perform similarly in finite samples to these alternative methods when they are used with optimal inputs, i.e. an epanechnikov kernel and optimally chosen bandwidth sequence. further simulation evidence demonstrates that, if the researcher modifies the inputs and chooses a larger bandwidth, our approach can even improve upon these optimized alternatives, asymptotically. we provide code in several languages.", "categories": "econ.em stat.me", "created": "2020-06-01", "updated": "", "authors": ["joris pinkse", "karl schurter"], "url": "https://arxiv.org/abs/2006.01328"}, {"title": "one step at a time: does gradualism build coordination?", "id": "2006.01386", "abstract": "this study investigates a potential mechanism to promote coordination. with theoretical guidance using a belief-based learning model, we conduct a multi-period, binary-choice, and weakest-link laboratory coordination experiment to study the effect of gradualism - increasing the required levels (stakes) of contributions slowly over time rather than requiring a high level of contribution immediately - on group coordination performance. we randomly assign subjects to three treatments: starting and continuing at a high stake, starting at a low stake but jumping to a high stake after a few periods, and starting at a low stake while gradually increasing the stakes over time (the gradualism treatment). we find that relative to the other two treatments, groups coordinate most successfully at high stakes in the gradualism treatment. we also find evidence that supports the belief-based learning model. these findings point to a simple mechanism for promoting successful voluntary coordination.", "categories": "econ.th physics.soc-ph", "created": "2020-06-02", "updated": "", "authors": ["maoliang ye", "jie zheng", "plamen nikolov", "sam asher"], "url": "https://arxiv.org/abs/2006.01386"}, {"title": "explaining the distribution of energy consumption at slow charging   infrastructure for electric vehicles from socio-economic data", "id": "2006.01672", "abstract": "here, we develop a data-centric approach enabling to analyse which activities, function, and characteristics of the environment surrounding the slow charging infrastructure impact the distribution of the electricity consumed at slow charging infrastructure. to gain a basic insight, we analysed the probabilistic distribution of energy consumption and its relation to indicators characterizing charging events. we collected geospatial datasets and utilizing statistical methods for data pre-processing, we prepared features modelling the spatial context in which the charging infrastructure operates. to enhance the statistical reliability of results, we applied the bootstrap method together with the lasso method that combines regression with variable selection ability. we evaluate the statistical distributions of the selected regression coefficients. we identified the most influential features correlated with energy consumption, indicating that the spatial context of the charging infrastructure affects its utilization pattern. many of these features are related to the economic prosperity of residents. application of the methodology to a specific class of charging infrastructure enables the differentiation of selected features, e.g. by the used rollout strategy. overall, the paper demonstrates the application of statistical methodologies to energy data and provides insights on factors potentially shaping the energy consumption that could be utilized when developing models to inform charging infrastructure deployment and planning of power grids.", "categories": "stat.ap cs.lg econ.em math.oc", "created": "2020-06-02", "updated": "2020-06-03", "authors": ["milan straka", "rui carvalho", "gijs van der poel", "\u013eubo\u0161 buzna"], "url": "https://arxiv.org/abs/2006.01672"}, {"title": "on the plausibility of the latent ignorability assumption", "id": "2006.01703", "abstract": "the estimation of the causal effect of an endogenous treatment based on an instrumental variable (iv) is often complicated by attrition, sample selection, or non-response in the outcome of interest. to tackle the latter problem, the latent ignorability (li) assumption imposes that attrition/sample selection is independent of the outcome conditional on the treatment compliance type (i.e. how the treatment behaves as a function of the instrument), the instrument, and possibly further observed covariates. as a word of caution, this note formally discusses the strong behavioral implications of li in rather standard iv models. we also provide an empirical illustration based on the job corps experimental study, in which the sensitivity of the estimated program effect to li and alternative assumptions about outcome attrition is investigated.", "categories": "econ.em", "created": "2020-06-02", "updated": "2020-06-03", "authors": ["martin huber"], "url": "https://arxiv.org/abs/2006.01703"}, {"title": "extractive contest design", "id": "2006.01808", "abstract": "we consider contest success functions (csfs) that extract contestants' values of the prize. in the case in which the values are observable to the contest designer, in the more-than-two-contestant or common-value subcase, we present a csf extractive in any equilibrium; in the other subcase, we present a csf extractive in some equilibrium, but there exists no csf extractive in any equilibrium. in the case in which the values are not observable, there exists no csf extractive in some equilibrium. in the case in which the values are observable and common, we present extractive a csf extractive in any equilibrium; we present a class of csfs extractive in some equilibrium, and this class can control the number of active contestants.", "categories": "econ.th", "created": "2020-06-02", "updated": "2020-06-02", "authors": ["tomohiko kawamori"], "url": "https://arxiv.org/abs/2006.01808"}, {"title": "subjective complexity under uncertainty", "id": "2006.01852", "abstract": "complexity of the problem of choosing among uncertain acts is a salient feature of many of the environments in which departures from expected utility theory are observed. i propose and axiomatize a model of choice under uncertainty in which the size of the partition with respect to which an act is measurable arises endogenously as a measure of subjective complexity. i derive a representation of incomplete simple bounds preferences in which acts that are complex from the perspective of the decision maker are bracketed by simple acts to which they are related by statewise dominance. the key axioms are motivated by a model of learning from limited data. i then consider choice behavior characterized by a \"cautious completion\" of simple bounds preferences, and discuss the relationship between this model and models of ambiguity aversion. i develop general comparative statics techniques, and explore applications to portfolio choice, contracting, and insurance choice.", "categories": "econ.th econ.em", "created": "2020-06-02", "updated": "2020-09-22", "authors": ["quitz\u00e9 valenzuela-stookey"], "url": "https://arxiv.org/abs/2006.01852"}, {"title": "reaping the informational surplus in bayesian persuasion", "id": "2006.02048", "abstract": "the bayesian persuasion model studies communication between an informed sender and a receiver with a payoff-relevant action, emphasizing the ability of a sender to extract maximal surplus from his informational advantage. in this paper we study a setting with multiple senders, but in which the receiver interacts with only one sender of his choice: senders commit to signals and the receiver then chooses, at the interim stage, with which sender to interact. our main result is that whenever senders are even slightly uncertain about each other's preferences, the receiver receives all the informational surplus in all equilibria of this game.", "categories": "cs.gt econ.th", "created": "2020-06-03", "updated": "", "authors": ["ronen gradwohl", "niklas hahn", "martin hoefer", "rann smorodinsky"], "url": "https://arxiv.org/abs/2006.02048"}, {"title": "time delay and investment decisions: evidence from an experiment in   tanzania", "id": "2006.02143", "abstract": "attitudes toward risk underlie virtually every important economic decision an individual makes. in this experimental study, i examine how introducing a time delay into the execution of an investment plan influences individuals' risk preferences. the field experiment proceeded in three stages: a decision stage, an execution stage and a payout stage. at the outset, in the decision stage (stage 1), each subject was asked to make an investment plan by splitting a monetary investment amount between a risky asset and a safe asset. subjects were informed that the investment plans they made in the decision stage are binding and will be executed during the execution stage (stage 2). the payout stage (stage 3) was the payout date. the timing of the decision stage and payout stage was the same for each subject, but the timing of the execution stage varied experimentally. i find that individuals who were assigned to execute their investment plans later (i.e., for whom there was a greater delay prior to the execution stage) invested a greater amount in the risky asset during the decision stage.", "categories": "econ.gn q-fin.ec", "created": "2020-06-02", "updated": "2020-06-04", "authors": ["plamen nikolov"], "url": "https://arxiv.org/abs/2006.02143"}, {"title": "a negative correlation strategy for bracketing in   difference-in-differences with application to the effect of voter   identification laws on voter turnout", "id": "2006.02423", "abstract": "the method of difference-in-differences (did) is widely used to study the causal effect of policy interventions in observational studies. did exploits a before and after comparison of the treated and control units to remove the bias due to time-invariant unmeasured confounders under the parallel trends assumption. estimates from did, however, will be biased if the outcomes for the treated and control units evolve differently in the absence of treatment, namely if the parallel trends assumption is violated. we propose a general identification strategy that leverages two groups of control units whose outcomes relative to the treated units exhibit a negative correlation, and achieves partial identification of the average treatment effect for the treated. the identified set is of a union bounds form that previously developed partial identification inference methods do not apply to. we develop a novel bootstrap method to construct valid confidence intervals for the identified set and parameter of interest when the identified set is of a union bounds form, and we establish the theoretical properties. we develop a simple falsification test and sensitivity analysis. we apply the proposed strategy for bracketing to an application on the effect of voter identification laws in georgia and indiana on turnout and find evidence that the laws increased turnout rates.", "categories": "stat.me econ.em stat.ap", "created": "2020-06-03", "updated": "", "authors": ["ting ye", "luke keele", "raiden hasegawa", "dylan s. small"], "url": "https://arxiv.org/abs/2006.02423"}, {"title": "testing finite moment conditions for the consistency and the root-n   asymptotic normality of the gmm and m estimators", "id": "2006.02541", "abstract": "common approaches to inference for structural and reduced-form parameters in empirical economic analysis are based on the consistency and the root-n asymptotic normality of the gmm and m estimators. the canonical consistency (respectively, root-n asymptotic normality) for these classes of estimators requires at least the first (respectively, second) moment of the score to be finite. in this article, we present a method of testing these conditions for the consistency and the root-n asymptotic normality of the gmm and m estimators. the proposed test controls size nearly uniformly over the set of data generating processes that are compatible with the null hypothesis. simulation studies support this theoretical result. applying the proposed test to the market share data from the dominick's finer foods retail chain, we find that a common \\textit{ad hoc} procedure to deal with zero market shares in analysis of differentiated products markets results in a failure to satisfy the conditions for both the consistency and the root-n asymptotic normality.", "categories": "econ.em", "created": "2020-06-03", "updated": "2020-09-02", "authors": ["yuya sasaki", "yulong wang"], "url": "https://arxiv.org/abs/2006.02541"}, {"title": "the pain of a new idea: do late bloomers response to extension service   in rural ethiopia?", "id": "2006.02846", "abstract": "the paper analyses the efficiency of extension programs in the adoption of chemical fertilisers in ethiopia between 1994 and 2004. fertiliser adoption provides a suitable strategy to ensure and stabilize food production in remote vulnerable areas. extension services programs have a long history in supporting the application of fertiliser. how-ever, their efficiency is questioned. in our analysis, we focus on seven villages with a considerable time lag in fertiliser diffusion. using matching techniques avoids sample selection bias in the comparison of treated (households received extension service) and controlled households. additionally to common factors, measures of culture, proxied by ethnicity and religion, aim to control for potential tensions between extension agents and peasants that hamper the efficiency of the program. we find a considerable impact of extension service on the first fertiliser adoption. the impact is consistent for five of seven villages.", "categories": "econ.em", "created": "2020-06-04", "updated": "", "authors": ["alexander jordan", "marco guerzoni"], "url": "https://arxiv.org/abs/2006.02846"}, {"title": "short-run health consequences of retirement and pension benefits:   evidence from china", "id": "2006.02900", "abstract": "this paper examines the impact of the new rural pension scheme (nrps) in china. exploiting the staggered implementation of an nrps policy expansion that began in 2009, we use a difference-in-difference approach to study the effects of the introduction of pension benefits on the health status, health behaviors, and healthcare utilization of rural chinese adults age 60 and above. the results point to three main conclusions. first, in addition to improvements in self-reported health, older adults with access to the pension program experienced significant improvements in several important measures of health, including mobility, self-care, usual activities, and vision. second, regarding the functional domains of mobility and self-care, we found that the females in the study group led in improvements over their male counterparts. third, in our search for the mechanisms that drive positive retirement program results, we find evidence that changes in individual health behaviors, such as a reduction in drinking and smoking, and improved sleep habits, play an important role. our findings point to the potential benefits of retirement programs resulting from social spillover effects. in addition, these programs may lessen the morbidity burden among the retired population.", "categories": "econ.gn q-fin.ec", "created": "2020-06-02", "updated": "", "authors": ["plamen nikolov", "alan adelman"], "url": "https://arxiv.org/abs/2006.02900"}, {"title": "coastal flood risk in the mortgage market: storm surge models'   predictions vs. flood insurance maps", "id": "2006.02977", "abstract": "prior literature has argued that flood insurance maps may not capture the extent of flood risk. this paper performs a granular assessment of coastal flood risk in the mortgage market by using physical simulations of hurricane storm surge heights instead of using fema's flood insurance maps. matching neighborhood-level predicted storm surge heights with mortgage files suggests that coastal flood risk may be large: originations and securitizations in storm surge areas have been rising sharply since 2012, while they remain stable when using flood insurance maps. every year, more than 50 billion dollars of originations occur in storm surge areas outside of insurance floodplains. the share of agency mortgages increases in storm surge areas, yet remains stable in the flood insurance 100-year floodplain. mortgages in storm surge areas are more likely to be complex: non-fully amortizing features such as interest-only or adjustable rates. households may also be more vulnerable in storm surge areas: median household income is lower, the share of african americans and hispanics is substantially higher, the share of individuals with health coverage is lower. price-to-rent ratios are declining in storm surge areas while they are increasing in flood insurance areas. this paper suggests that uncovering future financial flood risk requires scientific models that are independent of the flood insurance mapping process.", "categories": "econ.gn q-fin.ec", "created": "2020-06-04", "updated": "2020-06-09", "authors": ["amine ouazad"], "url": "https://arxiv.org/abs/2006.02977"}, {"title": "evaluating the effectiveness of regional lockdown policies in the   containment of covid-19: evidence from pakistan", "id": "2006.02987", "abstract": "to slow down the spread of covid-19, administrative regions within pakistan imposed complete and partial lockdown restrictions on socio-economic activities, religious congregations, and human movement. here we examine the impact of regional lockdown strategies on covid-19 outcomes. after conducting econometric analyses (regression discontinuity and negative binomial regressions) on official data from the national institute of health (nih) pakistan, we find that the strategies did not lead to a similar level of covid-19 caseload (positive cases and deaths) in all regions. in terms of reduction in the overall caseload (positive cases and deaths), compared to no lockdown, complete and partial lockdown appeared to be effective in four regions: balochistan, gilgit baltistan (gt), islamabad capital territory (ict), and azad jammu and kashmir (ajk). contrarily, complete and partial lockdowns did not appear to be effective in containing the virus in the three largest provinces of punjab, sindh, and khyber pakhtunkhwa (kpk). the observed regional heterogeneity in the effectiveness of lockdowns advocates for a careful use of lockdown strategies based on the demographic, social, and economic factors.", "categories": "econ.em physics.soc-ph q-bio.pe", "created": "2020-06-04", "updated": "", "authors": ["hamza umer", "muhammad salar khan"], "url": "https://arxiv.org/abs/2006.02987"}, {"title": "the importance of being discrete: on the (in-)accuracy of continuous   approximations in auction theory", "id": "2006.03016", "abstract": "while auction theory views bids and valuations as continuous variables, real-world auctions are necessarily discrete. in this paper, we use a combination of analytical and computational methods to investigate whether incorporating discreteness substantially changes the predictions of auction theory, focusing on the case of uniformly distributed valuations so that our results bear on the majority of auction experiments. in some cases, we find that introducing discreteness changes little. for example, the first-price auction with two bidders and an even number of values has a symmetric equilibrium that closely resembles its continuous counterpart and converges to its continuous counterpart as the discretisation goes to zero. in others, however, we uncover discontinuity results. for instance, introducing an arbitrarily small amount of discreteness into the all-pay auction makes its symmetric, pure-strategy equilibrium disappear; and appears (based on computational experiments) to rob the game of pure-strategy equilibria altogether. these results raise questions about the continuity approximations on which auction theory is based and prompt a re-evaluation of the experimental literature.", "categories": "econ.th", "created": "2020-06-04", "updated": "", "authors": ["itzhak rasooly", "carlos gavidia-calderon"], "url": "https://arxiv.org/abs/2006.03016"}, {"title": "digital currency and economic crises: helping states respond", "id": "2006.03023", "abstract": "the current crisis, at the time of writing, has had a profound impact on the financial world, introducing the need for creative approaches to revitalising the economy at the micro level as well as the macro level. in this informal analysis and design proposal, we describe how infrastructure for digital assets can serve as a useful monetary and fiscal policy tool and an enabler of existing tools in the future, particularly during crises, while aligning the trajectory of financial technology innovation toward a brighter future. we propose an approach to digital currency that would allow people without banking relationships to transact electronically and privately, including both internet purchases and point-of-sale purchases that are required to be cashless. we also propose an approach to digital currency that would allow for more efficient and transparent clearing and settlement, implementation of monetary and fiscal policy, and management of systemic risk. the digital currency could be implemented as central bank digital currency (cbdc), or it could be issued by the government and collateralised by public funds or treasury assets. our proposed architecture allows both manifestations and would be operated by banks and other money services businesses, operating within a framework overseen by government regulators. we argue that now is the time for action to undertake development of such a system, not only because of the current crisis but also in anticipation of future crises resulting from geopolitical risks, the continued globalisation of the digital economy, and the changing value and risks that technology brings.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-06-04", "updated": "2020-08-02", "authors": ["geoffrey goodell", "hazem danny al-nakib", "paolo tasca"], "url": "https://arxiv.org/abs/2006.03023"}, {"title": "inflation dynamics of financial shocks", "id": "2006.03301", "abstract": "we study the effects of financial shocks on the united states economy by using a bayesian structural vector autoregressive (svar) model that exploits the non-normalities in the data. we use this method to uniquely identify the model and employ inequality constraints to single out financial shocks. the results point to the existence of two distinct financial shocks that have opposing effects on inflation, which supports the idea that financial shocks are transmitted to the real economy through both demand and supply side channels.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["olli palm\u00e9n"], "url": "https://arxiv.org/abs/2006.03301"}, {"title": "capital and labor income pareto exponents across time and space", "id": "2006.03441", "abstract": "we estimate capital and labor income pareto exponents across 428 country-year observations that span 54 countries over half a century. we document two stylized facts: (i) capital income is more unequally distributed than labor income; namely, the capital exponent (1-3) is smaller than labor (2-5), and (ii) capital and labor exponents are nearly uncorrelated. to explain these findings, we build an incomplete market model with job ladders and capital income risk that gives rise to a capital income pareto exponent smaller than but nearly unrelated to the labor exponent. our results suggest the importance of distinguishing income and wealth inequality.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-03", "updated": "2020-09-29", "authors": ["tjeerd de vries", "alexis akira toda"], "url": "https://arxiv.org/abs/2006.03441"}, {"title": "economic properties of multi-product supply chains", "id": "2006.03467", "abstract": "we interpret multi-product supply chains (scs) as coordinated markets; under this interpretation, a sc optimization problem is a market clearing problem that allocates resources and associated economic values (prices) to different stakeholders that bid into the market (suppliers, consumers, transportation, and processing technologies). the market interpretation allows us to establish fundamental properties that explain how physical resources (primal variables) and associated economic values (dual variables) flow in the sc. we use duality theory to explain why incentivizing markets by forcing stakeholder participation (e.g., by imposing demand satisfaction or service provision constraints) yields artificial price behavior, inefficient allocations, and economic losses. to overcome these issues, we explore market incentive mechanisms that use bids; here, we introduce the concept of a stakeholder graph (a product-based representation of a supply chain) and show that this representation allows us to naturally determine minimum bids that activate the market. these results provide guidelines to design sc formulations that properly remunerate stakeholders and to design policy that foster market transactions. the results are illustrated using an urban waste management problem for a city of 100,000 residents.", "categories": "math.oc econ.th", "created": "2020-06-04", "updated": "2020-07-02", "authors": ["philip a. tominac", "victor m. zavala"], "url": "https://arxiv.org/abs/2006.03467"}, {"title": "commuting variability by wage groups in baton rouge 1990-2010", "id": "2006.03498", "abstract": "residential segregation recently has shifted to more class or income-based in the united states, and neighborhoods are undergoing significant changes such as commuting patterns over time. to better understand the commuting inequality across neighborhoods of different income levels, this research analyzes commuting variability (in both distance and time) across wage groups as well as stability over time using the ctpp data 1990-2010 in baton rouge. in comparison to previous work, commuting distance is estimated more accurately by monte carlo simulation of individual trips to mitigate aggregation error and scale effect. the results based on neighborhoods mean wage rate indicate that commuting behaviors vary across areas of different wage rates and such variability is captured by a convex shape. affluent neighborhoods tended to commute more but highest-wage neighborhoods retreated for less commuting. this trend remains relatively stable over time despite an overall transportation improvement in general. a complementary analysis based on the distribution of wage groups is conducted to gain more detailed insights and uncovers the lasting poor mobility (e.g., fewer location and transport options) of the lowest-wage workers in 1990-2010.", "categories": "stat.ap cs.cy econ.gn q-fin.ec", "created": "2020-05-30", "updated": "", "authors": ["yujie hu", "fahui wang", "chester wilmot"], "url": "https://arxiv.org/abs/2006.03498"}, {"title": "sovereign default risk and credit supply: evidence from the euro area", "id": "2006.03592", "abstract": "did sovereign default risk affect macroeconomic activity through firms' access to credit during the european sovereign debt crisis? we investigate this question by a estimating a structural panel vector autoregressive model for italy, spain, portugal, and ireland, where the sovereign risk shock is identified using sign restrictions. the results suggest that decline in the creditworthiness of the sovereign contributed to a fall in private lending and economic activity in several euro-area countries by reducing the value of banks' assets and crowding out private lending.", "categories": "econ.gn q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["olli palm\u00e9n"], "url": "https://arxiv.org/abs/2006.03592"}, {"title": "coordinated transaction scheduling in multi-area electricity markets:   equilibrium and learning", "id": "2006.03618", "abstract": "tie-line scheduling in multi-area power systems in the us largely proceeds through a market-based mechanism called coordinated transaction scheduling (cts). we analyze this market mechanism through a game-theoretic lens. our analysis characterizes the effects of market liquidity, market participants' forecasts about inter-area price spreads, transaction fees and interaction of cts markets with financial transmission rights. using real data, we empirically verify that cts bidders can employ simple learning algorithms to discover nash equilibria that support the conclusions drawn from the equilibrium analysis.", "categories": "cs.gt cs.sy econ.gn eess.sy q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["mariola ndrio", "subhonmesh bose", "ye guo", "lang tong"], "url": "https://arxiv.org/abs/2006.03618"}, {"title": "the effects of access to credit on productivity among microenterprises:   separating technological changes from changes in technical efficiency", "id": "2006.03650", "abstract": "improving productivity among farm microenterprises is important, especially in low-income countries where market imperfections are pervasive and resources are scarce. relaxing credit constraints can increase the productivity of farmers. using a field experiment involving microenterprises in bangladesh, we estimate the impact of access to credit on the overall productivity of rice farmers, and disentangle the total effect into technological change (frontier shift) and technical efficiency changes. we find that relative to the baseline rice output per decimal, access to credit results in, on average, approximately a 14 percent increase in yield, holding all other inputs constant. after decomposing the total effect into the frontier shift and efficiency improvement, we find that, on average, around 11 percent of the increase in output comes from changes in technology, or frontier shift, while the remaining 3 percent is attributed to improvements in technical efficiency. the efficiency gain is higher for modern hybrid rice varieties, and almost zero for traditional rice varieties. within the treatment group, the effect is greater among pure tenant and mixed-tenant farm households compared with farmers that only cultivate their own land.", "categories": "econ.gn q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["nusrat abedin jimi", "plamen nikolov", "mohammad abdul malek", "subal kumbhakar"], "url": "https://arxiv.org/abs/2006.03650"}, {"title": "past production constrains current energy demands: persistent scaling in   global energy consumption and implications for climate change mitigation", "id": "2006.03718", "abstract": "climate change has become intertwined with the global economy. here, we describe the importance of inertia to continued growth in energy consumption. drawing from thermodynamic arguments, and using 38 years of available statistics between 1980 to 2017, we find a persistent time-independent scaling between the historical time integral $w$ of world inflation-adjusted economic production $y$, or $w\\left(t\\right) = \\int_0^t y\\left(t'\\right)dt'$, and current rates of world primary energy consumption $\\mathcal e$, such that $\\lambda = \\mathcal{e}/w = 5.9\\pm0.1$ gigawatts per trillion 2010 us dollars. this empirical result implies that population expansion is a symptom rather than a cause of the current exponential rise in $\\mathcal e$ and carbon dioxide emissions $c$, and that it is past innovation of economic production efficiency $y/\\mathcal{e}$ that has been the primary driver of growth, at predicted rates that agree well with data. options for stabilizing $c$ are then limited to rapid decarbonization of $\\mathcal e$ through sustained implementation of over one gigawatt of renewable or nuclear power capacity per day. alternatively, assuming continued reliance on fossil fuels, civilization could shift to a steady-state economy that devotes economic production exclusively to maintenance rather than expansion. if this were instituted immediately, continual energy consumption would still be required, so atmospheric carbon dioxide concentrations would not balance natural sinks until concentrations exceeded 500 ppmv, and double pre-industrial levels if the steady-state was attained by 2030.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["timothy j. garrett", "matheus r. grasselli", "stephen keen"], "url": "https://arxiv.org/abs/2006.03718"}, {"title": "what factors drive individual misperceptions of the returns to schooling   in tanzania? some lessons for education policy", "id": "2006.03723", "abstract": "evidence on educational returns and the factors that determine the demand for schooling in developing countries is extremely scarce. building on previous studies that show individuals underestimating the returns to schooling, we use two surveys from tanzania to estimate both the actual and perceived schooling returns and subsequently examine what factors drive individual misperceptions regarding actual returns. using ordinary least squares and instrumental variable methods, we find that each additional year of schooling in tanzania increases earnings, on average, by 9 to 11 percent. we find that on average individuals underestimate returns to schooling by 74 to 79 percent and three factors are associated with these misperceptions: income, asset poverty and educational attainment. shedding light on what factors relate to individual beliefs about educational returns can inform policy on how to structure effective interventions in order to correct individual misperceptions.", "categories": "econ.gn q-fin.ec", "created": "2020-06-05", "updated": "", "authors": ["plamen nikolov", "nusrat jimi"], "url": "https://arxiv.org/abs/2006.03723"}, {"title": "false (and missed) discoveries in financial economics", "id": "2006.04269", "abstract": "multiple testing plagues many important questions in finance such as fund and factor selection. we propose a new way to calibrate both type i and type ii errors. next, using a double-bootstrap method, we establish a t-statistic hurdle that is associated with a specific false discovery rate (e.g., 5%). we also establish a hurdle that is associated with a certain acceptable ratio of misses to false discoveries (type ii error scaled by type i error), which effectively allows for differential costs of the two types of mistakes. evaluating current methods, we find that they lack power to detect outperforming managers.", "categories": "stat.me econ.em", "created": "2020-06-07", "updated": "", "authors": ["campbell r. harvey", "yan liu"], "url": "https://arxiv.org/abs/2006.04269"}, {"title": "on forward invariance in lyapunov stability theorem for local stability", "id": "2006.04280", "abstract": "forward invariance of a basin of attraction is often overlooked when using a lyapunov stability theorem to prove local stability; even if the lyapunov function decreases monotonically in a neighborhood of an equilibrium, the dynamic may escape from this neighborhood. in this note, we fix this gap by finding a smaller neighborhood that is forward invariant. this helps us to prove local stability more naturally without tracking each solution path. similarly, we prove a transitivity theorem about basins of attractions without requiring forward invariance.   keywords: lyapunov function, local stability, forward invariance, evolutionary dynamics.", "categories": "math.oc cs.gt cs.sy econ.th eess.sy math.ds", "created": "2020-06-07", "updated": "", "authors": ["dai zusai"], "url": "https://arxiv.org/abs/2006.04280"}, {"title": "envy-free relaxations for goods, chores, and mixed items", "id": "2006.04428", "abstract": "in fair division problems, we are given a set $s$ of $m$ items and a set $n$ of $n$ agents with individual preferences, and the goal is to find an allocation of items among agents so that each agent finds the allocation fair. there are several established fairness concepts and envy-freeness is one of the most extensively studied ones. however envy-free allocations do not always exist when items are indivisible and this has motivated relaxations of envy-freeness: envy-freeness up to one item (ef1) and envy-freeness up to any item (efx) are two well-studied relaxations. we consider the problem of finding ef1 and efx allocations for utility functions that are not necessarily monotone, and propose four possible extensions of different strength to this setting.   in particular, we present a polynomial-time algorithm for finding an ef1 allocation for two agents with arbitrary utility functions. an example is given showing that efx allocations need not exist for two agents with non-monotone, non-additive, identical utility functions. however, when all agents have monotone (not necessarily additive) identical utility functions, we prove that an efx allocation of chores always exists. as a step toward understanding the general case, we discuss two subclasses of utility functions: boolean utilities that are $\\{0,+1\\}$-valued functions, and negative boolean utilities that are $\\{0,-1\\}$-valued functions. for the latter, we give a polynomial time algorithm that finds an efx allocation when the utility functions are identical.", "categories": "econ.th cs.dm cs.gt", "created": "2020-06-08", "updated": "", "authors": ["krist\u00f3f b\u00e9rczi", "erika r. b\u00e9rczi-kov\u00e1cs", "endre boros", "fekadu tolessa gedefa", "naoyuki kamiyama", "telikepalli kavitha", "yusuke kobayashi", "kazuhisa makino"], "url": "https://arxiv.org/abs/2006.04428"}, {"title": "vertical vs. horizontal policy in a capabilities model of economic   development", "id": "2006.04624", "abstract": "against the background of renewed interest in vertical support policies targeting specific industries or technologies, we investigate the effects of vertical vs. horizontal policies in a combinatorial model of economic development. in the framework we propose, an economy develops by acquiring new capabilities allowing for the production of an ever greater variety of products with an increasing complexity. innovation policy can aim to expand the number of capabilities (vertical policy) or the ability to combine capabilities (horizontal policy). the model shows that for low-income countries, the two policies are complementary. for high-income countries that are specialised in the most complex products, focusing on horizontal policy only yields the highest returns. we reflect on the model results in the light of the contemporary debate on vertical policy.", "categories": "econ.gn q-fin.ec", "created": "2020-06-08", "updated": "", "authors": ["alje van dam", "koen frenken"], "url": "https://arxiv.org/abs/2006.04624"}, {"title": "heterogeneous effects of job displacement on earnings", "id": "2006.04968", "abstract": "this paper considers how the effect of job displacement varies across different individuals. in particular, our interest centers on features of the distribution of the individual-level effect of job displacement. identifying features of this distribution is particularly challenging -- e.g., even if we could randomly assign workers to be displaced or not, many of the parameters that we consider would not be point identified. we exploit our access to panel data, and our approach relies on comparing outcomes of displaced workers to outcomes the same workers would have experienced if they had not been displaced and if they maintained the same rank in the distribution of earnings as they had before they were displaced. using data from the displaced workers survey, we find that displaced workers earn about $157 per week less, on average, than they would have earned if they had not been displaced. we also find that there is substantial heterogeneity. we estimate that 42% of workers have higher earnings than they would have had if they had not been displaced and that a large fraction of workers have experienced substantially more negative effects than the average effect of displacement. finally, we also document major differences in the distribution of the effect of job displacement across education levels, sex, age, and counterfactual earnings levels. throughout the paper, we rely heavily on quantile regression. first, we use quantile regression as a flexible (yet feasible) first step estimator of conditional distributions and quantile functions that our main results build on. we also use quantile regression to study how covariates affect the distribution of the individual-level effect of job displacement.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-08", "updated": "2020-09-30", "authors": ["afrouz azadikhah jahromi", "brantly callaway"], "url": "https://arxiv.org/abs/2006.04968"}, {"title": "an optimal distributionally robust auction", "id": "2006.05192", "abstract": "an indivisible object may be sold to one of $n$ agents who know their valuations of the object. the seller would like to use a revenue-maximizing mechanism but her knowledge of the valuations' distribution is scarce: she knows only the means (which may be different) and an upper bound for valuations. valuations may be correlated.   using a constructive approach based on duality, we prove that a mechanism that maximizes the worst-case expected revenue among all deterministic dominant-strategy incentive compatible, ex post individually rational mechanisms is such that the object should be awarded to the agent with the highest linear score provided it is nonnegative. linear scores are bidder-specific linear functions of bids. the set of optimal mechanisms includes other mechanisms but all those have to be close to the optimal linear score auction in a certain sense. when means are high, all optimal mechanisms share the linearity property. second-price auction without a reserve is an optimal mechanism when the number of symmetric bidders is sufficiently high.", "categories": "econ.th cs.gt", "created": "2020-06-09", "updated": "2020-08-26", "authors": ["alex suzdaltsev"], "url": "https://arxiv.org/abs/2006.05192"}, {"title": "ensemble learning with statistical and structural models", "id": "2006.05308", "abstract": "statistical and structural modeling represent two distinct approaches to data analysis. in this paper, we propose a set of novel methods for combining statistical and structural models for improved prediction and causal inference. our first proposed estimator has the doubly robustness property in that it only requires the correct specification of either the statistical or the structural model. our second proposed estimator is a weighted ensemble that has the ability to outperform both models when they are both misspecified. experiments demonstrate the potential of our estimators in various settings, including fist-price auctions, dynamic models of entry and exit, and demand estimation with instrumental variables.", "categories": "econ.em cs.lg", "created": "2020-06-07", "updated": "", "authors": ["jiaming mao", "jingzhi xu"], "url": "https://arxiv.org/abs/2006.05308"}, {"title": "are weighted games sufficiently good for binary voting?", "id": "2006.05330", "abstract": "binary yes-no decisions in a legislative committee or a shareholder meeting are commonly modeled as a weighted game. however, there are noteworthy exceptions. e.g., the voting rules of the european council according to the treaty of lisbon use a more complicated construction. here we want to study the question if we loose much from a practical point of view, if we restrict ourselves to weighted games. to this end, we invoke power indices that measure the influence of a member in binary decision committees. more precisely, we compare the achievable power distributions of weighted games with those from a reasonable superset of weighted games", "categories": "cs.gt econ.th", "created": "2020-06-09", "updated": "", "authors": ["sascha kurz"], "url": "https://arxiv.org/abs/2006.05330"}, {"title": "designing stable elections: a survey", "id": "2006.05460", "abstract": "we survey the design of elections that are resilient to attempted interference by third parties. for example, suppose votes have been cast in an election between two candidates, and then each vote is randomly changed with a small probability, independently of the other votes. it is desirable to keep the outcome of the election the same, regardless of the changes to the votes. it is well known that the us electoral college system is about 5 times more likely to have a changed outcome due to vote corruption, when compared to a majority vote. in fact, mossel, o'donnell and oleszkiewicz proved in 2005 that the majority voting method is most stable to this random vote corruption, among voting methods where each person has a small influence on the election. we discuss some recent progress on the analogous result for elections between more than two candidates. in this case, plurality should be most stable to corruption in votes. we also survey results on adversarial election manipulation (where an adversary can select particular votes to change, perhaps in a non-random way), and we briefly discuss ranked choice voting methods (where a vote is a ranked list of candidates).", "categories": "math.pr cs.gt econ.th math.co", "created": "2020-06-09", "updated": "", "authors": ["steven heilman"], "url": "https://arxiv.org/abs/2006.05460"}, {"title": "bailout stigma", "id": "2006.05640", "abstract": "we develop a model of bailout stigma where accepting a bailout signals a firm's balance-sheet weakness and worsens its funding prospect. to avoid stigma, a firm with high-quality legacy assets either withdraws from subsequent financing after receiving a bailout or refuses a bailout altogether to send a favorable signal. the former leads to a short-lived stimulation with subsequent market freeze even worse than if there were no bailout. the latter revives the funding market, albeit with delay, to the level achievable without any stigma. strikingly, a bailout offer is most effective when many firms reject it (to build a favorable reputation) rather than accept it.", "categories": "q-fin.gn econ.th", "created": "2020-06-09", "updated": "", "authors": ["yeon-koo che", "chongwoo choe", "keeyoung rhee"], "url": "https://arxiv.org/abs/2006.05640"}, {"title": "a framework for modeling interdependencies among households, businesses,   and infrastructure systems; and their response to disruptions", "id": "2006.05678", "abstract": "urban systems, composed of households, businesses, and infrastructures, are continuously evolving and expanding. this has several implications because the impacts of disruptions, and the complexity and interdependence of systems, are rapidly increasing. hence, we face a challenge in how to improve our understanding about the interdependencies among those entities, as well as their responses to disruptions. the aims of this study were to (1) create an agent that mimics the metabolism of a business or household that obtains supplies from and provides output to infrastructure systems; (2) implement a network of agents that exchange resources, as coordinated with a price mechanism; and (3) test the responses of this prototype model to disruptions. our investigation resulted in the development of a business/household agent and a dynamically self-organizing mechanism of network coordination under disruption based on costs for production and transportation. simulation experiments confirmed the feasibility of this new model for analyzing responses to disruptions. among the nine disruption scenarios considered, in line with our expectations, the one combining the failures of infrastructure links and production processes had the most negative impact. we also identified areas for future research that focus on network topologies, mechanisms for resource allocation, and disruption generation.", "categories": "eess.sy cs.ma cs.si cs.sy econ.gn q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["mateusz iwo dubaniowski", "hans r. heinimann"], "url": "https://arxiv.org/abs/2006.05678"}, {"title": "a public-private insurance model for natural risk management: an   application to seismic and flood risks on residential buildings in italy", "id": "2006.05840", "abstract": "this paper proposes a public-private insurance scheme for earthquakes and floods in italy in which property-owners, the insurer and the government co-operate in risk financing. our model departs from the existing literature by describing a public-private insurance intended to relieve the financial burden that natural events place on governments, while at the same time assisting individuals and protecting the insurance business. hence, the business is aiming at maximizing social welfare rather than profits. given the limited amount of data available on natural risks, expected losses per individual have been estimated through risk-modeling. in order to evaluate the insurer's loss profile, spatial correlation among insured assets has been evaluated by means of the hoeffding bound for r-dependent random variables. though earthquakes generate expected losses that are almost six times greater than floods, we found that the amount of public funds needed to manage the two perils is almost the same. we argue that this result is determined by a combination of the risk aversion of individuals and the shape of the loss distribution. lastly, since earthquakes and floods are uncorrelated, we tested whether jointly managing the two perils can counteract the negative impact of spatial correlation. some benefit from risk diversification emerged, though the probability of the government having to inject further capital might be considerable. our findings suggest that, when not supported by the government, private insurance might either financially over-expose the insurer or set premiums so high that individuals would fail to purchase policies.", "categories": "econ.gn q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["selene perazzini", "giorgio stefano gnecco", "fabio pammolli"], "url": "https://arxiv.org/abs/2006.05840"}, {"title": "public-private partnership in the management of natural disasters: a   review", "id": "2006.05845", "abstract": "natural hazards can considerably impact the overall society of a country. as some degree of public sector involvement is always necessary to deal with the consequences of natural disasters, central governments have increasingly invested in proactive risk management planning. in order to empower and involve the whole society, some countries have established public-private partnerships, mainly with the insurance industry, with satisfactorily outcomes. although they have proven necessary and most often effective, the public-private initiatives have often incurred high debts or have failed to achieved the desired risk reduction objectives. we review the role of these partnerships in the management of natural risks, with particular attention to the insurance sector. among other country-specific issues, poor risk knowledge and weak governance have widely challenged the initiatives during the recent years, while the future is threatened by the uncertainty of climate change and unsustainable development. in order to strengthen the country's resilience, a greater involvement of all segments of the community, especially the weakest layers, is needed and the management of natural risks should be included in a sustainable development plan.", "categories": "econ.gn q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["selene perazzini"], "url": "https://arxiv.org/abs/2006.05845"}, {"title": "trading privacy for the greater social good: how did america react   during covid-19?", "id": "2006.05859", "abstract": "digital contact tracing and analysis of social distancing from smartphone location data are two prime examples of non-therapeutic interventions used in many countries to mitigate the impact of the covid-19 pandemic. while many understand the importance of trading personal privacy for the public good, others have been alarmed at the potential for surveillance via measures enabled through location tracking on smartphones. in our research, we analyzed massive yet atomic individual-level location data containing over 22 billion records from ten blue (democratic) and ten red (republican) cities in the u.s., based on which we present, herein, some of the first evidence of how americans responded to the increasing concerns that government authorities, the private sector, and public health experts might use individual-level location data to track the covid-19 spread. first, we found a significant decreasing trend of mobile-app location-sharing opt-out. whereas areas with more democrats were more privacy-concerned than areas with more republicans before the advent of the covid-19 pandemic, there was a significant decrease in the overall opt-out rates after covid-19, and this effect was more salient among democratic than republican cities. second, people who practiced social distancing (i.e., those who traveled less and interacted with fewer close contacts during the pandemic) were also less likely to opt-out, whereas the converse was true for people who practiced less social-distancing. this relationship also was more salient among democratic than republican cities. third, high-income populations and males, compared with low-income populations and females, were more privacy-conscientious and more likely to opt-out of location tracking.", "categories": "econ.em econ.gn q-fin.ec stat.ap", "created": "2020-06-10", "updated": "", "authors": ["anindya ghose", "beibei li", "meghanath macha", "chenshuo sun", "natasha ying zhang foutz"], "url": "https://arxiv.org/abs/2006.05859"}, {"title": "risk attitudes and human mobility during the covid-19 pandemic", "id": "2006.06078", "abstract": "behavioral responses to pandemics are less shaped by actual mortality or hospitalization risks than they are by risk attitudes. we explore human mobility patterns as a measure of behavioral responses during the covid-19 pandemic. our results indicate that risk-taking attitude is a critical factor in predicting reduction in human mobility and increase social confinement around the globe. we find that the sharp decline in movement after the who (world health organization) declared covid-19 to be a pandemic can be attributed to risk attitudes. our results suggest that regions with risk-averse attitudes are more likely to adjust their behavioral activity in response to the declaration of a pandemic even before most official government lockdowns. further understanding of the basis of responses to epidemics, e.g., precautionary behavior, will help improve the containment of the spread of the virus.", "categories": "econ.gn q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["ho fai chan", "ahmed skali", "david savage", "david stadelmann", "benno torgler"], "url": "https://arxiv.org/abs/2006.06078"}, {"title": "imf programs and economic growth in the drc: documentation, impact and   prospects", "id": "2006.06123", "abstract": "at the end of 2012 the international monetary fund (imf) has suspended its financial assistance to the democratic republic of the congo (drc). due to inflationary pressures which occurred in the last quarter of 2016, several decision-makers called for a reopening of a formal cooperation with the imf. this process was formally completed in december 2019. the restart of imf programs was greeted with satisfaction by politicians and widely commented in the media. however, recent history shows that the drc managed to achieve exceptional economic performance, between 2012 and 2016, without being in a formal cooperation with the imf. some people wonder whether imf assistance is a curse for recipient countries? we argue that the underlying problem has nothing to do with accepting or not the imf assistance, but rather in the ability of policy makers to establish effective leadership and good governance for the development and implementation supporting structural reforms.", "categories": "econ.gn math.ho q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["matata ponyo mapon", "jean-paul k. tsasa"], "url": "https://arxiv.org/abs/2006.06123"}, {"title": "secure: a social and environmental certificate for ai systems", "id": "2006.06217", "abstract": "in a world increasingly dominated by ai applications, an understudied aspect is the carbon and social footprint of these power-hungry algorithms that require copious computation and a trove of data for training and prediction. while profitable in the short-term, these practices are unsustainable and socially extractive from both a data-use and energy-use perspective. this work proposes an esg-inspired framework combining socio-technical measures to build eco-socially responsible ai systems. the framework has four pillars: compute-efficient machine learning, federated learning, data sovereignty, and a leedesque certificate.   compute-efficient machine learning is the use of compressed network architectures that show marginal decreases in accuracy. federated learning augments the first pillar's impact through the use of techniques that distribute computational loads across idle capacity on devices. this is paired with the third pillar of data sovereignty to ensure the privacy of user data via techniques like use-based privacy and differential privacy. the final pillar ties all these factors together and certifies products and services in a standardized manner on their environmental and social impacts, allowing consumers to align their purchase with their values.", "categories": "cs.cy cs.ai cs.lg econ.gn q-fin.ec", "created": "2020-06-11", "updated": "2020-07-19", "authors": ["abhishek gupta", "camylle lanteigne", "sara kingsley"], "url": "https://arxiv.org/abs/2006.06217"}, {"title": "re-evaluating cryptocurrencies' contribution to portfolio   diversification -- a portfolio analysis with special focus on german   investors", "id": "2006.06237", "abstract": "in this paper, we investigate whether mixing cryptocurrencies to a german investor portfolio improves portfolio diversification. we analyse this research question by applying a (mean variance) portfolio analysis using a toolbox consisting of (i) the comparison of descriptive statistics, (ii) graphical methods and (iii) econometric spanning tests. in contrast to most of the former studies we use a (broad) customized, equally-weighted cryptocurrency index (ewci) to capture the average development of a whole ex ante defined cryptocurrency universe and to mitigate possible survivorship biases in the data. according to glas/poddig (2018), this bias could have led to misleading results in some already existing studies. we find that cryptocurrencies can improve portfolio diversification in a few of the analyzed windows from our dataset (consisting of weekly observations from 2014-01-01 to 2019-05-31). however, we cannot confirm this pattern as the normal case. by including cryptocurrencies in their portfolios, investors predominantly cannot reach a significantly higher efficient frontier. these results also hold, if the non-normality of cryptocurrency returns is considered. moreover, we control for changes of the results, if transaction costs/illiquidities on the cryptocurrency market are additionally considered.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2020-06-11", "updated": "2020-08-06", "authors": ["tim schmitz", "ingo hoffmann"], "url": "https://arxiv.org/abs/2006.06237"}, {"title": "what drives inflation and how: evidence from additive mixed models   selected by caic", "id": "2006.06274", "abstract": "we analyze which forces explain inflation and how in a large panel of 124 countries from 1997 to 2015. models motivated by economic theory are compared to an approach based on model-based boosting and non-linearities are explicitly considered. we provide compelling evidence that the interaction of energy price and energy rents stand out among 40 explanatory variables. the output gap and globalization are also relevant drivers of inflation. credit and money growth, a country's inflation history and demographic changes are comparably less important while central bank related variables as well as political variables turn out to have the least empirical relevance. in a subset of countries public debt denomination and exchange rate arrangements also play a noteworthy role in the inflation process. by contrast, other public-debt variables and an inflation targeting regime have weaker explanatory power. finally, there is clear evidence of structural breaks in the effects since the financial crisis.", "categories": "stat.ap econ.em", "created": "2020-06-11", "updated": "", "authors": ["philipp baumann", "enzo rossi", "alexander volkmann"], "url": "https://arxiv.org/abs/2006.06274"}, {"title": "innovation and imitation", "id": "2006.06315", "abstract": "we study several models of growth driven by innovation and imitation by a continuum of firms, focusing on the interaction between the two. we first investigate a model on a technology ladder where innovation and imitation combine to generate a balanced growth path (bgp) with compact support, and with productivity distributions for firms that are truncated power-laws. we start with a simple model where firms can adopt technologies of other firms with higher productivities according to exogenous probabilities. we then study the case where the adoption probabilities depend on the probability distribution of productivities at each time. we finally consider models with a finite number of firms, which by construction have firm productivity distributions with bounded support. stochastic imitation and innovation can make the distance of the productivity frontier to the lowest productivity level fluctuate, and this distance can occasionally become large. alternatively, if we fix the length of the support of the productivity distribution because firms too far from the frontier cannot survive, the number of firms can fluctuate randomly.", "categories": "econ.th", "created": "2020-06-11", "updated": "2020-08-26", "authors": ["jess benhabib", "\u00e9ric brunet", "mildred hager"], "url": "https://arxiv.org/abs/2006.06315"}, {"title": "text as data: a machine learning-based approach to measuring uncertainty", "id": "2006.06457", "abstract": "the economic policy uncertainty index had gained considerable traction with both academics and policy practitioners. here, we analyse news feed data to construct a simple, general measure of uncertainty in the united states using a highly cited machine learning methodology. over the period january 1996 through may 2020, we show that the series unequivocally granger-causes the epu and there is no granger-causality in the reverse direction", "categories": "econ.em", "created": "2020-06-11", "updated": "", "authors": ["rickard nyman", "paul ormerod"], "url": "https://arxiv.org/abs/2006.06457"}, {"title": "reserve price optimization for first price auctions", "id": "2006.06519", "abstract": "the display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. in light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. in this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. we show that revenue in a first-price auction can be usefully decomposed into a \\emph{demand} component and a \\emph{bidding} component, and introduce techniques to reduce the variance of each component. we characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from google ad exchange.", "categories": "cs.gt cs.lg econ.em stat.ml", "created": "2020-06-11", "updated": "2020-06-28", "authors": ["zhe feng", "s\u00e9bastien lahaie", "jon schneider", "jinchao ye"], "url": "https://arxiv.org/abs/2006.06519"}, {"title": "data and incentives", "id": "2006.06543", "abstract": "many firms, such as banks and insurers, condition their level of service on a consumer's perceived \"quality,\" for instance their creditworthiness. increasingly, firms have access to consumer segmentations derived from auxiliary data on behavior, and can link outcomes across individuals in a segment for prediction. how does this practice affect consumer incentives to exert (socially-valuable) effort, e.g. to repay loans? we show that the impact of an identified linkage on behavior and welfare depends crucially on the structure of the linkage---namely, whether the linkage reflects quality (via correlations in types) or a shared circumstance (via common shocks to observed outcomes).", "categories": "econ.th cs.gt", "created": "2020-06-11", "updated": "", "authors": ["annie liang", "erik madsen"], "url": "https://arxiv.org/abs/2006.06543"}, {"title": "efficient democratic decisions via nondeterministic proportional   consensus", "id": "2006.06548", "abstract": "are there voting methods which (i) give everyone, including minorities, an equal share of effective power even if voters act strategically, (ii) promote consensus rather than polarization and inequality, and (iii) do not favour the status quo or rely too much on chance?   we show the answer is yes by describing two nondeterministic voting methods, one based on automatic bargaining over lotteries, the other on conditional commitments to approve compromise options. our theoretical analysis and agent-based simulation experiments suggest that with these, majorities cannot consistently suppress minorities as with deterministic methods, proponents of the status quo cannot block decisions as in consensus-based approaches, the resulting aggregate welfare is comparable to existing methods, and average randomness is lower than for other nondeterministic methods.", "categories": "econ.gn cs.gt cs.ma q-fin.ec", "created": "2020-06-10", "updated": "", "authors": ["jobst heitzig", "forest w. simmons"], "url": "https://arxiv.org/abs/2006.06548"}, {"title": "confidence sets for dynamic poverty indexes", "id": "2006.06595", "abstract": "in this study, we extend the research on the dynamic poverty indexes, namely the dynamic headcount ratio, the dynamic income-gap ratio, the dynamic gini and the dynamic sen, proposed in d'amico and regnault (2018). the contribution is twofold. first, we extend the computation of the dynamic gini index, thus the sen index accordingly, with the inclusion of the inequality within each class of poverty where people are classified according to their income. second, for each poverty index, we establish a central limit theorem that gives us the possibility to determine the confidence sets. an application to the italian income data from 1998 to 2012 confirms the effectiveness of the considered approach and the possibility to determine the evolution of poverty and inequality in real economies.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-11", "updated": "", "authors": ["guglielmo d'amico", "riccardo de blasis", "philippe regnault"], "url": "https://arxiv.org/abs/2006.06595"}, {"title": "the epidemic-driven collapse in a system with limited economic resource", "id": "2006.06642", "abstract": "we consider a possibility of socioeconomic collapse caused by the spread of epidemic in a basic dynamical model with negative feedback between the infected population size and a formal collective economic resource. the epidemic-resource coupling is supposed to be of activation type, with the recovery rate governed by the arrhenius-like law and resource playing the role of temperature. such a coupling can result in the collapsing effect opposite to thermal explosion because of the limited resource. in this case, the system can no longer stabilize and return to the stable pre- or post-epidemic states. we demonstrate that such a collapse can partially be mitigated by means of a negative resource or debt.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-06-11", "updated": "2020-08-15", "authors": ["i. s. gandzha", "o. v. kliushnichenko", "s. p. lukyanets"], "url": "https://arxiv.org/abs/2006.06642"}, {"title": "corona games: masks, social distancing and mechanism design", "id": "2006.06674", "abstract": "pandemic response is a complex affair. most governments employ a set of quasi-standard measures to fight covid-19 including wearing masks, social distancing, virus testing and contact tracing. we argue that some non-trivial factors behind the varying effectiveness of these measures are selfish decision-making and the differing national implementations of the response mechanism. in this paper, through simple games, we show the effect of individual incentives on the decisions made with respect to wearing masks and social distancing, and how these may result in a sub-optimal outcome. we also demonstrate the responsibility of national authorities in designing these games properly regarding the chosen policies and their influence on the preferred outcome. we promote a mechanism design approach: it is in the best interest of every government to carefully balance social good and response costs when implementing their respective pandemic response mechanism.", "categories": "econ.th cs.gt", "created": "2020-06-11", "updated": "2020-07-23", "authors": ["balazs pejo", "gergely biczok"], "url": "https://arxiv.org/abs/2006.06674"}, {"title": "delegation in veto bargaining", "id": "2006.06773", "abstract": "a proposer requires the approval of a veto player to change a status quo. preferences are single peaked. proposer is uncertain about vetoer's ideal point. we study proposer's optimal mechanism without transfers. vetoer is given a menu, or a delegation set, to choose from. the optimal delegation set balances the extent of proposer's compromise with the risk of a veto. under reasonable conditions, \"full delegation\" is optimal: vetoer can choose any action between the status quo and proposer's ideal action. this outcome largely nullifies proposer's bargaining power; vetoer frequently obtains her ideal point, and there is pareto efficiency despite asymmetric information. more generally, we identify when \"interval delegation\" is optimal. optimal interval delegation can be a pareto improvement over cheap talk. we derive comparative statics. vetoer receives less discretion when preferences are more likely to be aligned, by contrast to expertise-based delegation. methodologically, our analysis handles stochastic mechanisms.", "categories": "econ.th", "created": "2020-06-11", "updated": "", "authors": ["navin kartik", "andreas kleiner", "richard van weelden"], "url": "https://arxiv.org/abs/2006.06773"}, {"title": "incentives and efficiency in constrained allocation mechanisms", "id": "2006.06776", "abstract": "we study private-good allocation mechanisms where an arbitrary constraint delimits the set of feasible joint allocations. this generality provides a unified perspective over several prominent examples that can be parameterized as constraints in this model, including house allocation, roommate assignment, and social choice. we first characterize the set of two-agent strategy-proof and pareto efficient mechanisms, showing that every mechanism is a \"local dictatorship.\" for more than two agents, we leverage this result to provide a new characterization of group strategy-proofness. in particular, an n-agent mechanism is group strategy-proof if and only if all its two-agent marginal mechanisms (defined by holding fixed all but two agents' preferences) are individually strategy-proof and pareto efficient. to illustrate their usefulness, we apply these results to the roommates problem to discover the novel finding that all group strategy-proof and pareto efficient mechanisms are generalized serial dictatorships, a new class of mechanisms. our results also yield a simple new proof of the gibbard-satterthwaite theorem.", "categories": "econ.th", "created": "2020-06-11", "updated": "", "authors": ["joseph root", "david s. ahn"], "url": "https://arxiv.org/abs/2006.06776"}, {"title": "confidence interval for off-policy evaluation from dependent samples via   bandit algorithm: approach from standardized martingales", "id": "2006.06982", "abstract": "this study addresses the problem of off-policy evaluation (ope) from dependent samples obtained via the bandit algorithm. the goal of ope is to evaluate a new policy using historical data obtained from behavior policies generated by the bandit algorithm. because the bandit algorithm updates the policy based on past observations, the samples are not independent and identically distributed (i.i.d.). however, several existing methods for ope do not take this issue into account and are based on the assumption that samples are i.i.d. in this study, we address this problem by constructing an estimator from a standardized martingale difference sequence. to standardize the sequence, we consider using evaluation data or sample splitting with a two-step estimation. this technique produces an estimator with asymptotic normality without restricting a class of behavior policies. in an experiment, the proposed estimator performs better than existing methods, which assume that the behavior policy converges to a time-invariant policy.", "categories": "stat.ml cs.lg econ.em stat.me", "created": "2020-06-12", "updated": "", "authors": ["masahiro kato"], "url": "https://arxiv.org/abs/2006.06982"}, {"title": "seemingly unrelated regression with measurement error: estimation via   markov chain monte carlo and mean field variational bayes approximation", "id": "2006.07074", "abstract": "linear regression with measurement error in the covariates is a heavily studied topic, however, the statistics/econometrics literature is almost silent to estimating a multi-equation model with measurement error. this paper considers a seemingly unrelated regression model with measurement error in the covariates and introduces two novel estimation methods: a pure bayesian algorithm (based on markov chain monte carlo techniques) and its mean field variational bayes (mfvb) approximation. the mfvb method has the added advantage of being computationally fast and can handle big data. an issue pertinent to measurement error models is parameter identification, and this is resolved by employing a prior distribution on the measurement error variance. the methods are shown to perform well in multiple simulation studies, where we analyze the impact on posterior estimates arising due to different values of reliability ratio or variance of the true unobserved quantity used in the data generating process. the paper further implements the proposed algorithms in an application drawn from the health literature and shows that modeling measurement error in the data can improve model fitting.", "categories": "stat.me econ.em", "created": "2020-06-12", "updated": "", "authors": ["georges bresson", "anoop chaturvedi", "mohammad arshad rahman", "n/a shalabh"], "url": "https://arxiv.org/abs/2006.07074"}, {"title": "minimax estimation of conditional moment models", "id": "2006.07201", "abstract": "we develop an approach for estimating models described via conditional moment restrictions, with a prototypical application being non-parametric instrumental variable regression. we introduce a min-max criterion function, under which the estimation problem can be thought of as solving a zero-sum game between a modeler who is optimizing over the hypothesis space of the target model and an adversary who identifies violating moments over a test function space. we analyze the statistical estimation rate of the resulting estimator for arbitrary hypothesis spaces, with respect to an appropriate analogue of the mean squared error metric, for ill-posed inverse problems. we show that when the minimax criterion is regularized with a second moment penalty on the test function and the test function space is sufficiently rich, then the estimation rate scales with the critical radius of the hypothesis and test function spaces, a quantity which typically gives tight fast rates. our main result follows from a novel localized rademacher analysis of statistical learning problems defined via minimax objectives. we provide applications of our main results for several hypothesis spaces used in practice such as: reproducing kernel hilbert spaces, high dimensional sparse linear functions, spaces defined via shape constraints, ensemble estimators such as random forests, and neural networks. for each of these applications we provide computationally efficient optimization methods for solving the corresponding minimax problem (e.g. stochastic first-order heuristics for neural networks). in several applications, we show how our modified mean squared error rate, combined with conditions that bound the ill-posedness of the inverse problem, lead to mean squared error rates. we conclude with an extensive experimental analysis of the proposed methods.", "categories": "econ.em cs.lg math.st stat.ml stat.th", "created": "2020-06-12", "updated": "", "authors": ["nishanth dikkala", "greg lewis", "lester mackey", "vasilis syrgkanis"], "url": "https://arxiv.org/abs/2006.07201"}, {"title": "covid-19 and digital resilience: evidence from uber eats", "id": "2006.07204", "abstract": "we analyze how digital platforms can increase the survival rate of firms during a crisis by providing continuity in access to customers. using order-level data from uber technologies, we study how the covid-19 pandemic and the ensuing shutdown of businesses in the united states affected independent, small business restaurant supply and demand on the uber eats platform. we find evidence that small restaurants experience significant increases in total activity, orders per day, and orders per hour following the closure of the dine-in channel, and that these increases may be due to both demand-side and supply-side shocks. we document an increase in the intensity of competitive effects following the shock, showing that growth in the number of providers on a platform induces both market expansion and heightened inter-provider competition. our findings underscore the critical role that digital will play in creating business resilience in the post-covid economy, and provide new managerial insight into how supply-side and demand-side factors shape business performance on a platform.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-06-12", "updated": "", "authors": ["manav raj", "arun sundararajan", "calum you"], "url": "https://arxiv.org/abs/2006.07204"}, {"title": "predicting cell phone adoption metrics using satellite imagery", "id": "2006.07311", "abstract": "approximately half of the global population does not have access to the internet, even though digital access can reduce poverty by revolutionizing economic development opportunities. due to a lack of data, mobile network operators (mnos), governments and other digital ecosystem actors struggle to effectively determine if telecommunication investments are viable, especially in greenfield areas where demand is unknown. this leads to a lack of investment in network infrastructure, resulting in a phenomenon commonly referred to as the 'digital divide'. in this paper we present a method that uses publicly available satellite imagery to predict telecoms demand metrics, including cell phone adoption and spending on mobile services, and apply the method to malawi and ethiopia. a predictive machine learning approach can capture up to 40% of data variance, compared to existing approaches which only explain up to 20% of the data variance. the method is a starting point for developing more sophisticated predictive models of telecom infrastructure demand using publicly available satellite imagery and image recognition techniques. the evidence produced can help to better inform investment and policy decisions which aim to reduce the digital divide.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-06-12", "updated": "2020-06-22", "authors": ["edward j. oughton", "jatin mathur"], "url": "https://arxiv.org/abs/2006.07311"}, {"title": "algorithm for computing approximate nash equilibrium in continuous games   with application to continuous blotto", "id": "2006.07443", "abstract": "successful algorithms have been developed for computing nash equilibrium in a variety of finite game classes. however, solving continuous games---in which the pure strategy space is (potentially uncountably) infinite---is far more challenging. nonetheless, many real-world domains have continuous action spaces, e.g., where actions refer to an amount of time, money, or other resource that is naturally modeled as being real-valued as opposed to integral. we present a new algorithm for computing nash equilibrium strategies in continuous games. in addition to two-player zero-sum games, our algorithm also applies to multiplayer games and games of imperfect information. we experiment with our algorithm on a continuous imperfect-information blotto game, in which two players distribute resources over multiple battlefields. blotto games have frequently been used to model national security scenarios and have also been applied to electoral competition and auction theory. experiments show that our algorithm is able to quickly compute close approximations of nash equilibrium strategies for this game.", "categories": "cs.gt cs.ai cs.ma econ.th math.oc", "created": "2020-06-12", "updated": "2020-06-26", "authors": ["sam ganzfried"], "url": "https://arxiv.org/abs/2006.07443"}, {"title": "detangling robustness in high dimensions: composite versus   model-averaged estimation", "id": "2006.07457", "abstract": "robust methods, though ubiquitous in practice, are yet to be fully understood in the context of regularized estimation and high dimensions. even simple questions become challenging very quickly. for example, classical statistical theory identifies equivalence between model-averaged and composite quantile estimation. however, little to nothing is known about such equivalence between methods that encourage sparsity. this paper provides a toolbox to further study robustness in these settings and focuses on prediction. in particular, we study optimally weighted model-averaged as well as composite $l_1$-regularized estimation. optimal weights are determined by minimizing the asymptotic mean squared error. this approach incorporates the effects of regularization, without the assumption of perfect selection, as is often used in practice. such weights are then optimal for prediction quality. through an extensive simulation study, we show that no single method systematically outperforms others. we find, however, that model-averaged and composite quantile estimators often outperform least-squares methods, even in the case of gaussian model noise. real data application witnesses the method's practical use through the reconstruction of compressed audio signals.", "categories": "math.st econ.em stat.ml stat.th", "created": "2020-06-12", "updated": "", "authors": ["jing zhou", "gerda claeskens", "jelena bradic"], "url": "https://arxiv.org/abs/2006.07457"}, {"title": "horseshoe prior bayesian quantile regression", "id": "2006.07655", "abstract": "this paper extends the horseshoe prior of carvalho et al. (2010) to the bayesian quantile regression (hs-bqr) and provides a fast sampling algorithm that speeds up computation significantly in high dimensions. the performance of the hs-bqr is tested on large scale monte carlo simulations and an empirical application relevant to macroeoncomics. the monte carlo design considers several sparsity structures (sparse, dense, block) and error structures (i.i.d. errors and heteroskedastic errors). a number of lasso based estimators (frequentist and bayesian) are pitted against the hs-bqr to better gauge the performance of the method on the different designs. the hs-bqr yields just as good, or better performance than the other estimators considered when evaluated using coefficient bias and forecast error. we find that the hs-bqr is particularly potent in sparse designs and when estimating extreme quantiles. the simulations also highlight how the high dimensional quantile estimators fail to correctly identify the quantile function of the variables when both location and scale effects are present. in the empirical application, in which we evaluate forecast densities of us inflation, the hs-bqr provides well calibrated forecast densities whose individual quantiles, have the highest pseudo r squared, highlighting its potential for value-at-risk estimation.", "categories": "econ.em stat.ml", "created": "2020-06-13", "updated": "", "authors": ["david kohns", "tibor szendrei"], "url": "https://arxiv.org/abs/2006.07655"}, {"title": "carbon monitor: a near-real-time daily dataset of global co2 emission   from fossil fuel and cement production", "id": "2006.07690", "abstract": "we constructed a near-real-time daily co2 emission dataset, namely the carbon monitor, to monitor the variations of co2 emissions from fossil fuel combustion and cement production since january 1st 2019 at national level with near-global coverage on a daily basis, with the potential to be frequently updated. daily co2 emissions are estimated from a diverse range of activity data, including: hourly to daily electrical power generation data of 29 countries, monthly production data and production indices of industry processes of 62 countries/regions, daily mobility data and mobility indices of road transportation of 416 cities worldwide. individual flight location data and monthly data were utilised for aviation and maritime transportation sectors estimates. in addition, monthly fuel consumption data that corrected for daily air temperature of 206 countries were used for estimating the emissions from commercial and residential buildings. this carbon monitor dataset manifests the dynamic nature of co2 emissions through daily, weekly and seasonal variations as influenced by workdays and holidays, as well as the unfolding impacts of the covid-19 pandemic. the carbon monitor near-real-time co2 emission dataset shows a 7.8% decline of co2 emission globally from jan 1st to apr 30th in 2020 when compared with the same period in 2019, and detects a re-growth of co2 emissions by late april which are mainly attributed to the recovery of economy activities in china and partial easing of lockdowns in other countries. further, this daily updated co2 emission dataset could offer a range of opportunities for related scientific research and policy making.", "categories": "physics.soc-ph econ.gn physics.ao-ph physics.geo-ph q-fin.ec", "created": "2020-06-13", "updated": "", "authors": ["zhu liu", "philippe ciais", "zhu deng", "steven j. davis", "bo zheng", "yilong wang", "duo cui", "biqing zhu", "xinyu dou", "piyu ke", "taochun sun", "rui guo", "olivier boucher", "francois-marie breon", "chenxi lu", "runtao guo", "eulalie boucher", "frederic chevallier"], "url": "https://arxiv.org/abs/2006.07690"}, {"title": "synthetic interventions", "id": "2006.07691", "abstract": "we develop a method to help quantify the impact different levels of mobility restrictions could have had on covid-19 related deaths across nations. synthetic control (sc) has emerged as a standard tool in such scenarios to produce counterfactual estimates if a particular intervention had not occurred, using just observational data. however, it remains an important open problem of how to extend sc to obtain counterfactual estimates if a particular intervention had occurred - this is exactly the question of the impact of mobility restrictions stated above. as our main contribution, we introduce synthetic interventions (si), which helps resolve this open problem by allowing one to produce counterfactual estimates if there are multiple interventions of interest. we prove si produces consistent counterfactual estimates under a tensor factor model. our finite sample analysis shows the test error decays as $1/t_0$, where $t_0$ is the amount of observed pre-intervention data. as a special case, this improves upon the $1/\\sqrt{t_0}$ bound on test error for sc in prior works. our test error bound holds under a certain \"subspace inclusion\" condition; we furnish a data-driven hypothesis test with provable guarantees to check for this condition. this also provides a quantitative hypothesis test for when to use sc, currently absent in the literature. technically, we establish the parameter estimation and test error for principal component regression (a key subroutine in si and several sc variants) under the setting of error-in-variable regression decays as $1/t_0$, where $t_0$ is the number of samples observed; this improves the best prior test error bound of $1/\\sqrt{t_0}$. in addition to the covid-19 case study, we show how si can be used to run data-efficient, personalized randomized control trials using real data from a large e-commerce website and a large developmental economics study.", "categories": "econ.em cs.lg stat.ml", "created": "2020-06-13", "updated": "", "authors": ["anish agarwal", "abdullah alomar", "romain cosson", "devavrat shah", "dennis shen"], "url": "https://arxiv.org/abs/2006.07691"}, {"title": "optimal attention management: a tractable framework", "id": "2006.07729", "abstract": "a well-intentioned principal provides information to a rationally inattentive agent without internalizing the agent's cost of processing information. whatever information the principal makes available, the agent may choose to ignore some. we study optimal information provision in a tractable model with quadratic payoffs where full disclosure is not optimal. we characterize incentive-compatible information policies, that is, those to which the agent willingly pays full attention. in a leading example with three states, optimal disclosure involves information distortion at intermediate costs of attention. as the cost increases, optimal information abruptly changes from downplaying the state to exaggerating the state.", "categories": "econ.th", "created": "2020-06-13", "updated": "", "authors": ["elliot lipnowski", "laurent mathevet", "dong wei"], "url": "https://arxiv.org/abs/2006.07729"}, {"title": "nonparametric tests of tail behavior in stochastic frontier models", "id": "2006.07780", "abstract": "this article studies tail behavior for the error components in the stochastic frontier model, where one component has bounded support on one side, and the other has unbounded support on both sides. under weak assumptions on the error components, we derive nonparametric tests that the unbounded component distribution has thin tails and that the component tails are equivalent. the tests are useful diagnostic tools for stochastic frontier analysis. a simulation study and an application to a stochastic cost frontier for 6,100 us banks from 1998 to 2005 are provided. the new tests reject the normal or laplace distributional assumptions, which are commonly imposed in the existing literature.", "categories": "econ.em", "created": "2020-06-13", "updated": "", "authors": ["n/a william", "c. horrace", "yulong wang"], "url": "https://arxiv.org/abs/2006.07780"}, {"title": "representative committees of peers", "id": "2006.07837", "abstract": "a population of voters must elect representatives among themselves to decide on a sequence of possibly unforeseen binary issues. voters care only about the final decision, not the elected representatives. the disutility of a voter is proportional to the fraction of issues, where his preferences disagree with the decision.   while an issue-by-issue vote by all voters would maximize social welfare, we are interested in how well the preferences of the population can be approximated by a small committee.   we show that a k-sortition (a random committee of k voters with the majority vote within the committee) leads to an outcome within the factor 1+o(1/k) of the optimal social cost for any number of voters n, any number of issues $m$, and any preference profile.   for a small number of issues m, the social cost can be made even closer to optimal by delegation procedures that weigh committee members according to their number of followers. however, for large m, we demonstrate that the k-sortition is the worst-case optimal rule within a broad family of committee-based rules that take into account metric information about the preference profile of the whole population.", "categories": "cs.gt cs.ai econ.th", "created": "2020-06-14", "updated": "", "authors": ["reshef meir", "fedor sandomirskiy", "moshe tennenholtz"], "url": "https://arxiv.org/abs/2006.07837"}, {"title": "loss rate forecasting framework based on macroeconomic changes:   application to us credit card industry", "id": "2006.07911", "abstract": "a major part of the balance sheets of the largest us banks consists of credit card portfolios. hence, managing the charge-off rates is a vital task for the profitability of the credit card industry. different macroeconomic conditions affect individuals' behavior in paying down their debts. in this paper, we propose an expert system for loss forecasting in the credit card industry using macroeconomic indicators. we select the indicators based on a thorough review of the literature and experts' opinions covering all aspects of the economy, consumer, business, and government sectors. the state of the art machine learning models are used to develop the proposed expert system framework. we develop two versions of the forecasting expert system, which utilize different approaches to select between the lags added to each indicator. among 19 macroeconomic indicators that were used as the input, six were used in the model with optimal lags, and seven indicators were selected by the model using all lags. the features that were selected by each of these models covered all three sectors of the economy. using the charge-off data for the top 100 us banks ranked by assets from the first quarter of 1985 to the second quarter of 2019, we achieve mean squared error values of 1.15e-03 and 1.04e-03 using the model with optimal lags and the model with all lags, respectively. the proposed expert system gives a holistic view of the economy to the practitioners in the credit card industry and helps them to see the impact of different macroeconomic conditions on their future loss.", "categories": "stat.ml cs.lg econ.gn q-fin.ec", "created": "2020-06-14", "updated": "", "authors": ["sajjad taghiyeh", "david c lengacher", "robert b handfield"], "url": "https://arxiv.org/abs/2006.07911"}, {"title": "the energy representation of world gdp", "id": "2006.07938", "abstract": "the dependence of world gdp on current energy consumption and total energy produced over the previous period and materialized in the form of production infrastructure is studied. the dependence describes empirical data with high accuracy over the entire observation interval 1965-2018.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-06-14", "updated": "", "authors": ["boris m. dolgonosov"], "url": "https://arxiv.org/abs/2006.07938"}, {"title": "the cost of undisturbed landscapes", "id": "2006.08009", "abstract": "by 2030 austria aims to meet 100% of its electricity demand from domestic renewable sources, predominantly from wind and solar energy. while wind power reduces co2 emissions, it is also connected to negative impacts at the local level, such as interference with landscape aesthetics. nevertheless, wind power comes at lower system integration cost than solar power, so that it effectively reduces system cost. we quantify the opportunity cost of replacing wind turbines with solar power, using the power system model medea. our findings suggest that these cost of undisturbed landscapes are considerable, particularly when pv is not entirely rolled out as utility scale, open space installations. the opportunity cost is likely high enough to allow for significant compensation of the ones affected by local wind turbine externalities.", "categories": "econ.gn cs.sy eess.sy q-fin.ec", "created": "2020-06-14", "updated": "2020-06-19", "authors": ["sebastian wehrle", "johannes schmidt", "christian mikovits"], "url": "https://arxiv.org/abs/2006.08009"}, {"title": "multi-purchase behavior: modeling and optimization", "id": "2006.08055", "abstract": "we study the problem of modeling purchase of multiple items and utilizing it to display optimized recommendations, which is a central problem for online e-commerce platforms. rich personalized modeling of users and fast computation of optimal products to display given these models can lead to significantly higher revenues and simultaneously enhance the end user experience. we present a parsimonious multi-purchase family of choice models called the bundlemvl-k family, and develop a binary search based iterative strategy that efficiently computes optimized recommendations for this model. this is one of the first attempts at operationalizing multi-purchase class of choice models. we characterize structural properties of the optimal solution, which allow one to decide if a product is part of the optimal assortment in constant time, reducing the size of the instance that needs to be solved computationally. we also establish the hardness of computing optimal recommendation sets. we show one of the first quantitative links between modeling multiple purchase behavior and revenue gains. the efficacy of our modeling and optimization techniques compared to competing solutions is shown using several real world datasets on multiple metrics such as model fitness, expected revenue gains and run-time reductions. the benefit of taking multiple purchases into account is observed to be $6-8\\%$ in relative terms for the ta feng and uci shopping datasets when compared to the mnl model for instances with $\\sim 1500$ products. additionally, across $8$ real world datasets, the test log-likelihood fits of our models are on average $17\\%$ better in relative terms. the simplicity of our models and the iterative nature of our optimization technique allows practitioners meet stringent computational constraints while increasing their revenues in practical recommendation applications at scale.", "categories": "cs.ir cs.ai econ.th", "created": "2020-06-14", "updated": "", "authors": ["theja tulabandhula", "deeksha sinha", "prasoon patidar"], "url": "https://arxiv.org/abs/2006.08055"}, {"title": "reputation building under observational learning", "id": "2006.08068", "abstract": "i study a social learning model in which the object to learn is a strategic player's endogenous actions rather than an exogenous state. a patient seller faces a sequence of buyers and decides whether to build a reputation for supplying high quality products. each buyer does not have access to the seller's complete records, but can observe all previous buyers' actions, and some informative private signal about the seller's actions. i examine how the buyers' private signals affect the speed of social learning and the seller's incentives to establish reputations. when each buyer privately observes a bounded subset of the seller's past actions, the speed of learning is strictly positive but can vanish to zero as the seller becomes patient. as a result, reputation building can lead to low payoff for the patient seller and low social welfare. when each buyer observes an unboundedly informative private signal about the seller's current-period action, the speed of learning is uniformly bounded from below and a patient seller can secure high returns from building reputations. my results shed light on the effectiveness of various policies in accelerating social learning and encouraging sellers to establish good reputations.", "categories": "econ.th", "created": "2020-06-14", "updated": "2020-06-30", "authors": ["harry pei"], "url": "https://arxiv.org/abs/2006.08068"}, {"title": "repeated communication with private lying cost", "id": "2006.08069", "abstract": "i study repeated communication games between a patient sender and a sequence of receivers. the sender has persistent private information about his psychological cost of lying, and in every period, can privately observe the realization of an i.i.d. state before communication takes place. i characterize every type of sender's highest equilibrium payoff. when the highest lying cost in the support of the receivers' prior belief approaches the sender's benefit from lying, every type's highest equilibrium payoff in the repeated communication game converges to his equilibrium payoff in a one-shot bayesian persuasion game. i also show that in every sender-optimal equilibrium, no type of sender mixes between telling the truth and lying at every history. when there exist ethical types whose lying costs outweigh their benefits, i provide necessary and sufficient conditions for all non-ethical type senders to attain their optimal commitment payoffs. i identify an outside option effect through which the possibility of being ethical decreases every non-ethical type's payoff.", "categories": "econ.th", "created": "2020-06-14", "updated": "", "authors": ["harry pei"], "url": "https://arxiv.org/abs/2006.08069"}, {"title": "trust and betrayals: reputational payoffs and behaviors without   commitment", "id": "2006.08071", "abstract": "i study a repeated game in which a patient player (e.g., a seller) wants to win the trust of some myopic opponents (e.g., buyers) but can strictly benefit from betraying them. her benefit from betrayal is strictly positive and is her persistent private information. i characterize every type of patient player's highest equilibrium payoff. her persistent private information affects this payoff only through the lowest benefit in the support of her opponents' prior belief. i also show that in every equilibrium which is optimal for the patient player, her on-path behavior is nonstationary, and her long-run action frequencies are pinned down for all except two types. conceptually, my payoff-type approach incorporates a realistic concern that no type of reputation-building player is immune to reneging temptations. compared to commitment-type models, the incentive constraints for all types of patient player lead to a sharp characterization of her highest attainable payoff and novel predictions on her behaviors.", "categories": "econ.th", "created": "2020-06-14", "updated": "", "authors": ["harry pei"], "url": "https://arxiv.org/abs/2006.08071"}, {"title": "modeling and controlling the spread of epidemic with various social and   economic scenarios", "id": "2006.08375", "abstract": "we propose a novel model for describing the spreading processes, in particular, epidemics. our model is an extension of the siqr (susceptible-infected-quarantine-recovered) and sirp (susceptible-infected-recovered-pathogen) models used earlier to describe various scenarios of epidemic spread. as compared to the basic sir model, our model takes into account two possible routes of virus transmission: direct from the infected compartment to the susceptible compartment and indirect via some intermediate medium or fomites. the transmission rates are estimated in terms of the average distances between the individuals in selected social environments and characteristic relaxation times. we also introduce a resource activation function that reflects the load of the epidemics on economics and the limited capacity of the medical infrastructure. our model brings an advantage of building various control strategies to minimize the effect of the epidemic and can be applied to modeling the recent covid-19 outbreak.", "categories": "physics.soc-ph econ.gn q-bio.pe q-fin.ec", "created": "2020-06-12", "updated": "", "authors": ["s. p. lukyanets", "i. s. gandzha", "o. v. kliushnychenko"], "url": "https://arxiv.org/abs/2006.08375"}, {"title": "modeling joint lives within families", "id": "2006.08446", "abstract": "family history is usually seen as a significant factor insurance companies look at when applying for a life insurance policy. where it is used, family history of cardiovascular diseases, death by cancer, or family history of high blood pressure and diabetes could result in higher premiums or no coverage at all. in this article, we use massive (historical) data to study dependencies between life length within families. if joint life contracts (between a husband and a wife) have been long studied in actuarial literature, little is known about child and parents dependencies. we illustrate those dependencies using 19th century family trees in france, and quantify implications in annuities computations. for parents and children, we observe a modest but significant positive association between life lengths. it yields different estimates for remaining life expectancy, present values of annuities, or whole life insurance guarantee, given information about the parents (such as the number of parents alive). a similar but weaker pattern is observed when using information on grandparents.", "categories": "stat.ap econ.gn q-fin.ec", "created": "2020-06-15", "updated": "", "authors": ["olivier cabrignac", "arthur charpentier", "ewen gallic"], "url": "https://arxiv.org/abs/2006.08446"}, {"title": "v-, u-, l-, or w-shaped recovery after covid: insights from an agent   based model", "id": "2006.08469", "abstract": "we discuss the impact of a covid-like shock on a simple toy economy, described by the mark-0 agent-based model that we developed and discussed in a series of previous papers. we consider a mixed supply and demand shock, and show that depending on the shock parameters (amplitude and duration), our toy economy can display v-shaped, u-shaped or w-shaped recoveries, and even an l-shaped output curve with permanent output loss. this is due to the existence of a self-sustained \"bad\" state of the economy. we then discuss two policies that attempt to moderate the impact of the shock: giving easy credit to firms, and the so-called helicopter money, i.e. injecting new money into the households savings. we find that both policies are effective if strong enough, and we highlight the potential danger of terminating these policies too early. while we only discuss a limited number of scenarios, our model is flexible and versatile enough to allow for a much wider exploration, thus serving as a useful tool for the qualitative understanding of post-covid recovery. we provide an on-line version of the code at https://gitlab.com/sharma.dhruv/markovid .", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-06-15", "updated": "2020-09-09", "authors": ["dhruv sharma", "jean-philippe bouchaud", "stanislao gualdi", "marco tarzia", "francesco zamponi"], "url": "https://arxiv.org/abs/2006.08469"}, {"title": "measuring macroeconomic uncertainty: a cross-country analysis", "id": "2006.09007", "abstract": "this paper constructs internationally consistent measures of macroeconomic uncertainty. our econometric framework extracts uncertainty from revisions in data obtained from standardized national accounts. applying our model to quarterly post-wwii real-time data, we estimate macroeconomic uncertainty for 39 countries. the cross-country dimension of our uncertainty data allows us to identify the effects of uncertainty shocks on economic activity under different employment protection legislation. our empirical findings suggest that the effects of uncertainty shocks are stronger and more persistent in countries with low employment protection compared to countries with high employment protection. these empirical findings are in line with a theoretical model under varying firing cost.", "categories": "econ.em", "created": "2020-06-16", "updated": "", "authors": ["andreas dibiasi", "samad sarferaz"], "url": "https://arxiv.org/abs/2006.09007"}, {"title": "a demographic microsimulation model with an integrated household   alignment method", "id": "2006.09474", "abstract": "many dynamic microsimulation models have shown their ability to reasonably project detailed population and households using non-data based household formation and dissolution rules. although, those rules allow modellers to simplify changes in the household construction, they typically fall short in replicating household projections or if applied retrospectively the observed household numbers. consequently, such models with biased estimation for household size and other household related attributes lose their usefulness in applications that are sensitive to household size, such as in travel demand and housing demand modelling. nonetheless, these demographic microsimulation models with their associated shortcomings have been commonly used to assess various planning policies which can result in misleading judgements. in this paper, we contribute to the literature of population microsimulation by introducing a fully integrated system of models for different life event where a household alignment method adjusts household size distribution to closely align with any given target distribution. furthermore, some demographic events that are generally difficult to model, such as incorporating immigrant families into a population, can be included. we illustrated an example of the household alignment method and put it to test in a dynamic microsimulation model that we developed using dymiumcore, a general-purpose microsimulation toolkit in r, to show potential improvements and weaknesses of the method. the implementation of this model has been made publicly available on github.", "categories": "econ.gn physics.soc-ph q-fin.ec stat.ap", "created": "2020-06-15", "updated": "", "authors": ["amarin siripanich", "taha rashidi"], "url": "https://arxiv.org/abs/2006.09474"}, {"title": "adaptive, rate-optimal testing in instrumental variables models", "id": "2006.09587", "abstract": "this paper proposes simple, data-driven, optimal rate-adaptive inferences on a structural function in semi-nonparametric conditional moment restrictions. we consider two types of hypothesis tests based on leave-one-out sieve estimators. a structure-space test (st) uses a quadratic distance between the structural functions of endogenous variables; while an image-space test (it) uses a quadratic distance of the conditional moment from zero. for both tests, we analyze their respective classes of nonparametric alternative models that are separated from the null hypothesis by the minimax rate of testing. that is, the sum of the type i and the type ii errors of the test, uniformly over the class of nonparametric alternative models, cannot be improved by any other test. our new minimax rate of st differs from the known minimax rate of estimation in nonparametric instrumental variables (npiv) models. we propose computationally simple and novel exponential scan data-driven choices of sieve regularization parameters and adjusted chi-squared critical values. the resulting tests attain the minimax rate of testing, and hence optimally adapt to the unknown smoothness of functions and are robust to the unknown degree of ill-posedness (endogeneity). data-driven confidence sets are easily obtained by inverting the adaptive st. monte carlo studies demonstrate that our adaptive st has good size and power properties in finite samples for testing monotonicity or equality restrictions in npiv models. empirical applications to nonparametric multi-product demands with endogenous prices are presented.", "categories": "econ.em stat.me stat.ml", "created": "2020-06-16", "updated": "", "authors": ["christoph breunig", "xiaohong chen"], "url": "https://arxiv.org/abs/2006.09587"}, {"title": "combining experimental and observational data to estimate treatment   effects on long term outcomes", "id": "2006.09676", "abstract": "there has been an increase in interest in experimental evaluations to estimate causal effects, partly because their internal validity tends to be high. at the same time, as part of the big data revolution, large, detailed, and representative, administrative data sets have become more widely available. however, the credibility of estimates of causal effects based on such data sets alone can be low.   in this paper, we develop statistical methods for systematically combining experimental and observational data to obtain credible estimates of the causal effect of a binary treatment on a primary outcome that we only observe in the observational sample. both the observational and experimental samples contain data about a treatment, observable individual characteristics, and a secondary (often short term) outcome. to estimate the effect of a treatment on the primary outcome while addressing the potential confounding in the observational sample, we propose a method that makes use of estimates of the relationship between the treatment and the secondary outcome from the experimental sample. if assignment to the treatment in the observational sample were unconfounded, we would expect the treatment effects on the secondary outcome in the two samples to be similar. we interpret differences in the estimated causal effects on the secondary outcome between the two samples as evidence of unobserved confounders in the observational sample, and develop control function methods for using those differences to adjust the estimates of the treatment effects on the primary outcome.   we illustrate these ideas by combining data on class size and third grade test scores from the project star experiment with observational data on class size and both third and eighth grade test scores from the new york school system.", "categories": "stat.me econ.em", "created": "2020-06-17", "updated": "", "authors": ["susan athey", "raj chetty", "guido imbens"], "url": "https://arxiv.org/abs/2006.09676"}, {"title": "flexible mixture priors for time-varying parameter models", "id": "2006.10088", "abstract": "time-varying parameter (tvp) models often assume that the tvps evolve according to a random walk. this assumption, however, might be questionable since it implies that coefficients change smoothly and in an unbounded manner. in this paper, we relax this assumption by proposing a flexible law of motion for the tvps in large-scale vector autoregressions (vars). instead of imposing a restrictive random walk evolution of the latent states, we carefully design hierarchical mixture priors on the coefficients in the state equation. these priors effectively allow for discriminating between periods where coefficients evolve according to a random walk and times where the tvps are better characterized by a stationary stochastic process. moreover, this approach is capable of introducing dynamic sparsity by pushing small parameter changes towards zero if necessary. the merits of the model are illustrated by means of two applications. using synthetic data we show that our approach yields precise parameter estimates. when applied to us data, the model reveals interesting patterns of low-frequency dynamics in coefficients and forecasts well relative to a wide range of competing models.", "categories": "econ.em", "created": "2020-06-17", "updated": "", "authors": ["niko hauzenberger"], "url": "https://arxiv.org/abs/2006.10088"}, {"title": "nash sir: an economic-epidemiological model of strategic behavior during   a viral epidemic", "id": "2006.10109", "abstract": "this paper develops a nash-equilibrium extension of the classic sir model of infectious-disease epidemiology (\"nash sir\"), endogenizing people's decisions whether to engage in economic activity during a viral epidemic and allowing for complementarity in social-economic activity. an equilibrium epidemic is one in which nash equilibrium behavior during the epidemic generates the epidemic. there may be multiple equilibrium epidemics, in which case the epidemic trajectory can be shaped through the coordination of expectations, in addition to other sorts of interventions such as stay-at-home orders and accelerated vaccine development. an algorithm is provided to compute all equilibrium epidemics.", "categories": "econ.th", "created": "2020-06-17", "updated": "", "authors": ["david mcadams"], "url": "https://arxiv.org/abs/2006.10109"}, {"title": "gender inequality in research productivity during the covid-19 pandemic", "id": "2006.10194", "abstract": "we study the disproportionate impact of the lockdown as a result of the covid-19 outbreak on female and male academics' research productivity in social science. the lockdown has caused substantial disruptions to academic activities, requiring people to work from home. how this disruption affects productivity and the related gender equity is an important operations and societal question. we collect data from the largest open-access preprint repository for social science on 41,858 research preprints in 18 disciplines produced by 76,832 authors across 25 countries over a span of two years. we use a difference-in-differences approach leveraging the exogenous pandemic shock. our results indicate that, in the 10 weeks after the lockdown in the united states, although the total research productivity increased by 35%, female academics' productivity dropped by 13.9% relative to that of male academics. we also show that several disciplines drive such gender inequality. finally, we find that this intensified productivity gap is more pronounced for academics in top-ranked universities, and the effect exists in six other countries. our work points out the fairness issue in productivity caused by the lockdown, a finding that universities will find helpful when evaluating faculty productivity. it also helps organizations realize the potential unintended consequences that can arise from telecommuting.", "categories": "cs.dl econ.gn q-fin.ec", "created": "2020-06-17", "updated": "2020-07-23", "authors": ["ruomeng cui", "hao ding", "feng zhu"], "url": "https://arxiv.org/abs/2006.10194"}, {"title": "approximate maximum likelihood for complex structural models", "id": "2006.10245", "abstract": "indirect inference (i-i) is a popular technique for estimating complex parametric models whose likelihood function is intractable, however, the statistical efficiency of i-i estimation is questionable. while the efficient method of moments, gallant and tauchen (1996), promises efficiency, the price to pay for this efficiency is a loss of parsimony and thereby a potential lack of robustness to model misspecification. this stands in contrast to simpler i-i estimation strategies, which are known to display less sensitivity to model misspecification precisely due to their focus on specific elements of the underlying structural model. in this research, we propose a new simulation-based approach that maintains the parsimony of i-i estimation, which is often critical in empirical applications, but can also deliver estimators that are nearly as efficient as maximum likelihood. this new approach is based on using a constrained approximation to the structural model, which ensures identification and can deliver estimators that are nearly efficient. we demonstrate this approach through several examples, and show that this approach can deliver estimators that are nearly as efficient as maximum likelihood, when feasible, but can be employed in many situations where maximum likelihood is infeasible.", "categories": "econ.em q-fin.st stat.ap", "created": "2020-06-17", "updated": "", "authors": ["veronika czellar", "david t. frazier", "eric renault"], "url": "https://arxiv.org/abs/2006.10245"}, {"title": "sparse hp filter: finding kinks in the covid-19 contact rate", "id": "2006.10555", "abstract": "in this paper, we estimate the time-varying covid-19 contact rate of a susceptible-infected-recovered (sir) model. our measurement of the contact rate is constructed using data on actively infected, recovered and deceased cases. we propose a new trend filtering method that is a variant of the hodrick-prescott (hp) filter, constrained by the number of possible kinks. we term it the $\\textit{sparse hp filter}$ and apply it to daily data from five countries: canada, china, south korea, the uk and the us. our new method yields the kinks that are well aligned with actual events in each country. we find that the sparse hp filter provides a fewer kinks than the $\\ell_1$ trend filter, while both methods fitting data equally well. theoretically, we establish risk consistency of both the sparse hp and $\\ell_1$ trend filters. ultimately, we propose to use time-varying $\\textit{contact growth rates}$ to document and monitor outbreaks of covid-19.", "categories": "econ.em stat.ap", "created": "2020-06-18", "updated": "2020-07-29", "authors": ["sokbae lee", "yuan liao", "myung hwan seo", "youngki shin"], "url": "https://arxiv.org/abs/2006.10555"}, {"title": "conflict in africa during covid-19: social distancing, food   vulnerability and welfare response", "id": "2006.10696", "abstract": "we study the effect of social distancing, food vulnerability, welfare and labour covid-19 policy responses on riots, violence against civilians and food-related conflicts. our analysis uses georeferenced data for 24 african countries with monthly local prices and real-time conflict data reported in the armed conflict location and event data project (acled) from january 2015 until early may 2020. lockdowns and recent welfare policies have been implemented in light of covid-19, but in some contexts also likely in response to ongoing conflicts. to mitigate the potential risk of endogeneity, we use instrumental variables. we exploit the exogeneity of global commodity prices, and three variables that increase the risk of covid-19 and efficiency in response such as countries colonial heritage, male mortality rate attributed to air pollution and prevalence of diabetes in adults. we find that the probability of experiencing riots, violence against civilians, food-related conflicts and food looting has increased since lockdowns. food vulnerability has been a contributing factor. a 10% increase in the local price index is associated with an increase of 0.7 percentage points in violence against civilians. nonetheless, for every additional anti-poverty measure implemented in response to covid-19 the probability of experiencing violence against civilians, riots and food-related conflicts declines by approximately 0.2 percentage points. these anti-poverty measures also reduce the number of fatalities associated with these conflicts. overall, our findings reveal that food vulnerability has increased conflict risks, but also offer an optimistic view of the importance of the state in providing an extensive welfare safety net.", "categories": "econ.em physics.soc-ph", "created": "2020-06-18", "updated": "", "authors": ["roxana guti\u00e9rrez-romero"], "url": "https://arxiv.org/abs/2006.10696"}, {"title": "covid-19 response needs to broaden financial inclusion to curb the rise   in poverty", "id": "2006.10706", "abstract": "the ongoing covid-19 pandemic risks wiping out years of progress made in reducing global poverty. in this paper, we explore to what extent financial inclusion could help mitigate the increase in poverty using cross-country data across 78 low- and lower-middle-income countries. unlike other recent cross-country studies, we show that financial inclusion is a key driver of poverty reduction in these countries. this effect is not direct, but indirect, by mitigating the detrimental effect that inequality has on poverty. our findings are consistent across all the different measures of poverty used. our forecasts suggest that the world's population living on less than $1.90 per day could increase from 8% to 14% by 2021, pushing nearly 400 million people into poverty. however, urgent improvements in financial inclusion could substantially reduce the impact on poverty.", "categories": "econ.em", "created": "2020-06-18", "updated": "", "authors": ["mostak ahamed", "roxana guti\u00e9rrez-romero"], "url": "https://arxiv.org/abs/2006.10706"}, {"title": "a simple model of interbank trading with tiered remuneration", "id": "2006.10946", "abstract": "a negative interest rate policy is often accompanied by tiered remuneration, which allows for exemption from negative rates. this study proposes a basic model of interest rates formed in the interbank market with a tiering system. the results predicted by the model largely mirror actual market developments in late 2019, when the european central bank introduced, and the switzerland national bank modified, the tiering system.", "categories": "econ.gn q-fin.ec", "created": "2020-06-18", "updated": "", "authors": ["toshifumi nakamura"], "url": "https://arxiv.org/abs/2006.10946"}, {"title": "on the time trend of covid-19: a panel data study", "id": "2006.11060", "abstract": "in this paper, we study the trending behaviour of covid-19 data at country level, and draw attention to some existing econometric tools which are potentially helpful to understand the trend better in future studies. in our empirical study, we find that european countries overall flatten the curves more effectively compared to the other regions, while asia & oceania also achieve some success, but the situations are not as optimistic elsewhere. africa and america are still facing serious challenges in terms of managing the spread of the virus, and reducing the death rate, although in africa the virus spreads slower and has a lower death rate than the other regions. by comparing the performances of different countries, our results incidentally agree with gu et al. (2020), though different approaches and models are considered. for example, both works agree that countries such as usa, uk and italy perform relatively poorly; on the other hand, australia, china, japan, korea, and singapore perform relatively better.", "categories": "econ.em", "created": "2020-06-19", "updated": "2020-06-23", "authors": ["chaohua dong", "jiti gao", "oliver linton", "bin peng"], "url": "https://arxiv.org/abs/2006.11060"}, {"title": "mechanism of instrumental game theory in the legal process via   stochastic options pricing induction", "id": "2006.11061", "abstract": "economic theory has provided an estimable intuition in understanding the perplexing ideologies in law, in the areas of economic law, tort law, contract law, procedural law and many others. most legal systems require the parties involved in a legal dispute to exchange information through a process called discovery. the purpose is to reduce the relative optimisms developed by asymmetric information between the parties. like a head or tail phenomenon in stochastic processes, uncertainty in the adjudication affects the decisions of the parties in a legal negotiation. this paper therefore applies the principles of aleatory analysis to determine how negotiations fail in the legal process, introduce the axiological concept of optimal transaction cost and formulates a numerical methodology based on backwards induction and stochastic options pricing economics in estimating the reasonable and fair bargain in order to induce settlements thereby increasing efficiency and reducing social costs.", "categories": "econ.th", "created": "2020-06-19", "updated": "", "authors": ["kwadwo osei bonsu", "shoucan chen"], "url": "https://arxiv.org/abs/2006.11061"}, {"title": "sparse quantile regression", "id": "2006.11201", "abstract": "we consider both $\\ell _{0}$-penalized and $\\ell _{0}$-constrained quantile regression estimators. for the $\\ell _{0}$-penalized estimator, we derive an exponential inequality on the tail probability of excess quantile prediction risk and apply it to obtain non-asymptotic upper bounds on the mean-square parameter and regression function estimation errors. we also derive analogous results for the $\\ell _{0}$-constrained estimator. the resulting rates of convergence are minimax-optimal and the same as those for $\\ell _{1} $-penalized estimators. further, we characterize expected hamming loss for the $\\ell _{0}$-penalized estimator. we implement the proposed procedure via mixed integer linear programming and also a more scalable first-order approximation algorithm. we illustrate the finite-sample performance of our approach in monte carlo experiments and its usefulness in a real data application concerning conformal prediction of infant birth weights (with $% n\\approx 10^{3}$ and up to $p>10^{3}$). in sum, our $\\ell _{0}$-based method produces a much sparser estimator than the $\\ell _{1}$-penalized approach without compromising precision.", "categories": "stat.me econ.em", "created": "2020-06-19", "updated": "", "authors": ["le-yu chen", "sokbae lee"], "url": "https://arxiv.org/abs/2006.11201"}, {"title": "proper scoring rules for evaluating asymmetry in density forecasting", "id": "2006.11265", "abstract": "this paper proposes a novel asymmetric continuous probabilistic score (acps) for evaluating and comparing density forecasts. it extends the proposed score and defines a weighted version, which emphasizes regions of interest, such as the tails or the center of a variable's range. a test is also introduced to statistically compare the predictive ability of different forecasts. the acps is of general use in any situation where the decision maker has asymmetric preferences in the evaluation of the forecasts. in an artificial experiment, the implications of varying the level of asymmetry in the acps are illustrated. then, the proposed score and test are applied to assess and compare density forecasts of macroeconomic relevant datasets (us employment growth) and of commodity prices (oil and electricity prices) with particular focus on the recent covid-19 crisis period.", "categories": "stat.me econ.em stat.ap", "created": "2020-06-19", "updated": "2020-09-01", "authors": ["matteo iacopini", "francesco ravazzolo", "luca rossini"], "url": "https://arxiv.org/abs/2006.11265"}, {"title": "do methodological birds of a feather flock together?", "id": "2006.11346", "abstract": "quasi-experimental methods have proliferated over the last two decades, as researchers develop causal inference tools for settings in which randomization is infeasible. two popular such methods, difference-in-differences (did) and comparative interrupted time series (cits), compare observations before and after an intervention in a treated group to an untreated comparison group observed over the same period. both methods rely on strong, untestable counterfactual assumptions. despite their similarities, the methodological literature on cits lacks the mathematical formality of did. in this paper, we use the potential outcomes framework to formalize two versions of cits - a general version described by bloom (2005) and a linear version often used in health services research. we then compare these to two corresponding did formulations - one with time fixed effects and one with time fixed effects and group trends. we also re-analyze three previously published studies using these methods. we demonstrate that the most general versions of cits and did impute the same counterfactuals and estimate the same treatment effects. the only difference between these two designs is the language used to describe them and their popularity in distinct disciplines. we also show that these designs diverge when one constrains them using linearity (cits) or parallel trends (did). we recommend defaulting to the more flexible versions and provide advice to practitioners on choosing between the more constrained versions by considering the data-generating mechanism. we also recommend greater attention to specifying the outcome model and counterfactuals in papers, allowing for transparent evaluation of the plausibility of causal assumptions.", "categories": "stat.me econ.em", "created": "2020-06-19", "updated": "2020-07-08", "authors": ["carrie e. fry", "laura a. hatfield"], "url": "https://arxiv.org/abs/2006.11346"}, {"title": "valid causal inference with (some) invalid instruments", "id": "2006.11386", "abstract": "instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. but a key challenge when applying them is the reliance on untestable \"exclusion\" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. in this paper, we show how to perform consistent iv estimation despite violations of the exclusion assumption. in particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. the technique is simple to apply and is \"black-box\" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. as such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (cate) on complex, high dimensional data. experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated mendelian randomization problem.", "categories": "stat.me cs.lg econ.em stat.ml", "created": "2020-06-19", "updated": "", "authors": ["jason hartford", "victor veitch", "dhanya sridhar", "kevin leyton-brown"], "url": "https://arxiv.org/abs/2006.11386"}, {"title": "shifting policy strategy in keynesianism", "id": "2006.11749", "abstract": "this paper analyzes the evolution of keynesianism making use of concepts offered by imre lakatos. the keynesian \"hard core\" lies in its views regarding the instability of the market economy, its \"protective belt\" in the policy strategy for macroeconomic stabilization using fiscal policy and monetary policy. keynesianism developed as a policy program to counter classical liberalism, which attributes priority to the autonomy of the market economy and tries to limit the role of government. in general, the core of every policy program consists in an unfalsifiable worldview and a value judgment that remain unchanged. on the other hand, a policy strategy with a protective belt inevitably evolves owing to changes in reality and advances in scientific knowledge. this is why the keynesian policy strategy has shifted from being fiscal-led to one that is monetary-led because of the influence of monetarism; further, the great recession has even led to their integration.", "categories": "econ.gn q-fin.ec", "created": "2020-06-21", "updated": "", "authors": ["asahi noguchi"], "url": "https://arxiv.org/abs/2006.11749"}, {"title": "the economic costs of containing a pandemic", "id": "2006.11750", "abstract": "the coronavirus disease (covid-19) has caused one of the most serious social and economic losses to countries around the world since the spanish influenza pandemic of 1918 (during world war i). it has resulted in enormous economic as well as social costs, such as increased deaths from the spread of infection in a region. this is because public regulations imposed by national and local governments to deter the spread of infection inevitably involves a deliberate suppression of the level of economic activity. given this trade-off between economic activity and epidemic prevention, governments should execute public interventions to minimize social and economic losses from the pandemic. a major problem regarding the resultant economic losses is that it unequally impacts certain strata of the society. this raises an important question on how such economic losses should be shared equally across the society. at the same time, there is some antipathy towards economic compensation by means of public debt, which is likely to increase economic burden in the future. however, as paul samuelson once argued, much of the burden, whether due to public debt or otherwise, can only be borne by the present generation, and not by future generations.", "categories": "econ.gn q-fin.ec", "created": "2020-06-21", "updated": "", "authors": ["asahi noguchi"], "url": "https://arxiv.org/abs/2006.11750"}, {"title": "unified discrete-time factor stochastic volatility and continuous-time   ito models for combining inference based on low-frequency and high-frequency", "id": "2006.12039", "abstract": "this paper introduces unified models for high-dimensional factor-based ito process, which can accommodate both continuous-time ito diffusion and discrete-time stochastic volatility (sv) models by embedding the discrete sv model in the continuous instantaneous factor volatility process. we call it the sv-ito model. based on the series of daily integrated factor volatility matrix estimators, we propose quasi-maximum likelihood and least squares estimation methods. their asymptotic properties are established. we apply the proposed method to predict future vast volatility matrix whose asymptotic behaviors are studied. a simulation study is conducted to check the finite sample performance of the proposed estimation and prediction method. an empirical analysis is carried out to demonstrate the advantage of the sv-ito model in volatility prediction and portfolio allocation problems.", "categories": "stat.me econ.em", "created": "2020-06-22", "updated": "", "authors": ["donggyu kim", "xinyu song", "yazhen wang"], "url": "https://arxiv.org/abs/2006.12039"}, {"title": "impact of national lockdown on covid-19 deaths in select european   countries and the us using a changes-in-changes model", "id": "2006.12251", "abstract": "in this paper, we estimate the impact of national lockdown on covid-19 related total and daily deaths, per million people, in select european countries. in particular, we compare countries that imposed a nationwide lockdown (treatment group); belgium, denmark, france, germany, italy, norway, spain, united kingdom (uk), and the us, to sweden (control group) that did not impose national lockdown using a changes-in-changes (cic) estimation model. the key advantage of the cic model as compared to the standard difference-in-difference model is that cic allows for mean and variance of the outcomes to change over time in the absence of any policy intervention, and cic accounts for endogeneity in the choice of policy intervention. our results indicate that in contrast to sweden, which did not impose a national lockdown, germany, and to some extent, the us were the two countries where nationwide lockdown had a significant impact on the reduction in covid-19 related total and daily deaths per million people. in norway and denmark, there was no significant impact on total and daily deaths per million people relative to sweden. whereas in other countries; belgium, france, italy, spain, and the uk, the effect of the lockdown was in the opposite direction, that is, they experienced significantly higher covid-19 related total and daily deaths per million people, post the lockdown as compared to sweden. our results suggest that the impact of nationwide lockdown on covid-19 related total and daily deaths per million people varied from one country to another.", "categories": "physics.soc-ph econ.gn q-bio.pe q-fin.ec", "created": "2020-06-14", "updated": "", "authors": ["mudit kapoor", "shamika ravi"], "url": "https://arxiv.org/abs/2006.12251"}, {"title": "a pipeline for variable selection and false discovery rate control with   an application in labor economics", "id": "2006.12296", "abstract": "we introduce tools for controlled variable selection to economists. in particular, we apply a recently introduced aggregation scheme for false discovery rate (fdr) control to german administrative data to determine the parts of the individual employment histories that are relevant for the career outcomes of women. our results suggest that career outcomes can be predicted based on a small set of variables, such as daily earnings, wage increases in combination with a high level of education, employment status, and working experience.", "categories": "econ.em", "created": "2020-06-22", "updated": "2020-06-23", "authors": ["sophie-charlotte klose", "johannes lederer"], "url": "https://arxiv.org/abs/2006.12296"}, {"title": "stablecoins 2.0: economic foundations and risk-based models", "id": "2006.12388", "abstract": "stablecoins are one of the most widely capitalized type of cryptocurrency. however, their risks vary significantly according to their design and are often poorly understood. we seek to provide a sound foundation for stablecoin theory, with a risk-based functional characterization of the economic structure of stablecoins. first, we match existing economic models to the disparate set of custodial systems. next, we characterize the unique risks that emerge in non-custodial stablecoins and develop a model framework that unifies existing models from economics and computer science. we further discuss how this modeling framework is applicable to a wide array of cryptoeconomic systems, including cross-chain protocols, collateralized lending, and decentralized exchanges. these unique risks yield unanswered research questions that will form the crux of research in decentralized finance going forward.", "categories": "econ.gn cs.cr cs.ma q-fin.ec", "created": "2020-06-22", "updated": "2020-09-14", "authors": ["ariah klages-mundt", "dominik harz", "lewis gudgeon", "jun-you liu", "andreea minca"], "url": "https://arxiv.org/abs/2006.12388"}, {"title": "locally trimmed least squares: conventional inference in possibly   nonstationary models", "id": "2006.12595", "abstract": "a novel iv estimation method, that we term locally trimmed ls (ltls), is developed which yields estimators with (mixed) gaussian limit distributions in situations where the data may be weakly or strongly persistent. in particular, we allow for nonlinear predictive type of regressions where the regressor can be stationary short/long memory as well as nonstationary long memory process or a nearly integrated array. the resultant t-tests have conventional limit distributions (i.e. n(0; 1)) free of (near to unity and long memory) nuisance parameters. in the case where the regressor is a fractional process, no preliminary estimator for the memory parameter is required. therefore, the practitioner can conduct inference while being agnostic about the exact dependence structure in the data. the ltls estimator is obtained by applying certain chronological trimming to the ols instrument via the utilisation of appropriate kernel functions of time trend variables. the finite sample performance of ltls based t-tests is investigated with the aid of a simulation experiment. an empirical application to the predictability of stock returns is also provided.", "categories": "econ.em", "created": "2020-06-22", "updated": "", "authors": ["zhishui hu", "ioannis kasparis", "qiying wang"], "url": "https://arxiv.org/abs/2006.12595"}, {"title": "the social welfare implications of the zenga index", "id": "2006.12623", "abstract": "we introduce the social welfare implications of the zenga index, a recently proposed index of inequality. our proposal is derived by following the seminal book by son (2011) and the recent working paper by kakwani and son (2019). we compare the zenga based approach with the classical one, based on the lorenz curve and the gini coefficient, as well as the bonferroni index. we show that the social welfare specification based on the zenga uniformity curve presents some peculiarities that distinguish it from the other considered indexes. the social welfare specification presented here provides a deeper understanding of how the zenga index evaluates the inequality in a distribution.", "categories": "econ.gn q-fin.ec stat.me", "created": "2020-06-18", "updated": "", "authors": ["francesca greselin", "simone pellegrino", "achille vernizzi"], "url": "https://arxiv.org/abs/2006.12623"}, {"title": "the macroeconomy as a random forest", "id": "2006.12724", "abstract": "over the last decades, an impressive amount of non-linearities have been proposed to reconcile reduced-form macroeconomic models with the data. many of them boil down to have linear regression coefficients evolving through time: threshold/switching/smooth-transition regression; structural breaks and random walk time-varying parameters. while all of these schemes are reasonably plausible in isolation, i argue that those are much more in agreement with the data if they are combined. to this end, i propose macroeconomic random forests, which adapts the canonical machine learning (ml) algorithm to the problem of flexibly modeling evolving parameters in a linear macro equation. the approach exhibits clear forecasting gains over a wide range of alternatives and successfully predicts the drastic 2008 rise in unemployment. the obtained generalized time-varying parameters (gtvps) are shown to behave differently compared to random walk coefficients by adapting nicely to the problem at hand, whether it is regime-switching behavior or long-run structural change. by dividing the typical ml interpretation burden into looking at each tvp separately, i find that the resulting forecasts are, in fact, quite interpretable. an application to the us phillips curve reveals it is probably not flattening the way you think.", "categories": "econ.em stat.ml", "created": "2020-06-22", "updated": "", "authors": ["philippe goulet coulombe"], "url": "https://arxiv.org/abs/2006.12724"}, {"title": "the unbearable lightness of equilibria in a low interest rate   environment", "id": "2006.12966", "abstract": "structural models with no solution are incoherent, and those with multiple solutions are incomplete. we develop a method to study coherency and completeness conditions for linear dynamic forward-looking rational expectations models under an occasionally binding constraint. in the context of the simple new keynesian model with a zero lower bound, this method shows that the coherency and completeness condition generally violates the taylor principle. rational expectations require time-varying and correlated support restrictions on the distribution of the structural shocks. with appropriate restrictions, a very large number of equilibria can be supported.", "categories": "econ.gn q-fin.ec", "created": "2020-06-20", "updated": "", "authors": ["guido ascari", "sophocles mavroeidis"], "url": "https://arxiv.org/abs/2006.12966"}, {"title": "mitigating bias in online microfinance platforms: a case study on   kiva.org", "id": "2006.12995", "abstract": "over the last couple of decades in the lending industry, financial disintermediation has occurred on a global scale. traditionally, even for small supply of funds, banks would act as the conduit between the funds and the borrowers. it has now been possible to overcome some of the obstacles associated with such supply of funds with the advent of online platforms like kiva, prosper, lendingclub. kiva for example, works with micro finance institutions (mfis) in developing countries to build internet profiles of borrowers with a brief biography, loan requested, loan term, and purpose. kiva, in particular, allows lenders to fund projects in different sectors through group or individual funding. traditional research studies have investigated various factors behind lender preferences purely from the perspective of loan attributes and only until recently have some cross-country cultural preferences been investigated. in this paper, we investigate lender perceptions of economic factors of the borrower countries in relation to their preferences towards loans associated with different sectors. we find that the influence from economic factors and loan attributes can have substantially different roles to play for different sectors in achieving faster funding. we formally investigate and quantify the hidden biases prevalent in different loan sectors using recent tools from causal inference and regression models that rely on bayesian variable selection methods. we then extend these models to incorporate fairness constraints based on our empirical analysis and find that such models can still achieve near comparable results with respect to baseline regression models.", "categories": "econ.em cs.lg stat.ml", "created": "2020-06-19", "updated": "", "authors": ["soumajyoti sarkar", "hamidreza alvari"], "url": "https://arxiv.org/abs/2006.12995"}, {"title": "vocational training programs and youth labor market outcomes: evidence   from nepal", "id": "2006.13036", "abstract": "lack of skills is arguably one of the most important determinants of high levels of unemployment and poverty. in response, policymakers often initiate vocational training programs in effort to enhance skill formation among the youth. using a regression-discontinuity design, we examine a large youth training intervention in nepal. we find, twelve months after the start of the training program, that the intervention generated an increase in non-farm employment of 10 percentage points (itt estimates) and up to 31 percentage points for program compliers (late estimates). we also detect sizeable gains in monthly earnings. women who start self-employment activities inside their homes largely drive these impacts. we argue that low baseline educational levels and non-farm employment levels and nepal's social and cultural norms towards women drive our large program impacts. our results suggest that the program enables otherwise underemployed women to earn an income while staying at home - close to household errands and in line with the socio-cultural norms that prevent them from taking up employment outside the house.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-22", "updated": "", "authors": ["s. chakravarty", "m. lundberg", "p. nikolov", "j. zenker"], "url": "https://arxiv.org/abs/2006.13036"}, {"title": "bootstrapping $\\ell_p$-statistics in high dimensions", "id": "2006.13099", "abstract": "this paper considers a new bootstrap procedure to estimate the distribution of high-dimensional $\\ell_p$-statistics, i.e. the $\\ell_p$-norms of the sum of $n$ independent $d$-dimensional random vectors with $d \\gg n$ and $p \\in [1, \\infty]$. we provide a non-asymptotic characterization of the sampling distribution of $\\ell_p$-statistics based on gaussian approximation and show that the bootstrap procedure is consistent in the kolmogorov-smirnov distance under mild conditions on the covariance structure of the data. as an application of the general theory we propose a bootstrap hypothesis test for simultaneous inference on high-dimensional mean vectors. we establish its asymptotic correctness and consistency under high-dimensional alternatives, and discuss the power of the test as well as the size of associated confidence sets. we illustrate the bootstrap and testing procedure numerically on simulated data.", "categories": "math.st econ.em math.pr stat.th", "created": "2020-06-23", "updated": "2020-08-16", "authors": ["alexander giessing", "jianqing fan"], "url": "https://arxiv.org/abs/2006.13099"}, {"title": "what happens when separate and unequal school districts merge?", "id": "2006.13209", "abstract": "we study the welfare effects of school district consolidation, i.e. the integration of disjoint school districts into a centralised clearinghouse. we show theoretically that, in the worst-case scenario, district consolidation may unambiguously reduce students' welfare, even if the student-optimal stable matching is consistently chosen. however, on average all students experience expected welfare gains from district consolidation, particularly those who belong to smaller and over-demanded districts. using data from the hungarian secondary school assignment mechanism, we compute the actual welfare gains from district consolidation in budapest and compare these to our theoretical predictions. we empirically document substantial welfare gains from district consolidation for students, equivalent to attending a school five kilometres closer to the students' home addresses. as an important building block of our empirical strategy, we describe a method to consistently estimate students' preferences over schools and vice versa that does not fully assume that students report their preferences truthfully in the student-proposing deferred acceptance algorithm.", "categories": "econ.gn q-fin.ec", "created": "2020-06-22", "updated": "", "authors": ["robert aue", "thilo klein", "josue ortega"], "url": "https://arxiv.org/abs/2006.13209"}, {"title": "impact of covid-19 behavioral inertia on reopening strategies for new   york city transit", "id": "2006.13368", "abstract": "the covid-19 pandemic has affected travel behaviors and transportation system operations, and cities are grappling with what policies can be effective for a phased reopening shaped by social distancing. a baseline model was previously developed and calibrated for pre-covid conditions as matsim-nyc. a new covid model is calibrated that represents travel behavior during the covid-19 pandemic by recalibrating the population agendas to include work-from-home and re-estimating the mode choice model for matsim-nyc to fit observed traffic and transit ridership data. assuming the change in behavior exhibits inertia during reopening, we analyze the increase in car traffic due to the phased reopen plan guided by the state government of new york. four reopening phases and two reopening scenarios (with and without transit capacity restrictions) are analyzed. a phase 4 reopening with 100% transit capacity may only see as much as 73% of pre-covid ridership and an increase in the number of car trips by as much as 142% of pre-pandemic levels. limiting transit capacity to 50% would decrease transit ridership further from 73% to 64% while increasing car trips to as much as 143% of pre-pandemic levels. while the increase appears small, the impact on consumer surplus is disproportionately large due to already increased traffic congestion. many of the trips also get shifted to other modes like micromobility. the findings imply that a transit capacity restriction policy during reopening needs to be accompanied by (1) support for micromobility modes, particularly in non-manhattan boroughs, and (2) congestion alleviation policies that focus on reducing traffic in manhattan, such as cordon-based pricing.", "categories": "econ.gn cs.ma physics.soc-ph q-fin.ec", "created": "2020-06-23", "updated": "", "authors": ["ding wang", "brian yueshuai he", "jingqin gao", "joseph y. j. chow", "kaan ozbay", "shri iyer"], "url": "https://arxiv.org/abs/2006.13368"}, {"title": "design and evaluation of personalized free trials", "id": "2006.13420", "abstract": "free trial promotions, where users are given a limited time to try the product for free, are a commonly used customer acquisition strategy in the software as a service (saas) industry. we examine how trial length affect users' responsiveness, and seek to quantify the gains from personalizing the length of the free trial promotions. our data come from a large-scale field experiment conducted by a leading saas firm, where new users were randomly assigned to 7, 14, or 30 days of free trial. first, we show that the 7-day trial to all consumers is the best uniform policy, with a 5.59% increase in subscriptions. next, we develop a three-pronged framework for personalized policy design and evaluation. using our framework, we develop seven personalized targeting policies based on linear regression, lasso, cart, random forest, xgboost, causal tree, and causal forest, and evaluate their performances using the inverse propensity score (ips) estimator. we find that the personalized policy based on lasso performs the best, followed by the one based on xgboost. in contrast, policies based on causal tree and causal forest perform poorly. we then link a method's effectiveness in designing policy with its ability to personalize the treatment sufficiently without over-fitting (i.e., capture spurious heterogeneity). next, we segment consumers based on their optimal trial length and derive some substantive insights on the drivers of user behavior in this context. finally, we show that policies designed to maximize short-run conversions also perform well on long-run outcomes such as consumer loyalty and profitability.", "categories": "stat.ml cs.lg econ.em", "created": "2020-06-23", "updated": "", "authors": ["hema yoganarasimhan", "ebrahim barzegary", "abhishek pani"], "url": "https://arxiv.org/abs/2006.13420"}, {"title": "unified principal component analysis for sparse and dense functional   data under spatial dependency", "id": "2006.13489", "abstract": "we consider spatially dependent functional data collected under a geostatistics setting, where spatial locations are irregular and random. the functional response is the sum of a spatially dependent functional effect and a spatially independent functional nugget effect. observations on each function are made on discrete time points and contaminated with measurement errors. under the assumption of spatial stationarity and isotropy, we propose a tensor product spline estimator for the spatio-temporal covariance function. when a coregionalization covariance structure is further assumed, we propose a new functional principal component analysis method that borrows information from neighboring functions. the proposed method also generates nonparametric estimators for the spatial covariance functions, which can be used for functional kriging. under a unified framework for sparse and dense functional data, infill and increasing domain asymptotic paradigms, we develop the asymptotic convergence rates for the proposed estimators. advantages of the proposed approach are demonstrated through simulation studies and two real data applications representing sparse and dense functional data, respectively.", "categories": "stat.me econ.em math.st stat.co stat.th", "created": "2020-06-24", "updated": "", "authors": ["haozhe zhang", "yehua li"], "url": "https://arxiv.org/abs/2006.13489"}, {"title": "global sensitivity and domain-selective testing for functional-valued   responses: an application to climate economy models", "id": "2006.13850", "abstract": "complex computational models are increasingly used by business and governments for making decisions, such as how and where to invest to transition to a low carbon world. complexity arises with great evidence in the outputs generated by large scale models, and calls for the use of advanced sensitivity analysis techniques.   to our knowledge, there are no methods able to perform sensitivity analysis for outputs that are more complex than scalar ones and to deal with model uncertainty using a sound statistical framework.   the aim of this work is to address these two shortcomings by combining sensitivity and functional data analysis. we express output variables as smooth functions, employing a functional data analysis (fda) framework. we extend global sensitivity techniques to function-valued responses and perform significance testing over sensitivity indices.   we apply the proposed methods to computer models used in climate economics. while confirming the qualitative intuitions of previous works, we are able to test the significance of input assumptions and of their interactions. moreover, the proposed method allows to identify the time dynamics of sensitivity indices.", "categories": "stat.me econ.gn q-fin.ec", "created": "2020-06-24", "updated": "", "authors": ["matteo fontana", "massimo tavoni", "simone vantini"], "url": "https://arxiv.org/abs/2006.13850"}, {"title": "investor emotions and earnings announcements", "id": "2006.13934", "abstract": "armed with a decade of social media data, i explore the impact of investor emotions on earnings announcements. in particular, i test whether the emotional content of firm-specific messages posted on social media just prior to a firm's earnings announcement predicts its earnings and announcement returns. i find that investors are typically excited about firms that end up exceeding expectations, yet their enthusiasm results in lower announcement returns. specifically, a standard deviation increase in excitement is associated with an 7.8 basis points lower announcement return, which translates into an approximately -5.8% annualized loss. my findings confirm that emotions and market dynamics are closely related and highlight the importance of considering investor emotions when assessing a firm's short-term value.", "categories": "q-fin.pm cs.lg econ.gn q-fin.ec q-fin.gn", "created": "2020-06-24", "updated": "2020-06-25", "authors": ["domonkos f. vamossy"], "url": "https://arxiv.org/abs/2006.13934"}, {"title": "asset prices and capital share risks: theory and evidence", "id": "2006.14023", "abstract": "an asset pricing model using long-run capital share growth risk has recently been found to successfully explain u.s. stock returns. our paper adopts a recursive preference utility framework to derive an heterogeneous asset pricing model with capital share risks.while modeling capital share risks, we account for the elevated consumption volatility of high income stockholders. capital risks have strong volatility effects in our recursive asset pricing model. empirical evidence is presented in which capital share growth is also a source of risk for stock return volatility. we uncover contrasting unconditional and conditional asset pricing evidence for capital share risks.", "categories": "econ.em econ.th", "created": "2020-06-24", "updated": "", "authors": ["joseph p. byrne", "boulis m. ibrahim", "xiaoyu zong"], "url": "https://arxiv.org/abs/2006.14023"}, {"title": "optimizing voting order on sequential juries: a median voter theorem", "id": "2006.14045", "abstract": "we consider an odd-sized \"jury\", which votes sequentially between two states of nature (say a and b, or innocent and guilty) with the majority opinion determining the verdict. jurors have private information in the form of a signal in [-1,+1], with higher signals indicating a more likely. each juror has an ability in [0,1], which is proportional to the probability of a given a positive signal, an analog of condorcet's p for binary signals. we assume that jurors vote honestly for the alternative they view more likely, given their signal and prior voting, because they are experts who want to enhance their reputation (after their vote and actual state of nature is revealed). for a fixed set of jury abilities, the reliability of the verdict depends on the voting order. for a jury of size three, the optimal ordering is always as follows: middle ability first, then highest ability, then lowest. for sufficiently heterogeneous juries, sequential voting is more reliable than simultaneous voting and is in fact optimal (allowing for non-honest voting). when average ability is fixed, verdict reliability is increasing in heterogeneity.   for medium-sized juries, we find through simulation that the median ability juror should still vote first and the remaining ones should have increasing and then decreasing abilities.", "categories": "econ.th cs.gt math.oc", "created": "2020-06-24", "updated": "", "authors": ["steve alpern", "bo chen"], "url": "https://arxiv.org/abs/2006.14045"}, {"title": "dynamic effects of persistent shocks", "id": "2006.14047", "abstract": "we provide evidence that many narrative shocks used by prominent literature are persistent. we show that the two leading methods to estimate impulse responses to an independently identified shock (local projections and distributed lag models) treat persistence differently, hence identifying different objects. we propose corrections to re-establish the equivalence between local projections and distributed lag models, providing applied researchers with methods and guidance to estimate their desired object of interest. we apply these methods to well-known empirical work and find that how persistence is treated has a sizable impact on the estimates of dynamic effects.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-06-24", "updated": "", "authors": ["mario alloza", "jesus gonzalo", "carlos sanz"], "url": "https://arxiv.org/abs/2006.14047"}, {"title": "a model of the fed's view on inflation", "id": "2006.14110", "abstract": "we develop a medium-size semi-structural time series model of inflation dynamics that is consistent with the view - often expressed by central banks - that three components are important: a trend anchored by long-run expectations, a phillips curve and temporary fluctuations in energy prices. we find that a stable long-term inflation trend and a well identified steep phillips curve are consistent with the data, but they imply potential output declining since the new millennium and energy prices affecting headline inflation not only via the phillips curve but also via an independent expectational channel. a high-frequency energy price cycle can be related to global factors affecting the commodity market, and often overpowers the phillips curve thereby explaining the inflation puzzles of the last ten years.", "categories": "econ.em stat.ap", "created": "2020-06-24", "updated": "", "authors": ["thomas hasenzagl", "filippo pellegrino", "lucrezia reichlin", "giovanni ricco"], "url": "https://arxiv.org/abs/2006.14110"}, {"title": "robust and efficient approximate bayesian computation: a minimum   distance approach", "id": "2006.14126", "abstract": "in many instances, the application of approximate bayesian methods is hampered by two practical features: 1) the requirement to project the data down to low-dimensional summary, including the choice of this projection, which ultimately yields inefficient inference; 2) a possible lack of robustness to deviations from the underlying model structure. motivated by these efficiency and robustness concerns, we construct a new bayesian method that can deliver efficient estimators when the underlying model is well-specified, and which is simultaneously robust to certain forms of model misspecification. this new approach bypasses the calculation of summaries by considering a norm between empirical and simulated probability measures. for specific choices of the norm, we demonstrate that this approach can deliver point estimators that are as efficient as those obtained using exact bayesian inference, while also simultaneously displaying robustness to deviations from the underlying model assumptions.", "categories": "stat.me econ.em math.st stat.th", "created": "2020-06-24", "updated": "", "authors": ["david t. frazier"], "url": "https://arxiv.org/abs/2006.14126"}, {"title": "cointegration in large vars", "id": "2006.14179", "abstract": "the paper analyses cointegration in vector autoregressive processes (vars) for the cases when both the number of coordinates, $n$, and the number of time periods, $t$, are large and of the same order. we propose a way to examine a var for the presence of cointegration based on a modification of the johansen likelihood ratio test. the advantage of our procedure over the original johansen test and its finite sample corrections is that our test does not suffer from over-rejection. this is achieved through novel asymptotic theorems for eigenvalues of matrices in the test statistic in the regime of proportionally growing $n$ and $t$. our theoretical findings are supported by monte carlo simulations and an empirical illustration. moreover, we find a surprising connection with multivariate analysis of variance (manova) and explain why it emerges.", "categories": "econ.em math.pr math.st stat.th", "created": "2020-06-25", "updated": "", "authors": ["anna bykhovskaya", "vadim gorin"], "url": "https://arxiv.org/abs/2006.14179"}, {"title": "the uniqueness of dynamic groves mechanisms on restricted domains", "id": "2006.14190", "abstract": "this paper examines necessary and sufficient conditions for the uniqueness of dynamic groves mechanisms when the domain of valuations is restricted. our approach is to appropriately define the total valuation function, which is the expected discounted sum of each period's valuation function from the allocation and thus a dynamic counterpart of the static valuation function, and then to port the results for static groves mechanisms to the dynamic setting.", "categories": "econ.th", "created": "2020-06-25", "updated": "", "authors": ["kiho yoon"], "url": "https://arxiv.org/abs/2006.14190"}, {"title": "matching multidimensional types: theory and application", "id": "2006.14243", "abstract": "becker (1973) presents a bilateral matching model in which scalar types describe agents. for this framework, he establishes the conditions under which positive sorting between agents' attributes is the unique market outcome. becker's celebrated sorting result has been applied to address many economic questions. however, recent empirical studies in the fields of health, household, and labor economics suggest that agents have multiple outcome-relevant attributes. in this paper, i study a matching model with multidimensional types. i offer multidimensional generalizations of concordance and supermodularity to construct three multidimensional sorting patterns and two classes of multidimensional complementarities. for each of these sorting patterns, i identify the sufficient conditions which guarantee its optimality. in practice, we observe sorting patterns between observed attributes that are aggregated over unobserved characteristics. to reconcile theory with practice, i establish the link between production complementarities and the aggregated sorting patterns. finally, i examine the relationship between agents' health status and their spouses' education levels among u.s. households within the framework for multidimensional matching markets. preliminary analysis reveals a weak positive association between agents' health status and their spouses' education levels. this weak positive association is estimated to be a product of three factors: (a) an attraction between better-educated individuals, (b) an attraction between healthier individuals, and (c) a weak positive association between agents' health status and their education levels. the attraction channel suggests that the insurance risk associated with a two-person family plan is higher than the aggregate risk associated with two individual policies.", "categories": "econ.em econ.th", "created": "2020-06-25", "updated": "", "authors": ["veli safak"], "url": "https://arxiv.org/abs/2006.14243"}, {"title": "towards entrepreneurial ecosystem indicators : speed and acceleration", "id": "2006.14313", "abstract": "we suggest the use of indicators to analyze entrepreneurial ecosystems, in a way similar to ecological indicators: simple, measurable, and actionable characteristics, used to convey relevant information to stakeholders and policymakers. we define 3 possible such indicators: fundraising speed, acceleration and nth-year speed, all related to the ability of startups to develop more or less rapidly in a given ecosystem. results based on these 3 indicators for 6 prominent ecosystems (berlin, israel, london, new york, paris, silicon valley) exhibit markedly different situations and trajectories. altogether, they contribute to confirm that such indicators can help shed new and interesting light on entrepreneurial ecosystems, to the benefit of potentially more grounded policy decisions, and all the more so in otherwise blurred and somewhat cacophonic environments.", "categories": "econ.gn q-fin.ec", "created": "2020-06-25", "updated": "", "authors": ["th\u00e9ophile carniel", "jean-michel dalle"], "url": "https://arxiv.org/abs/2006.14313"}, {"title": "inference without smoothing for large panels with cross-sectional and   temporal dependence", "id": "2006.14409", "abstract": "this paper addresses inference in large panel data models in the presence of both cross-sectional and temporal dependence of unknown form. we are interested in making inferences that do not rely on the choice of any smoothing parameter as is the case with the often employed \"hac\" estimator for the covariance matrix. to that end, we propose a cluster estimator for the asymptotic covariance of the estimators and valid bootstrap schemes that do not require the selection of a bandwidth or smoothing parameter and accommodate the nonparametric nature of both temporal and cross-sectional dependence. our approach is based on the observation that the spectral representation of the fixed effect panel data model is such that the errors become approximately temporally uncorrelated. our proposed bootstrap schemes can be viewed as wild bootstraps in the frequency domain. we present some monte-carlo simulations to shed some light on the small sample performance of our inferential procedure.", "categories": "econ.em math.st stat.th", "created": "2020-06-25", "updated": "", "authors": ["j. hidalgo", "m. schafgans"], "url": "https://arxiv.org/abs/2006.14409"}, {"title": "comparative statics in multicriteria search models", "id": "2006.14452", "abstract": "mccall (1970) examines the search behaviour of an infinitely-lived and risk-neutral job seeker maximizing her lifetime earnings by accepting or rejecting real-valued scalar wage offers. in practice, job offers have multiple attributes, and job seekers solve a multicriteria search problem. this paper presents a multicriteria search model and new comparative statics results.", "categories": "econ.th", "created": "2020-06-25", "updated": "", "authors": ["veli safak"], "url": "https://arxiv.org/abs/2006.14452"}, {"title": "teamwise mean field competitions", "id": "2006.14472", "abstract": "this paper studies competitions with rank-based reward among a large number of teams. within each sizable team, we consider a mean-field contribution game in which each team member contributes to the jump intensity of a common poisson project process; across all teams, a mean field competition game is formulated on the rank of the completion time, namely the jump time of poisson project process, and the reward to each team is paid based on its ranking. on the layer of teamwise competition game, three optimization problems are introduced when the team size is determined by: (i) the team manager; (ii) the central planner; (iii) the team members' voting as partnership. we propose a relative performance criteria for each team member to share the team's reward and formulate some mean field games of mean field games, which are new to the literature. in all problems with homogeneous parameters, the equilibrium control of each worker and the equilibrium or optimal team size can be computed in an explicit manner, allowing us to analytically examine the impacts of some model parameters and discuss their economic implications. two numerical examples are also presented to illustrate the parameter dependence and comparison between different team size decision making.", "categories": "cs.gt econ.th math.oc", "created": "2020-06-24", "updated": "", "authors": ["xiang yu", "yuchong zhang", "zhou zhou"], "url": "https://arxiv.org/abs/2006.14472"}, {"title": "which random matching markets exhibit a stark effect of competition?", "id": "2006.14653", "abstract": "we revisit the popular random matching market model introduced by knuth (1976) and pittel (1989), and shown by ashlagi, kanoria and leshno (2013) to exhibit a \"stark effect of competition\", i.e., with any difference in the number of agents on the two sides, the short side agents obtain substantially better outcomes. we generalize the model to allow \"partially connected\" markets with each agent having an average degree $d$ in a random (undirected) graph. each agent has a (uniformly random) preference ranking over only their neighbors in the graph. we characterize stable matchings in large markets and find that the short side enjoys a significant advantage only for $d$ exceeding $\\log^2 n$ where $n$ is the number of agents on one side: for moderately connected markets with $d=o(\\log^2 n)$, we find that there is no stark effect of competition, with agents on both sides getting a $\\sqrt{d}$-ranked partner on average. notably, this regime extends far beyond the connectivity threshold of $d= \\theta(\\log n)$. in contrast, for densely connected markets with $d = \\omega(\\log^2 n)$, we find that the short side agents get $\\log n$-ranked partner on average, while the long side agents get a partner of (much larger) rank $d/\\log n$ on average. numerical simulations of our model confirm and sharpen our theoretical predictions. since preference list lengths in most real-world matching markets are much below $\\log^2 n$, our findings may help explain why available datasets do not exhibit a strong effect of competition.", "categories": "cs.gt econ.th", "created": "2020-06-25", "updated": "", "authors": ["yash kanoria", "seungki min", "pengyu qian"], "url": "https://arxiv.org/abs/2006.14653"}, {"title": "empirical mse minimization to estimate a scalar parameter", "id": "2006.14667", "abstract": "we consider the estimation of a scalar parameter, when two estimators are available. the first is always consistent. the second is inconsistent in general, but has a smaller asymptotic variance than the first, and may be consistent if an assumption is satisfied. we propose to use the weighted sum of the two estimators with the lowest estimated mean-squared error (mse). we show that this third estimator dominates the other two from a minimax-regret perspective: the maximum asymptotic-mse-gain one may incur by using this estimator rather than one of the other estimators is larger than the maximum asymptotic-mse-loss.", "categories": "math.st econ.em stat.th", "created": "2020-06-25", "updated": "", "authors": ["cl\u00e9ment de chaisemartin", "xavier d'haultf\u0153uille"], "url": "https://arxiv.org/abs/2006.14667"}, {"title": "identification and formal privacy guarantees", "id": "2006.14732", "abstract": "empirical economic research crucially relies on highly sensitive individual datasets. at the same time, increasing availability of public individual-level data makes it possible for adversaries to potentially de-identify anonymized records in sensitive research datasets. this increasing disclosure risk has incentivised large data curators, most notably the us census bureau and several large companies including apple, facebook and microsoft to look for algorithmic solutions to provide formal non-disclosure guarantees for their secure data. the most commonly accepted formal data security concept in the computer science community is differential privacy. it restricts the interaction of researchers with the data by allowing them to issue queries to the data. the differential privacy mechanism then replaces the actual outcome of the query with a randomised outcome.   while differential privacy does provide formal data security guarantees, its impact on the identification of empirical economic models and on the performance of estimators in those models has not been sufficiently studied. since privacy protection mechanisms are inherently finite-sample procedures, we define the notion of identifiability of the parameter of interest as a property of the limit of experiments. it is linked to the asymptotic behavior in measure of differentially private estimators.   we demonstrate that particular instances of regression discontinuity design and average treatment effect may be problematic for inference with differential privacy because their estimators can only be ensured to converge weakly with their asymptotic limit remaining random and, thus, may not be estimated consistently. this result is clearly supported by our simulation evidence. our analysis suggests that many other estimators that rely on nuisance parameters may have similar properties with the requirement of differential privacy.", "categories": "econ.em stat.me", "created": "2020-06-25", "updated": "", "authors": ["tatiana komarova", "denis nekipelov"], "url": "https://arxiv.org/abs/2006.14732"}, {"title": "kuhn's equivalence theorem for games in intrinsic form", "id": "2006.14838", "abstract": "we state and prove kuhn's equivalence theorem for a new representation of games, the intrinsic form. first, we introduce games in intrinsic form where information is represented by $\\sigma$-fields over a product set. for this purpose, we adapt to games the intrinsic representation that witsenhausen introduced in control theory. those intrinsic games do not require an explicit description of the play temporality, as opposed to extensive form games on trees. second, we prove, for this new and more general representation of games, that behavioral and mixed strategies are equivalent under perfect recall (kuhn's theorem). as the intrinsic form replaces the tree structure with a product structure, the handling of information is easier. this makes the intrinsic form a new valuable tool for the analysis of games with information.", "categories": "math.oc cs.gt econ.th", "created": "2020-06-26", "updated": "", "authors": ["benjamin heymann", "michel de lara", "jean-philippe chancelier"], "url": "https://arxiv.org/abs/2006.14838"}, {"title": "the welfare of ramsey optimal policy facing auto-regressive shocks", "id": "2006.14842", "abstract": "with non-controllable auto-regressive shocks, the welfare of ramsey optimal policy is the solution of a single riccati equation of a linear quadratic regulator. the existing theory by hansen and sargent (2007) refers to an additional sylvester equation but miss another equation for computing the block matrix weighting the square of non-controllable variables in the welfare function. there is no need to simulate impulse response functions over a long period, to compute period loss functions and to sum their discounted value over this long period, as currently done so far. welfare is computed for the case of the new-keynesian phillips curve with an auto-regressive cost-push shock. jel classification numbers: c61, c62, c73, e47, e52, e61, e63.", "categories": "math.oc econ.gn q-fin.ec", "created": "2020-06-26", "updated": "", "authors": ["jean-bernard chatelain", "kirsten ralf"], "url": "https://arxiv.org/abs/2006.14842"}, {"title": "social welfare in search games with asymmetric information", "id": "2006.14860", "abstract": "we consider games in which players search for a hidden prize, and they have asymmetric information about the prize location. we study the social payoff in equilibria of these games. we present sufficient conditions for the existence of an equilibrium that yields the first-best payoff (i.e., the highest social payoff under any strategy profile), and we characterize the first-best payoff. the results have interesting implications for innovation contests and r&d races.", "categories": "econ.th", "created": "2020-06-26", "updated": "", "authors": ["gilad bavly", "yuval heller", "amnon schreiber"], "url": "https://arxiv.org/abs/2006.14860"}, {"title": "status quo bias beats the decoy effect and reverses attitudes toward   risk", "id": "2006.14868", "abstract": "inertia and context-dependent choice effects are well-studied classes of behavioural phenomena. while much is known about these effects individually, little is known about whether one of them \"dominates\" another. knowledge of any such dominance is important for effective choice architecture and for accurate descriptive modelling. we initiate this empirical investigation with a lab experiment on choice under risk that was designed to test for dominance between *status quo bias* and the *decoy effect*. we find that the former unambiguously prevails over the latter and is powerful enough to make the average subject switch from being risk averse to being risk-seeking. the observed reversal in risk attitudes is explainable by a large class of kozsegi-rabin (2006) reference-dependent preferences.", "categories": "econ.gn q-fin.ec", "created": "2020-06-26", "updated": "", "authors": ["miguel costa-gomes", "georgios gerasimou"], "url": "https://arxiv.org/abs/2006.14868"}, {"title": "revealing choice bracketing", "id": "2006.14869", "abstract": "in a decision problem comprised of multiple choices, a person may fail to take into account the interdependencies between her choices. to understand how people make decisions in such problems we design a novel experiment and revealed preference tests that determine how each subject brackets her choices. in separate portfolio allocation under risk, social allocation, and induced-utility shopping experiments, we find that 40-43\\% of our subjects are consistent with narrow bracketing while only 0-15\\% are consistent with broad bracketing. classifying subjects while adjusting for models' predictive precision, 73\\% of subjects are best described by narrow bracketing, 14\\% by broad bracketing, and 5\\% by intermediate cases.", "categories": "econ.th", "created": "2020-06-26", "updated": "2020-09-30", "authors": ["andrew ellis", "david j freeman"], "url": "https://arxiv.org/abs/2006.14869"}, {"title": "endogenous treatment effect estimation with some invalid and irrelevant   instruments", "id": "2006.14998", "abstract": "instrumental variables (iv) regression is a popular method for the estimation of the endogenous treatment effects. conventional iv methods require all the instruments are relevant and valid. however, this is impractical especially in high-dimensional models when we consider a large set of candidate ivs. in this paper, we propose an iv estimator robust to the existence of both the invalid and irrelevant instruments (called r2ive) for the estimation of endogenous treatment effects. this paper extends the scope of kang et al. (2016) by considering a true high-dimensional iv model and a nonparametric reduced form equation. it is shown that our procedure can select the relevant and valid instruments consistently and the proposed r2ive is root-n consistent and asymptotically normal. monte carlo simulations demonstrate that the r2ive performs favorably compared to the existing high-dimensional iv estimators (such as, naive (fan and zhong, 2018) and sisvive (kang et al., 2016)) when invalid instruments exist. in the empirical study, we revisit the classic question of trade and growth (frankel and romer, 1999).", "categories": "econ.em stat.ap stat.me", "created": "2020-06-26", "updated": "", "authors": ["qingliang fan", "yaqian wu"], "url": "https://arxiv.org/abs/2006.14998"}, {"title": "real-time real economic activity: exiting the great recession and   entering the pandemic recession", "id": "2006.15183", "abstract": "we study the real-time signals provided by the aruoba-diebold-scotti index of business conditions (ads) for tracking economic activity at high frequency. we start with exit from the great recession, comparing the evolution of real-time vintage beliefs to a \"final\" late-vintage chronology. we then consider entry into the pandemic recession, again tracking the evolution of real-time vintage beliefs. ads swings widely as its underlying economic indicators swing widely, but the emerging ads path as of this writing (late june) indicates a return to growth in may. the trajectory of the nascent recovery, however, is highly uncertain -- particularly as covid-19 spreads in the south and west -- and could be revised or eliminated as new data arrive.", "categories": "econ.em econ.gn q-fin.ec stat.ap", "created": "2020-06-26", "updated": "", "authors": ["francis x. diebold"], "url": "https://arxiv.org/abs/2006.15183"}, {"title": "biased-belief equilibrium", "id": "2006.15306", "abstract": "we investigate how distorted, yet structured, beliefs can persist in strategic situations. specifically, we study two-player games in which each player is endowed with a biased-belief function that represents the discrepancy between a player's beliefs about the opponent's strategy and the actual strategy. our equilibrium condition requires that (i) each player choose a best-response strategy to his distorted belief about the opponent's strategy, and (ii) the distortion functions form best responses to one another. we obtain sharp predictions and novel insights into the set of stable outcomes and their supporting stable biases in various classes of games.", "categories": "econ.th", "created": "2020-06-27", "updated": "", "authors": ["yuval heller", "eyal winter"], "url": "https://arxiv.org/abs/2006.15306"}, {"title": "coevolution of deception and preferences: darwin and nash meet   machiavelli", "id": "2006.15308", "abstract": "we develop a framework in which individuals' preferences coevolve with their abilities to deceive others about their preferences and intentions. specifically, individuals are characterised by (i) a level of cognitive sophistication and (ii) a subjective utility function. increased cognition is costly, but higher-level individuals have the advantage of being able to deceive lower-level opponents about their preferences and intentions in some of the matches. in the remaining matches, the individuals observe each other's preferences. our main result shows that, essentially, only efficient outcomes can be stable. moreover, under additional mild assumptions, we show that an efficient outcome is stable if and only if the gain from unilateral deviation is smaller than the effective cost of deception in the environment.", "categories": "econ.th", "created": "2020-06-27", "updated": "", "authors": ["yuval heller", "erik mohlin"], "url": "https://arxiv.org/abs/2006.15308"}, {"title": "a closed-form solution to the risk-taking motivation of subordinated   debtholders", "id": "2006.15309", "abstract": "black and cox (1976) claim that the value of junior debt is increasing in asset risk when the firm's value is low. we show, using closed-form solution, that the junior debt's value is hump-shaped. this has interesting implications for the market-discipline role of banks' junior debt.", "categories": "econ.th", "created": "2020-06-27", "updated": "", "authors": ["yuval heller", "n/a sharonpeleg-lazar", "alon raviv"], "url": "https://arxiv.org/abs/2006.15309"}, {"title": "observations on cooperation", "id": "2006.15310", "abstract": "we study environments in which agents are randomly matched to play a prisoner's dilemma, and each player observes a few of the partner's past actions against previous opponents. we depart from the existing related literature by allowing a small fraction of the population to be commitment types. the presence of committed agents destabilizes previously proposed mechanisms for sustaining cooperation. we present a novel intuitive combination of strategies that sustains cooperation in various environments. moreover, we show that under an additional assumption of stationarity, this combination of strategies is essentially the unique mechanism to support full cooperation, and it is robust to various perturbations. finally, we extend the results to a setup in which agents also observe actions played by past opponents against the current partner, and we characterize which observation structure is optimal for sustaining cooperation.", "categories": "econ.th", "created": "2020-06-27", "updated": "", "authors": ["yuval heller", "erik mohlin"], "url": "https://arxiv.org/abs/2006.15310"}, {"title": "quantitative statistical robustness for tail-dependent law invariant   risk measures", "id": "2006.15491", "abstract": "when estimating the risk of a financial position with empirical data or monte carlo simulations via a tail-dependent law invariant risk measure such as the conditional value-at-risk (cvar), it is important to ensure the robustness of the statistical estimator particularly when the data contain noise. kr\u007fatscher et al. [1] propose a new framework to examine the qualitative robustness of estimators for tail-dependent law invariant risk measures on orlicz spaces, which is a step further from earlier work for studying the robustness of risk measurement procedures by cont et al. [2]. in this paper, we follow the stream of research to propose a quantitative approach for verifying the statistical robustness of tail-dependent law invariant risk measures. a distinct feature of our approach is that we use the fortet-mourier metric to quantify the variation of the true underlying probability measure in the analysis of the discrepancy between the laws of the plug-in estimators of law invariant risk measure based on the true data and perturbed data, which enables us to derive an explicit error bound for the discrepancy when the risk functional is lipschitz continuous with respect to a class of admissible laws. moreover, the newly introduced notion of lipschitz continuity allows us to examine the degree of robustness for tail-dependent risk measures. finally, we apply our quantitative approach to some well-known risk measures to illustrate our theory.", "categories": "q-fin.rm econ.em math.pr", "created": "2020-06-27", "updated": "", "authors": ["wei wang", "huifu xu", "tiejun ma"], "url": "https://arxiv.org/abs/2006.15491"}, {"title": "treatment effects in interactive fixed effects models", "id": "2006.15780", "abstract": "this paper considers identifying and estimating the average treatment effect on the treated (att) in interactive fixed effects models. we focus on the case where there is a single unobserved time-invariant variable whose effect is allowed to change over time, though we also allow for time fixed effects and unobserved individual-level heterogeneity. the models that we consider in this paper generalize many commonly used models in the treatment effects literature including difference in differences and individual-specific linear trend models. unlike the majority of the literature on interactive fixed effects models, we do not require the number of time periods to go to infinity to consistently estimate the att. our main identification result relies on having the effect of some time invariant covariate (e.g., race or sex) not vary over time. using our approach, we show that the att can be identified with as few as three time periods and with panel or repeated cross sections data.", "categories": "econ.em", "created": "2020-06-28", "updated": "", "authors": ["brantly callaway", "sonia karami"], "url": "https://arxiv.org/abs/2006.15780"}, {"title": "all things equal? social networks as a mechanism for discrimination", "id": "2006.15988", "abstract": "i study labor markets in which firms can hire via job referrals. despite full equality in the initial time period (e.g., equal ability, employment, wages, and network structure), unequal wages and employment still emerge over time between majority and minority workers, due to homophily---the well-documented tendency for people to associate more with others similar to themselves. this inequality can be mitigated by minority workers having more social ties or a \"stronger-knit\" network. hence, this paper uncovers a direct mechanism for discriminatory outcomes that neither relies on past inequality nor on discriminatory motives (i.e., neither of the prevailing economic models of taste-based and statistical discrimination). these findings introduce multiple policy implications, including disproving a primary justification for \"colorblind\" policies---namely disproving the position that such policies are inherently merit-enhancing.", "categories": "econ.gn q-fin.ec", "created": "2020-06-26", "updated": "", "authors": ["chika o. okafor"], "url": "https://arxiv.org/abs/2006.15988"}, {"title": "visualizing and comparing distributions with half-disk density strips", "id": "2006.16063", "abstract": "we propose a user-friendly graphical tool, the half-disk density strip (hdds), for visualizing and comparing probability density functions. the hdds exploits color shading for representing a distribution in an intuitive way. in univariate settings, the half-disk density strip allows to immediately discern the key characteristics of a density, such as symmetry, dispersion, and multi-modality. in the multivariate settings, we define hdds tables to generalize the concept of contingency tables. it is an array of half-disk density strips, which compactly displays the univariate marginal and conditional densities of a variable of interest, together with the joint and marginal densities of the conditioning variables. moreover, hddss are by construction well suited to easily compare pairs of densities. to highlight the concrete benefits of the proposed methods, we show how to use hddss for analyzing income distribution and life-satisfaction, conditionally on continuous and categorical controls, from survey data. the code for implementing hdds methods is made available through a dedicated r package.", "categories": "stat.me econ.em stat.ap", "created": "2020-06-29", "updated": "", "authors": ["carlo romano marcello alessandro santagiustina", "matteo iacopini"], "url": "https://arxiv.org/abs/2006.16063"}, {"title": "pay transparency and cracks in the glass ceiling", "id": "2006.16099", "abstract": "each year since 2018, more than 10,000 uk firms have been required to publicly disclose their gender pay gap and gender composition. this paper studies how this transparency policy affects the occupational outcomes and wages of male and female workers. theoretically, pay transparency represents an information shock that alters the bargaining power of male and female employees vis-\\`a-vis the firm in an asymmetric way. as women are currently underpaid, this shock may improve women's relative outcomes. we test these theoretical predictions using a difference-in-difference strategy that exploits the variation in the uk mandate across firm size and time. our results show that pay transparency increases the probability that women are hired in above-median-wage occupations by 5 percent compared to the pre-policy mean. additionally, it leads to a 2.8 percent decrease in male real hourly pay in treated firms compared to control ones. combining the difference-in-difference strategy with a text analysis of job listings, we also find suggestive evidence that treated firms in industries with a high gender pay gap become more likely to post wage information than firms in the control group.", "categories": "econ.gn q-fin.ec", "created": "2020-06-25", "updated": "", "authors": ["emma duchini", "stefania simion", "arthur turrell"], "url": "https://arxiv.org/abs/2006.16099"}, {"title": "reputation for playing mixed actions: a characterization theorem", "id": "2006.16206", "abstract": "i study a reputation model in which a patient player privately observes a persistent state that affects his myopic opponents' payoffs, and can be one of the several commitment types that plays the same (possibly mixed) action over time. the main result is a characterization of the set of environments under which the patient player obtains at least his commitment payoff in all equilibria regardless of his stage-game payoff function. my result implies that small perturbations to a pure commitment action can lead to a discontinuous change in the patient player's equilibrium payoff. the main technical contribution is to use martingale techniques to construct a non-stationary strategy under which the patient player can avoid signaling negative information about the state while at the same time, matching the long-run frequency of his actions to the mixed commitment action and convincing his opponents that his action is close to the commitment action in almost all periods.", "categories": "econ.th", "created": "2020-06-29", "updated": "", "authors": ["harry pei"], "url": "https://arxiv.org/abs/2006.16206"}, {"title": "estimation of covid-19 prevalence from serology tests: a partial   identification approach", "id": "2006.16214", "abstract": "we propose a partial identification method for estimating disease prevalence from serology studies. our data are results from antibody tests in some population sample, where the test parameters, such as the true/false positive rates, are unknown. our method scans the entire parameter space, and rejects parameter values using the joint data density as the test statistic. the proposed method is conservative for marginal inference, in general, but its key advantage over more standard approaches is that it is valid in finite samples even when the underlying model is not point identified. moreover, our method requires only independence of serology test results, and does not rely on asymptotic arguments, normality assumptions, or other approximations. we use recent covid-19 serology studies in the us, and show that the parameter confidence set is generally wide, and cannot support definite conclusions. specifically, recent serology studies from california suggest a prevalence anywhere in the range 0%-2% (at the time of study), and are therefore inconclusive. however, this range could be narrowed down to 0.7%-1.5% if the actual false positive rate of the antibody test was indeed near its empirical estimate (~0.5%). in another study from new york state, covid-19 prevalence is confidently estimated in the range 13%-17% in mid-april of 2020, which also suggests significant geographic variation in covid-19 exposure across the us. combining all datasets yields a 5%-8% prevalence range. our results overall suggest that serology testing on a massive scale can give crucial information for future policy design, even when such tests are imperfect and their parameters unknown.", "categories": "stat.me econ.em stat.ap", "created": "2020-06-29", "updated": "", "authors": ["panos toulis"], "url": "https://arxiv.org/abs/2006.16214"}, {"title": "inference in bayesian additive vector autoregressive tree models", "id": "2006.16333", "abstract": "vector autoregressive (var) models assume linearity between the endogenous variables and their lags. this linearity assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. as a solution, we propose combining var with bayesian additive regression tree (bart) models. the resulting bayesian additive vector autoregressive tree (bavart) model is capable of capturing arbitrary non-linear relations between the endogenous variables and the covariates without much input from the researcher. since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. using synthetic and real data, we demonstrate the advantages of our methods. for eurozone data, we show that our nonparametric approach improves upon commonly used forecasting models and that it produces impulse responses to an uncertainty shock that are consistent with established findings in the literature.", "categories": "econ.em stat.ml stat.ot", "created": "2020-06-29", "updated": "", "authors": ["florian huber", "luca rossini"], "url": "https://arxiv.org/abs/2006.16333"}, {"title": "mixed logit models and network formation", "id": "2006.16516", "abstract": "the study of network formation is pervasive in economics, sociology, and many other fields. in this paper, we model network formation as a ``choice'' that is made by nodes in a network to connect to other nodes. we study these ``choices'' using discrete-choice models, in which an agent chooses between two or more discrete alternatives. one framework for studying network formation is the multinomial logit (mnl) model. we highlight limitations of the mnl model on networks that are constructed from empirical data. we employ the ``repeated choice'' (rc) model to study network formation \\cite{trainrevelt97mixedlogit}. we argue that the rc model overcomes important limitations of the mnl model and is well-suited to study network formation. we also illustrate how to use the rc model to accurately study network formation using both synthetic and real-world networks. using synthetic networks, we also compare the performance of the mnl model and the rc model; we find that the rc model estimates the data-generation process of our synthetic networks more accurately than the mnl model. we provide examples of qualitatively interesting questions -- the presence of homophily in a teen friendship network and the fact that new patents are more likely to cite older, more cited, and similar patents -- for which the rc model allows us to achieve insights.", "categories": "cs.si econ.th physics.soc-ph stat.ml", "created": "2020-06-30", "updated": "", "authors": ["harsh gupta", "mason a. porter"], "url": "https://arxiv.org/abs/2006.16516"}, {"title": "the yannelis-prabhakar theorem on upper semi-continuous selections in   paracompact spaces: extensions and applications", "id": "2006.16681", "abstract": "in a 1983 paper, yannelis-prabhakar rely on michael's selection theorem to guarantee a continuous selection in the context of the existence of maximal elements and equilibria in abstract economies. in this tribute to nicholas yannelis, we root this paper in chapter ii of yannelis' 1983 rochester ph.d. dissertation, and identify its pioneering application of the paracompactness condition to current and ongoing work of yannelis and his co-authors, and to mathematical economics more generally. we move beyond the literature to provide a necessary and sufficient condition for upper semi-continuous local and global selections of correspondences, and to provide application to five domains of yannelis' interests: berge's maximum theorem, the gale-nikaido-debreu lemma, the gale-mckenzie survival assumption, shafer's non-transitive setting, and the anderson-khan-rashid approximate existence theorem. the last resonates with chapter vi of the yannelis' dissertation.", "categories": "econ.th math.fa math.gn", "created": "2020-06-30", "updated": "", "authors": ["m. ali khan", "metin uyanik"], "url": "https://arxiv.org/abs/2006.16681"}, {"title": "turbulence on the global economy influenced by artificial intelligence   and foreign policy inefficiencies", "id": "2006.16911", "abstract": "it is said that data and information are the new oil. one, who handles the data, handles the emerging future of the global economy. complex algorithms and intelligence-based filter programs are utilized to manage, store, handle and maneuver vast amounts of data for the fulfillment of specific purposes. this paper seeks to find the bridge between artificial intelligence and its impact on the international policy implementation in the light of geopolitical influence, global economy and the future of labor markets. we hypothesize that the distortion in the labor markets caused by artificial intelligence can be mitigated by a collaborative international foreign policy on the deployment of ai in the industrial circles. we, in this paper, then proceed to propose a disposition for the essentials of ai-based foreign policy and implementation, while asking questions such as 'could ai become the real invisible hand discussed by economists?'.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-06-19", "updated": "", "authors": ["kwadwo osei bonsu", "jie song"], "url": "https://arxiv.org/abs/2006.16911"}, {"title": "interdependence in active mobility adoption: joint modelling and   motivational spill-over in walking, cycling and bike-sharing", "id": "2006.16920", "abstract": "active mobility, traditionally referring to modes requiring physical activity to operate, offers an array of physical, emotional, and social well-being benefits. however, with the proliferation of the sharing economy, new nonmotorized means of transport are entering the fold, complementing some existing mobility options while competing with others. the purpose of this research is to investigate the adoption of three active travel modes, namely walking, cycling and bike-sharing, in a joint modeling framework. the analysis is based on an adaptation of the stages of change framework, which originates from the health behavior sciences. the development of a multivariate ordered probit model drawing on u.s. survey data provides well-needed insights into individuals preparedness to adopt multiple active modes as a function of personal, neighborhood and psychosocial factors. the joint model structures reveal different levels of interdependence among active mobility choices. the strongest positive association is found for walking and cycling adoption, whereas other joint model effects are less evident. identifying strongly with active mobility, experiences with multimodal travel, possessing better navigational skills, along with supportive local community norms are the factors that appear to drive the joint adoption decisions. this study contributes to the understanding of how decisions within the same functional domain are related.", "categories": "cs.cy econ.em", "created": "2020-06-24", "updated": "", "authors": ["m said", "a biehl", "a stathopoulos"], "url": "https://arxiv.org/abs/2006.16920"}, {"title": "the equilibrium existence duality: equilibrium with indivisibilities &   income effects", "id": "2006.16939", "abstract": "we show that, with indivisible goods, the existence of competitive equilibrium fundamentally depends on agents' substitution effects, not their income effects. our equilibrium existence duality allows us to transport results on the existence of competitive equilibrium from settings with transferable utility to settings with income effects. one consequence is that net substitutability---which is a strictly weaker condition than gross substitutability---is sufficient for the existence of competitive equilibrium. we also extend the ``demand types'' classification of valuations to settings with income effects and give necessary and sufficient conditions for a pattern of substitution effects to guarantee the existence of competitive equilibrium.", "categories": "econ.th cs.gt", "created": "2020-06-30", "updated": "", "authors": ["elizabeth baldwin", "omer edhan", "ravi jagadeesan", "paul klemperer", "alexander teytelboym"], "url": "https://arxiv.org/abs/2006.16939"}, {"title": "inference in differences-in-differences with few treated units and   spatial correlation", "id": "2006.16997", "abstract": "we consider the problem of inference in differences-in-differences models when there are few treated units and errors are spatially correlated. we first show that, when there is a single treated unit, existing inference methods designed for settings with few treated and many control units remain asymptotically valid when errors are strongly mixing. however, these methods are invalid with more than one treated unit. we propose asymptotically valid, though generally conservative, inference methods for settings with more than one treated unit. our findings are relevant even for settings in which the number of treated units is usually considered as large enough to rely on clustered robust standard errors.", "categories": "econ.em", "created": "2020-06-30", "updated": "2020-08-09", "authors": ["bruno ferman"], "url": "https://arxiv.org/abs/2006.16997"}, {"title": "regression discontinuity design with multivalued treatments", "id": "2007.00185", "abstract": "we study identification and estimation in the regression discontinuity design (rdd) with a multivalued treatment variable. we also allow for the inclusion of covariates. we show that without additional information, treatment effects are not identified. we give necessary and sufficient conditions that lead to identification of lates as well as of weighted averages of the conditional lates. we show that if the first stage discontinuities of the multiple treatments conditional on covariates are linearly independent, then it is possible to identify multivariate weighted averages of the treatment effects with convenient identifiable weights. if, moreover, treatment effects do not vary with some covariates or a flexible parametric structure can be assumed, it is possible to identify (in fact, over-identify) all the treatment effects. the over-identification can be used to test these assumptions. we propose a simple estimator, which can be programmed in packaged software as a two-stage least squares regression, and packaged standard errors and tests can also be used. finally, we implement our approach to identify the effects of different types of insurance coverage on health care utilization, as in card, dobkin and maestas (2008).", "categories": "econ.em stat.me", "created": "2020-06-30", "updated": "", "authors": ["carolina caetano", "gregorio caetano", "juan carlos escanciano"], "url": "https://arxiv.org/abs/2007.00185"}, {"title": "when are google data useful to nowcast gdp? an approach via   pre-selection and shrinkage", "id": "2007.00273", "abstract": "we analyse whether, and when, a large set of google search data can be useful to increase euro area gdp nowcasting accuracy once we control for information contained in official variables. to deal with these data we propose an estimator that combines variable pre-selection and ridge regularization and study its theoretical properties. we show that in a period of cyclical stability google data convey useful information for real-time nowcasting of gdp growth at the beginning of the quarter when macroeconomic information is lacking. on the other hand, in periods that exhibit a sudden downward shift in gdp growth rate, including google search data in the information set improves nowcasting accuracy even when official macroeconomic information is available.", "categories": "econ.em", "created": "2020-07-01", "updated": "", "authors": ["laurent ferrara", "anna simoni"], "url": "https://arxiv.org/abs/2007.00273"}, {"title": "multi-objective optimal control of dynamic integrated model of climate   and economy: evolution in action", "id": "2007.00449", "abstract": "one of the widely used models for studying economics of climate change is the dynamic integrated model of climate and economy (dice), which has been developed by professor william nordhaus, one of the laureates of the 2018 nobel memorial prize in economic sciences. originally a single-objective optimal control problem has been defined on dice dynamics, which is aimed to maximize the social welfare. in this paper, a bi-objective optimal control problem defined on dice model, objectives of which are maximizing social welfare and minimizing the temperature deviation of atmosphere. this multi-objective optimal control problem solved using non-dominated sorting genetic algorithm ii (nsga-ii) also it is compared to previous works on single-objective version of the problem. the resulting pareto front rediscovers the previous results and generalizes to a wide range of non-dominant solutions to minimize the global temperature deviation while optimizing the economic welfare. the previously used single-objective approach is unable to create such a variety of possibilities, hence, its offered solution is limited in vision and reachable performance. beside this, resulting pareto-optimal set reveals the fact that temperature deviation cannot go below a certain lower limit, unless we have significant technology advancement or positive change in global conditions.", "categories": "econ.gn cs.ne cs.sy eess.sy q-fin.ec", "created": "2020-06-29", "updated": "", "authors": ["mostapha kalami heris", "shahryar rahnamayan"], "url": "https://arxiv.org/abs/2007.00449"}, {"title": "robust communication on networks", "id": "2007.00457", "abstract": "we consider sender-receiver games, where the sender and the receiver are two distinct nodes in a communication network. communication between the sender and the receiver is thus indirect. we ask when it is possible to robustly implement the equilibrium outcomes of the direct communication game as equilibrium outcomes of indirect communication games on the network. robust implementation requires that: (i) the implementation is independent of the preferences of the intermediaries and (ii) the implementation is guaranteed at all histories consistent with unilateral deviations by the intermediaries. robust implementation of direct communication is possible if and only if either the sender and receiver are directly connected or there exist two disjoint paths between the sender and the receiver.", "categories": "econ.th cs.gt", "created": "2020-07-01", "updated": "", "authors": ["marie laclau", "ludovic renou", "xavier venel"], "url": "https://arxiv.org/abs/2007.00457"}, {"title": "barriers to grid-connected battery systems: evidence from the spanish   electricity market", "id": "2007.00486", "abstract": "electrical energy storage is considered essential for the future energy system to solve the intermittency problems caused by renewable energy sources such as wind and solar power. among all the energy storage technologies, battery systems may provide flexibility in a more distributed and decentralized way. in countries with deregulated electricity markets, grid-connected battery systems should participate in the electric power system and interact with other market players. in this study, the market designs of both wholesale markets and ancillary services in spain are introduced, and the barriers to grid-connected battery storage are investigated under its specific market and regulatory framework. finally, the numerical and empirical analysis suggests that the high cycle cost for battery is still the main barrier for grid-connected battery systems, and the flexibility offered by such systems would be currently the most promising comparative advantage for this novel technology. additionally, a correct recognition of the barriers and advantages by all the stakeholders, including the system/market operator, policy maker, investor, and project manager, is the key factor to promote battery storage technologies in grid-connected applications. for market and system regulators, more efforts are necessary to reduce the barriers from regulatory framework, promote pilot projects, and design appropriate market products or services that adequately address the flexibility provided by different technologies.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-06-28", "updated": "", "authors": ["yu hu", "david soler soneira", "mar\u00eda jes\u00fas s\u00e1nchez"], "url": "https://arxiv.org/abs/2007.00486"}, {"title": "project selection with partially verifiable information", "id": "2007.00907", "abstract": "we consider a problem where the principal chooses a project from a set of available projects for the agent to work on. each project provides some profit to the principal and some payoff to the agent and these profits and payoffs are the agent's private information. the principal has a belief over these values and his problem is to find an incentive compatible mechanism without using transfers that maximizes expected profit. importantly, we assume partial verifiability so that the agent cannot report a project to be more profitable to the principal than it actually is. in this setup, we find a neat characterization of the set of incentive compatible mechanisms. using this characterization, we find the optimal mechanism for the principal when there are two projects. within a subclass of incentive compatible mechanisms, we show that a single cutoff mechanism is optimal and conjecture that it is the optimal incentive compatible mechanism.", "categories": "econ.th", "created": "2020-07-02", "updated": "", "authors": ["sumit goel", "wade hann-caruthers"], "url": "https://arxiv.org/abs/2007.00907"}, {"title": "emergency powers in response to covid-19: policy diffusion, democracy,   and preparedness", "id": "2007.00933", "abstract": "we examine covid-19-related states of emergency (soes) using data on 180 countries in the period january 1 through june 12, 2020. the results suggest that states' declaration of soes is driven by both external and internal factors. a permissive regional environment, characterized by many and simultaneously declared soes, may have diminished reputational and political costs, making employment of emergency powers more palatable for a wider range of governments. at the same time, internal characteristics, specifically democratic institutions and pandemic preparedness, shaped governments' decisions. weak democracies with poor pandemic preparedness were considerably more likely to opt for soes than dictatorships and robust democracies with higher preparedness. we find no significant association between pandemic impact, measured as national covid-19-related deaths, and soes, suggesting that many states adopted soes proactively before the disease spread locally.", "categories": "econ.gn q-fin.ec", "created": "2020-07-02", "updated": "", "authors": ["magnus lundgren", "mark klamberg", "karin sundstr\u00f6m", "julia dahlqvist"], "url": "https://arxiv.org/abs/2007.00933"}, {"title": "from fear to hate: how the covid-19 pandemic sparks racial animus in the   united states", "id": "2007.01448", "abstract": "we estimate the effect of the coronavirus (covid-19) pandemic on racial animus, as measured by google searches and twitter posts including a commonly used anti-asian racial slur. our empirical strategy exploits the plausibly exogenous variation in the timing of the first covid-19 diagnosis across regions in the united states. we find that the first local diagnosis leads to an immediate increase in racist google searches and twitter posts, with the latter mainly coming from existing twitter users posting the slur for the first time. this increase could indicate a rise in future hate crimes, as we document a strong correlation between the use of the slur and anti-asian hate crimes using historic data. moreover, we find that the rise in the animosity is directed at asians rather than other minority groups and is stronger on days when the connection between the disease and asians is more salient, as proxied by president trump's tweets mentioning china and covid-19 at the same time. in contrast, the negative economic impact of the pandemic plays little role in the initial increase in racial animus. our results suggest that de-emphasizing the connection between the disease and a particular racial group can be effective in curbing current and future racial animus.", "categories": "econ.gn cs.si q-fin.ec", "created": "2020-07-02", "updated": "", "authors": ["runjing lu", "yanying sheng"], "url": "https://arxiv.org/abs/2007.01448"}, {"title": "anonymous, non-manipulable, binary social choice", "id": "2007.01552", "abstract": "let v be a finite society whose members express weak orderings (hence also indifference, possibly) about two alternatives. we show a simple representation formula that is valid for all, and only, anonymous, non-manipulable, binary social choice functions on v . the number of such functions is $2^{n+1}$ if v contains $n$ agents.", "categories": "econ.th", "created": "2020-07-03", "updated": "", "authors": ["achille basile", "surekha rao", "k. p. s. bhaskara rao"], "url": "https://arxiv.org/abs/2007.01552"}, {"title": "learning utilities and equilibria in non-truthful auctions", "id": "2007.01722", "abstract": "in non-truthful auctions, agents' utility for a strategy depends on the strategies of the opponents and also the prior distribution over their private types; the set of bayes nash equilibria generally has an intricate dependence on the prior. using the first price auction as our main demonstrating example, we show that $\\tilde o(n / \\epsilon^2)$ samples from the prior with $n$ agents suffice for an algorithm to learn the interim utilities for all monotone bidding strategies. as a consequence, this number of samples suffice for learning all approximate equilibria. we give almost matching (up to polylog factors) lower bound on the sample complexity for learning utilities. we also consider settings where agents must pay a search cost to discover their own types. drawing on a connection between this setting and the first price auction, discovered recently by kleinberg et al. (2016), we show that $\\tilde o(n / \\epsilon^2)$ samples suffice for utilities and equilibria to be estimated in a near welfare-optimal descending auction in this setting. en route, we improve the sample complexity bound, recently obtained by guo et al. (2019), for the pandora's box problem, which is a classical model for sequential consumer search.", "categories": "cs.gt cs.lg econ.th", "created": "2020-07-03", "updated": "", "authors": ["hu fu", "tao lin"], "url": "https://arxiv.org/abs/2007.01722"}, {"title": "spatial iterated prisoner's dilemma as a transformation semigroup", "id": "2007.01896", "abstract": "the prisoner's dilemma (pd) is a game-theoretic model studied in a wide array of fields to understand the emergence of cooperation between rational self-interested agents. in this work, we formulate a spatial iterated pd as a discrete-event dynamical system where agents play the game in each time-step and analyse it algebraically using krohn-rhodes algebraic automata theory using a computational implementation of the holonomy decomposition of transformation semigroups. in each iteration all players adopt the most profitable strategy in their immediate neighbourhood. perturbations resetting the strategy of a given player provide additional generating events for the dynamics. our initial study shows that the algebraic structure, including how natural subsystems comprising permutation groups acting on the spatial distributions of strategies, arise in certain parameter regimes for the pay-off matrix, and are absent for other parameter regimes. differences in the number of group levels in the holonomy decomposition (an upper bound for krohn-rhodes complexity) are revealed as more pools of reversibility appear when the temptation to defect is at an intermediate level. algebraic structure uncovered by this analysis can be interpreted to shed light on the dynamics of the spatial iterated pd.", "categories": "math.ds econ.th math.gr physics.soc-ph", "created": "2020-07-03", "updated": "2020-07-27", "authors": ["isaiah farahbakhsh", "chrystopher l. nehaniv"], "url": "https://arxiv.org/abs/2007.01896"}, {"title": "binary relations in mathematical economics: on the continuity,   additivity and monotonicity postulates in eilenberg, villegas and degroot", "id": "2007.01952", "abstract": "this chapter examines how positivity and order play out in two important questions in mathematical economics, and in so doing, subjects the postulates of continuity, additivity and monotonicity to closer scrutiny. two sets of results are offered: the first departs from eilenberg's (1941) necessary and sufficient conditions on the topology under which an anti-symmetric, complete, transitive and continuous binary relation exists on a topologically connected space; and the second, from degroot's (1970) result concerning an additivity postulate that ensures a complete binary relation on a {\\sigma}-algebra to be transitive. these results are framed in the registers of order, topology, algebra and measure-theory; and also beyond mathematics in economics: the exploitation of villegas' notion of monotonic continuity by arrow-chichilnisky in the context of savage's theorem in decision theory, and the extension of diamond's impossibility result in social choice theory by basu-mitra. as such, this chapter has a synthetic and expository motivation, and can be read as a plea for inter-disciplinary conversations, connections and collaboration.", "categories": "econ.th math.gn", "created": "2020-07-03", "updated": "", "authors": ["m. ali khan", "metin uyanik"], "url": "https://arxiv.org/abs/2007.01952"}, {"title": "scenarios for a post-covid-19 world airline network", "id": "2007.02109", "abstract": "the airline industry was severely hit by the covid-19 crisis with an average demand decrease of about $64\\%$ (iata, april 2020) which triggered already several bankruptcies of airline companies all over the world. while the robustness of the world airline network (wan) was mostly studied as an homogeneous network, we introduce a new tool for analyzing the impact of a company failure: the `airline company network' where two airlines are connected if they share at least one route segment. using this tool, we observe that the failure of companies well connected with others has the largest impact on the connectivity of the wan. we then explore how the global demand reduction affects airlines differently, and provide an analysis of different scenarios if its stays low and does not come back to its pre-crisis level. using traffic data from the official aviation guide (oag) and simple assumptions about customer's airline choice strategies, we find that the local effective demand can be much lower than the average one, especially for companies that are not monopolistic and share their segments with larger companies. even if the average demand comes back to $60\\%$ of the total capacity, we find that between $46\\%$ and $59\\%$ of the companies could experience a reduction of more than $50\\%$ of their traffic, depending on the type of competitive advantage that drives customer's airline choice. these results highlight how the complex competitive structure of the wan weakens its robustness when facing such a large crisis.", "categories": "physics.soc-ph cond-mat.dis-nn econ.gn q-fin.ec", "created": "2020-07-04", "updated": "", "authors": ["jiachen ye", "peng ji", "marc barthelemy"], "url": "https://arxiv.org/abs/2007.02109"}, {"title": "off-policy exploitability-evaluation and equilibrium-learning in   two-player zero-sum markov games", "id": "2007.02141", "abstract": "off-policy evaluation (ope) is the problem of evaluating new policies using historical data obtained from a different policy. off-policy learning (opl), on the other hand, is the problem of finding an optimal policy using historical data. in recent ope and opl contexts, most of the studies have focused on one-player cases, and not on more than two-player cases. in this study, we propose methods for ope and opl in two-player zero-sum markov games. for ope, we estimate exploitability that is often used as a metric for determining how close a strategy profile is to a nash equilibrium in two-player zero-sum games. for opl, we calculate maximin policies as nash equilibrium strategies over the historical data. we prove the exploitability estimation error bounds for ope and regret bounds for opl based on the doubly robust and double reinforcement learning estimators. finally, we demonstrate the effectiveness and performance of the proposed methods through experiments.", "categories": "cs.lg cs.gt econ.em stat.ml", "created": "2020-07-04", "updated": "", "authors": ["kenshi abe", "yusuke kaneko"], "url": "https://arxiv.org/abs/2007.02141"}, {"title": "robust causal inference under covariate shift via worst-case   subpopulation treatment effects", "id": "2007.02411", "abstract": "we propose the worst-case treatment effect (wte) across all subpopulations of a given size, a conservative notion of topline treatment effect. compared to the average treatment effect (ate), whose validity relies on the covariate distribution of collected data, wte is robust to unanticipated covariate shifts, and positive findings guarantee uniformly valid treatment effects over subpopulations. we develop a semiparametrically efficient estimator for the wte, leveraging machine learning-based estimates of the heterogeneous treatment effect and propensity score. by virtue of satisfying a key (neyman) orthogonality property, our estimator enjoys central limit behavior---oracle rates with true nuisance parameters---even when estimates of nuisance parameters converge at slower rates. for both randomized trials and observational studies, we establish a semiparametric efficiency bound, proving that our estimator achieves the optimal asymptotic variance. on real datasets where robustness to covariate shift is of core concern, we illustrate the non-robustness of ate under even mild distributional shift, and demonstrate that the wte guards against brittle findings that are invalidated by unanticipated covariate shifts.", "categories": "stat.ml cs.lg econ.em", "created": "2020-07-05", "updated": "2020-07-20", "authors": ["sookyo jeong", "hongseok namkoong"], "url": "https://arxiv.org/abs/2007.02411"}, {"title": "forecasting with bayesian grouped random effects in panel data", "id": "2007.02435", "abstract": "in this paper, we estimate and leverage latent constant group structure to generate the point, set, and density forecasts for short dynamic panel data. we implement a nonparametric bayesian approach to simultaneously identify coefficients and group membership in the random effects which are heterogeneous across groups but fixed within a group. this method allows us to flexibly incorporate subjective prior knowledge on the group structure that potentially improves the predictive accuracy. in monte carlo experiments, we demonstrate that our bayesian grouped random effects (bgre) estimators produce accurate estimates and score predictive gains over standard panel data estimators. with a data-driven group structure, the bgre estimators exhibit comparable accuracy of clustering with the kmeans algorithm and outperform a two-step bayesian grouped estimator whose group structure relies on kmeans. in the empirical analysis, we apply our method to forecast the investment rate across a broad range of firms and illustrate that the estimated latent group structure improves forecasts relative to standard panel data estimators.", "categories": "econ.em", "created": "2020-07-05", "updated": "2020-10-04", "authors": ["boyuan zhang"], "url": "https://arxiv.org/abs/2007.02435"}, {"title": "spectral targeting estimation of $\\lambda$-garch models", "id": "2007.02588", "abstract": "this paper presents a novel estimator of orthogonal garch models, which combines (eigenvalue and -vector) targeting estimation with stepwise (univariate) estimation. we denote this the spectral targeting estimator. this two-step estimator is consistent under finite second order moments, while asymptotic normality holds under finite fourth order moments. the estimator is especially well suited for modelling larger portfolios: we compare the empirical performance of the spectral targeting estimator to that of the quasi maximum likelihood estimator for five portfolios of 25 assets. the spectral targeting estimator dominates in terms of computational complexity, being up to 57 times faster in estimation, while both estimators produce similar out-of-sample forecasts, indicating that the spectral targeting estimator is well suited for high-dimensional empirical applications.", "categories": "econ.em", "created": "2020-07-06", "updated": "", "authors": ["simon hetland"], "url": "https://arxiv.org/abs/2007.02588"}, {"title": "teacher-to-classroom assignment and student achievement", "id": "2007.02653", "abstract": "we study the effects of counterfactual teacher-to-classroom assignments on average student achievement in elementary and middle schools in the us. we use the measures of effective teaching (met) experiment to semiparametrically identify the average reallocation effects (ares) of such assignments. our findings suggest that changes in within-district teacher assignments could have appreciable effects on student achievement. unlike policies which require hiring additional teachers (e.g., class-size reduction measures), or those aimed at changing the stock of teachers (e.g., vam-guided teacher tenure policies), alternative teacher-to-classroom assignments are resource neutral; they raise student achievement through a more efficient deployment of existing teachers.", "categories": "econ.em", "created": "2020-07-06", "updated": "2020-09-01", "authors": ["bryan s. graham", "geert ridder", "petra thiemann", "gema zamarro"], "url": "https://arxiv.org/abs/2007.02653"}, {"title": "bridging the covid-19 data and the epidemiological model using time   varying parameter sird model", "id": "2007.02726", "abstract": "this paper extends the canonical model of epidemiology, sird model, to allow for time varying parameters for real-time measurement of the stance of the covid-19 pandemic. time variation in model parameters is captured using the generalized autoregressive score modelling structure designed for the typically daily count data related to pandemic. the resulting specification permits a flexible yet parsimonious model structure with a very low computational cost. this is especially crucial at the onset of the pandemic when the data is scarce and the uncertainty is abundant. full sample results show that countries including us, brazil and russia are still not able to contain the pandemic with the us having the worst performance. furthermore, iran and south korea are likely to experience the second wave of the pandemic. a real-time exercise show that the proposed structure delivers timely and precise information on the current stance of the pandemic ahead of the competitors that use rolling window. this, in turn, transforms into accurate short-term predictions of the active cases. we further modify the model to allow for unreported cases. results suggest that the effects of the presence of these cases on the estimation results diminish towards the end of sample with the increasing number of testing.", "categories": "q-bio.pe econ.em stat.ap", "created": "2020-07-03", "updated": "", "authors": ["cem cakmakli", "yasin simsek"], "url": "https://arxiv.org/abs/2007.02726"}, {"title": "semi-nonparametric latent class choice model with a flexible class   membership component: a mixture model approach", "id": "2007.02739", "abstract": "this study presents a semi-nonparametric latent class choice model (lccm) with a flexible class membership component. the proposed model formulates the latent classes using mixture models as an alternative approach to the traditional random utility specification with the aim of comparing the two approaches on various measures including prediction accuracy and representation of heterogeneity in the choice process. mixture models are parametric model-based clustering techniques that have been widely used in areas such as machine learning, data mining and patter recognition for clustering and classification problems. an expectation-maximization (em) algorithm is derived for the estimation of the proposed model. using two different case studies on travel mode choice behavior, the proposed model is compared to traditional discrete choice models on the basis of parameter estimates' signs, value of time, statistical goodness-of-fit measures, and cross-validation tests. results show that mixture models improve the overall performance of latent class choice models by providing better out-of-sample prediction accuracy in addition to better representations of heterogeneity without weakening the behavioral and economic interpretability of the choice models.", "categories": "econ.em cs.lg stat.me", "created": "2020-07-06", "updated": "", "authors": ["georges sfeir", "maya abou-zeid", "filipe rodrigues", "francisco camara pereira", "isam kaysi"], "url": "https://arxiv.org/abs/2007.02739"}, {"title": "dynamic awareness", "id": "2007.02823", "abstract": "we investigate how to model the beliefs of an agent who becomes more aware. we use the framework of halpern and rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $\\phi$ in state $s$ of a model $m$, she transitions to state $s^*$ in a model $m^*$. we then discuss how such a model can be applied to information disclosure.", "categories": "cs.ai cs.lo econ.th", "created": "2020-07-06", "updated": "", "authors": ["joseph y. halpern", "evan piermont"], "url": "https://arxiv.org/abs/2007.02823"}, {"title": "the effects of taxes on wealth inequality in artificial chemistry models   of economic activity", "id": "2007.02934", "abstract": "we consider a number of artificial chemistry models for economic activity and what consequences they have for the formation of economic inequality. we are particularly interested in what tax measures are effective in dampening economic inequality. by starting from well-known kinetic exchange models, we examine different scenarios for reducing the tendency of economic activity models to form unequal wealth distribution in equilibrium.", "categories": "q-fin.gn cs.ma econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-03", "updated": "", "authors": ["wolfgang banzhaf"], "url": "https://arxiv.org/abs/2007.02934"}, {"title": "the home office in times of covid-19 pandemic and its impact in the   labor supply", "id": "2007.02935", "abstract": "we lightly modify eriksson's (1996) model to accommodate the home office in a simple model of endogenous growth. by home office we mean any working activity carried out away from the workplace which is assumed to be fixed. due to the strong mobility restrictions imposed on citizens during the covid-19 pandemic, we allow the home office to be located at home. at the home office, however, in consequence of the fear and anxiety workers feel because of covid-19, they become distracted and spend less time working. we show that in the long run, the intertemporal elasticity of substitution of the home-office labor is sufficiently small only if the intertemporal elasticity of substitution of the time spent on distracting activities is small enough also.", "categories": "econ.gn q-fin.ec", "created": "2020-07-04", "updated": "2020-07-08", "authors": ["jos\u00e9 nilmar alves de oliveira", "jaime orrillo", "franklin gamboa"], "url": "https://arxiv.org/abs/2007.02935"}, {"title": "real-time estimation of the short-run impact of covid-19 on economic   activity using electricity market data", "id": "2007.03477", "abstract": "the covid-19 pandemic has caused more than 8 million confirmed cases and 500,000 death to date. in response to this emergency, many countries have introduced a series of social-distancing measures including lockdowns and businesses' temporary shutdowns, in an attempt to curb the spread of the infection. accordingly, the pandemic has been generating unprecedent disruption on practically every aspect of society. this paper demonstrates that high-frequency electricity market data can be used to estimate the causal, short-run impact of covid-19 on the economy. in the current uncertain economic conditions, timeliness is essential. unlike official statistics, which are published with a delay of a few months, with our approach one can monitor virtually every day the impact of the containment policies, the extent of the recession and measure whether the monetary and fiscal stimuli introduced to address the crisis are being effective. we illustrate our methodology on daily data for the italian day-ahead power market. not surprisingly, we find that the containment measures caused a significant reduction in economic activities and that the gdp at the end of in may 2020 is still about 11% lower that what it would have been without the outbreak.", "categories": "econ.gn q-fin.ec", "created": "2020-07-07", "updated": "", "authors": ["carlo fezzi", "valeria fanghella"], "url": "https://arxiv.org/abs/2007.03477"}, {"title": "inverse reinforcement learning for sequential hypothesis testing and   search", "id": "2007.03481", "abstract": "this paper considers a novel formulation of inverse reinforcement learning~(irl) with behavioral economics constraints to address inverse sequential hypothesis testing (sht) and inverse search in bayesian agents. we first estimate the stopping and search costs by observing the actions of these agents using bayesian revealed preference from microeconomics and rational inattention from behavioral economics. we also solve the inverse problem of the more general rationally inattentive sht where the agent incorporates controlled sensing by optimally choosing from various sensing modes. second, we design statistical hypothesis tests with bounded type-i and type-ii error probabilities to detect if the agents are bayesian utility maximizers when their actions are measured in noise. by dynamically tuning the prior specified to the agents, we formulate an {\\em active} irl framework which enhances these detection tests and minimizes their type-ii and type-i error probabilities of utility maximization detection. finally, we give a finite sample complexity result which provides finite sample bounds on the error probabilities of the detection tests.", "categories": "cs.lg cs.sy econ.th eess.sy stat.ml", "created": "2020-07-07", "updated": "", "authors": ["kunal pattanayak", "vikram krishnamurthy"], "url": "https://arxiv.org/abs/2007.03481"}, {"title": "the cathedral and the starship: learning from the middle ages for future   long-duration projects", "id": "2007.03654", "abstract": "a popular analogue used in the space domain is that of historical building projects, notably cathedrals that took decades and in some cases centuries to complete. cathedrals are often taken as archetypes for long-term projects. in this article, i will explore the cathedral from the point of view of project management and systems architecting and draw implications for long-term projects in the space domain, notably developing a starship. i will show that the popular image of a cathedral as a continuous long-term project is in contradiction to the current state of research. more specifically, i will show that for the following propositions: the cathedrals were built based on an initial detailed master plan; building was a continuous process that adhered to the master plan; investments were continuously provided for the building process. although initial plans might have existed, the construction process took often place in multiple campaigns, sometimes separated by decades. such interruptions made knowledge-preservation very challenging. the reason for the long stretches of inactivity was mostly due to a lack of funding. hence, the availability of funding coincided with construction activity. these findings paint a much more relevant picture of cathedral building for long-duration projects today: how can a project be completed despite a range of uncertainties regarding loss in skills, shortage in funding, and interruptions? it is concluded that long-term projects such as an interstellar exploration program can take inspiration from cathedrals by developing a modular architecture, allowing for extensibility and flexibility, thinking about value delivery at an early point, and establishing mechanisms and an organization for stable funding.", "categories": "econ.gn physics.pop-ph physics.space-ph q-fin.ec", "created": "2020-07-07", "updated": "", "authors": ["andreas m. hein"], "url": "https://arxiv.org/abs/2007.03654"}, {"title": "a dynamic choice model with heterogeneous decision rules: application in   estimating the user cost of rail crowding", "id": "2007.03682", "abstract": "crowding valuation of subway riders is an important input to various supply-side decisions of transit operators. the crowding cost perceived by a transit rider is generally estimated by capturing the trade-off that the rider makes between crowding and travel time while choosing a route. however, existing studies rely on static compensatory choice models and fail to account for inertia and the learning behaviour of riders. to address these challenges, we propose a new dynamic latent class model (dlcm) which (i) assigns riders to latent compensatory and inertia/habit classes based on different decision rules, (ii) enables transitions between these classes over time, and (iii) adopts instance-based learning theory to account for the learning behaviour of riders. we use the expectation-maximisation algorithm to estimate dlcm, and the most probable sequence of latent classes for each rider is retrieved using the viterbi algorithm. the proposed dlcm can be applied in any choice context to capture the dynamics of decision rules used by a decision-maker. we demonstrate its practical advantages in estimating the crowding valuation of an asian metro's riders. to calibrate the model, we recover the daily route preferences and in-vehicle crowding experiences of regular metro riders using a two-month-long smart card and vehicle location data. the results indicate that the average rider follows the compensatory rule on only 25.5% of route choice occasions. dlcm estimates also show an increase of 47% in metro riders' valuation of travel time under extremely crowded conditions relative to that under uncrowded conditions.", "categories": "stat.ap econ.em", "created": "2020-07-07", "updated": "", "authors": ["prateek bansal", "daniel h\u00f6rcher", "daniel j. graham"], "url": "https://arxiv.org/abs/2007.03682"}, {"title": "stability in repeated matching markets", "id": "2007.03794", "abstract": "this paper develops a framework for repeated matching markets. the model departs from the gale-shapley matching model by having a fixed set of long-lived hospitals match with a new generation of short-lived residents in every period. i show that there are two kinds of hospitals in this repeated environment: some hospitals can be motivated dynamically to voluntarily reduce their hiring capacity, potentially making more residents available to rural hospitals; the others, however, are untouchable even with repeated interaction and must obtain the same match as they do in a static matching. in large matching markets with correlated preferences, at most a vanishingly small fraction of the hospitals are untouchable. the vast majority of hospitals can be motivated using dynamic incentives.", "categories": "econ.th cs.gt", "created": "2020-07-07", "updated": "", "authors": ["ce liu"], "url": "https://arxiv.org/abs/2007.03794"}, {"title": "max-sum tests for cross-sectional dependence of high-demensional panel   data", "id": "2007.03911", "abstract": "we consider a testing problem for cross-sectional dependence for high-dimensional panel data, where the number of cross-sectional units is potentially much larger than the number of observations. the cross-sectional dependence is described through a linear regression model. we study three tests named the sum test, the max test and the max-sum test, where the latter two are new. the sum test is initially proposed by breusch and pagan (1980). we design the max and sum tests for sparse and non-sparse residuals in the linear regressions, respectively.and the max-sum test is devised to compromise both situations on the residuals. indeed, our simulation shows that the max-sum test outperforms the previous two tests. this makes the max-sum test very useful in practice where sparsity or not for a set of data is usually vague. towards the theoretical analysis of the three tests, we have settled two conjectures regarding the sum of squares of sample correlation coefficients asked by pesaran (2004 and 2008). in addition, we establish the asymptotic theory for maxima of sample correlations coefficients appeared in the linear regression model for panel data, which is also the first successful attempt to our knowledge. to study the max-sum test, we create a novel method to show asymptotic independence between maxima and sums of dependent random variables. we expect the method itself is useful for other problems of this nature. finally, an extensive simulation study as well as a case study are carried out. they demonstrate advantages of our proposed methods in terms of both empirical powers and robustness for residuals regardless of sparsity or not.", "categories": "math.st econ.em stat.th", "created": "2020-07-08", "updated": "", "authors": ["long feng", "tiefeng jiang", "binghui liu", "wei xiong"], "url": "https://arxiv.org/abs/2007.03911"}, {"title": "network effects and the appointment of female board members in japan", "id": "2007.03980", "abstract": "we investigate the dynamics in the networks of japanese corporates and its interplay with the appointment of female board members. we find that firms with female board members show homophily with respect to gender and often have above average profitability. we also find that new appointments of women are more likely at boards which observe female board members at other firms to which they are tied by either ownership relations or corporate board interlocks.", "categories": "econ.gn q-fin.ec q-fin.gn q-fin.st", "created": "2020-07-08", "updated": "", "authors": ["matthias raddant", "hiroshi takahashi"], "url": "https://arxiv.org/abs/2007.03980"}, {"title": "optimal decision rules for weak gmm", "id": "2007.04050", "abstract": "this paper derives the limit experiment for nonlinear gmm models with weak and partial identification. we propose a theoretically-motivated class of default priors on a non-parametric nuisance parameter. these priors imply computationally tractable bayes decision rules in the limit problem, while leaving the prior on the structural parameter free to be selected by the researcher. we further obtain quasi-bayes decision rules as the limit of sequences in this class, and derive weighted average power-optimal identification-robust frequentist tests. finally, we prove a bernstein-von mises-type result for the quasi-bayes posterior under weak and partial identification.", "categories": "econ.em", "created": "2020-07-08", "updated": "2020-07-18", "authors": ["isaiah andrews", "anna mikusheva"], "url": "https://arxiv.org/abs/2007.04050"}, {"title": "talents from abroad. foreign managers and productivity in the united   kingdom", "id": "2007.04055", "abstract": "in this paper, we test the contribution of foreign management on firms' competitiveness. we use a novel dataset on the careers of 165,084 managers employed by 13,106 companies in the united kingdom in the period 2009-2017. we find that domestic manufacturing firms become, on average, between 7% and 12% more productive after hiring the first foreign managers, whereas foreign-owned firms register no significant improvement. in particular, we test that previous industry-specific experience is the primary driver of productivity gains in domestic firms (15.6%), in a way that allows the latter to catch up with foreign-owned firms. managers from the european union are highly valuable, as they represent about half of the recruits in our data. our identification strategy combines matching techniques, difference-in-difference, and pre-recruitment trends to challenge reverse causality. results are robust to placebo tests and to different estimators of total factor productivity. eventually, we argue that upcoming limits to the mobility of foreign talents after the brexit event can hamper the allocation of productive managerial resources.", "categories": "econ.em", "created": "2020-07-08", "updated": "", "authors": ["dimitrios exadaktylos", "massimo riccaboni", "armando rungi"], "url": "https://arxiv.org/abs/2007.04055"}, {"title": "difference-in-differences estimators of intertemporal treatment effects", "id": "2007.04267", "abstract": "we consider the estimation of the effect of a policy or treatment, using panel data where different groups of units are exposed to the treatment at different times. we focus on parameters aggregating instantaneous and dynamic treatment effects, with a clear welfare interpretation. we show that under parallel trends conditions, these parameters can be unbiasedly estimated by a weighted average of differences-in-differences, provided that at least one group is always untreated, and another group is always treated. our estimators are valid if the treatment effect is heterogeneous, contrary to the commonly-used event-study regression.", "categories": "econ.em", "created": "2020-07-08", "updated": "2020-08-11", "authors": ["cl\u00e9ment de chaisemartin", "xavier d'haultf\u0153uille"], "url": "https://arxiv.org/abs/2007.04267"}, {"title": "efficient covariate balancing for the local average treatment effect", "id": "2007.04346", "abstract": "this paper develops an empirical balancing approach for the estimation of treatment effects under two-sided noncompliance using a binary conditionally independent instrumental variable. the method weighs both treatment and outcome information with inverse probabilities to produce exact finite sample balance across instrument level groups. it is free of functional form assumptions on the outcome or the treatment selection step. by tailoring the loss function for the instrument propensity scores, the resulting treatment effect estimates exhibit both low bias and a reduced variance in finite samples compared to conventional inverse probability weighting methods. the estimator is automatically weight normalized and has similar bias properties compared to conventional two-stage least squares estimation under constant causal effects for the compliers. we provide conditions for asymptotic normality and semiparametric efficiency and demonstrate how to utilize additional information about the treatment selection step for bias reduction in finite samples. the method can be easily combined with regularization or other statistical learning approaches to deal with a high-dimensional number of observed confounding variables. monte carlo simulations suggest that the theoretical advantages translate well to finite samples. the method is illustrated in an empirical example.", "categories": "econ.em", "created": "2020-07-08", "updated": "", "authors": ["phillip heiler"], "url": "https://arxiv.org/abs/2007.04346"}, {"title": "nice guys don't always finish last: succeeding in hierarchical   organizations", "id": "2007.04435", "abstract": "what are the chances of an ethical individual rising through the ranks of a political party or a corporation in the presence of unethical peers? to answer this question, i consider a four-player two-stage elimination tournament, in which players are partitioned into those willing to be involved in sabotage behavior and those who are not. i show that, under certain conditions, the latter are more likely to win the tournament.", "categories": "econ.gn q-fin.ec", "created": "2020-07-07", "updated": "2020-09-07", "authors": ["doron klunover"], "url": "https://arxiv.org/abs/2007.04435"}, {"title": "time series analysis of covid-19 infection curve: a change-point   perspective", "id": "2007.04553", "abstract": "in this paper, we model the trajectory of the cumulative confirmed cases and deaths of covid-19 (in log scale) via a piecewise linear trend model. the model naturally captures the phase transitions of the epidemic growth rate via change-points and further enjoys great interpretability due to its semiparametric nature. on the methodological front, we advance the nascent self-normalization (sn) technique (shao, 2010) to testing and estimation of a single change-point in the linear trend of a nonstationary time series. we further combine the sn-based change-point test with the not algorithm (baranowski et al., 2019) to achieve multiple change-point estimation. using the proposed method, we analyze the trajectory of the cumulative covid-19 cases and deaths for 30 major countries and discover interesting patterns with potentially relevant implications for effectiveness of the pandemic responses by different countries. furthermore, based on the change-point detection algorithm and a flexible extrapolation function, we design a simple two-stage forecasting scheme for covid-19 and demonstrate its promising performance in predicting cumulative deaths in the u.s.", "categories": "econ.em physics.soc-ph stat.ap", "created": "2020-07-09", "updated": "", "authors": ["feiyu jiang", "zifeng zhao", "xiaofeng shao"], "url": "https://arxiv.org/abs/2007.04553"}, {"title": "line-up elections: parallel voting with shared candidate pool", "id": "2007.04960", "abstract": "we introduce the model of line-up elections which captures parallel or sequential single-winner elections with a shared candidate pool. the goal of a line-up election is to find a high-quality assignment of a set of candidates to a set of positions such that each position is filled by exactly one candidate and each candidate fills at most one position. a score for each candidate-position pair is given as part of the input, which expresses the qualification of the candidate to fill the position. we propose several voting rules for line-up elections and analyze them from an axiomatic and an empirical perspective using real-world data from the popular video game fifa.", "categories": "cs.gt econ.th", "created": "2020-07-09", "updated": "", "authors": ["niclas boehmer", "robert bredereck", "piotr faliszewski", "andrzej kaczmarczyk", "rolf niedermeier"], "url": "https://arxiv.org/abs/2007.04960"}, {"title": "an\\'alise dos fatores determinantes para o decreto de pris\\~ao   preventiva em casos envolvendo acusa\\c{c}\\~oes por roubo, tr\\'afico de drogas   e furto: um estudo no \\^ambito das cidades mais populosas do paran\\'a", "id": "2007.04962", "abstract": "based on the theoretical assumptions of the labeling approach, critical criminology and behavioral economics, taking into account that almost half of the brazilian prison population is composed of individuals who are serving pre-trial detention, it was sought to assess the characteristics of the flagranteated were presented as determinants to influence the subjectivity of the judges in the decision to determine, or not, the custody of these. the research initially adopted a deductive methodology, based on the principle that external objective factors encourage magistrates to decide in a certain sense. it was then focused on the identification of which characteristics of the flagranteated and allegedly committed crimes would be relevant to guide such decisions. subsequently, after deduction of such factors, an inductive methodology was adopted, analyzing the data from the theoretical assumptions pointed out. during the research, 277 decisions were analyzed, considering decision as individual decision and not custody hearing. this sample embarked decisions of six judges among the largest cities of the state of paran\\'a and concerning the crimes of theft, robbery and drug trafficking. it was then concluded that the age, gender, social class and type of accusation that the flagranteated suffered are decisive for the decree of his provisional arrest, being that, depending on the judge competent in the case, the chances of the decree can increase in up to 700%, taking into account that the circumstantial and causal variables are constant. given the small sample size, as far as the number of judges is concerned, more extensive research is needed so that conclusions of national validity can be developed.", "categories": "econ.gn q-fin.ec", "created": "2020-07-09", "updated": "", "authors": ["giovane cerezuela policeno", "mario edson passerino fischer da silva", "vitor pestana ostrensky"], "url": "https://arxiv.org/abs/2007.04962"}, {"title": "intelligent credit limit management in consumer loans based on causal   inference", "id": "2007.05188", "abstract": "nowadays consumer loan plays an important role in promoting the economic growth, and credit cards are the most popular consumer loan. one of the most essential parts in credit cards is the credit limit management. traditionally, credit limits are adjusted based on limited heuristic strategies, which are developed by experienced professionals. in this paper, we present a data-driven approach to manage the credit limit intelligently. firstly, a conditional independence testing is conducted to acquire the data for building models. based on these testing data, a response model is then built to measure the heterogeneous treatment effect of increasing credit limits (i.e. treatments) for different customers, who are depicted by several control variables (i.e. features). in order to incorporate the diminishing marginal effect, a carefully selected log transformation is introduced to the treatment variable. moreover, the model's capability can be further enhanced by applying a non-linear transformation on features via gbdt encoding. finally, a well-designed metric is proposed to properly measure the performances of compared methods. the experimental results demonstrate the effectiveness of the proposed approach.", "categories": "cs.lg cs.ce econ.em stat.ml", "created": "2020-07-10", "updated": "", "authors": ["hang miao", "kui zhao", "zhun wang", "linbo jiang", "quanhui jia", "yanming fang", "quan yu"], "url": "https://arxiv.org/abs/2007.05188"}, {"title": "a semiparametric network formation model with unobserved linear   heterogeneity", "id": "2007.05403", "abstract": "this paper analyzes a semiparametric model of network formation in the presence of unobserved agent-specific heterogeneity. the objective is to identify and estimate the preference parameters associated with homophily on observed attributes when the distributions of the unobserved factors are not parametrically specified. this paper offers two main contributions to the literature on network formation. first, it establishes a new point identification result for the vector of parameters that relies on the existence of a special repressor. the identification proof is constructive and characterizes a closed-form for the parameter of interest. second, it introduces a simple two-step semiparametric estimator for the vector of parameters with a first-step kernel estimator. the estimator is computationally tractable and can be applied to both dense and sparse networks. moreover, i show that the estimator is consistent and has a limiting normal distribution as the number of individuals in the network increases. monte carlo experiments demonstrate that the estimator performs well in finite samples and in networks with different levels of sparsity.", "categories": "econ.em", "created": "2020-07-10", "updated": "2020-08-30", "authors": ["luis e. candelaria"], "url": "https://arxiv.org/abs/2007.05403"}, {"title": "is there a golden parachute in sannikov's principal-agent problem?", "id": "2007.05529", "abstract": "this paper provides a complete review of the continuous-time optimal contracting problem introduced by sannikov, in the extended context allowing for possibly different discount rates of both parties. the agent's problem is to seek for optimal effort, given the compensation scheme proposed by the principal over a random horizon. then, given the optimal agent's response, the principal determines the best compensation scheme in terms of running payment, retirement, and lump-sum payment at retirement. a golden parachute is a situation where the agent ceases any effort at some positive stopping time, and receives a payment afterwards, possibly under the form of a lump-sum payment, or of a continuous stream of payments. we show that a golden parachute only exists in certain specific circumstances. this is in contrast with the results claimed by sannikov, where the only requirement is a positive agent's marginal cost of effort at zero. namely, we show that there is no golden parachute if this parameter is too large. similarly, in the context of a concave marginal utility, there is no golden parachute if the agent's utility function has a too negative curvature at zero. in the general case, we provide a rigorous analysis of this problem, and we prove that an agent with positive reservation utility is either never retired by the principal, or retired above some given threshold (as in sannikov's solution). in particular, different discount factors induce naturally a face-lifted utility function, which allows to reduce the whole analysis to a setting similar to the equal-discount rates one. finally, we also confirm that an agent with small reservation utility does have an informational rent, meaning that the principal optimally offers him a contract with strictly higher utility value.", "categories": "econ.th math.oc math.pr", "created": "2020-07-10", "updated": "", "authors": ["dylan possama\u00ef", "nizar touzi"], "url": "https://arxiv.org/abs/2007.05529"}, {"title": "pension benefits, early retirement and human capital depreciation in   late adulthood", "id": "2007.05884", "abstract": "historically, economists have mainly focused on human capital accumulation and considerably less so on the causes and consequences of human capital depreciation in late adulthood. studying human capital depreciation over the life cycle has powerful economic consequences for decision-making in old age. using data from the introduction of a retirement program in china, we examine how the introduction of a retirement program influences individual cognition. we find large negative effects of pension benefits on cognitive functioning among the elderly. we detect the most substantial impact of the program to be on delayed recall, which is a significant predictor of the onset of dementia. we show suggestive evidence that the program leads to larger negative impacts among women. we show that retirement plays a significant role in explaining cognitive decline at older ages.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2020-07-11", "updated": "2020-08-19", "authors": ["plamen nikolov", "alan adelman"], "url": "https://arxiv.org/abs/2007.05884"}, {"title": "contracting over persistent information", "id": "2007.05983", "abstract": "we consider a dynamic moral hazard problem between a principal and an agent, where the sole instrument the principal has to incentivize the agent is the disclosure of information. the principal aims at maximizing the (discounted) number of times the agent chooses a particular action, e.g., to work hard. we show that there exists an optimal contract, where the principal stops disclosing information as soon as its most preferred action is a static best reply for the agent or else continues disclosing information until the agent perfectly learns the principal's private information. if the agent perfectly learns the state, he learns it in finite time with probability one; the more patient the agent, the later he learns it.", "categories": "econ.th", "created": "2020-07-12", "updated": "", "authors": ["wei zhao", "claudio mezzetti", "ludovic renou", "tristan tomala"], "url": "https://arxiv.org/abs/2007.05983"}, {"title": "an adversarial approach to structural estimation", "id": "2007.06169", "abstract": "we propose a new simulation-based estimation method, adversarial estimation, for structural models. the estimator is formulated as the solution to a minimax problem between a generator (which generates synthetic observations using the structural model) and a discriminator (which classifies if an observation is synthetic). the discriminator maximizes the accuracy of its classification while the generator minimizes it. we show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. we advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence. we apply our method to the elderly's saving decision model and show that including gender and health profiles in the discriminator uncovers the bequest motive as an important source of saving across the wealth distribution, not only for the rich.", "categories": "econ.em cs.lg math.st stat.me stat.ml stat.th", "created": "2020-07-12", "updated": "", "authors": ["tetsuya kaji", "elena manresa", "guillaume pouliot"], "url": "https://arxiv.org/abs/2007.06169"}, {"title": "equilibrium refinement in finite evidence games", "id": "2007.06403", "abstract": "evidence games study situations where a sender persuades a receiver by selectively disclosing hard evidence about an unknown state of the world. evidence games often have multiple equilibria. hart et al. (2017) propose to focus on truth-leaning equilibria, i.e., perfect bayesian equilibria where the sender prefers disclosing truthfully when indifferent, and the receiver takes off-path disclosure at face value. they show that a truth-leaning equilibrium is an equilibrium of a perturbed game where the sender has an infinitesimal reward for truth-telling. we show that, when the receiver's action space is finite, truth-leaning equilibrium may fail to exist, and it is not equivalent to equilibrium of the perturbed game. to restore existence, we introduce a disturbed game with a small uncertainty about the receiver's payoff. a purifiable equilibrium is a truth-leaning equilibrium in an infinitesimally disturbed game. it exists and features a simple characterization. a truth-leaning equilibrium that is also purifiable is an equilibrium of the perturbed game.", "categories": "econ.th", "created": "2020-07-13", "updated": "", "authors": ["shaofei jiang"], "url": "https://arxiv.org/abs/2007.06403"}, {"title": "incentivizing narrow-spectrum antibiotic development with refunding", "id": "2007.06468", "abstract": "the rapid rise of antibiotic resistance is a serious threat to global public health. without further incentives, pharmaceutical companies have little interest in developing antibiotics, since the success probability is low and development costs are huge. the situation is exacerbated by the \"antibiotics dilemma\": developing narrow-spectrum antibiotics against resistant bacteria is most beneficial for society, but least attractive for companies since their usage is more limited than for broad-spectrum drugs and thus sales are low. starting from a general mathematical framework for the study of antibiotic-resistance dynamics with an arbitrary number of antibiotics, we identify efficient treatment protocols and introduce a market-based refunding scheme that incentivizes pharmaceutical companies to develop narrow-spectrum antibiotics: successful companies can claim a refund from a newly established antibiotics fund that partially covers their development costs. the proposed refund involves a fixed and variable part. the latter (i) increases with the use of the new antibiotic for currently resistant strains in comparison with other newly developed antibiotics for this purpose---the resistance premium---and (ii) decreases with the use of this antibiotic for non-resistant bacteria. we outline how such a refunding scheme can solve the antibiotics dilemma and cope with various sources of uncertainty inherent in antibiotic r\\&d. finally, connecting our refunding approach to the recently established antimicrobial resistance (amr) action fund, we discuss how the antibiotics fund can be financed.", "categories": "q-bio.pe econ.gn q-fin.ec", "created": "2020-07-13", "updated": "", "authors": ["lucas b\u00f6ttcher", "hans gersbach"], "url": "https://arxiv.org/abs/2007.06468"}, {"title": "the new digital platforms: merger control in pakistan", "id": "2007.06535", "abstract": "the pakistan competition policy, as in many other countries, was originally designed to regulate business conduct in traditional markets and for tangible goods and services. however, the development and proliferation of the internet has led to the emergence of digital companies which have disrupted many sectors of the economy. these platforms provide digital infrastructure for a range of services including search engines, marketplaces, and social networking sites. the digital economy poses a myriad of challenges for competition authorities worldwide, especially with regard to digital mergers and acquisitions (m&as). while some jurisdictions such as the european union and the united states have taken significant strides in regulating technological m&as, there is an increasing need for developing countries such as pakistan to rethink their competition policy tools. this paper investigates whether merger reviews in the pakistan digital market are informed by the same explanatory variables as in the traditional market, by performing an empirical comparative analysis of the competition commission of pakistan's (ccp's) m&a decisions between 2014 and 2019. the findings indicate the ccp applies the same decision factors in reviewing both traditional and digital m&as. as such, this paper establishes a basis for igniting the policy and economic debate of regulating the digital platform industry in pakistan.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-06-22", "updated": "", "authors": ["shahzada aamir mushtaq", "wang yuhui"], "url": "https://arxiv.org/abs/2007.06535"}, {"title": "do online courses provide an equal educational value compared to   in-person classroom teaching? evidence from us survey data using quantile   regression", "id": "2007.06994", "abstract": "education has traditionally been classroom-oriented with a gradual growth of online courses in recent times. however, the outbreak of the covid-19 pandemic has dramatically accelerated the shift to online classes. associated with this learning format is the question: what do people think about the educational value of an online course compared to a course taken in-person in a classroom? this paper addresses the question and presents a bayesian quantile analysis of public opinion using a nationally representative survey data from the united states. our findings show that previous participation in online courses and full-time employment status favor the educational value of online courses. we also find that the older demographic and females have a greater propensity for online education. in contrast, highly educated individuals have a lower willingness towards online education vis-\\`a-vis traditional classes. besides, covariate effects show heterogeneity across quantiles which cannot be captured using probit or logit models.", "categories": "econ.gn q-fin.ec", "created": "2020-07-14", "updated": "", "authors": ["manini ojha", "mohammad arshad rahman"], "url": "https://arxiv.org/abs/2007.06994"}, {"title": "polarization in networks: identification-alienation framework", "id": "2007.07061", "abstract": "we introduce a model of polarization in networks as a unifying framework for the measurement of polarization that covers a wide range of applications. we consider a sufficiently general setup for this purpose: node- and edge-weighted, undirected, and connected networks. we generalize the axiomatic characterization of esteban and ray (1994) and show that only a particular instance within this class can be used justifiably to measure polarization in networks.", "categories": "econ.th cs.si", "created": "2020-07-14", "updated": "2020-09-25", "authors": ["kenan huremovic", "ali ozkes"], "url": "https://arxiv.org/abs/2007.07061"}, {"title": "a more robust t-test", "id": "2007.07065", "abstract": "standard inference about a scalar parameter estimated via gmm amounts to applying a t-test to a particular set of observations. if the number of observations is not very large, then moderately heavy tails can lead to poor behavior of the t-test. this is a particular problem under clustering, since the number of observations then corresponds to the number of clusters, and heterogeneity in cluster sizes induces a form of heavy tails. this paper combines extreme value theory for the smallest and largest observations with a normal approximation for the average of the remaining observations to construct a more robust alternative to the t-test. the new test is found to control size much more successfully in small samples compared to existing methods. analytical results in the canonical inference for the mean problem demonstrate that the new test provides a refinement over the full sample t-test under more than two but less than three moments, while the bootstrapped t-test does not.", "categories": "econ.em math.st stat.th", "created": "2020-07-14", "updated": "", "authors": ["ulrich k. mueller"], "url": "https://arxiv.org/abs/2007.07065"}, {"title": "time-equitable dynamic tolling scheme for single bottlenecks", "id": "2007.07091", "abstract": "dynamic tolls present an opportunity for municipalities to eliminate congestion and fund infrastructure. imposing tolls that regulate travel along a public highway through monetary fees raise worries of inequity. in this article, we introduce the concept of time poverty, emphasize its value in policy-making in the same ways income poverty is already considered, and argue the potential equity concern posed by time-varying tolls that produce time poverty. we also compare the cost burdens of a no-toll, system optimal toll, and a proposed ``time-equitable\" toll on heterogeneous traveler groups using an analytical vickrey bottleneck model where travelers make departure time decisions to arrive at their destination at a fixed time. we show that the time-equitable toll is able to eliminate congestion while creating equitable travel patterns amongst traveler groups.", "categories": "econ.th", "created": "2020-07-11", "updated": "", "authors": ["john w helsel", "venktesh pandey", "stephen d. boyles"], "url": "https://arxiv.org/abs/2007.07091"}, {"title": "degrees of individual and groupwise backward and forward responsibility   in extensive-form games with ambiguity, and their application to social   choice problems", "id": "2007.07352", "abstract": "many real-world situations of ethical relevance, in particular those of large-scale social choice such as mitigating climate change, involve not only many agents whose decisions interact in complicated ways, but also various forms of uncertainty, including quantifiable risk and unquantifiable ambiguity. in such problems, an assessment of individual and groupwise moral responsibility for ethically undesired outcomes or their responsibility to avoid such is challenging and prone to the risk of under- or overdetermination of responsibility. in contrast to existing approaches based on strict causation or certain deontic logics that focus on a binary classification of `responsible' vs `not responsible', we here present several different quantitative responsibility metrics that assess responsibility degrees in units of probability. for this, we use a framework based on an adapted version of extensive-form game trees and an axiomatic approach that specifies a number of potentially desirable properties of such metrics, and then test the developed candidate metrics by their application to a number of paradigmatic social choice situations. we find that while most properties one might desire of such responsibility metrics can be fulfilled by some variant, an optimal metric that clearly outperforms others has yet to be found.", "categories": "econ.th cs.ai", "created": "2020-07-09", "updated": "", "authors": ["jobst heitzig", "sarah hiller"], "url": "https://arxiv.org/abs/2007.07352"}, {"title": "keynesian models of depression. supply shocks and the covid-19 crisis", "id": "2007.07353", "abstract": "the objective of this work is twofold: to expand the depression models proposed by tobin and analyse a supply shock, such as the covid-19 pandemic, in this keynesian conceptual environment. the expansion allows us to propose the evolution of all endogenous macroeconomic variables. the result obtained is relevant due to its theoretical and practical implications. a quantity or keynesian adjustment to the shock produces a depression through the effect on aggregate demand. this depression worsens in the medium/long-term. it is accompanied by increases in inflation, inflation expectations and the real interest rate. a stimulus tax policy is also recommended, as well as an active monetary policy to reduce real interest rates. on the other hand, the pricing or marshallian adjustment foresees a more severe and rapid depression in the short-term. there would be a reduction in inflation and inflation expectations, and an increase in the real interest rates. the tax or monetary stimulus measures would only impact inflation. this result makes it possible to clarify and assess the resulting depression, as well as propose policies. finally, it offers conflicting predictions that allow one of the two models to be falsified.", "categories": "econ.gn q-fin.ec", "created": "2020-07-11", "updated": "2020-07-17", "authors": ["ignacio escanuela romana"], "url": "https://arxiv.org/abs/2007.07353"}, {"title": "multivariate circulant singular spectrum analysis", "id": "2007.07561", "abstract": "multivariate circulant singular spectrum analysis (m-cissa) is a non-parametric automated technique that allows to extract signals of time series at any frequency specified beforehand. it is useful to uncover co-movements at different frequencies and understand their commonalities and specificities. we apply m-cissa to understand the main drivers and co-movements of energy commodity prices at trend and cyclical frequencies that are key to assess energy policy at different time horizons. we clearly distinguish the detached behaviour of us natural gas from the rest of energy commodities at both frequencies while coals and japan natural gas only decouple at the cyclical frequency.", "categories": "eess.sp econ.em", "created": "2020-07-15", "updated": "", "authors": ["juan b\u00f3galo", "pilar poncela", "eva senra"], "url": "https://arxiv.org/abs/2007.07561"}, {"title": "prophylaxis of epidemic spreading with transient dynamics", "id": "2007.07580", "abstract": "we investigate the containment of epidemic spreading in networks from a normative point of view. we consider a susceptible/infected model in which agents can invest in order to reduce the contagiousness of network links. in this setting, we study the relationships between social efficiency, individual behaviours and network structure. first, we exhibit an upper bound on the price of anarchy and prove that the level of inefficiency can scale up to linearly with the number of agents. second, we prove that policies of uniform reduction of interactions satisfy some optimality conditions in a vast range of networks. in setting where no central authority can enforce such stringent policies, we consider as a type of second-best policy the shift from a local to a global game by allowing agents to subsidise investments in contagiousness reduction in the global rather than in the local network. we then characterise the scope for pareto improvement opened by such policies through a notion of price of autarky, measuring the ratio between social welfare at a global and a local equilibrium. overall, our results show that individual behaviours can be extremely inefficient in the face of epidemic propagation but that policy can take advantage of the network structure to design efficient containment policies.", "categories": "econ.th cs.gt", "created": "2020-07-15", "updated": "", "authors": ["geraldine bouveret", "antoine mandel"], "url": "https://arxiv.org/abs/2007.07580"}, {"title": "failures of contingent thinking", "id": "2007.07703", "abstract": "in this paper, we provide a theoretical framework to analyze an agent who misinterprets or misperceives the true decision problem she faces. within this framework, we show that a wide range of behavior observed in experimental settings manifest as failures to perceive implications, in other words, to properly account for the logical relationships between various payoff relevant contingencies. we present behavioral characterizations corresponding to several benchmarks of logical sophistication and show how it is possible to identify which implications the agent fails to perceive. thus, our framework delivers both a methodology for assessing an agent's level of contingent thinking and a strategy for identifying her beliefs in the absence full rationality.", "categories": "cs.ai econ.th", "created": "2020-07-15", "updated": "", "authors": ["evan piermont", "peio zuazo-garin"], "url": "https://arxiv.org/abs/2007.07703"}, {"title": "sketching for two-stage least squares estimation", "id": "2007.07781", "abstract": "when there is so much data that they become a computation burden, it is not uncommon to compute quantities of interest using a sketch of data of size $m$ instead of the full sample of size $n$. this paper investigates the implications for two-stage least squares (2sls) estimation when the sketches are obtained by a computationally efficient method known as countsketch. we obtain three results. first, we establish conditions under which given the full sample, a sketched 2sls estimate can be arbitrarily close to the full-sample 2sls estimate with high probability. second, we give conditions under which the sketched 2sls estimator converges in probability to the true parameter at a rate of $m^{-1/2}$ and is asymptotically normal. third, we show that the asymptotic variance can be consistently estimated using the sketched sample and suggest methods for determining an inference-conscious sketch size $m$. the sketched 2sls estimator is used to estimate returns to education.", "categories": "stat.ml cs.lg econ.em", "created": "2020-07-15", "updated": "", "authors": ["sokbae lee", "serena ng"], "url": "https://arxiv.org/abs/2007.07781"}, {"title": "covid-19 induced economic uncertainty: a comparison between the united   kingdom and the united states", "id": "2007.07839", "abstract": "the purpose of this study is to investigate the effects of the covid-19 pandemic on economic policy uncertainty in the us and the uk. the impact of the increase in covid-19 cases and deaths in the country, and the increase in the number of cases and deaths outside the country may vary. to examine this, the study employs bootstrap ardl cointegration approach from march 8, 2020 to may 24, 2020. according to the bootstrap ardl results, a long-run equilibrium relationship is confirmed for five out of the 10 models. the long-term coefficients obtained from the ardl models suggest that an increase in covid-19 cases and deaths outside of the uk and the us has a significant effect on economic policy uncertainty. the us is more affected by the increase in the number of covid-19 cases. the uk, on the other hand, is more negatively affected by the increase in the number of covid-19 deaths outside the country than the increase in the number of cases. moreover, another important finding from the study demonstrates that covid-19 is a factor of great uncertainty for both countries in the short-term.", "categories": "econ.gn physics.soc-ph q-fin.ec stat.me", "created": "2020-06-28", "updated": "", "authors": ["ugur korkut pata"], "url": "https://arxiv.org/abs/2007.07839"}, {"title": "dynamic networks in large financial and economic systems", "id": "2007.07842", "abstract": "we propose new measures to characterize dynamic network connections in large financial and economic systems. in doing so, our measures allow one to describe and understand causal network structures that evolve throughout time and over horizons using variance decomposition matrices from time-varying parameter var (tvp var) models. these methods allow researchers and practitioners to examine network connections over any horizon of interest whilst also being applicable to a wide range of economic and financial data. our empirical application redefines the meaning of big in big data, in the context of tvp var models, and track dynamic connections among illiquidity ratios of all s\\&p500 constituents. we then study the information content of these measures for the market return and real economy.", "categories": "econ.em", "created": "2020-07-14", "updated": "", "authors": ["jozef barunik", "michael ellington"], "url": "https://arxiv.org/abs/2007.07842"}, {"title": "social capital and resilience make an employee cooperate for coronavirus   measures and lower his/her turnover intention", "id": "2007.07963", "abstract": "an important theme is how to maximize the cooperation of employees when dealing with crisis measures taken by the company. therefore, to find out what kind of employees have cooperated with the company's measures in the current corona (covid-19) crisis, and what effect the cooperation has had to these employees/companies to get hints for preparing for the next crisis, the pass analysis was carried out using awareness data obtained from a questionnaire survey conducted on 2,973 employees of japanese companies in china. the results showed that employees with higher social capital and resilience were more supportive of the company's measures against corona and that employees who were more supportive of corona measures were less likely to leave their jobs. however, regarding fatigue and anxiety about the corona felt by employees, it was shown that it not only works to support cooperation in corona countermeasures but also enhances the turnover intention. this means that just by raising the anxiety of employees, even if a company achieves the short-term goal of having them cooperate with the company's countermeasures against corona, it may not reach the longer-term goal by making them increase their intention to leave. it is important for employees to be aware of the crisis and to fear it properly. but more than that, it should be possible for the company to help employees stay resilient, build good relationships with them, and increase their social capital to make them support crisis measurement of the company most effectively while keeping their turnover intention low.", "categories": "econ.gn q-fin.ec", "created": "2020-07-15", "updated": "2020-07-23", "authors": ["keisuke kokubun", "yoshiaki ino", "kazuyoshi ishimura"], "url": "https://arxiv.org/abs/2007.07963"}, {"title": "global representation of late model: a separability result", "id": "2007.08106", "abstract": "this paper studies the latent index representation of the conditional late model, making explicit the role of covariates in treatment selection. we find that if the directions of the monotonicity condition are the same across all values of the conditioning covariate, which is often assumed in the literature, then the treatment choice equation has to satisfy a separability condition between the instrument and the covariate. this global representation result establishes testable restrictions imposed on the way covariates enter the treatment choice equation. we later extend the representation theorem to incorporate multiple ordered levels of treatment.", "categories": "econ.em", "created": "2020-07-16", "updated": "", "authors": ["yu-chang chen", "haitian xie"], "url": "https://arxiv.org/abs/2007.08106"}, {"title": "government spending and multi-category treatment effects:the modified   conditional independence assumption", "id": "2007.08396", "abstract": "i devise a novel approach to evaluate the effectiveness of fiscal policy in the short run with multi-category treatment effects and inverse probability weighting based on the potential outcome framework. this study's main contribution to the literature is the proposed modified conditional independence assumption to improve the evaluation of fiscal policy. using this approach, i analyze the effects of government spending on the us economy from 1992 to 2019. the empirical study indicates that large fiscal contraction generates a negative effect on the economic growth rate, and small and large fiscal expansions realize a positive effect. however, these effects are not significant in the traditional multiple regression approach. i conclude that this new approach significantly improves the evaluation of fiscal policy.", "categories": "econ.em", "created": "2020-07-16", "updated": "2020-08-09", "authors": ["koiti yano"], "url": "https://arxiv.org/abs/2007.08396"}, {"title": "is segregating anti-vaxxers a good idea?", "id": "2007.08523", "abstract": "we study and si-type model, with the possibility of vaccination, where the population is partitioned between pro-vaxxers and anti-vaxxers. we show that, during the outbreak of a disease, segregating people that are against vaccination from the rest of the population decreases the speed of recovery and may increase the number of cases. then, we include endogenous choices based on the tradeoff between the cost of vaccinating and the risk of getting infected. we show that the results remain valid under endogenous choices, unless people are too flexible in determining their identity towards vaccination.", "categories": "q-bio.pe econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-16", "updated": "2020-09-18", "authors": ["matteo bizzarri", "fabrizio panebianco", "paolo pin"], "url": "https://arxiv.org/abs/2007.08523"}, {"title": "learning from manipulable signals", "id": "2007.08762", "abstract": "we study a dynamic stopping game between a principal and an agent. the agent is privately informed about his type. the principal learns about the agent's type from a noisy performance measure, which can be manipulated by the agent via a costly and hidden action. we fully characterize the unique markov equilibrium of this game. we find that terminations/market crashes are often preceded by a spike in (expected) performance. our model also predicts that, due to endogenous signal manipulation, too much transparency can inhibit learning. as the players get arbitrarily patient, the principal elicits no useful information from the observed signal.", "categories": "econ.th", "created": "2020-07-17", "updated": "2020-09-07", "authors": ["mehmet ekmekci", "leandro gorno", "lucas maestri", "jian sun", "dong wei"], "url": "https://arxiv.org/abs/2007.08762"}, {"title": "tractable constrained optimization over multiple attributes under the   multinomial logit and nested logit model", "id": "2007.09193", "abstract": "in classical optimization problems under discrete choice models, the decision maker can control at most one attribute for each choice. usually, this attribute is the price of a product. however, price is not the only attribute that can impact both the product's market share and profit margin. this paper studies a generalization of these problems, which allows multiple attributes to be controlled for each choice of product. past research showed that we can solve the classical optimization problems efficiently, using methods like the market share reformulation. however, these methods do not apply to the generalized problems. we showed that such problems are still tractable, and can be solved efficiently as conic programs. this argument applies to optimization problems under multinomial logit models, and applies to problems under nested logit models when certain assumptions hold. these generalized problems are very common in retail and transportation industries. this paper allows practitioners to solve such problems directly using commercial solvers.", "categories": "math.oc econ.em econ.th", "created": "2020-07-17", "updated": "", "authors": ["hongzhang shao", "anton j. kleywegt"], "url": "https://arxiv.org/abs/2007.09193"}, {"title": "how flexible is that functional form? quantifying the restrictiveness of   theories", "id": "2007.09213", "abstract": "we propose a new way to quantify the restrictiveness of an economic model, based on how well the model fits simulated, hypothetical data sets. the data sets are drawn at random from a distribution that satisfies some application-dependent content restrictions (such as that people prefer more money to less). models that can fit almost all hypothetical data well are not restrictive. to illustrate our approach, we evaluate the restrictiveness of two widely-used behavioral models, cumulative prospect theory and the poisson cognitive hierarchy model, and explain how restrictiveness reveals new insights about them.", "categories": "econ.th cs.gt econ.em", "created": "2020-07-17", "updated": "", "authors": ["drew fudenberg", "wayne gao", "annie liang"], "url": "https://arxiv.org/abs/2007.09213"}, {"title": "absentee and economic impact of low-level fine particulate matter and   ozone exposure in k-12 students", "id": "2007.09230", "abstract": "high air pollution levels are associated with school absences. however, low level pollution impact on individual school absences are under-studied. we modelled pm2.5 and ozone concentrations at 36 schools from july 2015 to june 2018 using data from a dense, research grade regulatory sensor network. we determined exposures and daily absences at each school. we used generalized estimating equations model to retrospectively estimate rate ratios for association between outdoor pollutant concentrations and school absences. we estimated lost school revenue, productivity, and family economic burden. pm2.5 and ozone concentrations and absence rates vary across the school district. pollution exposure were associated with as high a rate ratio of 1.02 absences per ug/m$^3$ and 1.01 per ppb increase for pm2.5 and ozone, respectively. significantly, even pm2.5 and ozone exposure below regulatory standards (<12.1 ug/m$^3$ and <55 ppb) was associated with positive rate ratios of absences: 1.04 per ug/m$^3$ and 1.01 per ppb increase, respectively. granular local measurements enabled demonstration of air pollution impacts that varied between schools undetectable with averaged pollution levels. reducing pollution by 50% would save $452,000 per year districtwide. pollution reduction benefits would be greatest in schools located in socioeconomically disadvantaged areas. exposures to air pollution, even at low levels, are associated with increased school absences. heterogeneity in exposure, disproportionately affecting socioeconomically disadvantaged schools, points to the need for fine resolution exposure estimation. the economic cost of absences associated with air pollution is substantial even excluding indirect costs such as hospital visits and medication. these findings may help inform decisions about recess during severe pollution events and regulatory considerations for localized pollution sources.", "categories": "econ.gn cs.cy q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["daniel l. mendoza", "cheryl s. pirozzi", "erik t. crosman", "theodore g. liou", "yue zhang", "jessica j. cleeves", "stephen c. bannister", "william r. l. anderegg", "robert paine"], "url": "https://arxiv.org/abs/2007.09230"}, {"title": "inequalities for (p,a,b)-convex functions and their applications", "id": "2007.09258", "abstract": "convex analysis is fundamental to proving inequalities that have a wide variety of applications in economics and mathematics. in this paper we provide inequalities for functions that are, intuitively, \"very\" convex. these inequalities are simple to apply and can be used to generalize and extend previous results or to derive new results. we apply our inequalities to various applications from different fields, including risk aversion, risk measures, moment generating functions, and log-likelihood functions.", "categories": "math.oc econ.th math.pr", "created": "2020-07-17", "updated": "", "authors": ["bar light"], "url": "https://arxiv.org/abs/2007.09258"}, {"title": "authoritarian governments appear to manipulate covid data", "id": "2007.09566", "abstract": "because sars-cov-2 (covid-19) statistics affect economic policies and political outcomes, governments have an incentive to control them. manipulation may be less likely in democracies, which have checks to ensure transparency. we show that data on disease burden bear indicia of data modification by authoritarian governments relative to democratic governments. first, data on covid-19 cases and deaths from authoritarian governments show significantly less variation from a 7 day moving average. because governments have no reason to add noise to data, lower deviation is evidence that data may be massaged. second, data on covid-19 deaths from authoritarian governments do not follow benford's law, which describes the distribution of leading digits of numbers. deviations from this law are used to test for accounting fraud. smoothing and adjustments to covid-19 data may indicate other alterations to these data and a need to account for such alterations when tracking the disease.", "categories": "econ.gn q-fin.ec", "created": "2020-07-18", "updated": "", "authors": ["mudit kapoor", "anup malani", "shamika ravi", "arnav agrawal"], "url": "https://arxiv.org/abs/2007.09566"}, {"title": "only time will tell: credible dynamic signaling", "id": "2007.09568", "abstract": "this paper explores a model of dynamic signaling without commitment. it is known that separating equilibria do not exist if the sender cannot commit to future costly actions, since no single action can have enough weight to be an effective signal. this paper, however, shows that informative and payoff-relevant signaling can occur even without commitment and without resorting to unreasonable off-path beliefs. such signaling can only happen through attrition, when the weakest type mixes between revealing own type and pooling with the stronger types. the possibility of full information revelation in the limit hence depends crucially on the assumptions about the state space. we illustrate the results by exploring a model of dynamic price signaling and show that prices may be informative of product quality even if the seller cannot commit to future prices, with both high and low prices being able to signal high quality.", "categories": "econ.th", "created": "2020-07-18", "updated": "", "authors": ["egor starkov"], "url": "https://arxiv.org/abs/2007.09568"}, {"title": "a maximum theorem for incomplete preferences", "id": "2007.09781", "abstract": "we extend berge's maximum theorem to allow for incomplete preferences (i.e., reflexive and transitive binary relations which fail to be complete). we first provide a simple version of the maximum theorem for convex feasible sets and a fixed preference. then, we show that if, in addition to the traditional continuity assumptions, a new continuity property for the domains of comparability holds, the limits of maximal elements along a sequence of decision problems are maximal elements in the limit problem. while this new continuity property for the domains of comparability is sufficient, it is not generally necessary. however, we provide conditions under which it is necessary and sufficient for maximality and minimality to be preserved by limits.", "categories": "econ.th math.fa", "created": "2020-07-19", "updated": "2020-09-26", "authors": ["leandro gorno", "alessandro rivello"], "url": "https://arxiv.org/abs/2007.09781"}, {"title": "permutation-based tests for discontinuities in event studies", "id": "2007.09837", "abstract": "we propose using a permutation test to detect discontinuities in an underlying economic model at a cutoff point. relative to the existing literature, we show that this test is well suited for event studies based on time-series data. the test statistic measures the distance between the empirical distribution functions of observed data in two local subsamples on the two sides of the cutoff. critical values are computed via a standard permutation algorithm. under a high-level condition that the observed data can be coupled by a collection of conditionally independent variables, we establish the asymptotic validity of the permutation test, allowing the sizes of the local subsamples to be either be fixed or grow to infinity. in the latter case, we also establish that the permutation test is consistent. we demonstrate that our high-level condition can be verified in a broad range of problems in the infill asymptotic time-series setting, which justifies using the permutation test to detect jumps in economic variables such as volatility, trading activity, and liquidity. an empirical illustration on a recent sample of daily s&p 500 returns is provided.", "categories": "econ.em", "created": "2020-07-19", "updated": "", "authors": ["federico a. bugni", "jia li"], "url": "https://arxiv.org/abs/2007.09837"}, {"title": "social capital may mediate the relationship between social distance and   covid-19 prevalence", "id": "2007.09939", "abstract": "the threat of the new coronavirus (covid-19) is increasing. regarding the difference in the infection rate observed in each region, in addition to studies seeking the cause due to differences in the social distance (population density), there is an increasing trend toward studies seeking the cause due to differences in social capital. however, studies have not yet been conducted on whether social capital could influence the infection rate even if it controls the effect of population density. therefore, in this paper, we analyzed the relationship between infection rate, population density, and social capital using statistical data for each prefecture. statistical analysis showed that social capital not only correlates with infection rates and population densities but still has a negative correlation with infection rates controlling for the effects of population density. besides, controlling the relationship between variables by mean age showed that social capital had a greater correlation with infection rate than population density. in other words, social capital mediates the correlation between population density and infection rates. this means that social distance alone is not enough to deter coronavirus infection, and social capital needs to be recharged.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-20", "updated": "2020-07-21", "authors": ["keisuke kokubun"], "url": "https://arxiv.org/abs/2007.09939"}, {"title": "competing bandits: the perils of exploration under competition", "id": "2007.10144", "abstract": "most online platforms strive to learn from interactions with consumers, and many engage in exploration: making potentially suboptimal choices for the sake of acquiring new information. we initiate a study of the interplay between exploration and competition: how such platforms balance the exploration for learning and the competition for consumers. here consumers play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing platforms.   we consider a stylized duopoly model in which two firms face the same problem instance of multi-armed bandits. users arrive one by one and choose between the two firms, so that each firm makes progress on its bandit instance only if it is chosen. we study whether and to what extent competition incentivizes the adoption of better bandit algorithms, and whether it leads to welfare increases for consumers. we find that stark competition induces firms to commit to a \"greedy\" bandit algorithm that leads to low consumer welfare. however, we find that weakening competition by providing firms with some \"free\" consumers incentivizes better exploration strategies and increases consumer welfare. we investigate two channels for weakening the competition: relaxing the rationality of consumers and giving one firm a first-mover advantage. we provide a mix of theoretical results and numerical simulations. our findings are closely related to the \"competition vs. innovation\" relationship, a well-studied theme in economics. they also elucidate the first-mover advantage in the digital economy by exploring the role that data can play as a barrier to entry in online markets.", "categories": "cs.gt cs.lg econ.th", "created": "2020-07-20", "updated": "2020-09-29", "authors": ["guy aridor", "yishay mansour", "aleksandrs slivkins", "zhiwei steven wu"], "url": "https://arxiv.org/abs/2007.10144"}, {"title": "variable selection in macroeconomic forecasting with many predictors", "id": "2007.10160", "abstract": "in the data-rich environment, using many economic predictors to forecast a few key variables has become a new trend in econometrics. the commonly used approach is factor augment (fa) approach. in this paper, we pursue another direction, variable selection (vs) approach, to handle high-dimensional predictors. vs is an active topic in statistics and computer science. however, it does not receive as much attention as fa in economics. this paper introduces several cutting-edge vs methods to economic forecasting, which includes: (1) classical greedy procedures; (2) l1 regularization; (3) gradient descent with sparsification and (4) meta-heuristic algorithms. comprehensive simulation studies are conducted to compare their variable selection accuracy and prediction performance under different scenarios. among the reviewed methods, a meta-heuristic algorithm called sequential monte carlo algorithm performs the best. surprisingly the classical forward selection is comparable to it and better than other more sophisticated algorithms. in addition, we apply these vs methods on economic forecasting and compare with the popular fa approach. it turns out for employment rate and cpi inflation, some vs methods can achieve considerable improvement over fa, and the selected predictors can be well explained by economic theories.", "categories": "econ.em stat.ap", "created": "2020-07-20", "updated": "", "authors": ["zhenzhong wang", "zhengyuan zhu", "cindy yu"], "url": "https://arxiv.org/abs/2007.10160"}, {"title": "the impacts of incarceration on crime", "id": "2007.10268", "abstract": "this paper reviews the research on the impacts of incarceration on crime. where data availability permits, reviewed studies are replicated and reanalyzed. among three dozen studies i reviewed, i obtained or reconstructed the data and code for eight. replication and reanalysis revealed significant methodological concerns in seven and led to major reinterpretations of four. i estimate that, at typical policy margins in the united states today, decarceration has zero net impact on crime outside of prison. that estimate is uncertain, but at least as much evidence suggests that decarceration reduces crime as increases it. the crux of the matter is that tougher sentences hardly deter crime, and that while imprisoning people temporarily stops them from committing crime outside prison walls, it also tends to increase their criminality after release. as a result, \"tough-on-crime\" initiatives can reduce crime in the short run but cause offsetting harm in the long run. a cost-benefit analysis finds that even under a devil's advocate reading of this evidence, in which incarceration does reduce crime in u.s., it is unlikely to increase aggregate welfare.", "categories": "econ.gn q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["david roodman"], "url": "https://arxiv.org/abs/2007.10268"}, {"title": "the domestic economic impacts of immigration", "id": "2007.10269", "abstract": "this paper critically reviews the research on the impact of immigration on employment and wages of natives in wealthy countries--where \"natives\" includes previous immigrants and their descendants. while written for a non-technical audience, the paper engages with technical issues and probes one particularly intense scholarly debate in an appendix. while the available evidence is not definitive, it paints a consistent picture. industrial economies can generally absorb migrants quickly, in part because capital is mobile and flexible, in part because immigrants are consumers as well as producers. thus, long-term average impacts are probably zero at worst. and the long-term may come quickly, especially if migration inflows are predictable to investors. possibly, skilled immigration boosts productivity and wages for many others. around the averages, there are distributional effects. among low-income \"native\" workers, the ones who stand to lose the most are those who most closely resemble new arrivals, in being immigrants themselves, being low-skill, being less assimilated, and, potentially, undocumented. but native workers and earlier immigrants tend to benefit from the arrival of workers different from them, who complement more than compete with them in production. thus skilled immigration can offset the effects of low-skill immigration on natives and earlier immigrants.", "categories": "econ.gn q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["david roodman"], "url": "https://arxiv.org/abs/2007.10269"}, {"title": "the impacts of alcohol taxes: a replication review", "id": "2007.10270", "abstract": "this paper reviews the research on the impacts of alcohol taxation outcomes such as heavy drinking and mortality. where data availability permits, reviewed studies are replicated and reanalyzed. despite weaknesses in the majority of studies, and despite seeming disagreements among the more credible one--ones based on natural experiments--we can be reasonably confident that taxing alcohol reduces drinking in general and problem drinking in particular. the larger and cleaner the underlying natural experiment, the more apt a study is to detect impacts on drinking. estimates from the highest-powered study settings, such as in alaska in 2002 and finland in 2004, suggest an elasticity of mortality with respect to price of -1 to -3. a 10% price increase in the us would, according to this estimate, save 2,000-6,000 lives and 48,000-130,000 years of life each year.", "categories": "econ.gn q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["david roodman"], "url": "https://arxiv.org/abs/2007.10270"}, {"title": "the historical impact of anthropogenic climate change on global   agricultural productivity", "id": "2007.10415", "abstract": "agricultural research has fostered productivity growth, but the historical influence of anthropogenic climate change on that growth has not been quantified. we develop a robust econometric model of weather effects on global agricultural total factor productivity (tfp) and combine this model with counterfactual climate scenarios to evaluate impacts of past climate trends on tfp. our baseline model indicates that anthropogenic climate change has reduced global agricultural tfp by about 21% since 1961, a slowdown that is equivalent to losing the last 9 years of productivity growth. the effect is substantially more severe (a reduction of ~30-33%) in warmer regions such as africa and latin america and the caribbean. we also find that global agriculture has grown more vulnerable to ongoing climate change.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ariel ortiz-bobea", "toby r. ault", "carlos m. carrillo", "robert g. chambers", "david b. lobell"], "url": "https://arxiv.org/abs/2007.10415"}, {"title": "filtered and unfiltered treatment effects with targeting instruments", "id": "2007.10432", "abstract": "multivalued treatments are commonplace in applications. we explore the use of discrete-valued instruments to control for selection bias in this setting. we establish conditions under which counterfactual averages and treatment effects are identified for heterogeneous complier groups. these conditions require a combination of assumptions that restrict both the unobserved heterogeneity in treatment assignment and how the instruments target the treatments. we introduce the concept of filtered treatment, which takes into account limitations in the analyst's information. finally, we illustrate the usefulness of our framework by applying it to data from the student achievement and retention project and the head start impact study.", "categories": "econ.em stat.me", "created": "2020-07-20", "updated": "", "authors": ["sokbae lee", "bernard salani\u00e9"], "url": "https://arxiv.org/abs/2007.10432"}, {"title": "the risk spillover from economic policy uncertainties to the european   union emission trading scheme", "id": "2007.10564", "abstract": "the european union emission trading scheme is a carbon emission allowance trading system designed by europe to achieve emission reduction targets. the amount of carbon emission caused by production activities is closely related to the socio-economic environment. therefore, from the perspective of economic policy uncertainty, this article constructs the garch-midas-euepu and garch-midas-gepu models for the impact of european and global economic policy uncertainty on carbon price fluctuations. the results show that both european and global economic policy uncertainty will exacerbate the volatility of carbon price returns, with the latter having a stronger impact. moreover, the volatility of carbon price returns can be forecasted better with the predictor of global economic policy uncertainty. this research can provide some implications for market managers in grasping market trends and helping participants control the risk of fluctuations in carbon allowances.", "categories": "econ.gn econ.em q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["jiqiang wang", "jianfeng guo", "peng-fei dai", "yinpeng liu", "ying fan"], "url": "https://arxiv.org/abs/2007.10564"}, {"title": "continuous-time incentives in hierarchies", "id": "2007.10758", "abstract": "this paper studies continuous-time optimal contracting in a hierarchy problem which generalises the model of sung (2015). the hierarchy is modeled by a series of interlinked principal-agent problems, leading to a sequence of stackelberg equilibria. more precisely, the principal can contract with the managers to incentivise them to act in her best interest, despite only observing the net benefits of the total hierarchy. managers in turn subcontract with the agents below them. both agents and managers independently control in continuous time a stochastic process representing their outcome. first, we show through a continuous-time adaptation of sung's model that, even if the agents only control the drift of their outcome, their manager controls the volatility of their continuation utility. this first simple example justifies the use of recent results on optimal contracting for drift and volatility control, and therefore the theory of second-order backward stochastic differential equations, developed in the theoretical part of this paper, dedicated to a more general model. the comprehensive approach we outline highlights the benefits of considering a continuous-time model and opens the way to obtain comparative statics. we also explain how the model can be extended to a large-scale principal-agent hierarchy. since the principal's problem can be reduced to only an $m$-dimensional state space and a $2m$-dimensional control set, where $m$ is the number of managers immediately below her, and is therefore independent of the size of the hierarchy below these managers, the dimension of the problem does not explode.", "categories": "math.oc econ.th math.pr", "created": "2020-07-21", "updated": "", "authors": ["emma hubert"], "url": "https://arxiv.org/abs/2007.10758"}, {"title": "lasso inference for high-dimensional time series", "id": "2007.10952", "abstract": "the desparsified lasso is a high-dimensional estimation method which provides uniformly valid inference. we extend this method to a time series setting under near-epoch dependence (ned) assumptions allowing for non-gaussian, serially correlated and heteroskedastic processes, where the number of regressors can possibly grow faster than the time dimension. we first derive an oracle inequality for the (regular) lasso, relaxing the commonly made exact sparsity assumption to a weaker alternative, which permits many small but non-zero parameters. the weak sparsity coupled with the ned assumption means this inequality can also be applied to the (inherently misspecified) nodewise regressions performed in the desparsified lasso. this allows us to establish the uniform asymptotic normality of the desparsified lasso under general conditions. additionally, we show consistency of a long-run variance estimator, thus providing a complete set of tools for performing inference in high-dimensional linear time series models. finally, we perform a simulation exercise to demonstrate the small sample properties of the desparsified lasso in common time series settings.", "categories": "econ.em math.st stat.th", "created": "2020-07-21", "updated": "", "authors": ["robert adamek", "stephan smeekes", "ines wilms"], "url": "https://arxiv.org/abs/2007.10952"}, {"title": "a canon of probabilistic rationality", "id": "2007.11386", "abstract": "we prove that a random choice rule satisfies luce's choice axiom if and only if its support, the set of \"alternatives that can be chosen,\" is a choice correspondence that satisfies the weak axiom of revealed preference, and random choice occurs according to a stochastic tie breaking among optimizers that satisfies renyi's conditioning axiom. our result shows that the choice axiom is, in a precise formal sense, a probabilistic version of the weak axiom. it thus supports luce's view of his own axiom as a \"canon of probabilistic rationality.\"", "categories": "econ.th", "created": "2020-07-17", "updated": "", "authors": ["simone cerreia-vioglio", "fabio maccheroni", "massimo marinacci", "aldo rustichini"], "url": "https://arxiv.org/abs/2007.11386"}, {"title": "the impact of life-saving interventions on fertility", "id": "2007.11388", "abstract": "many interventions in global health save lives. one criticism sometimes lobbed at these interventions invokes the spirit of malthus. the good done, the charge goes, is offset by the harm of spreading the earth's limited resources more thinly: more people, and more misery per person. to the extent this holds, the net benefit of savings lives is lower than it appears at first. on the other hand, if lower mortality, especially in childhood, leads families to have fewer children, life-saving interventions could reduce population. this document critically reviews the evidence. it finds that the impact of life-saving interventions on fertility and population growth varies by context, and is rarely greater than 1:1. in places where lifetime births/woman has been converging to 2 or lower, saving one child's life should lead parents to avert a birth they would otherwise have. the impact of mortality drops on fertility will be nearly 1:1, so population growth will hardly change. in the increasingly exceptional locales where couples appear not to limit fertility much, such as niger and mali, the impact of saving a life on total births will be smaller, and may come about mainly through the biological channel of lactational amenorrhea. here, mortality-drop-fertility-drop ratios of 1:0.5 and 1:0.33 appear more plausible. but in the long-term, it would be surprising if these few countries do not join the rest of the world in the transition to lower and more intentionally controlled fertility.", "categories": "econ.gn q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["david roodman"], "url": "https://arxiv.org/abs/2007.11388"}, {"title": "examining the drivers of business cycle divergence between euro area and   romania", "id": "2007.11407", "abstract": "this research aims to provide an explanatory analyses of the business cycles divergence between euro area and romania, respectively its drivers, since the synchronisation of output-gaps is one of the most important topic in the context of a potential emu accession. according to the estimates, output-gaps synchronisation entered on a downward path in the subperiod 2010-2017, compared to 2002-2009. the paper demonstrates there is a negative relationship between business cycles divergence and three factors (economic structure convergence, wage structure convergence and economic openness), but also a positive relationship between it and its autoregressive term, respectively the gdp per capita convergence.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.11407"}, {"title": "the impact of private sector credit on income inequalities in european   union (15 member states)", "id": "2007.11408", "abstract": "this paper aims to provide a comprehensive analysis on the income inequalities recorded in the eu-15 in the 1995-2014 period and to estimate the impact of private sector credit on income disparities. in order to estimate the impact, i used the panel data technique with 15 cross-sections for the first 15 member states of the european union, applying generalized error correction model.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.11408"}, {"title": "the impact of government health and education expenditure on income   inequality in european union", "id": "2007.11409", "abstract": "this research aims to provide an overview of the existing inequalities and their drivers in the member states of the european union as well as their developements in the 2002-2008 and 2009- 2015 sub-periods. it also analyses the impact of health and education government spending on income inequality in the european union over the 2002-2015 period. in this context, i applied the estimated generalized least squares method using panel data for the 28-member states of the european union.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.11409"}, {"title": "the effect of young people not in employment, education or training, on   poverty rate in european union", "id": "2007.11435", "abstract": "this paper aims to estimate the effect of young people who are not in employment, education or training (neets rate) on the people at risk of poverty rate in the european union. statistical data covering the 2010-2016 period for all eu-28 member states have been used. regarding the methodology, the study was performed by using panel estimated generalized least squares method, weighted by period sur option. the effect of neets rate on poverty rate proved to be positive and statistically significant in european union, since this indicator includes two main areas which are extremely relevant for poverty dimension. firstly, young unemployment rate was one of the main channels through which the financial crisis has affected the population income. secondly, it accounts for the educational system coverage and its skills deficiencies.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.11435"}, {"title": "the implications of institutional specificities on the income   inequalities drivers in european union", "id": "2007.11436", "abstract": "this paper aims to review the different impacts of income inequality drivers on the gini coefficient, depending on institutional specificities. in this context, we divided the european union member states in two clusters (the cluster of member states with inclusive institutions / extractive institutions) using the institutional pillar as a clustering criterion. in both cases, we assesed the impact of income inequality drivers on gini coefficient by using a fixed effects model in order to examine the role and importance of the institutions in the dynamics of income disparities.the models were estimated by applying the panel estimated generalized least squares (egls) method, this being weighted by cross-section weights option. the separate assessment of the income inequality reactivity to the change in its determinants according to the institutional criterion represents a new approach in this field of research and the results show that the impact of moderating income inequality strategies is limitedin the case of member states with extractive institutions.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu", "ion dobre", "dumitru alexandru bodislav", "carmen valentina radulescu", "sorin burlacu"], "url": "https://arxiv.org/abs/2007.11436"}, {"title": "a comprehensive view of the manifestations of aggregate demand and   aggregate supply shocks in greece, ireland, italy and portugal", "id": "2007.11439", "abstract": "the main goal of the paper is to extract the aggregate demand and aggregate supply shocks in greece, ireland, italy and portugal, as well as to examine the correlation among the two types of shocks. the decomposition of the shocks was achieved by using a structural vector autoregression that analyses the relationship between the evolution of the gross domestic product and inflation in the period 1997-2015. the goal of the paper is to confirm the aggregate demand - aggregate supply model in the above-mentioned economies.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.11439"}, {"title": "how happy are my neighbours? modelling spatial spillover effects of   well-being", "id": "2007.11580", "abstract": "this article uses data of subjective life satisfaction aggregated to the community level in canada and examines the spatial interdependencies and spatial spillovers of community happiness. a theoretical model of utility is presented. using spatial econometric techniques, we find that the utility of community, proxied by subjective measures of life satisfaction, is affected both by the utility of neighbouring communities as well as by the latter's average household income and unemployment rate. shared cultural traits and institutions may justify such spillovers. the results are robust to the different binary contiguity spatial weights matrices used and to the various econometric models. clusters of both high-high and low-low in life satisfaction communities are also found based on the moran's i test", "categories": "econ.gn q-fin.ec", "created": "2020-07-22", "updated": "", "authors": ["thanasis ziogas", "dimitris ballas", "sierdjan koster", "arjen edzes"], "url": "https://arxiv.org/abs/2007.11580"}, {"title": "the mode treatment effect", "id": "2007.11606", "abstract": "mean, median, and mode are three essential measures of the centrality of probability distributions. in program evaluation, the average treatment effect (mean) and the quantile treatment effect (median) have been intensively studied in the past decades. the mode treatment effect, however, has long been neglected in program evaluation. this paper fills the gap by discussing both the estimation and inference of the mode treatment effect. i propose both traditional kernel and machine learning methods to estimate the mode treatment effect. i also derive the asymptotic properties of the proposed estimators and find that both estimators follow the asymptotic normality but with the rate of convergence slower than the regular rate $\\sqrt{n}$, which is different from the rates of the classical average and quantile treatment effect estimators.", "categories": "econ.em", "created": "2020-07-22", "updated": "", "authors": ["neng-chieh chang"], "url": "https://arxiv.org/abs/2007.11606"}, {"title": "nursing home staff networks and covid-19", "id": "2007.11789", "abstract": "nursing homes and other long term-care facilities account for a disproportionate share of covid-19 cases and fatalities worldwide. outbreaks in u.s. nursing homes have persisted despite nationwide visitor restrictions beginning in mid-march. an early report issued by the centers for disease control and prevention identified staff members working in multiple nursing homes as a likely source of spread from the life care center in kirkland, washington to other skilled nursing facilities. the full extent of staff connections between nursing homes---and the crucial role these connections serve in spreading a highly contagious respiratory infection---is currently unknown given the lack of centralized data on cross-facility nursing home employment. in this paper, we perform the first large-scale analysis of nursing home connections via shared staff using device-level geolocation data from 30 million smartphones, and find that 7 percent of smartphones appearing in a nursing home also appeared in at least one other facility---even after visitor restrictions were imposed. we construct network measures of nursing home connectedness and estimate that nursing homes have, on average, connections with 15 other facilities. controlling for demographic and other factors, a home's staff-network connections and its centrality within the greater network strongly predict covid-19 cases. traditional federal regulatory metrics of nursing home quality are unimportant in predicting outbreaks, consistent with recent research. results suggest that eliminating staff linkages between nursing homes could reduce covid-19 infections in nursing homes by 44 percent.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-23", "updated": "2020-07-24", "authors": ["m. keith chen", "judith a. chevalier", "elisa f. long"], "url": "https://arxiv.org/abs/2007.11789"}, {"title": "deep dynamic factor models", "id": "2007.11887", "abstract": "we propose a novel deep neural net framework - that we refer to as deep dynamic factor model (d2fm) -, to encode the information available, from hundreds of macroeconomic and financial time-series into a handful of unobserved latent states. while similar in spirit to traditional dynamic factor models (dfms), differently from those, this new class of models allows for nonlinearities between factors and observables due to the deep neural net structure. however, by design, the latent states of the model can still be interpreted as in a standard factor model. in an empirical application to the forecast and nowcast of economic conditions in the us, we show the potential of this framework in dealing with high dimensional, mixed frequencies and asynchronously published time series data. in a fully real-time out-of-sample exercise with us data, the d2fm improves over the performances of a state-of-the-art dfm.", "categories": "econ.em cs.lg", "created": "2020-07-23", "updated": "", "authors": ["paolo andreini", "cosimo izzo", "giovanni ricco"], "url": "https://arxiv.org/abs/2007.11887"}, {"title": "(unintended) consequences of export restrictions on medical goods during   the covid-19 pandemic", "id": "2007.11941", "abstract": "in the first half of 2020, several countries have responded to the challenges posed by the covid-19 pandemic by restricting their export of medical supplies. such measures are meant to increase the domestic availability of critical goods, and are commonly used in times of crisis. yet, not much is known about their impact, especially on countries imposing them. here we show that export bans are, by and large, counterproductive. using a model of shock diffusion through the network of international trade, we simulate the impact of restrictions under different scenarios. we observe that while they would be beneficial to a country implementing them in isolation, their generalized use makes most countries worse off relative to a no-ban scenario. as a corollary, we estimate that prices increase in many countries imposing the restrictions. we also find that the cost of restraining from export bans is small, even when others continue to implement them. finally, we document a change in countries' position within the international trade network, suggesting that export bans have geopolitical implications.", "categories": "physics.soc-ph econ.gn q-fin.ec q-fin.tr", "created": "2020-07-23", "updated": "", "authors": ["marco grassia", "giuseppe mangioni", "stefano schiavo", "silvio traverso"], "url": "https://arxiv.org/abs/2007.11941"}, {"title": "dominant resource fairness with meta-types", "id": "2007.11961", "abstract": "inspired by the recent covid-19 pandemic, we study a generalization of the multi-resource allocation problem with heterogeneous demands and leontief utilities. unlike existing settings, we allow each agent to specify a constraint to only accept allocations from a subset of the total supply for each resource. such constraints often arise from location constraints (e.g. among all of the volunteer nurses, only those who live nearby can work at hospital a due to commute constraints. so hospital a can only receive allocations of volunteers from a subset of the total supply). this can also model a type of substitute effect where some agents need 1 unit of resource a \\emph{or} b, but some other agents specifically want a, and some specifically want b. we propose a new mechanism called group dominant resource fairness which determines the allocations by solving a small number of linear programs. the proposed method satisfies pareto optimality, envy-freeness, strategy-proofness, and a notion of sharing incentive for our setting. to the best of our knowledge this is the first mechanism to achieve all four properties in our setting. furthermore, we show numerically that our method scales better to large problems than alternative approaches. finally, although motivated by the problem of medical resource allocation in a pandemic, our mechanism can be applied more broadly to resource allocation under leontief utilities with accessibility constraints.", "categories": "econ.th cs.gt", "created": "2020-07-21", "updated": "2020-09-23", "authors": ["steven yin", "shatian wang", "lingyi zhang", "christian kroer"], "url": "https://arxiv.org/abs/2007.11961"}, {"title": "the societal and ethical relevance of computational creativity", "id": "2007.11973", "abstract": "in this paper, we provide a philosophical account of the value of creative systems for individuals and society. we characterize creativity in very broad philosophical terms, encompassing natural, existential, and social creative processes, such as natural evolution and entrepreneurship, and explain why creativity understood in this way is instrumental for advancing human well-being in the long term. we then explain why current mainstream ai tends to be anti-creative, which means that there are moral costs of employing this type of ai in human endeavors, although computational systems that involve creativity are on the rise. in conclusion, there is an argument for ethics to be more hospitable to creativity-enabling ai, which can also be in a trade-off with other values promoted in ai ethics, such as its explainability and accuracy.", "categories": "cs.ai econ.gn q-fin.ec", "created": "2020-07-23", "updated": "", "authors": ["michele loi", "eleonora vigan\u00f2", "lonneke van der plas"], "url": "https://arxiv.org/abs/2007.11973"}, {"title": "the relationship between the economic and financial crises and   unemployment rate in the european union -- how institutions affected their   linkage", "id": "2007.12007", "abstract": "this paper aims to estimate the impact of economic and financial crises on the unemployment rate in the european union, taking also into consideration the institutional specificities, since unemployment was the main channel through which the economic and financial crisis influenced the social developments.. in this context, i performed two institutional clusters depending on their inclusive or extractive institutional features and, in each cases, i computed the crisis effect on unemployment rate over the 2003-2017 period. both models were estimated by using panel estimated generalized least squares method, and are weighted by period sur option in order to remove, in advance the possible inconveniences of the models. the institutions proved to be a relevant criterion that drives the impact of economic and financial crises on the unemployment rate, highlighting that countries with inclusive institutions are less vulnerable to economic shocks and are more resilient than countries with extractive institutions. the quality of institutions was also found to have a significant effect on the response of unemployment rate to the dynamic of its drivers.", "categories": "econ.gn q-fin.ec", "created": "2020-07-20", "updated": "", "authors": ["ionut jianu"], "url": "https://arxiv.org/abs/2007.12007"}, {"title": "generating empirical core size distributions of hedonic games using a   monte carlo method", "id": "2007.12127", "abstract": "data analytics allows an analyst to gain insight into underlying populations through the use of various computational approaches, including monte carlo methods. this paper discusses an approach to apply monte carlo methods to hedonic games. hedonic games have gain popularity over the last two decades leading to several research articles that are concerned with the necessary, sufficient, or both conditions of the existence of a core partition. researchers have used analytical methods for this work. we propose that using a numerical approach will give insights that might not be available through current analytical methods. in this paper, we describe an approach to representing hedonic games, with strict preferences, in a matrix form that can easily be generated; that is, a hedonic game with randomly generated preferences for each player. using this generative approach, we were able to create and solve, i.e., find any core partitions, of millions of hedonic games. our monte carlo experiment generated games with up to thirteen players. the results discuss the distribution form of the core size of the games of a given number of players. we also discuss computational considerations. our numerical study of hedonic games gives insight into the underlying properties of hedonic games.", "categories": "econ.th", "created": "2020-07-23", "updated": "", "authors": ["andrew j. collins", "sheida etemadidavan", "wael khallouli"], "url": "https://arxiv.org/abs/2007.12127"}, {"title": "online appendix & additional results for the determinants of social   connectedness in europe", "id": "2007.12177", "abstract": "in this online appendix we provide additional information and analyses to support \"the determinants of social connectedness in europe.\" we include a number of case studies illustrating how language, history, and other factors have shaped european social networks. we also look at the effects of social connectedness. our results provide empirical support for theoretical models that suggest social networks play an important role in individuals' travel decisions. we study variation in the degree of connectedness of regions to other european countries, finding a negative correlation between euroscepticism and greater levels of international connection.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-23", "updated": "", "authors": ["michael bailey", "drew johnston", "theresa kuchler", "dominic russel", "bogdan state", "johannes stroebel"], "url": "https://arxiv.org/abs/2007.12177"}, {"title": "understanding the dynamics emerging from infodemics: a call to action   for interdisciplinary research", "id": "2007.12226", "abstract": "research on infodemics, i.e., the rapid spread of (mis)information related to a hazardous event, such as the covid-19 pandemic, requires the integration of a multiplicity of scientific disciplines. the dynamics emerging from infodemics have the potential to generate complex behavioral patterns. in order to react appropriately, it is of ultimate importance for the fields of business and economics to understand the dynamics emerging from it. in the short run, dynamics might lead to an adaptation in household spending or to a shift in buying behavior towards online providers. in the long run, changes in investments, consumer behavior, and markets are to be expected. we argue that the dynamics emerge from complex interactions among multiple factors, such as information and misinformation accessible for individuals and the formation and revision of beliefs. (mis)information accessible to individuals is, amongst others, affected by algorithms specifically designed to provide personalized information, while automated fact-checking algorithms can help reduce the amount of circulating misinformation. the formation and revision of individual (and probably false) beliefs and individual fact-checking and interpretation of information are heavily affected by linguistic patterns inherent to information during pandemics and infodemics and further factors, such as affect, intuition and motives. we argue that, in order to get a deep(er) understanding of the dynamics emerging from infodemics, the fields of business and economics should integrate the perspectives of computer science and information systems, (computational) linguistics, and cognitive science into the wider context of economic systems (e.g., organizations, markets or industries) and propose a way to do so.", "categories": "physics.soc-ph cs.si econ.gn math.ds q-fin.ec", "created": "2020-07-23", "updated": "2020-10-01", "authors": ["stephan leitner", "bartosz gula", "dietmar jannach", "ulrike krieg-holz", "friederike wall"], "url": "https://arxiv.org/abs/2007.12226"}, {"title": "bootur: an r package for bootstrap unit root tests", "id": "2007.12249", "abstract": "unit root tests form an essential part of any time series analysis. we provide practitioners with a single, unified framework for comprehensive and reliable unit root testing in the r package bootur. the package's backbone is the popular augmented dickey-fuller (adf) test paired with a union of rejections principle, which can be performed directly on single time series or multiple (including panel) time series. accurate inference is ensured through the use of bootstrap methods.the package addresses the needs of both novice users, by providing user-friendly and easy-to-implement functions with sensible default options, as well as expert users, by giving full user-control to adjust the tests to one's desired settings. our openmp-parallelized efficient c++ implementation ensures that all unit root tests are scalable to datasets containing many time series.", "categories": "econ.em stat.co", "created": "2020-07-23", "updated": "2020-09-02", "authors": ["stephan smeekes", "ines wilms"], "url": "https://arxiv.org/abs/2007.12249"}, {"title": "home advantage in the brazilian elite football: verifying managers'   capacity to outperform their disadvantage", "id": "2007.12255", "abstract": "home advantage (ha) in football, soccer is well documented in the literature; however, the explanation for such phenomenon is yet to be completed, as this paper demonstrates that it is possible to overcome such disadvantage through managerial planning and intervention (tactics), an effect so far absent in the literature. to accomplish that, this study develops an integrative theoretical model of team performance to explain ha based on prior literature, pushing its limits to unfold the manager role in such a persistent pattern of performance in soccer. data on one decade of the national tournament of brazil was obtained from public sources, including information about matches and coaches of all 12 teams who played these competitions. our conceptual modeling allows an empirical analysis of ha and performance that covers the effects of tactics, presence of supporters in matches and team fatigue via logistic regression. the results confirm the ha in the elite division of brazilian soccer across all levels of comparative technical quality, a new variable introduced to control for a potential technical gap between teams (something that would turn the managerial influence null). further analysis provides evidence that highlights managerial capacity to block the ha effect above and beyond the influences of fatigue (distance traveled) and density of people in the matches. this is the case of coaches abel braga, marcelo fernandes and roger machado, who were capable to reverse ha when playing against teams of similar quality. overall, the home advantage diminishes as the comparative quality increases but disappears only when the two teams are of extremely different technical quality.", "categories": "econ.gn q-fin.ec", "created": "2020-07-23", "updated": "", "authors": ["carlos denner dos santos", "jessica alves"], "url": "https://arxiv.org/abs/2007.12255"}, {"title": "effects of dynamic capability and marketing strategy on the   organizational performance of the banking sector in makassar, indonesia", "id": "2007.12433", "abstract": "the dynamic capability and marketing strategy are challenges to the banking sector in indonesia. this study uses a survey method solving 39 banks in makassar. data collection was conducted of questionnaires. the results show that, the dynamic capability has a positive yet insignificant impact on the organizational performance, the marketing strategy has a positive and significant effect on organizational performance and, dynamic capability and marketing strategy have a positive and significant effect on the organization's performance in the banking sector in makassar. keywords : dynamic capability, marketing strategy, organizational performance, banking", "categories": "econ.gn q-fin.ec", "created": "2020-07-24", "updated": "", "authors": ["akhmad muhammadin", "rashila ramli", "syamsul ridjal", "muhlis kanto", "syamsul alam", "hamzah idris"], "url": "https://arxiv.org/abs/2007.12433"}, {"title": "the role of global economic policy uncertainty in predicting crude oil   futures volatility: evidence from a two-factor garch-midas model", "id": "2007.12838", "abstract": "this paper aims to examine whether the global economic policy uncertainty (gepu) and uncertainty changes have different impacts on crude oil futures volatility. we establish single-factor and two-factor models under the garch-midas framework to investigate the predictive power of gepu and gepu changes excluding and including realized volatility. the findings show that the models with rolling-window specification perform better than those with fixed-span specification. for single-factor models, the gepu index and its changes, as well as realized volatility, are consistent effective factors in predicting the volatility of crude oil futures. specially, gepu changes have stronger predictive power than the gepu index. for two-factor models, gepu is not an effective forecast factor for the volatility of wti crude oil futures or brent crude oil futures. the two-factor model with gepu changes contains more information and exhibits stronger forecasting ability for crude oil futures market volatility than the single-factor models. the gepu changes are indeed the main source of long-term volatility of the crude oil futures.", "categories": "q-fin.st econ.em", "created": "2020-07-24", "updated": "", "authors": ["peng-fei dai", "xiong xiong", "wei-xing zhou"], "url": "https://arxiv.org/abs/2007.12838"}, {"title": "myopic equilibria, the spanning property, and sublime bundles", "id": "2007.12876", "abstract": "for a set-valued function $f$ on a compact subset $w$ of a manifold, spanning is a topological property that implies that $f(x) \\ne 0$ for interior points $x$ of $w$. a myopic equilibrium applies when for each action there is a payoff whose functional value is not necessarily affine in the strategy space. we show that if the payoffs satisfy the spanning property, then there exist a myopic equilibrium (though not necessarily a nash equilibrium). furthermore, given a parametrized collection of games and the spanning property to the structure of payoffs in that collection, the resulting myopic equilibria and their payoffs have the spanning property with respect to that parametrization. this is a far reaching extension of the kohberg-mertens structure theorem. there are at least four useful applications, when payoffs are exogenous to a finite game tree (for example a finitely repeated game followed by an infinitely repeated game), when one wants to understand a game strategically entirely with behaviour strategies, when one wants to extends the subgame concept to subsets of a game tree that are known in common, and for evolutionary game theory. the proofs involve new topological results asserting that spanning is preserved by relevant operations on set-valued functions.", "categories": "econ.th math.at", "created": "2020-07-25", "updated": "", "authors": ["robert simon", "stanislaw spiez", "henryk torunczyk"], "url": "https://arxiv.org/abs/2007.12876"}, {"title": "catastrophe by design in population games: destabilizing wasteful   locked-in technologies", "id": "2007.12877", "abstract": "in multi-agent environments in which coordination is desirable, the history of play often causes lock-in at sub-optimal outcomes. notoriously, technologies with a significant environmental footprint or high social cost persist despite the successful development of more environmentally friendly and/or socially efficient alternatives. the displacement of the status quo is hindered by entrenched economic interests and network effects. to exacerbate matters, the standard mechanism design approaches based on centralized authorities with the capacity to use preferential subsidies to effectively dictate system outcomes are not always applicable to modern decentralized economies. what other types of mechanisms are feasible? in this paper, we develop and analyze a mechanism that induces transitions from inefficient lock-ins to superior alternatives. this mechanism does not exogenously favor one option over another -- instead, the phase transition emerges endogenously via a standard evolutionary learning model, q-learning, where agents trade-off exploration and exploitation. exerting the same transient influence to both the efficient and inefficient technologies encourages exploration and results in irreversible phase transitions and permanent stabilization of the efficient one. on a technical level, our work is based on bifurcation and catastrophe theory, a branch of mathematics that deals with changes in the number and stability properties of equilibria. critically, our analysis is shown to be structurally robust to significant and even adversarially chosen perturbations to the parameters of both our game and our behavioral model.", "categories": "cs.gt econ.th math.ds", "created": "2020-07-25", "updated": "", "authors": ["stefanos leonardos", "iosif sakos", "costas courcoubetis", "georgios piliouras"], "url": "https://arxiv.org/abs/2007.12877"}, {"title": "visibility graph analysis of economy policy uncertainty indices", "id": "2007.12880", "abstract": "uncertainty plays an important role in the global economy. in this paper, the economic policy uncertainty (epu) indices of the united states and china are selected as the proxy variable corresponding to the uncertainty of national economic policy. by adopting the visibility graph algorithm, the four economic policy uncertainty indices of the united states and china are mapped into complex networks, and the topological properties of the corresponding networks are studied. the hurst exponents of all the four indices are within $\\left[0.5,1\\right]$, which implies that the economic policy uncertainty is persistent. the degree distributions of the epu networks have power-law tails and are thus scale-free. the average clustering coefficients of the four epu networks are high and close to each other, while these networks exhibit weak assortative mixing. we also find that the epu network in united states based on daily data shows the small-world feature since the average shortest path length increases logarithmically with the network size such that $l\\left(n\\right)=0.626\\ln n+0.405$. our research highlights the possibility to study the epu from the view angle of complex networks.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2020-07-25", "updated": "", "authors": ["peng-fei dai", "xiong xiong", "wei-xing zhou"], "url": "https://arxiv.org/abs/2007.12880"}, {"title": "a well-timed switch from local to global agreements accelerates climate   change mitigation", "id": "2007.13238", "abstract": "recent attempts at cooperating on climate change mitigation highlight the limited efficacy of large-scale agreements, when commitment to mitigation is costly and initially rare. bottom-up approaches using region-specific mitigation agreements promise greater success, at the cost of slowing global adoption. here, we show that a well-timed switch from regional to global negotiations dramatically accelerates climate mitigation compared to using only local, only global, or both agreement types simultaneously. this highlights the scale-specific roles of mitigation incentives: local incentives capitalize on regional differences (e.g., where recent disasters incentivize mitigation) by committing early-adopting regions, after which global agreements draw in late-adopting regions. we conclude that global agreements are key to overcoming the expenses of mitigation and economic rivalry among regions but should be attempted once regional agreements are common. gradually up-scaling efforts could likewise accelerate mitigation at smaller scales, for instance when costly ecosystem restoration initially faces limited public and legislative support.", "categories": "nlin.ao econ.gn q-fin.ec", "created": "2020-07-26", "updated": "", "authors": ["vadim a. karatayev", "v\u00edtor v. vasconcelos", "anne-sophie lafuite", "simon a. levin", "chris t. bauch", "madhur anand"], "url": "https://arxiv.org/abs/2007.13238"}, {"title": "scalable bayesian estimation in the multinomial probit model", "id": "2007.13247", "abstract": "the multinomial probit model is a popular tool for analyzing choice behaviour as it allows for correlation between choice alternatives. because current model specifications employ a full covariance matrix of the latent utilities for the choice alternatives, they are not scalable to a large number of choice alternatives. this paper proposes a factor structure on the covariance matrix, which makes the model scalable to large choice sets. the main challenge in estimating this structure is that the model parameters require identifying restrictions. we identify the parameters by a trace-restriction on the covariance matrix, which is imposed through a reparametrization of the factor structure. we specify interpretable prior distributions on the model parameters and develop an mcmc sampler for parameter estimation. the proposed approach substantially improves performance in large choice sets relative to existing multinomial probit specifications. applications to purchase data show the economic importance of including a large number of choice alternatives in consumer choice analysis.", "categories": "econ.em", "created": "2020-07-26", "updated": "", "authors": ["ruben loaiza-maya", "didier nibbering"], "url": "https://arxiv.org/abs/2007.13247"}, {"title": "total error and variability measures for the quarterly workforce   indicators and lehd origin-destination employment statistics in onthemap", "id": "2007.13275", "abstract": "we report results from the first comprehensive total quality evaluation of five major indicators in the u.s. census bureau's longitudinal employer-household dynamics (lehd) program quarterly workforce indicators (qwi): total flow-employment, beginning-of-quarter employment, full-quarter employment, average monthly earnings of full-quarter employees, and total quarterly payroll. beginning-of-quarter employment is also the main tabulation variable in the lehd origin-destination employment statistics (lodes) workplace reports as displayed in onthemap (otm), including onthemap for emergency management. we account for errors due to coverage; record-level non-response; edit and imputation of item missing data; and statistical disclosure limitation. the analysis reveals that the five publication variables under study are estimated very accurately for tabulations involving at least 10 jobs. tabulations involving three to nine jobs are a transition zone, where cells may be fit for use with caution. tabulations involving one or two jobs, which are generally suppressed on fitness-for-use criteria in the qwi and synthesized in lodes, have substantial total variability but can still be used to estimate statistics for untabulated aggregates as long as the job count in the aggregate is more than 10.", "categories": "econ.em stat.me", "created": "2020-07-26", "updated": "", "authors": ["kevin l. mckinney", "andrew s. green", "lars vilhuber", "john m. abowd"], "url": "https://arxiv.org/abs/2007.13275"}, {"title": "the wage premium of communist party membership: evidence from china", "id": "2007.13549", "abstract": "social status and political connections could confer large economic benefits to an individual. previous studies focused on china examine the relationship between communist party membership and earnings and find a positive correlation. however, this correlation may be partly or totally spurious, thereby generating upwards-biased estimates of the importance of political party membership. using data from three surveys spanning more than three decades, we estimate the causal effect of chinese party membership on monthly earnings in in china. we find that, on average, membership in the communist party of china increases monthly earnings and we find evidence that the wage premium has grown in recent years. we explore for potential mechanisms and we find suggestive evidence that improvements in one's social network, acquisition of job-related qualifications and improvement in one's social rank and life satisfaction likely play an important role. (jel d31, j31, p2)", "categories": "econ.gn q-fin.ec", "created": "2020-07-23", "updated": "", "authors": ["plamen nikolov", "hongjian wang", "kevin acker"], "url": "https://arxiv.org/abs/2007.13549"}, {"title": "are low frequency macroeconomic variables important for high frequency   electricity prices?", "id": "2007.13566", "abstract": "we analyse the importance of low frequency hard and soft macroeconomic information, respectively the industrial production index and the manufacturing purchasing managers' index surveys, for forecasting high-frequency daily electricity prices in two of the main european markets, germany and italy. we do that by means of mixed-frequency models, introducing a bayesian approach to reverse unrestricted midas models (ru-midas). despite the general parsimonious structure of standard midas models, the ru-midas has a large set of parameters when several predictors are considered simultaneously and bayesian inference is useful for imposing parameter restrictions. we study the forecasting accuracy for different horizons (from $1$ day ahead to $28$ days ahead) and by considering different specifications of the models. results indicate that the macroeconomic low frequency variables are more important for short horizons than for longer horizons. moreover, accuracy increases by combining hard and soft information, and using only surveys gives less accurate forecasts than using only industrial production data.", "categories": "q-fin.st econ.em", "created": "2020-07-24", "updated": "", "authors": ["claudia foroni", "francesco ravazzolo", "luca rossini"], "url": "https://arxiv.org/abs/2007.13566"}, {"title": "the spectral approach to linear rational expectations models", "id": "2007.13804", "abstract": "this paper considers linear rational expectations models in the frequency domain. two classical results drive the entire theory: the kolmogorov-cram\\'{e}r spectral representation theorem and wiener-hopf factorization. the paper develops necessary and sufficient conditions for existence and uniqueness of particular and generic systems. the space of all solutions is characterized as an affine space in the frequency domain, which sheds light on the variety of solution methods considered in the literature. it is shown that solutions are not generally continuous with respect to the parameters of the models. this motivates regularized solutions with theoretically guaranteed smoothness properties. as an application, the limiting gaussian likelihood functions of solutions is derived analytically and its properties are studied. the paper finds that non-uniqueness leads to highly irregular likelihood functions and recommends either restricting the parameter space to the region of uniqueness or employing regularization.", "categories": "econ.em math.st stat.th", "created": "2020-07-27", "updated": "2020-08-13", "authors": ["majid m. al-sadoon"], "url": "https://arxiv.org/abs/2007.13804"}, {"title": "economic reality, economic media and individuals' expectations", "id": "2007.13823", "abstract": "this paper investigates the relationship between economic media sentiment and individuals' expetations and perceptions about economic conditions. we test if economic media sentiment granger-causes individuals' expectations and opinions concerning economic conditions, controlling for macroeconomic variables. we develop a measure of economic media sentiment using a supervised machine learning method on a data set of swedish economic media during the period 1993-2017. we classify the sentiment of 179,846 media items, stemming from 1,071 unique media outlets, and use the number of news items with positive and negative sentiment to construct a time series index of economic media sentiment. our results show that this index granger-causes individuals' perception of macroeconomic conditions. this indicates that the way the economic media selects and frames macroeconomic news matters for individuals' aggregate perception of macroeconomic reality.", "categories": "econ.gn q-fin.ec", "created": "2020-07-27", "updated": "", "authors": ["kristoffer persson"], "url": "https://arxiv.org/abs/2007.13823"}, {"title": "local projection inference is simpler and more robust than you think", "id": "2007.13888", "abstract": "applied macroeconomists often compute confidence intervals for impulse responses using local projections, i.e., direct linear regressions of future outcomes on current covariates. this paper proves that local projection inference robustly handles two issues that commonly arise in applications: highly persistent data and the estimation of impulse responses at long horizons. we consider local projections that control for lags of the variables in the regression. we show that lag-augmented local projections with normal critical values are asymptotically valid uniformly over (i) both stationary and non-stationary data, and also over (ii) a wide range of response horizons. moreover, lag augmentation obviates the need to correct standard errors for serial correlation in the regression residuals. hence, local projection inference is arguably both simpler than previously thought and more robust than standard autoregressive inference, whose validity is known to depend sensitively on the persistence of the data and on the length of the horizon.", "categories": "econ.em", "created": "2020-07-27", "updated": "", "authors": ["jos\u00e9 luis montiel olea", "mikkel plagborg-m\u00f8ller"], "url": "https://arxiv.org/abs/2007.13888"}, {"title": "leveraging the power of place: a data-driven decision helper to improve   the location decisions of economic immigrants", "id": "2007.13902", "abstract": "a growing number of countries have established programs to attract immigrants who can contribute to their economy. research suggests that an immigrant's initial arrival location plays a key role in shaping their economic success. yet immigrants currently lack access to personalized information that would help them identify optimal destinations. instead, they often rely on availability heuristics, which can lead to the selection of sub-optimal landing locations, lower earnings, elevated outmigration rates, and concentration in the most well-known locations. to address this issue and counteract the effects of cognitive biases and limited information, we propose a data-driven decision helper that draws on behavioral insights, administrative data, and machine learning methods to inform immigrants' location decisions. the decision helper provides personalized location recommendations that reflect immigrants' preferences as well as data-driven predictions of the locations where they maximize their expected earnings given their profile. we illustrate the potential impact of our approach using backtests conducted with administrative data that links landing data of recent economic immigrants from canada's express entry system with their earnings retrieved from tax records. simulations across various scenarios suggest that providing location recommendations to incoming economic immigrants can increase their initial earnings and lead to a mild shift away from the most populous landing destinations. our approach can be implemented within existing institutional structures at minimal cost, and offers governments an opportunity to harness their administrative data to improve outcomes for economic immigrants.", "categories": "cs.cy cs.lg econ.gn q-fin.ec stat.ap", "created": "2020-07-27", "updated": "", "authors": ["jeremy ferwerda", "nicholas adams-cohen", "kirk bansak", "jennifer fei", "duncan lawrence", "jeremy m. weinstein", "jens hainmueller"], "url": "https://arxiv.org/abs/2007.13902"}, {"title": "equilibrium behaviors in reputation games", "id": "2007.14002", "abstract": "we examine a patient player's behavior when he can build a reputation in front of a sequence of myopic opponents. with positive probability, the patient player is a commitment type who mechanically plays his stackelberg action in every period. we characterize the patient player's action frequencies in equilibrium. our results clarify the extent to which reputation effects can refine the patient player's equilibrium behavior.", "categories": "econ.th", "created": "2020-07-28", "updated": "2020-08-03", "authors": ["yingkai li", "harry pei"], "url": "https://arxiv.org/abs/2007.14002"}, {"title": "heterogeneity and the dynamic effects of aggregate shocks", "id": "2007.14022", "abstract": "using a semi-structural approach, the paper identifies how heterogeneity and financial frictions affect the transmission of aggregate shocks. approximating a heterogeneous agent model around the representative agent allocation can successfully trace the aggregate and distributional dynamics and can be consistent with alternative mechanisms. employing spanish macroeconomic data as well as firm and household survey data, the paper finds that frictions on both consumption and investment have rich interactions with aggregate shocks. the response of heterogeneity amplifies or dampens these effects depending on the type of the shock. both dispersion in consumption shares and the marginal revenue product of firms, as well as the proportion of investment constrained firms are key determinants of the fiscal multiplier.", "categories": "econ.gn q-fin.ec", "created": "2020-07-28", "updated": "", "authors": ["andreas tryphonides"], "url": "https://arxiv.org/abs/2007.14022"}, {"title": "epidemic response to physical distancing policies and their impact on   the outbreak risk", "id": "2007.14620", "abstract": "we introduce a theoretical framework that highlights the impact of physical distancing variables such as human mobility and physical proximity on the evolution of epidemics and, crucially, on the reproduction number. in particular, in response to the coronavirus disease (covid-19) pandemic, countries have introduced various levels of 'lockdown' to reduce the number of new infections. specifically we use a collisional approach to an infection-age structured model described by a renewal equation for the time homogeneous evolution of epidemics. as a result, we show how various contributions of the lockdown policies, namely physical proximity and human mobility, reduce the impact of sars-cov-2 and mitigate the risk of disease resurgence. we check our theoretical framework using real-world data on physical distancing with two different data repositories, obtaining consistent results. finally, we propose an equation for the effective reproduction number which takes into account types of interactions among people, which may help policy makers to improve remote-working organizational structure.", "categories": "physics.soc-ph econ.gn q-bio.pe q-fin.ec", "created": "2020-07-29", "updated": "2020-07-31", "authors": ["fabio vanni", "david lambert", "luigi palatella"], "url": "https://arxiv.org/abs/2007.14620"}, {"title": "a convergence analysis of the price of anarchy in atomic congestion   games", "id": "2007.14769", "abstract": "this paper provides a comprehensive convergence analysis of the poa of both pure and mixed nash equilibria in atomic congestion games with unsplittable demands.", "categories": "cs.gt econ.th", "created": "2020-07-28", "updated": "", "authors": ["zijun wu", "rolf h. moehring", "chunying ren", "dachuan xu"], "url": "https://arxiv.org/abs/2007.14769"}, {"title": "are less developed countries more likely to manipulate data during   pandemics? evidence from newcomb-benford law", "id": "2007.14841", "abstract": "we use the newcomb-benford law to test if countries manipulate reported data during the covid-19 pandemic. we find that democratic countries, countries with the higher gross domestic product (gdp) per capita, higher healthcare expenditures, and better universal healthcare coverage are less likely to deviate from the newcomb-benford law. the relationship holds for the cumulative number of deaths and for the cumulative number of total cases but is more pronounced for the death toll. the findings are robust for the second digit tests, for a sub-sample of countries with regional data, and during the previous swine flu (h1n1) 2009-2010 pandemic.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2020-07-29", "updated": "", "authors": ["vadim s. balashov", "yuxing yan", "xiaodi zhu"], "url": "https://arxiv.org/abs/2007.14841"}, {"title": "learning what they think vs. learning what they do: the   micro-foundations of vicarious learning", "id": "2007.15264", "abstract": "vicarious learning is a vital component of organizational learning. we theorize and model two fundamental processes underlying vicarious learning: observation of actions (learning what they do) vs. belief sharing (learning what they think). the analysis of our model points to three key insights. first, vicarious learning through either process is beneficial even when no agent in a system of vicarious learners begins with a knowledge advantage. second, vicarious learning through belief sharing is not universally better than mutual observation of actions and outcomes. specifically, enabling mutual observability of actions and outcomes is superior to sharing of beliefs when the task environment features few alternatives with large differences in their value and there are no time pressures. third, symmetry in vicarious learning in fact adversely affects belief sharing but improves observational learning. all three results are shown to be the consequence of how vicarious learning affects self-confirming biased beliefs.", "categories": "econ.th cs.ai", "created": "2020-07-30", "updated": "2020-07-30", "authors": ["sanghyun park", "phanish puranam"], "url": "https://arxiv.org/abs/2007.15264"}, {"title": "equilibrium oil market share under the covid-19 pandemic", "id": "2007.15265", "abstract": "equilibrium models for energy markets under uncertain demand and supply have attracted considerable attentions. this paper focuses on modelling crude oil market share under the covid-19 pandemic using two-stage stochastic equilibrium. we describe the uncertainties in the demand and supply by random variables and provide two types of production decisions (here-and-now and wait-and-see). the here-and-now decision in the first stage does not depend on the outcome of random events to be revealed in the future and the wait-and-see decision in the second stage is allowed to depend on the random events in the future and adjust the feasibility of the here-and-now decision in rare unexpected scenarios such as those observed during the covid-19 pandemic. we develop a fast algorithm to find a solution of the two-stage stochastic equilibrium. we show the robustness of the two-stage stochastic equilibrium model for forecasting the oil market share using the real market data from january 2019 to may 2020.", "categories": "math.oc econ.gn q-fin.ec", "created": "2020-07-30", "updated": "", "authors": ["xiaojun chen", "yun shi", "xiaozhou wang"], "url": "https://arxiv.org/abs/2007.15265"}, {"title": "measuring the effectiveness of us monetary policy during the covid-19   recession", "id": "2007.15419", "abstract": "the covid-19 recession that started in march 2020 led to an unprecedented decline in economic activity across the globe. to fight this recession, policy makers in central banks engaged in expansionary monetary policy. this paper asks whether the measures adopted by the us federal reserve (fed) have been effective in boosting real activity and calming financial markets. to measure these effects at high frequencies, we propose a novel mixed frequency vector autoregressive (mf-var) model. this model allows us to combine weekly and monthly information within an unified framework. our model combines a set of macroeconomic aggregates such as industrial production, unemployment rates and inflation with high frequency information from financial markets such as stock prices, interest rate spreads and weekly information on the feds balance sheet size. the latter set of high frequency time series is used to dynamically interpolate the monthly time series to obtain weekly macroeconomic measures. we use this setup to simulate counterfactuals in absence of monetary stimulus. the results show that the monetary expansion caused higher output growth and stock market returns, more favorable long-term financing conditions and a depreciation of the us dollar compared to a no-policy benchmark scenario.", "categories": "econ.em stat.ap", "created": "2020-07-30", "updated": "", "authors": ["martin feldkircher", "florian huber", "michael pfarrhofer"], "url": "https://arxiv.org/abs/2007.15419"}, {"title": "signaling with private monitoring", "id": "2007.15514", "abstract": "we study dynamic signaling when the informed party does not observe the signals generated by her actions. a long-run player signals her type continuously over time to a myopic second player who privately monitors her behavior; in turn, the myopic player transmits his private inferences back through an imperfect public signal of his actions. preferences are linear-quadratic and the information structure is gaussian. we construct linear markov equilibria using belief states up to the long-run player's $\\textit{second-order belief}$. because of the private monitoring, this state is an explicit function of the long-run player's past play. a novel separation effect then emerges through this second-order belief channel, altering the traditional signaling that arises when beliefs are public. applications to models of leadership, reputation, and trading are examined.", "categories": "econ.th", "created": "2020-07-30", "updated": "", "authors": ["gonzalo cisternas", "aaron kolb"], "url": "https://arxiv.org/abs/2007.15514"}, {"title": "combining distributive ethics and causal inference to make trade-offs   between austerity and population health", "id": "2007.15550", "abstract": "the international monetary fund (imf) provides financial assistance to its member-countries in economic turmoil, but requires at the same time that these countries reform their public policies. in several contexts, these reforms are at odds with population health. while researchers have empirically analyzed the consequences of these reforms on health, no analysis exist on identifying fair tradeoffs between consequences on population health and economic outcomes. our article analyzes and identifies the principles governing these tradeoffs. first, this article reviews existing policy-evaluation studies, which show, on balance, that imf policies frequently cause adverse effects on child health and material standards in the pursuit of macroeconmic improvement. second, this article discusses four theories in distributive ethics (maximization, egalitarianianism, prioritarianiasm, and sufficientarianism) to identify which is the most compatible with the core mission of the imf, that is, improved macroeconomics (articles of agreement) while at the same time balancing consequences on health. using a distributive-ethics analyses of imf polices, we argue that sufficientarianism is the most compatible theory. third, this article offer a qualitative rearticulation of the articles of agreement, and formalize sufficientarian principles in the language of causal inference. we also offer a framework on how to empirically measure, from observational data, the extent that imf policies trade off fairly between population health and economic outcomes. we conclude with policy recommendations and suggestions for future research.", "categories": "econ.gn q-fin.ec", "created": "2020-07-30", "updated": "2020-08-10", "authors": ["adel daoud", "anders herlitz", "sv subramanian"], "url": "https://arxiv.org/abs/2007.15550"}, {"title": "job market effects of covid-19 on urban ukrainian households", "id": "2007.15704", "abstract": "the employment status of billions of people has been affected by the covid epidemic around the globe. new evidence is needed on how to mitigate the job market crisis, but there exists only a handful of studies mostly focusing on developed countries. we fill in this gap in the literature by using novel data from ukraine, a transition country in eastern europe, which enacted strict quarantine policies early on. we model four binary outcomes to identify respondents (i) who are not working during quarantine, (ii) those who are more likely to work from home, (iii) respondents who are afraid of losing a job, and, finally, (iv) survey participants who have savings for 1 month or less if quarantine is further extended. our findings suggest that respondents employed in public administration, programming and it, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine. females, better educated respondents, and those who lived in kyiv were more likely to work remotely. working in the public sector also made people more confident about their future employment perspectives. although our findings are limited to urban households only, they provide important early evidence on the correlates of job market outcomes, expectations, and financial security, indicating potential deterioration of socio-economic inequalities.", "categories": "econ.gn q-fin.ec", "created": "2020-07-30", "updated": "", "authors": ["tymofii brik", "maksym obrizan"], "url": "https://arxiv.org/abs/2007.15704"}, {"title": "truthful equilibria in generalized common agency models", "id": "2007.15942", "abstract": "in this paper i discuss truthful equilibria in common agency models. specifically, i provide general conditions under which truthful equilibria are plausible, easy to calculate and efficient. these conditions generalize similar results in the literature and allow the use of truthful equilibria in novel economic applications. moreover, i provide two such applications. the first application is a market game in which multiple sellers sell a uniform good to a single buyer. the second application is a lobbying model in which there are externalities in contributions between lobbies. this last example indicates that externalities between principals do not necessarily prevent efficient equilibria. in this regard, this paper provides a set of conditions, under which, truthful equilibria in common agency models with externalities are efficient.", "categories": "econ.th", "created": "2020-07-31", "updated": "", "authors": ["ilias boultzis"], "url": "https://arxiv.org/abs/2007.15942"}, {"title": "the hansen ratio in mean--variance portfolio theory", "id": "2007.15980", "abstract": "it is shown that the ratio between the mean and the $l^2$-norm leads to a particularly parsimonious description of the mean-variance efficient frontier and the dual pricing kernel restrictions known as the hansen-jagannathan (hj) bounds. because this ratio has not appeared in economic theory previously, it seems appropriate to name it the hansen ratio. the initial treatment of the mean-variance theory via the hansen ratio is extended in two directions, to monotone mean-variance preferences and to arbitrary hilbert space setting. a multiperiod example with iid returns is also discussed.", "categories": "q-fin.pm econ.gn q-fin.ec q-fin.gn q-fin.mf", "created": "2020-07-31", "updated": "", "authors": ["ale\u0161 \u010dern\u00fd"], "url": "https://arxiv.org/abs/2007.15980"}, {"title": "on single point forecasts for fat-tailed variables", "id": "2007.16096", "abstract": "we discuss common errors and fallacies when using naive \"evidence based\" empiricism and point forecasts for fat-tailed variables, as well as the insufficiency of using naive first-order scientific methods for tail risk management. we use the covid-19 pandemic as the background for the discussion and as an example of a phenomenon characterized by a multiplicative nature, and what mitigating policies must result from the statistical properties and associated risks. in doing so, we also respond to the points raised by ioannidis et al. (2020).", "categories": "physics.soc-ph econ.gn q-fin.ec stat.ap stat.me", "created": "2020-07-31", "updated": "", "authors": ["nassim nicholas taleb", "yaneer bar-yam", "pasquale cirillo"], "url": "https://arxiv.org/abs/2007.16096"}, {"title": "lookahead and hybrid sample allocation procedures for multiple attribute   selection decisions", "id": "2007.16119", "abstract": "attributes provide critical information about the alternatives that a decision-maker is considering. when their magnitudes are uncertain, the decision-maker may be unsure about which alternative is truly the best, so measuring the attributes may help the decision-maker make a better decision. this paper considers settings in which each measurement yields one sample of one attribute for one alternative. when given a fixed number of samples to collect, the decision-maker must determine which samples to obtain, make the measurements, update prior beliefs about the attribute magnitudes, and then select an alternative. this paper presents the sample allocation problem for multiple attribute selection decisions and proposes two sequential, lookahead procedures for the case in which discrete distributions are used to model the uncertain attribute magnitudes. the two procedures are similar but reflect different quality measures (and loss functions), which motivate different decision rules: (1) select the alternative with the greatest expected utility and (2) select the alternative that is most likely to be the truly best alternative. we conducted a simulation study to evaluate the performance of the sequential procedures and hybrid procedures that first allocate some samples using a uniform allocation procedure and then use the sequential, lookahead procedure. the results indicate that the hybrid procedures are effective; allocating many (but not all) of the initial samples with the uniform allocation procedure not only reduces overall computational effort but also selects alternatives that have lower average opportunity cost and are more often truly best.", "categories": "econ.th cs.ai", "created": "2020-07-31", "updated": "", "authors": ["jeffrey w. herrmann", "kunal mehta"], "url": "https://arxiv.org/abs/2007.16119"}, {"title": "ergodic annealing", "id": "2008.00234", "abstract": "simulated annealing is the crowning glory of markov chain monte carlo methods for the solution of np-hard optimization problems in which the cost function is known. here, by replacing the metropolis engine of simulated annealing with a reinforcement learning variation -- that we call macau algorithm -- we show that the simulated annealing heuristic can be very effective also when the cost function is unknown and has to be learned by an artificial agent.", "categories": "cs.ai econ.th math.pr stat.ml", "created": "2020-08-01", "updated": "", "authors": ["carlo baldassi", "fabio maccheroni", "massimo marinacci", "marco pirazzini"], "url": "https://arxiv.org/abs/2008.00234"}, {"title": "male earnings volatility in lehd before, during, and after the great   recession", "id": "2008.00253", "abstract": "this paper is part of a coordinated collection of papers on prime-age male earnings volatility. each paper produces a similar set of statistics for the same reference population using a different primary data source. our primary data source is the census bureau's longitudinal employer-household dynamics (lehd) infrastructure files. using lehd data from 1998 to 2016, we create a well-defined population frame to facilitate accurate estimation of temporal changes comparable to designed longitudinal samples of people. we show that earnings volatility, excluding increases during recessions, has declined over the analysis period, a finding robust to various sensitivity analyses. although we find volatility is declining, the effect is not homogeneous, particularly for workers with tenuous labor force attachment for whom volatility is increasing. these \"not stable\" workers have earnings volatility approximately 30 times larger than stable workers, but more important for earnings volatility trends we observe a large increase in the share of stable employment from 60% in 1998 to 67% in 2016, which we show to largely be responsible for the decline in overall earnings volatility. to further emphasize the importance of not stable and/or low earning workers we also conduct comparisons with the psid and show how changes over time in the share of workers at the bottom tail of the cross-sectional earnings distributions can produce either declining or increasing earnings volatility trends.", "categories": "econ.gn q-fin.ec stat.ap", "created": "2020-08-01", "updated": "2020-08-12", "authors": ["kevin l. mckinney", "john m. abowd"], "url": "https://arxiv.org/abs/2008.00253"}, {"title": "simpler proofs for approximate factor models of large dimensions", "id": "2008.00254", "abstract": "estimates of the approximate factor model are increasingly used in empirical work. their theoretical properties, studied some twenty years ago, also laid the ground work for analysis on large dimensional panel data models with cross-section dependence. this paper presents simplified proofs for the estimates by using alternative rotation matrices, exploiting properties of low rank matrices, as well as the singular value decomposition of the data in addition to its covariance structure. these simplifications facilitate interpretation of results and provide a more friendly introduction to researchers new to the field. new results are provided to allow linear restrictions to be imposed on factor models.", "categories": "econ.em stat.me", "created": "2020-08-01", "updated": "", "authors": ["jushan bai", "serena ng"], "url": "https://arxiv.org/abs/2008.00254"}, {"title": "what can we learn about sars-cov-2 prevalence from testing and hospital   data?", "id": "2008.00298", "abstract": "measuring the prevalence of active sars-cov-2 infections is difficult because tests are conducted on a small and non-random segment of the population. but people admitted to the hospital for non-covid reasons are tested at very high rates, even though they do not appear to be at elevated risk of infection. this sub-population may provide valuable evidence on prevalence in the general population. we estimate upper and lower bounds on the prevalence of the virus in the general population and the population of non-covid hospital patients under weak assumptions on who gets tested, using indiana data on hospital inpatient records linked to sars-cov-2 virological tests. the non-covid hospital population is tested fifty times as often as the general population. by mid-june, we estimate that prevalence was between 0.01 and 4.1 percent in the general population and between 0.6 to 2.6 percent in the non-covid hospital population. we provide and test conditions under which this non-covid hospitalization bound is valid for the general population. the combination of clinical testing data and hospital records may contain much more information about the state of the epidemic than has been previously appreciated. the bounds we calculate for indiana could be constructed at relatively low cost in many other states.", "categories": "econ.em stat.ap", "created": "2020-08-01", "updated": "", "authors": ["daniel w. sacks", "nir menachemi", "peter embi", "coady wing"], "url": "https://arxiv.org/abs/2008.00298"}, {"title": "fair allocation of vaccines, ventilators and antiviral treatments:   leaving no ethical value behind in health care rationing", "id": "2008.00374", "abstract": "covid-19 has revealed several limitations of existing mechanisms for rationing scarce medical resources under emergency scenarios. many argue that they abandon various ethical values such as equity by discriminating against disadvantaged communities. illustrating that these limitations are aggravated by a restrictive choice of mechanism, we formulate pandemic rationing of medical resources as a new application of market design and propose a reserve system as a resolution. we develop a general theory of reserve design, introduce new concepts such as cutoff equilibria and smart reserves, extend previously-known ones such as sequential reserve matching, and relate these concepts to current debates.", "categories": "econ.th", "created": "2020-08-01", "updated": "", "authors": ["parag a. pathak", "tayfun s\u00f6nmez", "m. utku \u00fcnver", "m. bumin yenmez"], "url": "https://arxiv.org/abs/2008.00374"}, {"title": "robust sequential search", "id": "2008.00502", "abstract": "we study sequential search without priors. our interest lies in decision rules that are close to being optimal under each prior and after each history. we call these rules dynamically robust. the search literature employs optimal rules based on cutoff strategies that are not dynamically robust. we derive dynamically robust rules and show that their performance exceeds 1/2 of the optimum against binary environments and 1/4 of the optimum against all environments. this performance improves substantially with the outside option value, for instance, it exceeds 2/3 of the optimum if the outside option exceeds 1/6 of the highest possible alternative.", "categories": "econ.th", "created": "2020-08-02", "updated": "", "authors": ["karl h. schlag", "andriy zapechelnyuk"], "url": "https://arxiv.org/abs/2008.00502"}, {"title": "design-based uncertainty for quasi-experiments", "id": "2008.00602", "abstract": "social scientists are often interested in estimating causal effects in settings where all units in the population are observed (e.g. all 50 us states). design-based approaches, which view the treatment as the random object of interest, may be more appealing than standard sampling-based approaches in such contexts. this paper develops a design-based theory of uncertainty suitable for quasi-experimental settings, in which the researcher estimates the treatment effect as if treatment was randomly assigned, but in reality treatment probabilities may depend in unknown ways on the potential outcomes. we first study the properties of the simple difference-in-means (sdim) estimator. the sdim is unbiased for a finite-population design-based analog to the average treatment effect on the treated (att) if treatment probabilities are uncorrelated with the potential outcomes in a finite population sense. we further derive expressions for the variance of the sdim estimator and a central limit theorem under sequences of finite populations with growing sample size. we then show how our results can be applied to analyze the distribution and estimand of difference-in-differences (did) and two-stage least squares (2sls) from a design-based perspective when treatment is not completely randomly assigned.", "categories": "econ.em stat.me", "created": "2020-08-02", "updated": "2020-08-04", "authors": ["ashesh rambachan", "jonathan roth"], "url": "https://arxiv.org/abs/2008.00602"}, {"title": "a spatial multinomial logit model for analysing urban expansion", "id": "2008.00673", "abstract": "the paper proposes a bayesian multinomial logit model to analyse spatial patterns of urban expansion. the specification assumes that the log-odds of each class follow a spatial autoregressive process. using recent advances in bayesian computing, our model allows for a computationally efficient treatment of the spatial multinomial logit model. this allows us to assess spillovers between regions and across land use classes. in a series of monte carlo studies, we benchmark our model against other competing specifications. the paper also showcases the performance of the proposed specification using european regional data. our results indicate that spatial dependence plays a key role in land sealing process of cropland and grassland. moreover, we uncover land sealing spillovers across multiple classes of arable land.", "categories": "econ.em", "created": "2020-08-03", "updated": "", "authors": ["tam\u00e1s krisztin", "philipp piribauer", "michael w\u00f6gerer"], "url": "https://arxiv.org/abs/2008.00673"}, {"title": "estimating tvp-var models with time invariant long-run multipliers", "id": "2008.00718", "abstract": "the main goal of this paper is to develop a methodology for estimating time varying parameter vector auto-regression (tvp-var) models with a timeinvariant long-run relationship between endogenous variables and changes in exogenous variables. we propose a gibbs sampling scheme for estimation of model parameters as well as time-invariant long-run multiplier parameters. further we demonstrate the applicability of the proposed method by analyzing examples of the norwegian and russian economies based on the data on real gdp, real exchange rate and real oil prices. our results show that incorporating the time invariance constraint on the long-run multipliers in tvp-var model helps to significantly improve the forecasting performance.", "categories": "econ.em stat.ap", "created": "2020-08-03", "updated": "", "authors": ["denis belomestny", "ekaterina krymova", "andrey polbin"], "url": "https://arxiv.org/abs/2008.00718"}, {"title": "testing error distribution by kernelized stein discrepancy in   multivariate time series models", "id": "2008.00747", "abstract": "knowing the error distribution is important in many multivariate time series applications. to alleviate the risk of error distribution mis-specification, testing methodologies are needed to detect whether the chosen error distribution is correct. however, the majority of the existing tests only deal with the multivariate normal distribution for some special multivariate time series models, and they thus can not be used to testing for the often observed heavy-tailed and skewed error distributions in applications. in this paper, we construct a new consistent test for general multivariate time series models, based on the kernelized stein discrepancy. to account for the estimation uncertainty and unobserved initial values, a bootstrap method is provided to calculate the critical values. our new test is easy-to-implement for a large scope of multivariate error distributions, and its importance is illustrated by simulated and real data.", "categories": "econ.em stat.me", "created": "2020-08-03", "updated": "", "authors": ["donghang luo", "ke zhu", "huan gong", "dong li"], "url": "https://arxiv.org/abs/2008.00747"}, {"title": "testing semi-strong form efficiency of the prewar japanese stock market", "id": "2008.00860", "abstract": "this paper examines fama's (1970) semi-strong form efficient market hypothesis (emh) in the prewar japanese stock market using a new dataset. we particularly focus on the relationship between the prewar japanese stock market and several government policy interventions to explore whether the semi-strong form stock market efficiency evolves over time. to capture the long-run impact of government policy interventions against stock markets, we measure the time-varying joint degree of market efficiency and the time-varying impulse responses based on ito et al.'s (2014; 2017) generalized least squares-based time-varying vector autoregressive model. the empirical results reveal that (1) the joint degree of market efficiency in the prewar japanese stock market fluctuated over time because of external events such as policy changes and wars, (2) the semi-strong form emh is almost supported in the prewar japanese stock market, and (3) the markets rapidly reflect the information of the external events through time. therefore, we conclude that lo's (2004) adaptive market hypothesis is supported in the prewar japanese stock market even if we consider that the public information affects the stock market.", "categories": "q-fin.st econ.gn q-fin.ec q-fin.pr", "created": "2020-08-03", "updated": "", "authors": ["kenichi hirayama", "akihiko noda"], "url": "https://arxiv.org/abs/2008.00860"}, {"title": "existence and uniqueness of recursive utilities without boundedness", "id": "2008.00963", "abstract": "this paper derives primitive, easily verifiable sufficient conditions for existence and uniqueness of recursive utilities for a number of important classes of preferences. in order to accommodate models commonly used in practice, we allow both the statespace and per-period utilities to be unbounded. for many of the models we study, existence and uniqueness is established under a single \"thin tail\" condition on the distribution of growth in per-period utilities. we illustrate our approach with applications to robust preferences, models of ambiguity aversion and learning about hidden states, dynamic discrete choice models, and epstein-zin preferences.", "categories": "econ.th", "created": "2020-07-30", "updated": "", "authors": ["timothy m. christensen"], "url": "https://arxiv.org/abs/2008.00963"}, {"title": "making decisions under model misspecification", "id": "2008.01071", "abstract": "we use decision theory to confront uncertainty that is sufficiently broad to incorporate \"models as approximations.\" we presume the existence of a featured collection of what we call \"structured models\" that have explicit substantive motivations. the decision maker confronts uncertainty through the lens of these models, but also views these models as simplifications, and hence, as misspecified. we extend min-max analysis under model ambiguity to incorporate the uncertainty induced by acknowledging that the models used in decision-making are simplified approximations. formally, we provide an axiomatic rationale for a decision criterion that incorporates model misspecification concerns.", "categories": "econ.th math.oc", "created": "2020-08-01", "updated": "2020-08-27", "authors": ["simone cerreia-vioglio", "lars peter hansen", "fabio maccheroni", "massimo marinacci"], "url": "https://arxiv.org/abs/2008.01071"}, {"title": "weighted accuracy algorithmic approach in counteracting fake news and   disinformation", "id": "2008.01535", "abstract": "as the world is becoming more dependent on the internet for information exchange, some overzealous journalists, hackers, bloggers, individuals and organizations tend to abuse the gift of free information environment by polluting it with fake news, disinformation and pretentious content for their own agenda. hence, there is the need to address the issue of fake news and disinformation with utmost seriousness. this paper proposes a methodology for fake news detection and reporting through a constraint mechanism that utilizes the combined weighted accuracies of four machine learning algorithms.", "categories": "cs.cl cs.si econ.gn q-fin.ec", "created": "2020-07-30", "updated": "2020-08-04", "authors": ["kwadwo osei bonsu"], "url": "https://arxiv.org/abs/2008.01535"}, {"title": "distributionally robust pricing in independent private value auctions", "id": "2008.01618", "abstract": "a seller chooses a reserve price in a second-price auction to maximize worst-case expected revenue when she knows only the mean of value distribution and an upper bound on either values themselves or variance. values are private and iid. using an indirect technique, we prove that it is always optimal to set the reserve price to the seller's own valuation. however, the maxmin reserve price may not be unique. if the number of bidders is sufficiently high, all prices below the seller's valuation, including zero, are also optimal. a second-price auction with the reserve equal to seller's value (or zero) is an asymptotically optimal mechanism (among all ex post individually rational mechanisms) as the number of bidders grows without bound.", "categories": "econ.th cs.gt", "created": "2020-08-04", "updated": "2020-08-07", "authors": ["alex suzdaltsev"], "url": "https://arxiv.org/abs/2008.01618"}, {"title": "anxiety for the pandemic and trust in financial markets", "id": "2008.01649", "abstract": "the covid-19 pandemic has generated disruptive changes in many fields. here we focus on the relationship between the anxiety felt by people during the pandemic and the trust in the future performance of financial markets. precisely, we move from the idea that the volume of google searches about \"coronavirus\" can be considered as a proxy of the anxiety and, jointly with the stock index prices, can be used to produce mood indicators -- in terms of pessimism and optimism -- at country level. we analyse the \"very high human developed countries\" according to the human development index plus china and their respective main stock market indexes. namely, we propose both a temporal and a global measure of pessimism and optimism and provide accordingly a classification of indexes and countries. the results show the existence of different clusters of countries and markets in terms of pessimism and optimism. moreover, specific regimes along the time emerge, with an increasing optimism spreading during the mid of june 2020. furthermore, countries with different government responses to the pandemic have experienced different levels of mood indicators, so that countries with less strict lockdown had a higher level of optimism.", "categories": "q-fin.st econ.gn q-fin.ec", "created": "2020-08-01", "updated": "", "authors": ["roy cerqueti", "valerio ficcadenti"], "url": "https://arxiv.org/abs/2008.01649"}, {"title": "macroeconomic data transformations matter", "id": "2008.01714", "abstract": "from a purely predictive standpoint, rotating the predictors' matrix in a low-dimensional linear regression setup does not alter predictions. however, when the forecasting technology either uses shrinkage or is non-linear, it does. this is precisely the fabric of the machine learning (ml) macroeconomic forecasting environment. pre-processing of the data translates to an alteration of the regularization -- explicit or implicit -- embedded in ml algorithms. we review old transformations and propose new ones, then empirically evaluate their merits in a substantial pseudo-out-sample exercise. it is found that traditional factors should almost always be included in the feature matrix and moving average rotations of the data can provide important gains for various forecasting targets.", "categories": "econ.em stat.ap stat.ml", "created": "2020-08-04", "updated": "", "authors": ["philippe goulet coulombe", "maxime leroux", "dalibor stevanovic", "st\u00e9phane surprenant"], "url": "https://arxiv.org/abs/2008.01714"}, {"title": "understanding the relationship between social distancing policies,   traffic volume, air quality, and the prevalence of covid-19 outcomes in urban   neighborhoods", "id": "2008.01828", "abstract": "in response to the covid-19 pandemic, governments have implemented policies to curb the spread of the novel virus. little is known about how these policies impact various groups in society. this paper explores the relationship between social distancing policies, traffic volumes and air quality and how they impact various socioeconomic groups. this study aims to understand how disparate communities respond to stay-at-home orders and other social distancing policies to understand how human behavior in response to policy may play a part in the prevalence of covid-19 positive cases. we collected data on traffic density, air quality, socio-economic status, and positive cases rates of covid-19 for each zip code of salt lake county, utah (usa) between february 17 and june 12, 2020. we studied the impact of social distancing policies across three periods of policy implementation. we found that wealthier and whiter zip codes experienced a greater reduction in traffic and air pollution during the stay-at-home period. however, air quality did not necessarily follow traffic volumes in every case due to the complexity of interactions between emissions and meteorology. we also found a strong relationship between lower socioeconomic status and positive covid-19 rates. this study provides initial evidence for social distancing's effectiveness in limiting the spread of covid-19, while providing insight into how socioeconomic status has compounded vulnerability during this crisis. behavior restrictions disproportionately benefit whiter and wealthier communities both through protection from spread of covid-19 and reduction in air pollution. such findings may be further compounded by the impacts of air pollution, which likely exacerbate covid-19 transmission and mortality rates. policy makers need to consider adapting social distancing policies to maximize equity in health protection.", "categories": "physics.soc-ph cs.cy econ.gn q-bio.pe q-fin.ec", "created": "2020-07-15", "updated": "", "authors": ["daniel l. mendoza", "tabitha m. benney", "rajive ganguli", "rambabu pothina", "benjamin krick", "cheryl s. pirozzi", "erik t. crosman", "yue zhang"], "url": "https://arxiv.org/abs/2008.01828"}, {"title": "geometry of anonymous binary social choices that are strategy-proof", "id": "2008.02041", "abstract": "let $v$ be society whose members express preferences about two alternatives, indifference included. identifying anonymous binary social choice functions with binary functions $f=f(k,m)$ defined over the integer triangular grid $g=\\{(k,m)\\in \\mathbb{n}_0\\times\\mathbb{n}_0 : k+m\\le |v|\\} $, we show that every strategy-proof, anonymous social choice function can be described geometrically by listing, in a sequential manner, groups of segments of g, of equal (maximum possible) length, alternately horizontal and vertical, representative of preference profiles that determine the collective choice of one of the two alternatives. indeed, we show that every function which is anonymous and strategy-proof can be described in terms of a sequence of nonnegative integers $(q_1, q_2, \\cdots, q_s)$ corresponding to the cardinalities of the mentioned groups of segments. we also analyze the connections between our present representation with another of our earlier representations involving sequences of majority quotas.   a python code is available with the authors for the implementation of any such social choice function.", "categories": "econ.th math.co", "created": "2020-08-05", "updated": "", "authors": ["achille basile", "surekha rao", "k. p. s. bhaskara rao"], "url": "https://arxiv.org/abs/2008.02041"}, {"title": "identifying opportunities to improve the network of immigration legal   services providers", "id": "2008.02230", "abstract": "immigration legal services providers (isps) are a principal source of support for low-income immigrants seeking immigration benefits. yet there is scant quantitative evidence on the prevalence and geographic distribution of isps in the united states. to fill this gap, we construct a comprehensive, nationwide database of 2,138 geocoded isp offices that offer low- or no-cost legal services to low-income immigrants. we use spatial optimization methods to analyze the geographic network of isps and measure isps' proximity to the low-income immigrant population. because both isps and immigrants are highly concentrated in major urban areas, most low-income immigrants live close to an isp. however, we also find a sizable fraction of low-income immigrants in underserved areas, which are primarily in midsize cities in the south. this reflects both a general skew in non-governmental organization service provision and the more recent arrival of immigrants in these largely southern destinations. finally, our optimization analysis suggests significant gains from placing new isps in underserved areas to maximize the number of low-income immigrants who live near an isp. overall, our results provide vital information to immigrants, funders, and policymakers about the current state of the isp network and opportunities to improve it.", "categories": "econ.gn q-fin.ec", "created": "2020-08-05", "updated": "", "authors": ["vasil yasenov", "david hausman", "michael hotard", "duncan lawrence", "alexandra siegel", "jessica s. wolff", "david d. laitin", "jens hainmueller"], "url": "https://arxiv.org/abs/2008.02230"}, {"title": "applying data synthesis for longitudinal business data across three   countries", "id": "2008.02246", "abstract": "data on businesses collected by statistical agencies are challenging to protect. many businesses have unique characteristics, and distributions of employment, sales, and profits are highly skewed. attackers wishing to conduct identification attacks often have access to much more information than for any individual. as a consequence, most disclosure avoidance mechanisms fail to strike an acceptable balance between usefulness and confidentiality protection. detailed aggregate statistics by geography or detailed industry classes are rare, public-use microdata on businesses are virtually inexistant, and access to confidential microdata can be burdensome. synthetic microdata have been proposed as a secure mechanism to publish microdata, as part of a broader discussion of how to provide broader access to such data sets to researchers. in this article, we document an experiment to create analytically valid synthetic data, using the exact same model and methods previously employed for the united states, for data from two different countries: canada (leap) and germany (bhp). we assess utility and protection, and provide an assessment of the feasibility of extending such an approach in a cost-effective way to other data.", "categories": "econ.em", "created": "2020-07-24", "updated": "", "authors": ["m. jahangir alam", "benoit dostie", "j\u00f6rg drechsler", "lars vilhuber"], "url": "https://arxiv.org/abs/2008.02246"}, {"title": "on the size control of the hybrid test for predictive ability", "id": "2008.02318", "abstract": "we show that the hybrid test for superior predictability is not pointwise asymptotically of level under standard conditions, and may lead to rejection rates over 11% when the significance level $\\alpha$ is 5% in a simple case. we propose a modified hybrid test which is uniformly asymptotically of level $\\alpha$ by properly adapting the generalized moment selection method.", "categories": "econ.em", "created": "2020-08-05", "updated": "", "authors": ["deborah kim"], "url": "https://arxiv.org/abs/2008.02318"}, {"title": "teaching economics with interactive browser-based models", "id": "2008.02581", "abstract": "interactive simulation toolkits come in handy when teaching macroeconomic models by facilitating an easy understanding of underlying economic concepts and offering an intuitive approach to the models' comparative statics. based on the example of the is-lm model, this paper demonstrates innovative browser-based features well-suited for the shift in education to online platforms accelerated by covid-19. the free and open-source code can be found alongside the standalone html files for the ad-as and the solow growth model at https://gitlab.tu-berlin.de/chair-of-macroeconomics/.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-08-06", "updated": "", "authors": ["juan dominguez-moran", "rouven geismar"], "url": "https://arxiv.org/abs/2008.02581"}, {"title": "an upper bound for functions of estimators in high dimensions", "id": "2008.02636", "abstract": "we provide an upper bound as a random variable for the functions of estimators in high dimensions. this upper bound may help establish the rate of convergence of functions in high dimensions. the upper bound random variable may converge faster, slower, or at the same rate as estimators depending on the behavior of the partial derivative of the function. we illustrate this via three examples. the first two examples use the upper bound for testing in high dimensions, and third example derives the estimated out-of-sample variance of large portfolios. all our results allow for a larger number of parameters, p, than the sample size, n.", "categories": "econ.em", "created": "2020-08-06", "updated": "", "authors": ["mehmet caner", "xu han"], "url": "https://arxiv.org/abs/2008.02636"}, {"title": "pricing group membership", "id": "2008.03102", "abstract": "we consider a model where agents differ in their `types' which determines their voluntary contribution towards a public good. we analyze what the equilibrium composition of groups are under centralized and centralized choice. we show that there exists a top-down sorting equilibrium i.e. an equilibrium where there exists a set of prices which leads to groups that can be ordered by level of types, with the first k types in the group with the highest price and so on. this exists both under decentralized and centralized choosing. we also analyze the model with endogenous group size and examine under what conditions is top-down sorting socially efficient. we illustrate when integration (i.e. mixing types so that each group's average type if the same) is socially better than top-down sorting. finally, we show that top down sorting is efficient even when groups compete among themselves.", "categories": "econ.th cs.gt", "created": "2020-08-05", "updated": "", "authors": ["siddhartha bandyopadhyay", "antonio cabrales"], "url": "https://arxiv.org/abs/2008.03102"}, {"title": "covid-19: what if immunity wanes?", "id": "2008.03283", "abstract": "using a simple economic model in which social-distancing reduces contagion, we study the implications of waning immunity for the epidemiological dynamics and social activity. if immunity wanes, we find that covid-19 likely becomes endemic and that social-distancing is here to stay until the discovery of a vaccine or cure. but waning immunity does not necessarily change optimal actions on the onset of the pandemic. decentralized equilibria are virtually independent of waning immunity until close to peak infections. for centralized equilibria, the relevance of waning immunity decreases in the probability of finding a vaccine or cure, the costs of infection (e.g., infection-fatality rate), and the presence of other npis that lower contagion (e.g., quarantining and mask use). in simulations calibrated to july 2020, our model suggests that waning immunity is virtually unimportant for centralized equilibria until at least 2021. this provides vital time for individuals and policymakers to learn about immunity against sars-cov-2 before it becomes critical.", "categories": "econ.gn physics.soc-ph q-bio.pe q-fin.ec", "created": "2020-08-07", "updated": "", "authors": ["m. alper \u00e7enesiz", "lu\u00eds guimar\u00e3es"], "url": "https://arxiv.org/abs/2008.03283"}, {"title": "radner equilibrium and systems of quadratic bsdes with discontinuous   generators", "id": "2008.03500", "abstract": "motivated by an equilibrium problem, we establish the existence of a solution for a family of markovian backward stochastic differential equations with quadratic nonlinearity and discontinuity in $z$. using unique continuation and backward uniqueness, we show that the set of discontinuity has measure zero. in a continuous-time stochastic model of an endowment economy, we prove the existence of an incomplete radner equilibrium with nondegenerate endogenous volatility.", "categories": "math.pr econ.th q-fin.mf", "created": "2020-08-08", "updated": "2020-09-02", "authors": ["luis escauriaza", "daniel c. schwarz", "hao xing"], "url": "https://arxiv.org/abs/2008.03500"}, {"title": "machine learning panel data regressions with an application to   nowcasting price earnings ratios", "id": "2008.03600", "abstract": "this paper introduces structured machine learning regressions for prediction and nowcasting with panel data consisting of series sampled at different frequencies. motivated by the empirical problem of predicting corporate earnings for a large cross-section of firms with macroeconomic, financial, and news time series sampled at different frequencies, we focus on the sparse-group lasso regularization. this type of regularization can take advantage of the mixed frequency time series panel data structures and we find that it empirically outperforms the unstructured machine learning methods. we obtain oracle inequalities for the pooled and fixed effects sparse-group lasso panel data estimators recognizing that financial and economic data exhibit heavier than gaussian tails. to that end, we leverage on a novel fuk-nagaev concentration inequality for panel data consisting of heavy-tailed $\\tau$-mixing processes which may be of independent interest in other high-dimensional panel data settings.", "categories": "econ.em math.st stat.ap stat.me stat.ml stat.th", "created": "2020-08-08", "updated": "", "authors": ["andrii babii", "ryan t. ball", "eric ghysels", "jonas striaukas"], "url": "https://arxiv.org/abs/2008.03600"}, {"title": "aggression in the workplace makes social distance difficult", "id": "2008.04131", "abstract": "the spread of new coronavirus (covid-19) infections continues to increase. the practice of social distance attracts attention as a measure to prevent the spread of infection, but it is difficult for some occupations. therefore, in previous studies, the scale of factors that determine social distance has been developed. however, it was not clear how to select the items among them, and it seemed to be somewhat arbitrary. in response to this trend, this paper extracted eight scales by performing exploratory factor analysis based on certain rules while eliminating arbitrariness as much as possible. they were adverse conditions, leadership, information processing, response to aggression, mechanical movement, autonomy, communication with the outside, and horizontal teamwork. of these, adverse conditions, response to aggression, and horizontal teamwork had a positive correlation with physical proximity, and information processing, mechanical movement, autonomy, and communication with the outside had a negative correlation with physical proximity. furthermore, as a result of multiple regression analysis, it was shown that response to aggression, not the mere teamwork assumed in previous studies, had the greatest influence on physical proximity.", "categories": "econ.gn q-fin.ec", "created": "2020-08-10", "updated": "2020-08-11", "authors": ["keisuke kokubun"], "url": "https://arxiv.org/abs/2008.04131"}, {"title": "the decision-conflict and multicriteria logit", "id": "2008.04229", "abstract": "we study two tractable random non-forced choice models that explain behavioural patterns suggesting that the choice-deferral outside option is often selected when people find it hard to decide between the market alternatives available to them, even when these are few and desirable. the *decision-conflict logit* extends the logit model with an outside option by assigning a menu-dependent value to that option. this value captures the degree of complexity/decision difficulty at the relevant menu and allows for the choice probability of the outside option to either increase or decrease when the menu is expanded, depending on how many as well as how attractive options are added to it. the *multicriteria logit* is a special case of this model and introduces multiple utility functions that jointly predict behaviour in a multiplicative-logit way. every multicriteria logit admits a simple discrete-choice formulation.", "categories": "econ.th econ.em", "created": "2020-08-10", "updated": "2020-09-28", "authors": ["georgios gerasimou"], "url": "https://arxiv.org/abs/2008.04229"}, {"title": "purely bayesian counterfactuals versus newcomb's paradox", "id": "2008.04256", "abstract": "this paper proposes a careful separation between an entity's epistemic system and their decision system. crucially, bayesian counterfactuals are estimated by the epistemic system; not by the decision system. based on this remark, i prove the existence of newcomb-like problems for which an epistemic system necessarily expects the entity to make a counterfactually bad decision. i then address (a slight generalization of) newcomb's paradox. i solve the specific case where the player believes that the predictor applies bayes rule with a supset of all the data available to the player. i prove that the counterfactual optimality of the 1-box strategy depends on the player's prior on the predictor's additional data. if these additional data are not expected to reduce sufficiently the predictor's uncertainty on the player's decision, then the player's epistemic system will counterfactually prefer to 2-box. but if the predictor's data is believed to make them quasi-omniscient, then 1-box will be counterfactually preferred. implications of the analysis are then discussed. more generally, i argue that, to better understand or design an entity, it is useful to clearly separate the entity's epistemic, decision, but also data collection, reward and maintenance systems, whether the entity is human, algorithmic or institutional.", "categories": "econ.th cs.ai", "created": "2020-08-10", "updated": "", "authors": ["l\u00ea nguy\u00ean hoang"], "url": "https://arxiv.org/abs/2008.04256"}, {"title": "nonparametric prediction with spatial data", "id": "2008.04269", "abstract": "we describe a nonparametric prediction algorithm for spatial data. the algorithm is based on a flexible exponential representation of the model characterized via the spectral density function. we provide theoretical results demonstrating that our predictors have desired asymptotic properties. finite sample performance is assessed in a monte carlo study that also compares our algorithm to a rival nonparametric method based on the infinite ar representation of the dynamics of the data. we apply our method to a real data set in an empirical example that predicts house prices in los angeles.", "categories": "econ.em stat.me", "created": "2020-08-10", "updated": "", "authors": ["abhimanyu gupta", "javier hidalgo"], "url": "https://arxiv.org/abs/2008.04269"}, {"title": "connected incomplete preferences", "id": "2008.04401", "abstract": "the standard model of choice in economics is the maximization of a complete and transitive preference relation over a fixed set of alternatives. while completeness of preferences is usually regarded as a strong assumption, weakening it requires care to ensure that the resulting model still has enough structure to yield interesting results. this paper takes a step in this direction by studying the class of \"connected preferences\", that is, preferences that may fail to be complete but have connected maximal domains of comparability. we offer four new results. theorem 1 identifies a basic necessary condition for a continuous preference to be connected in the sense above, while theorem 2 provides sufficient conditions. building on the latter, theorem 3 characterizes the maximal domains of comparability. finally, theorem 4 presents conditions that ensure that maximal domains are arc-connected.", "categories": "econ.th", "created": "2020-08-10", "updated": "", "authors": ["leandro gorno", "alessandro rivello"], "url": "https://arxiv.org/abs/2008.04401"}, {"title": "measuring energy-saving technological change: international trends and   differences", "id": "2008.04639", "abstract": "technological change is essential for balancing economic growth and environmental sustainability. this study measures and documents energy-saving technological change to understand its trends in advanced countries over recent decades. we estimate aggregate production functions with factor-augmenting technology using cross-country panel data and shift-share instruments, thereby measuring and documenting energy-saving technological change. our results show how energy-saving technological change varies across countries over time and the extent to which it contributes to economic growth in 12 oecd countries from the years 1978 to 2005.", "categories": "econ.gn q-fin.ec", "created": "2020-08-11", "updated": "", "authors": ["emiko inoue", "hiroya taniguchi", "ken yamada"], "url": "https://arxiv.org/abs/2008.04639"}, {"title": "convergence rate of estimators of clustered panel models with   misclassification", "id": "2008.04708", "abstract": "we study kmeans clustering estimation of panel data models with a latent group structure and $n$ units and $t$ time periods under long panel asymptotics. we show that the group-specific coefficients can be estimated at the parametric root $nt$ rate even if error variances diverge as $t \\to \\infty$ and some units are asymptotically misclassified. this limit case approximates empirically relevant settings and is not covered by existing asymptotic results.", "categories": "econ.em math.st stat.th", "created": "2020-08-11", "updated": "", "authors": ["andreas dzemski", "ryo okui"], "url": "https://arxiv.org/abs/2008.04708"}, {"title": "counting the costs of covid-19: why future treatment option values   matter", "id": "2008.04850", "abstract": "i critique a recent analysis (miles, stedman & heald, 2020) of covid-19 lockdown costs and benefits, focussing on the united kingdom (uk). miles et al. (2020) argue that the march-june uk lockdown was more costly than the benefit of lives saved, evaluated using the nice threshold of {\\pounds}30000 for a quality-adjusted life year (qaly) and that the costs of a lockdown for 13 weeks from mid-june would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-september.   i note here two key problems that significantly affect their estimates and cast doubt on their conclusions. firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. that is, they assume indifference between mid-september states of 13 or 7500 weekly deaths and corresponding infection rates. this seems indefensible unless one assumes that (a) there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, (b) notwithstanding temporary lockdowns, covid-19 will very likely propagate until herd immunity. even under these assumptions it is very questionable. secondly, they ignore the costs of serious illness, possible long-term lowering of life quality and expectancy for survivors. these are uncertain, but plausibly at least as large as the costs in deaths.   in summary, policy on tackling covid-19 cannot be rationally made without estimating probabilities of future medical interventions and long-term illness costs. more work on modelling these uncertainties is urgently needed.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-07-06", "updated": "2020-09-19", "authors": ["adrian kent"], "url": "https://arxiv.org/abs/2008.04850"}, {"title": "bookmakers' mispricing of the disappeared home advantage in the german   bundesliga after the covid-19 break", "id": "2008.05417", "abstract": "the outbreak of covid-19 in march 2020 led to a shutdown of economic activities in europe. this included the sports sector, since public gatherings were prohibited. the german bundesliga was among the first sport leagues realising a restart without spectators. several recent studies suggest that the home advantage of teams was eroded for the remaining matches. our paper analyses the reaction by bookmakers to the disappearance of such home advantage. we show that bookmakers had problems to adjust the betting odds in accordance to the disappeared home advantage, opening opportunities for profitable betting strategies.", "categories": "econ.gn q-fin.ec", "created": "2020-08-12", "updated": "2020-08-20", "authors": ["christian deutscher", "david winkelmann", "marius \u00f6tting"], "url": "https://arxiv.org/abs/2008.05417"}, {"title": "intertemporal collective household models: identification in short   panels with unobserved heterogeneity in resource shares", "id": "2008.05507", "abstract": "we provide a new full-commitment intertemporal collective household model to estimate resource shares, defined as the fraction of household expenditure enjoyed by household members. our model implies nonlinear time-varying household quantity demand functions that depend on fixed effects.   we provide new econometric results showing identification of a large class of models that includes our household model. we cover fixed-$t$ panel models where the response variable is an unknown monotonic function of a linear latent variable with fixed effects, regressors, and a nonparametric error term. the function may be weakly monotonic and time-varying, and the fixed effects are unrestricted. we identify the structural parameters and features of the distribution of fixed effects. in our household model, these correspond to features of the distribution of resource shares.   using bangladeshi data, we show: women's resource shares decline with household budgets; and, half the variation in women's resource shares is due to unobserved heterogeneity.", "categories": "econ.em", "created": "2020-08-12", "updated": "", "authors": ["irene botosaru", "chris muris", "krishna pendakur"], "url": "https://arxiv.org/abs/2008.05507"}, {"title": "a dynamic ordered logit model with fixed effects", "id": "2008.05517", "abstract": "we study a fixed-$t$ panel data logit model for ordered outcomes that accommodates fixed effects and state dependence. we provide identification results for the autoregressive parameter, regression coefficients, and the threshold parameters in this model. our results require only four observations on the outcome variable. we provide conditions under which a composite conditional maximum likelihood estimator is consistent and asymptotically normal. we use our estimator to explore the determinants of self-reported health in a panel of european countries over the period 2003-2016. we find that: (i) the autoregressive parameter is positive and analogous to a linear ar(1) coefficient of about 0.25, indicating persistence in health status; (ii) the association between income and health becomes insignificant once we control for unobserved heterogeneity and persistence.", "categories": "econ.em", "created": "2020-08-12", "updated": "", "authors": ["chris muris", "pedro raposo", "sotiris vandoros"], "url": "https://arxiv.org/abs/2008.05517"}, {"title": "on the origin(s) of the term \"big data\"", "id": "2008.05835", "abstract": "i investigate the origin(s) of the term \"big data,\" in industry and academics, and in computer science and econometrics. the term probably originated in lunch-table conversations at silicon graphics inc. (sgi) in the mid 1990s, in which john mashey figured prominently. the first significant (and independent) academic references are arguably weiss and indurkhya (1998) in computer science and diebold (2000) in econometrics. an unpublished 2001 research note by douglas laney at gartner enriched the concept significantly. the big data phenomenon continues unabated.", "categories": "econ.em cs.cy", "created": "2020-08-13", "updated": "2020-09-06", "authors": ["francis x. diebold"], "url": "https://arxiv.org/abs/2008.05835"}, {"title": "exact solutions for a solow-swan model with non-constant returns to   scale", "id": "2008.05875", "abstract": "the solow-swan model is shortly reviewed from a mathematical point of view. by considering non-constant returns to scale, we obtain a general solution strategy. we then compute the exact solution for the cobb-douglas production function, for both the classical model and the von bertalanffy model. numerical simulations are provided.", "categories": "econ.th", "created": "2020-08-13", "updated": "", "authors": ["nicol\u00f2 cangiotti", "mattia sensi"], "url": "https://arxiv.org/abs/2008.05875"}, {"title": "modelling the expected probability of correct assignment under   uncertainty", "id": "2008.05878", "abstract": "when making important decisions such as choosing health insurance or a school, people are often uncertain what levels of attributes will suit their true preference. after choice, they might realize that their uncertainty resulted in a mismatch: choosing a sub-optimal alternative, while another available alternative better matches their needs.   we study here the overall impact, from a central planner's perspective, of decisions under such uncertainty. we use the representation of voronoi tessellations to locate all individuals and alternatives in an attribute space. we provide an expression for the probability of correct match, and calculate, analytically and numerically, the average percentage of matches. we test dependence on the level of uncertainty and location.   we find overall considerable mismatch even for low uncertainty - a possible concern for policy makers. we further explore a commonly used practice - allocating service representatives to assist individuals' decisions. we show that within a given budget and uncertainty level, the effective allocation is for individuals who are close to the boundary between several voronoi cells, but are not right on the boundary.", "categories": "physics.soc-ph econ.th", "created": "2020-08-12", "updated": "", "authors": ["tom dvir", "renana peres", "ze\u00e9v rudnick"], "url": "https://arxiv.org/abs/2008.05878"}, {"title": "on social welfare orders satisfying anonymity and asymptotic density-one   pareto", "id": "2008.05879", "abstract": "we study the nature (i.e., constructive as opposed to non-constructive) of social welfare orders on infinite utility streams, and their representability by means of real-valued functions. we assume finite anonymity and introduce a new efficiency concept we refer to as asymptotic density-one pareto. we characterize the existence of representable and constructive social welfare orders (satisfying the above properties) in terms of easily verifiable conditions on the feasible set of one-period utilities.", "categories": "econ.th math.lo", "created": "2020-08-11", "updated": "", "authors": ["ram sewak dubey", "giorgio laguzzi", "francesco ruscitti"], "url": "https://arxiv.org/abs/2008.05879"}, {"title": "effect of pop-up bike lanes on cycling in european cities", "id": "2008.05883", "abstract": "the bicycle is a low-cost means of transport linked to low risk of covid-19 transmission. governments have incentivized cycling by redistributing street space as part of their post-lockdown strategies. here, we evaluate the impact of provisional bicycle infrastructure on cycling traffic in european cities. we scrape daily bicycle counts spanning over a decade from 736 bicycle counters in 106 european cities. we combine this with data on announced and completed pop-up bike lane road work projects. on average 11.5 kilometers of provisional pop-up bike lanes have been built per city. each kilometer has increased cycling in a city by 0.6%. we calculate that the new infrastructure will generate $2.3 billion in health benefits per year, if cycling habits are sticky.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-08-06", "updated": "2020-09-05", "authors": ["sebastian kraus", "nicolas koch"], "url": "https://arxiv.org/abs/2008.05883"}, {"title": "the p-innovation ecosystems model", "id": "2008.05885", "abstract": "in this paper, we propose a spatially constrained clustering problem belonging to the family of \"p-regions\" problems. our formulation is motivated by the recent developments of economic complexity on the evolution of the economic output through key interactions among industries within economic regions. the objective of this model consists in aggregating a set of geographic areas into a prescribed number of regions (so-called innovation ecosystems) such that the resulting regions preserve the most relevant interactions among industries. we formulate the p-innovation ecosystems model as a mixed-integer programming (mip) problem and propose a heuristic solution approach. we explore a case involving the municipalities of colombia to illustrate how such a model can be applied and used for policy and regional development.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-08-02", "updated": "", "authors": ["r. church", "j. c. duque", "d. e. restrepo"], "url": "https://arxiv.org/abs/2008.05885"}, {"title": "a spatial stochastic sir model for transmission networks with   application to covid-19 epidemic in china", "id": "2008.06051", "abstract": "governments around the world have implemented preventive measures against the spread of the coronavirus disease (covid-19). in this study, we consider a multivariate discrete-time markov model to analyze the propagation of covid-19 across 33 provincial regions in china. this approach enables us to evaluate the effect of mobility restriction policies on the spread of the disease. we use data on daily human mobility across regions and apply the bayesian framework to estimate the proposed model. the results show that the spread of the disease in china was predominately driven by community transmission within regions and the lockdown policy introduced by local governments curbed the spread of the pandemic. further, we document that hubei was only the epicenter of the early epidemic stage. secondary epicenters, such as beijing and guangdong, had already become established by late january 2020, and the disease spread out to connected regions. the transmission from these epicenters substantially declined following the introduction of human mobility restrictions across regions.", "categories": "q-bio.pe econ.gn physics.soc-ph q-fin.ec stat.ap", "created": "2020-08-13", "updated": "2020-08-16", "authors": ["tatsushi oka", "wei wei", "dan zhu"], "url": "https://arxiv.org/abs/2008.06051"}, {"title": "bounding disease prevalence by bounding selectivity and accuracy of   tests: the case of covid-19", "id": "2008.06178", "abstract": "i propose novel partial identification bounds on disease prevalence from information on test rate and test yield. the approach broadly follows recent work by \\cite{mm20} on covid-19, but starts from user-specified bounds on (i) test accuracy, in particular sensitivity, (ii) the extent to which tests are targeted, formalized as restriction on the effect of true status on the odds ratio of getting tested and thereby embeddable in logit specifications. the motivating application is to the covid-19 pandemic but the strategy may also be useful elsewhere. evaluated on data from the pandemic's early stage, even the weakest of the novel bounds are reasonably informative. for example, they place the infection fatality rate for italy well above the one of influenza by mid-april.", "categories": "econ.em stat.ap", "created": "2020-08-13", "updated": "", "authors": ["j\u00f6rg stoye"], "url": "https://arxiv.org/abs/2008.06178"}, {"title": "optimal selection of the number of control units in knn algorithm to   estimate average treatment effects", "id": "2008.06564", "abstract": "we propose a simple approach to optimally select the number of control units in k nearest neighbors (knn) algorithm focusing in minimizing the mean squared error for the average treatment effects. our approach is non-parametric where confidence intervals for the treatment effects were calculated using asymptotic results with bias correction. simulation exercises show that our approach gets relative small mean squared errors, and a balance between confidence intervals length and type i error. we analyzed the average treatment effects on treated (atet) of participation in 401(k) plans on accumulated net financial assets confirming significant effects on amount and positive probability of net asset. our optimal k selection produces significant narrower atet confidence intervals compared with common practice of using k=1.", "categories": "econ.em", "created": "2020-08-14", "updated": "", "authors": ["andr\u00e9s ram\u00edrez-hassan", "raquel vargas-correa", "gustavo garc\u00eda", "daniel londo\u00f1o"], "url": "https://arxiv.org/abs/2008.06564"}, {"title": "no covid-19 climate \"silver lining\" in the u.s. power sector: co$_2$   emissions reductions not statistically significant, additional risk to coal   generators is minimal", "id": "2008.06660", "abstract": "recent studies conclude that the global coronavirus (covid-19) pandemic decreased power sector co$_2$ emissions worldwide. we analyze the statistical significance of co$_2$ emissions reductions in the u.s power sector from march through july 2020 and we present model-informed expectations of the likelihood of sustained reductions in co$_2$ emissions. we use gaussian process (gp) regression to assess whether co$_2$ emissions reductions would have occurred with reasonable probability in the absence of covid-19 considering uncertainty due to factors unrelated to the pandemic. we show that co$_2$ emissions are lower than levels expected in the absence of covid-19 for each month from march through july 2020 but that those monthly reductions are not statistically significant considering hypothesis tests at 5% significance levels. to make predictions about whether covid-19-related co$_2$ emissions reductions will be sustained in the power sector, we assess the relative impacts of the pandemic on electricity generation (e) and on the carbon intensity of electricity supply (c/e). e is on average 2.9\\% lower than what we would expect in the absence of covid-19 from march through july 2020. we expect e to rebound alongside a recovery of the u.s. economy. c/e is determined to be 2.7% lower on average than what we would expect in the absence of covid-19 from march through july 2020. reductions in c/e are mostly attributable to reductions in the share of coal-fired electricity generation. we analyze the expected profitability through 2021 of the 845 coal-fired power plant units operating in the u.s. we find that only 76 of those units, representing 1.3% of total coal generation capacity, were expected to be profitable prior to covid-19 but are no longer expected to be profitable. we conclude that covid-19 is unlikely to have a material impact on u.s. power sector co$_2$ emissions in the long-run.", "categories": "econ.gn physics.soc-ph q-fin.ec stat.ap", "created": "2020-08-15", "updated": "", "authors": ["max luke", "priyanshi somani", "turner cotterman", "dhruv suri", "stephen j. lee"], "url": "https://arxiv.org/abs/2008.06660"}, {"title": "to bag is to prune", "id": "2008.07063", "abstract": "it is notoriously hard to build a bad random forest (rf). concurrently, rf is perhaps the only standard ml algorithm that blatantly overfits in-sample without any consequence out-of-sample. standard arguments cannot rationalize this paradox. i propose a new explanation: bootstrap aggregation and model perturbation as implemented by rf automatically prune a (latent) true underlying tree. more generally, there is no need to tune the stopping point of a properly randomized ensemble of greedily optimized base learners. thus, boosting and mars are eligible for automatic (implicit) tuning. i empirically demonstrate the property, with simulated and real data, by reporting that these new completely overfitting ensembles yield an out-of-sample performance equivalent to that of their tuned counterparts -- or better.", "categories": "stat.ml cs.lg econ.em", "created": "2020-08-16", "updated": "2020-09-14", "authors": ["philippe goulet coulombe"], "url": "https://arxiv.org/abs/2008.07063"}, {"title": "analysing a built-in advantage in asymmetric darts contests using causal   machine learning", "id": "2008.07165", "abstract": "we analyse a sequential contest with two players in darts where one of the contestants enjoys a technical advantage. using methods from the causal machine learning literature, we analyse the built-in advantage, which is the first-mover having potentially more but never less moves. our empirical findings suggest that the first-mover has an 8.6 percentage points higher probability to win the match induced by the technical advantage. contestants with low performance measures and little experience have the highest built-in advantage. with regard to the fairness principle that contestants with equal abilities should have equal winning probabilities, this contest is ex-ante fair in the case of equal built-in advantages for both competitors and a randomized starting right. nevertheless, the contest design produces unequal probabilities of winning for equally skilled contestants because of asymmetries in the built-in advantage associated with social pressure for contestants competing at home and away.", "categories": "econ.em", "created": "2020-08-17", "updated": "", "authors": ["daniel goller"], "url": "https://arxiv.org/abs/2008.07165"}, {"title": "verification results for age-structured models of economic-epidemics   dynamics", "id": "2008.07335", "abstract": "in this paper we propose a macro-dynamic age-structured set-up for the analysis of epidemics/economic dynamics in continuous time. the resulting optimal control problem is reformulated in an infinite dimensional hilbert space framework where we perform the basic steps of dynamic programming approach. our main result is a verification theorem which allows to guess the feedback form of optimal strategies. this will be a departure point to discuss the behavior of the models of the family we introduce and their policy implications.", "categories": "econ.th math.oc", "created": "2020-08-17", "updated": "", "authors": ["giorgio fabbri", "fausto gozzi", "giovanni zanco"], "url": "https://arxiv.org/abs/2008.07335"}, {"title": "mobility and social efficiency", "id": "2008.07650", "abstract": "this is a general competitive analysis paper. a model is presented that describes how an individual with a physical disability, or mobility impairment, would go about utility maximization. these results are then generalized. subsequently, a selection of disability policies from canada and the united states are compared to the insights of the model, and it is shown that there are sources of inefficiency in many north american disability support systems.", "categories": "econ.th", "created": "2020-08-17", "updated": "", "authors": ["ryan steven kostiuk"], "url": "https://arxiv.org/abs/2008.07650"}, {"title": "a relation analysis of markov decision process frameworks", "id": "2008.07820", "abstract": "we study the relation between different markov decision process (mdp) frameworks in the machine learning and econometrics literatures, including the standard mdp, the entropy and general regularized mdp, and stochastic mdp, where the latter is based on the assumption that the reward function is stochastic and follows a given distribution. we show that the entropy-regularized mdp is equivalent to a stochastic mdp model, and is strictly subsumed by the general regularized mdp. moreover, we propose a distributional stochastic mdp framework by assuming that the distribution of the reward function is ambiguous. we further show that the distributional stochastic mdp is equivalent to the regularized mdp, in the sense that they always yield the same optimal policies. we also provide a connection between stochastic/regularized mdp and constrained mdp. our work gives a unified view on several important mdp frameworks, which would lead new ways to interpret the (entropy/general) regularized mdp frameworks through the lens of stochastic rewards and vice-versa. given the recent popularity of regularized mdp in (deep) reinforcement learning, our work brings new understandings of how such algorithmic schemes work and suggest ideas to develop new ones.", "categories": "math.oc cs.lg econ.em", "created": "2020-08-18", "updated": "", "authors": ["tien mai", "patrick jaillet"], "url": "https://arxiv.org/abs/2008.07820"}, {"title": "peer effects and endogenous social interactions", "id": "2008.07886", "abstract": "we introduce an approach to deal with self-selection of peers in the linear-in-means model. contrary to the existing proposals we do not require to specify a model for how the selection of peers comes about. rather, we exploit two restrictions that are inherent to many such specifications to construct intuitive instrumental variables. these restrictions are that link decisions that involve a given individual are not all independent of one another, but that they are independent of the link behavior between other pairs of individuals. a two-stage least-squares estimator of the linear-in-means model is then readily obtained.", "categories": "econ.em", "created": "2020-08-18", "updated": "", "authors": ["koen jochmans"], "url": "https://arxiv.org/abs/2008.07886"}, {"title": "learning structure in nested logit models", "id": "2008.08048", "abstract": "this paper introduces a new data-driven methodology for nested logit structure discovery. nested logit models allow the modeling of positive correlations between the error terms of the utility specifications of the different alternatives in a discrete choice scenario through the specification of a nesting structure. current nested logit model estimation practices require an a priori specification of a nesting structure by the modeler. in this we work we optimize over all possible specifications of the nested logit model that are consistent with rational utility maximization. we formulate the problem of learning an optimal nesting structure from the data as a mixed integer nonlinear programming (minlp) optimization problem and solve it using a variant of the linear outer approximation algorithm. we exploit the tree structure of the problem and utilize the latest advances in integer optimization to bring practical tractability to the optimization problem we introduce. we demonstrate the ability of our algorithm to correctly recover the true nesting structure from synthetic data in a monte carlo experiment. in an empirical illustration using a stated preference survey on modes of transportation in the u.s. state of massachusetts, we use our algorithm to obtain an optimal nesting tree representing the correlations between the unobserved effects of the different travel mode choices. we provide our implementation as a customizable and open-source code base written in the julia programming language.", "categories": "stat.me cs.lg econ.em", "created": "2020-08-18", "updated": "", "authors": ["youssef m. aboutaleb", "moshe ben-akiva", "patrick jaillet"], "url": "https://arxiv.org/abs/2008.08048"}, {"title": "bounds on distributional treatment effect parameters using panel data   with an application on job displacement", "id": "2008.08117", "abstract": "this paper develops new techniques to bound distributional treatment effect parameters that depend on the joint distribution of potential outcomes -- an object not identified by standard identifying assumptions such as selection on observables or even when treatment is randomly assigned. i show that panel data and an additional assumption on the dependence between untreated potential outcomes for the treated group over time (i) provide more identifying power for distributional treatment effect parameters than existing bounds and (ii) provide a more plausible set of conditions than existing methods that obtain point identification. i apply these bounds to study heterogeneity in the effect of job displacement during the great recession. using standard techniques, i find that workers who were displaced during the great recession lost on average 34\\% of their earnings relative to their counterfactual earnings had they not been displaced. using the methods developed in the current paper, i also show that the average effect masks substantial heterogeneity across workers.", "categories": "econ.em", "created": "2020-08-18", "updated": "", "authors": ["brantly callaway"], "url": "https://arxiv.org/abs/2008.08117"}, {"title": "a novel approach to predictive accuracy testing in nested environments", "id": "2008.08387", "abstract": "we introduce a new approach for comparing the predictive accuracy of two nested models that bypasses the difficulties caused by the degeneracy of the asymptotic variance of forecast error loss differentials used in the construction of commonly used predictive comparison statistics. our approach continues to rely on the out of sample mse loss differentials between the two competing models, leads to nuisance parameter free gaussian asymptotics and is shown to remain valid under flexible assumptions that can accommodate heteroskedasticity and the presence of mixed predictors (e.g. stationary and local to unit root). a local power analysis also establishes its ability to detect departures from the null in both stationary and persistent settings. simulations calibrated to common economic and financial applications indicate that our methods have strong power with good size control across commonly encountered sample sizes.", "categories": "econ.em", "created": "2020-08-19", "updated": "", "authors": ["jean-yves pitarakis"], "url": "https://arxiv.org/abs/2008.08387"}, {"title": "axioms for defeat in democratic elections", "id": "2008.08451", "abstract": "we propose six axioms concerning when one candidate should defeat another in a democratic election involving two or more candidates. five of the axioms are widely satisfied by known voting procedures. the sixth axiom is a weakening of kenneth arrow's famous condition of the independence of irrelevant alternatives (iia). we call this weakening coherent iia. we prove that the five axioms plus coherent iia single out a voting procedure studied in our recent work: split cycle. in particular, split cycle is the most resolute voting procedure satisfying the six axioms for democratic defeat. in addition, we analyze how split cycle escapes arrow's impossibility theorem and related impossibility results.", "categories": "econ.th cs.gt cs.ma", "created": "2020-08-15", "updated": "", "authors": ["wesley h. holliday", "eric pacuit"], "url": "https://arxiv.org/abs/2008.08451"}, {"title": "are temporary value-added tax reductions passed on to consumers?   evidence from germany's stimulus", "id": "2008.08511", "abstract": "this paper provides the first estimates of the pass-through rate of the ongoing temporary value-added tax (vat) reduction, which is part of the german fiscal response to covid-19. using a unique dataset containing the universe of price changes at fuel stations in germany and france in june and july 2020, we employ a difference-in-differences strategy and find that pass-through is fast and substantial but remains incomplete for all fuel types. furthermore, we find a high degree of heterogeneity between the pass-through estimates for different fuel types. our results are consistent with the interpretation that pass-through rates are higher for customer groups who are more likely to exert competitive pressure by shopping for lower prices. our results have important implications for the effectiveness of the stimulus measure and the cost-effective design of unconventional fiscal policy.", "categories": "econ.gn q-fin.ec", "created": "2020-08-19", "updated": "", "authors": ["felix montag", "alina sagimuldina", "monika schnitzer"], "url": "https://arxiv.org/abs/2008.08511"}, {"title": "competing persuaders in zero-sum games", "id": "2008.08517", "abstract": "we study a bayesian persuasion game with multiple senders employing conditionally independent experiments. senders have zero-sum preferences over what information is revealed. we characterize when a set of states cannot be pooled in any equilibrium, and in particular, when the state is (fully) revealed in every equilibrium. the state must be fully revealed in every equilibrium if and only if sender utility functions are sufficiently nonlinear. in the binary-state case, the state is fully revealed in every equilibrium if and only if some sender has nontrivial preferences. our takeaway is that `most' zero-sum sender preferences result in full revelation.", "categories": "econ.th", "created": "2020-08-19", "updated": "", "authors": ["dilip ravindran", "zhihan cui"], "url": "https://arxiv.org/abs/2008.08517"}, {"title": "reforming the state-based forward guidance through wage growth rate   threshold: evidence from frb/us simulations", "id": "2008.08705", "abstract": "i have analyzed the practicality of the evans rule in the state based forward guidance and possible ways to reform it. i examined the biases, measurement errors, and other limitations extant in the unemployment and the inflation rate in the evans rule. using time series analysis, i calibrated the thresholds of eci wage growth and the employment to population ratio and investigated the relationship between other labor utilization variables. then i imposed various shocks and constructed impulse response functions to contrast the paths of eight macroeconomic variables under three scenarios. the results suggest that under the wage growth rate scenario, the federal funds rate lift off earlier than under the current evans rule.", "categories": "econ.gn q-fin.ec", "created": "2020-08-19", "updated": "", "authors": ["sudiksha joshi"], "url": "https://arxiv.org/abs/2008.08705"}, {"title": "positionality-weighted aggregation methods on cumulative voting", "id": "2008.08759", "abstract": "the issue in solving social problems is how to respect minority opinions, which are often ignored in general majority rules. to build consensus on pluralistic values and make social choices in consideration of minority opinions, we propose aggregation methods that give weighting to the minority's positionality on cardinal cumulative voting. based on quadratic and linear voting, we formulated three weighted aggregation methods that differ in the ratio of votes to cumulative points and the weighting of the minority to all members, and calculated the frequency distributions of the aggregation results, assuming that the distributions of votes follow normal distributions. from these calculation results, we found that minority opinions are likely to be reflected as weighting increases proportionally in two of the above three methods. this means that sen and gotoh's idea of considering the social position of unfortunate people on ordinal ranking, that welfare economics considers under an axiomatic approach, was shown by weighting the minority's positionality on cardinal voting. in addition, we can know the contents such as the number and positionality of the minority from the analysis of the aggregation results. it will be useful for promoting mutual understanding between the majority and minority by visualizing the contents of the proposed aggregation methods interactively in the consensus-building process. with the further development of information technology, the consensus building on cardinal choices based on big data will be necessary. we would like to use the proposed aggregation methods for making social choices for pluralistic values such as social, environmental, and economic.", "categories": "econ.gn cs.cy physics.soc-ph q-fin.ec", "created": "2020-08-19", "updated": "", "authors": ["takeshi kato", "yasuhiro asa", "misa owa"], "url": "https://arxiv.org/abs/2008.08759"}, {"title": "west australian pandemic response: the black swan of black swans", "id": "2008.08918", "abstract": "the covid-19 pandemic has been described as the global challenge of our time, an enormous human tragedy with dramatic economic impacts. this paper describes the response and expected recovery process for western australia, where a rapid and effective response was implemented. this has enabled an early transition into an expected recovery both in health and economic terms. the positive lessons learned from this experience are documented as they emerge in order to support other states and nations as they address this issue globally in the near-term and consider enduring improvements for the longer term. while the authors have personal experience in the wa context, wider observations across australia and selected international benchmarks are also included. key lessons include the importance of good health advice in australia's interest; timely, synchronized and aligned action at all levels of government; a program of well communicated, aligned health and economic measures which support all in society allowing a very high level of appropriate community behaviour, ensuring the health system was not overloaded; innovation in telehealth, testing, pandemic modelling, and integrated operations which also allowed essential industries to continue; and strong border and travel controls with highly effective isolation preventing community spread, ultimately enabling rapid elimination of the disease from the hospital system. in combination, these demonstrate that in the case of western australia the result of first eliminating the disease from the community, and then reopening the economy progressively at a strong pace, has enabled a world leading outcome in both in health and economic terms. the lessons from this experience are widely applicable, shareable both as supporting service to other regions and through knowledge transfer.", "categories": "q-bio.pe econ.gn q-fin.ec", "created": "2020-08-17", "updated": "", "authors": ["david cavanagh", "mark hoey", "andrew clark", "michael small", "paul bailey", "jon watson"], "url": "https://arxiv.org/abs/2008.08918"}, {"title": "the vigilant eating rule: a general approach for probabilistic economic   design with constraints", "id": "2008.08991", "abstract": "we consider the problem of probabilistic allocation of objects under ordinal preferences. our main contribution is an allocation mechanism, called the vigilant eating rule (ver), that applies to nearly arbitrary feasibility constraints. it is constrained ordinally efficient, can be computed efficiently for a large class of constraints, and treats agents equally if they have the same preferences and are subject to the same constraints. when the set of feasible allocations is convex, we also present a characterization of our rule based on ordinal egalitarianism. our general results concerning ver do not just apply to allocation problems but to any collective choice problem in which agents have ordinal preferences over discrete outcomes. as a case study, we assume objects have priorities for agents and apply ver to sets of probabilistic allocations that are constrained by stability. ver coincides with the (extended) probabilistic serial rule when priorities are flat and the agent proposing deterministic deferred acceptance algorithm when preferences and priorities are strict. while ver always returns a stable and constrained efficient allocation, it fails to be strategyproof, unconstrained efficient, and envy-free. we show, however, that each of these three properties is incompatible with stability and constrained efficiency.", "categories": "econ.th", "created": "2020-08-20", "updated": "", "authors": ["haris aziz", "florian brandl"], "url": "https://arxiv.org/abs/2008.08991"}, {"title": "inference for moment inequalities: a constrained moment selection   procedure", "id": "2008.09021", "abstract": "inference in models where the parameter is defined by moment inequalities is of interest in many areas of economics. this paper develops a new method for improving the performance of generalized moment selection (gms) testing procedures in finite-samples. the method modifies gms tests by tilting the empirical distribution in its moment selection step by an amount that maximizes the empirical likelihood subject to the restrictions of the null hypothesis. we characterize sets of population distributions on which a modified gms test is (i) asymptotically equivalent to its non-modified version to first-order, and (ii) superior to its non-modified version according to local power when the sample size is large enough. an important feature of the proposed modification is that it remains computationally feasible even when the number of moment inequalities is large. we report simulation results that show the modified tests control size well, and have markedly improved local power over their non-modified counterparts.", "categories": "econ.em", "created": "2020-08-20", "updated": "2020-08-25", "authors": ["rami v. tabri", "christopher d. walker"], "url": "https://arxiv.org/abs/2008.09021"}, {"title": "coverage optimal empirical likelihood inference for regression   discontinuity design", "id": "2008.09263", "abstract": "this paper proposes an empirical likelihood inference method for a general framework that covers various types of treatment effect parameters in regression discontinuity designs (rdd) . our method can be applied for standard sharp and fuzzy rdds, rdds with categorical outcomes, augmented sharp and fuzzy rdds with covariates and testing problems that involve multiple rdd treatment effect parameters. our method is based on the first-order conditions from local polynomial fitting and avoids explicit asymptotic variance estimation. we investigate both firstorder and second-order asymptotic properties and derive the coverage optimal bandwidth which minimizes the leading term in the coverage error expansion. in some cases, the coverage optimal bandwidth has a simple explicit form, which the wald-type inference method usually lacks. we also find that bartlett corrected empirical likelihood inference further improves the coverage accuracy. easily implementable coverage optimal bandwidth selector and bartlett correction are proposed for practical use. we conduct monte carlo simulations to assess finite-sample performance of our method and also apply it to two real datasets to illustrate its usefulness.", "categories": "econ.em", "created": "2020-08-20", "updated": "", "authors": ["jun ma", "zhengfei yu"], "url": "https://arxiv.org/abs/2008.09263"}, {"title": "estimation of the number of irregular foreigners in poland using   non-linear count regression models", "id": "2008.09407", "abstract": "population size estimation requires access to unit-level data in order to correctly apply capture-recapture methods. unfortunately, for reasons of confidentiality access to such data may be limited. to overcome this issue we apply and extend the hierarchical poisson-gamma model proposed by zhang (2008), which initially was used to estimate the number of irregular foreigners in norway.   the model is an alternative to the current capture-recapture approach as it does not require linking multiple sources and is solely based on aggregated administrative data that include (1) the number of apprehended irregular foreigners, (2) the number of foreigners who faced criminal charges and (3) the number of foreigners registered in the central population register. the model explicitly assumes a relationship between the unauthorized and registered population, which is motivated by the interconnection between these two groups. this makes the estimation conditionally dependent on the size of regular population, provides interpretation with analogy to registered population and makes the estimated parameter more stable over time.   in this paper, we modify the original idea to allow for covariates and flexible count distributions in order to estimate the number of irregular foreigners in poland in 2019. we also propose a parametric bootstrap for estimating standard errors of estimates. based on the extended model we conclude that in as of 31.03.2019 and 30.09.2019 around 15,000 and 20,000 foreigners and were residing in poland without valid permits. this means that those apprehended by the polish border guard account for around 15-20% of the total.", "categories": "stat.ap econ.gn q-fin.ec stat.me", "created": "2020-08-21", "updated": "", "authors": ["maciej ber\u0119sewicz", "katarzyna pawlukiewicz"], "url": "https://arxiv.org/abs/2008.09407"}, {"title": "a theoretical look at electre tri-nb", "id": "2008.09484", "abstract": "electre tri is a set of methods designed to sort alternatives evaluated on several attributes into ordered categories. the original electre tri-b method uses one limiting profile per category. a more recent method, electre tri-nb, allows one to use several limiting profiles for each category. we investigate the properties of electre tri-nb. when the number of limiting profiles used to define each category is not restricted, electre tri-nb is easy to characterize axiomatically and is found to be equivalent to several other methods proposed in the literature. we extend this result in various directions.", "categories": "econ.th", "created": "2020-08-20", "updated": "", "authors": ["denis bouyssou", "thierry marchant", "marc pirlot"], "url": "https://arxiv.org/abs/2008.09484"}, {"title": "optimal rating design", "id": "2008.09529", "abstract": "we study the design of optimal rating systems in the presence of adverse selection and moral hazard. buyers and sellers interact in a competitive market where goods are vertically differentiated according to their qualities. sellers differ in their cost of quality provision, which is private information to them. an intermediary observes sellers' quality and chooses a rating system, i.e., a signal of quality for buyers, in order to incentivize sellers to produce high-quality goods. we provide a full characterization of the set of payoffs and qualities that can arise in equilibrium under an arbitrary rating system. we use this characterization to analyze pareto optimal rating systems when seller's quality choice is deterministic and random.", "categories": "econ.th", "created": "2020-08-21", "updated": "2020-09-07", "authors": ["maryam saeedi", "ali shourideh"], "url": "https://arxiv.org/abs/2008.09529"}, {"title": "search for a moving target in a competitive environment", "id": "2008.09653", "abstract": "we consider a discrete-time dynamic search game in which a number of players compete to find an invisible object that is moving according to a time-varying markov chain. we examine the subgame perfect equilibria of these games. the main result of the paper is that the set of subgame perfect equilibria is exactly the set of greedy strategy profiles, i.e. those strategy profiles in which the players always choose an action that maximizes their probability of immediately finding the object. we discuss various variations and extensions of the model.", "categories": "math.oc cs.gt econ.th math.pr", "created": "2020-08-21", "updated": "2020-08-25", "authors": ["benoit duvocelle", "j\u00e1nos flesch", "hui min shi", "dries vermeulen"], "url": "https://arxiv.org/abs/2008.09653"}, {"title": "constrained trading networks", "id": "2008.09757", "abstract": "trades based on bilateral (indivisible) contracts can be represented by a network. vertices correspond to agents while arcs represent the non-price elements of a bilateral contract. given prices for each arc, agents choose the incident arcs that maximize their utility. we enlarge the model to allow for polymatroidal constraints on the set of contracts that may be traded which can be interpreted as modeling limited one for-one substitution. we show that for two-sided markets there exists a competitive equilibrium however for multi-sided markets this may not be possible.", "categories": "econ.th cs.gt", "created": "2020-08-22", "updated": "", "authors": ["can kizilkale", "rakesh vohra"], "url": "https://arxiv.org/abs/2008.09757"}, {"title": "competitive ride-sourcing market with a third-party integrator", "id": "2008.09815", "abstract": "recently, some transportation service providers attempt to integrate the ride services offered by multiple independent ride-sourcing platforms, and passengers are able to request ride through such third-party integrators or connectors and receive service from any one of the platforms. this novel business model, termed as third-party platform-integration in this paper, has potentials to alleviate the cost of market fragmentation due to the demand splitting among multiple platforms. while most existing studies focus on the operation strategies for one single monopolist platform, much less is known about the competition and platform-integration as well as the implications on operation strategy and system efficiency. in this paper, we propose mathematical models to describe the ride-sourcing market with multiple competing platforms and compare system performance metrics between two market scenarios, i.e., with and without platform-integration, at nash equilibrium as well as social optimum. we find that platform-integration can increase total realized demand and social welfare at both nash equilibrium and social optimum, but may not necessarily generate a greater profit when vehicle supply is sufficiently large or/and market is too fragmented. we show that the market with platform-integration generally achieves greater social welfare. on one hand, the integrator in platform-integration is able to generate a thicker market and reduce matching frictions; on the other hand, multiple platforms are still competing by independently setting their prices, which help to mitigate monopoly mark-up as in the monopoly market.", "categories": "econ.gn cs.gt q-fin.ec", "created": "2020-08-22", "updated": "", "authors": ["yaqian zhou", "hai yang", "jintao ke", "hai wang", "xinwei li"], "url": "https://arxiv.org/abs/2008.09815"}, {"title": "lindahl equilibrium as a collective choice rule", "id": "2008.09932", "abstract": "a collective choice problem is a finite set of social alternatives and a finite set of economic agents with vnm utility functions. we associate a public goods economy with each collective choice problem and establish the existence and efficiency of (equal income) lindahl equilibrium allocations. we interpret collective choice problems as cooperative bargaining problems and define a set-valued solution concept, {\\it the equitable solution} (es). we provide axioms that characterize es and show that es contains the nash bargaining solution. our main result shows that the set of es payoffs is the same a the set of lindahl equilibrium payoffs. we consider two applications: in the first, we show that in a large class of matching problems without transfers the set of lindahl equilibrium payoffs is the same as the set of (equal income) walrasian equilibrium payoffs. in our second application, we show that in any discrete exchange economy without transfers every walrasian equilibrium payoff is a lindahl equilibrium payoff of the corresponding collective choice market. moreover, for any cooperative bargaining problem, it is possible to define a set of commodities so that the resulting economy's utility possibility set is that bargaining problem {\\it and} the resulting economy's set of walrasian equilibrium payoffs is the same as the set of lindahl equilibrium payoffs of the corresponding collective choice market.", "categories": "econ.th", "created": "2020-08-22", "updated": "2020-08-26", "authors": ["faruk gul", "wolfgang pesendorfer"], "url": "https://arxiv.org/abs/2008.09932"}, {"title": "implications of the tradeoff between inside and outside social status in   group choice", "id": "2008.10145", "abstract": "we investigate a group choice problem of agents pursuing social status. we assume heterogeneous agents want to signal their private information (ability, income, patience, altruism, etc.) to others, facing tradeoff between \"outside status\" (desire to be perceived in prestigious group from outside observers) and \"inside status\" (desire to be perceived talented from peers inside their group). to analyze the tradeoff, we develop two stage signaling model in which each agent firstly chooses her group and secondly chooses her action in the group she chose. they face binary choice problems both in group and action choices. using cutoff strategy, we construct an partially separating equilibrium such that there are four populations: (i) choosing high group with strong incentive for action in the group, (ii) high group with weak incentive, (iii) low group with strong incentive, and (iv) low group with weak incentive. by comparative statics results, we find some spillover effects from a certain group to another, on how four populations change, when a policy is taken in each group. these results have rich implications for group choice problems like school, firm or residential preference.", "categories": "econ.gn q-fin.ec", "created": "2020-08-23", "updated": "", "authors": ["takaaki hamada"], "url": "https://arxiv.org/abs/2008.10145"}, {"title": "finite-sample average bid auction", "id": "2008.10217", "abstract": "the paper studies the problem of auction design in a setting where the auctioneer accesses the knowledge of the valuation distribution only through statistical samples. a new framework is established that combines the statistical decision theory with mechanism design. two optimality criteria, maxmin, and equivariance, are studied along with their implications on the form of auctions. the simplest form of the equivariant auction is the average bid auction, which set individual reservation prices proportional to the average of other bids and historical samples. this form of auction can be motivated by the gamma distribution, and it sheds new light on the estimation of the optimal price, an irregular parameter. theoretical results show that it is often possible to use the regular parameter population mean to approximate the optimal price. an adaptive average bid estimator is developed under this idea, and it has the same asymptotic properties as the empirical myerson estimator. the new proposed estimator has a significantly better performance in terms of value at risk and expected shortfall when the sample size is small.", "categories": "econ.em econ.th", "created": "2020-08-24", "updated": "", "authors": ["haitian xie"], "url": "https://arxiv.org/abs/2008.10217"}, {"title": "transaction costs: economies of scale, optimum, equilibrium and   efficiency", "id": "2008.10348", "abstract": "the aim of this article is to propose a core game theory model of transaction costs wherein it is indicated how direct costs determine the probability of loss and subsequent transaction costs. the existence of optimum is proven, and the way in which exposure influences the location of the optimum is demonstrated. the decisions are described as a two-player game and it is discussed how the transaction cost sharing rule determines whether the optimum point of transaction costs is the same as the equilibrium of the game. a game modelling dispute between actors regarding changing the share of transaction costs to be paid by each party is also presented. requirements of efficient transaction cost sharing rules are defined, and it is posited that a solution exists which is not unique. policy conclusions are also devised based on principles of design of institutions to influence the nature of transaction costs.", "categories": "econ.th", "created": "2020-08-24", "updated": "", "authors": ["l\u00e1szl\u00f3 k\u00e1llay", "tibor tak\u00e1cs", "l\u00e1szl\u00f3 trautmann"], "url": "https://arxiv.org/abs/2008.10348"}, {"title": "on the equivalence between the kinetic ising model and discrete   autoregressive processes", "id": "2008.10666", "abstract": "binary random variables are the building blocks used to describe a large variety of systems, from magnetic spins to financial time series and neuron activity. in statistical physics the kinetic ising model has been introduced to describe the dynamics of the magnetic moments of a spin lattice, while in time series analysis discrete autoregressive processes have been designed to capture the multivariate dependence structure across binary time series. in this article we provide a rigorous proof of the equivalence between the two models in the range of a unique and invertible map unambiguously linking one model parameters set to the other. our result finds further justification acknowledging that both models provide maximum entropy distributions of binary time series with given means, auto-correlations, and lagged cross-correlations of order one.", "categories": "cond-mat.stat-mech econ.em physics.data-an", "created": "2020-08-24", "updated": "", "authors": ["carlo campajola", "fabrizio lillo", "piero mazzarisi", "daniele tantari"], "url": "https://arxiv.org/abs/2008.10666"}, {"title": "interacting regional policies in containing a disease", "id": "2008.10745", "abstract": "regional quarantine policies, in which a portion of a population surrounding infections are locked down, are an important tool to contain disease. however, jurisdictional governments -- such as cities, counties, states, and countries -- act with minimal coordination across borders. we show that a regional quarantine policy's effectiveness depends upon whether (i) the network of interactions satisfies a balanced-growth condition, (ii) infections have a short delay in detection, and (iii) the government has control over and knowledge of the necessary parts of the network (no leakage of behaviors). as these conditions generally fail to be satisfied, especially when interactions cross borders, we show that substantial improvements are possible if governments are proactive: triggering quarantines in reaction to neighbors' infection rates, in some cases even before infections are detected internally. we also show that even a few lax governments -- those that wait for nontrivial internal infection rates before quarantining -- impose substantial costs on the whole system. our results illustrate the importance of understanding contagion across policy borders and offer a starting point in designing proactive policies for decentralized jurisdictions.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-08-24", "updated": "2020-09-01", "authors": ["arun g. chandrasekhar", "paul goldsmith-pinkham", "matthew o. jackson", "samuel thau"], "url": "https://arxiv.org/abs/2008.10745"}, {"title": "drivers learn city-scale dynamic equilibrium", "id": "2008.10775", "abstract": "understanding collective human behavior and dynamics at urban-scale has drawn broad interest in physics, engineering, and social sciences. social physics often adopts a statistical perspective and treats individuals as interactive elementary units,while the economics perspective sees individuals as strategic decision makers. here we provide a microscopic mechanism of city-scale dynamics,interpret the collective outcome in a thermodynamic framework,and verify its various implications empirically. we capture the decisions of taxi drivers in a game-theoretic model,prove the existence, uniqueness, and global asymptotic stability of nash equilibrium. we offer a macroscopic view of this equilibrium with laws of thermodynamics. with 870 million trips of over 50k drivers in new york city,we verify this equilibrium in space and time,estimate an empirical constitutive relation,and examine the learning process at individual and collective levels. connecting two perspectives,our work shows a promising approach to understand collective behavior of subpopulations.", "categories": "physics.soc-ph econ.gn math.ds q-fin.ec", "created": "2020-08-24", "updated": "", "authors": ["ruda zhang", "roger ghanem"], "url": "https://arxiv.org/abs/2008.10775"}, {"title": "characterizing pareto optima: sequential utilitarian welfare   maximization", "id": "2008.10819", "abstract": "we characterize pareto optimality via sequential utilitarian welfare maximization: a utility vector u is pareto optimal if and only if there exists a finite sequence of nonnegative (and eventually positive) welfare weights such that $u$ maximizes utilitarian welfare with each successive welfare weights among the previous set of maximizers. the characterization can be further related to maximization of a piecewise-linear concave social welfare function and sequential bargaining among agents a la generalized nash bargaining. we provide conditions enabling simpler utilitarian characterizations and a version of the second welfare", "categories": "econ.th", "created": "2020-08-25", "updated": "", "authors": ["yeon-koo che", "jinwoo kim", "fuhito kojima", "christopher thomas ryan"], "url": "https://arxiv.org/abs/2008.10819"}, {"title": "the impact of sodomy law repeals on crime", "id": "2008.10926", "abstract": "we exploit variation in the timing of decriminalization of same-sex sexual intercourse across u.s. states to estimate the impact of these law changes on crime through difference-in-difference and event-study models. we provide the first evidence that sodomy law repeals led to a decline in the number of arrests for disorderly conduct, prostitution, and other sex offenses. furthermore, we show that these repeals led to a reduction in arrests for drug and alcohol consumption.", "categories": "econ.gn q-fin.ec", "created": "2020-08-25", "updated": "", "authors": ["riccardo ciacci", "dario sansone"], "url": "https://arxiv.org/abs/2008.10926"}, {"title": "an energy-based macroeconomic model validated by global historical   series since 1820", "id": "2008.10967", "abstract": "global historical series spanning the last two centuries recently became available for primary energy consumption (pec) and gross domestic product (gdp). based on a thorough analysis of the data, we propose a new, simple macroeconomic model whereby physical power is fueling economic power. from 1820 to 1920, the linearity between global pec and world gdp justifies basic equations where, originally, pec incorporates unskilled human labor that consumes and converts energy from food. in a consistent model, both physical capital and human capital are fed by pec and represent a form of stored energy. in the following century, from 1920 to 2016, gdp grows quicker than pec. periods of quasi-linearity of the two variables are separated by distinct jumps, which can be interpreted as radical technology shifts. the gdp to pec ratio accumulates game-changing innovation, at an average growth rate proportional to pec. these results seed alternative strategies for modeling and for political management of the climate crisis and the energy transition.", "categories": "econ.gn q-fin.ec", "created": "2020-08-25", "updated": "2020-09-24", "authors": ["herve bercegol", "henri benisty"], "url": "https://arxiv.org/abs/2008.10967"}, {"title": "powerful inference", "id": "2008.11140", "abstract": "we develop an inference method for a (sub)vector of parameters identified by conditional moment restrictions, which are implied by economic models such as rational behavior and euler equations. building on bierens (1990), we propose penalized maximum statistics and combine bootstrap inference with model selection. our method is optimized to be powerful against a set of local alternatives of interest by solving a data-dependent max-min problem for tuning parameter selection. we demonstrate the efficacy of our method by a proof of concept using two empirical examples: rational unbiased reporting of ability status and the elasticity of intertemporal substitution.", "categories": "econ.em math.st stat.th", "created": "2020-08-25", "updated": "", "authors": ["xiaohong chen", "sokbae lee", "myung hwan seo"], "url": "https://arxiv.org/abs/2008.11140"}, {"title": "formula to determine the countries equilibrium exchange rate with the   dollar and proposal for a second bretton woods conference", "id": "2008.11275", "abstract": "this paper presents the way in which can be determined the exchange rates that simultaneously balance the trade balances of all countries that trade with each other within a common market. a mathematical synthesis between the theory of comparative advantages of ricardo and mill and the new theory on international trade of paul krugman is also presented in this paper. this mathematical synthesis shows that these theories are complementary. also presented in this paper is a proposal to organize a common market of the american hemisphere. this economic alliance would allow to establish a political alliance for the common defense of the entire american hemisphere. the formula we developed in this paper to determine the exchange rates of the countries solves the problem that mexico, canada, europe, japan and china currently experience with the united states in relation to the deficits and surpluses in the trade balance of the countries and their consequent impediment so that stable growth of international trade can be achieved.", "categories": "econ.gn q-fin.ec", "created": "2020-08-24", "updated": "", "authors": ["walter h. bruckman"], "url": "https://arxiv.org/abs/2008.11275"}, {"title": "potential impacts of ballast water regulations on international trade,   shipping patterns, and the global economy: an integrated transportation and   economic modeling assessment", "id": "2008.11334", "abstract": "global ballast water management regulations aiming to decrease aquatic species invasion require actions that can increase shipping costs. we employ an integrated shipping cost and global economic modeling approach to investigate the impacts of ballast water regulations on bilateral trade, national economies, and shipping patterns. given the potential need for more stringent regulation at regional hotspots of species invasions, this work considers two ballast water treatment policy scenarios: implementation of current international regulations, and a possible stricter regional regulation that targets ships traveling to and from the united states while other vessels continue to face current standards. we find that ballast water management compliance costs under both scenarios lead to modest negative impacts on international trade and national economies overall. however, stricter regulations applied to u.s. ports are expected to have large negative impacts on bilateral trade of several specific commodities for a few countries. trade diversion causes decreased u.s. imports of some products, leading to minor economic welfare losses.", "categories": "econ.gn q-fin.ec", "created": "2020-08-25", "updated": "", "authors": ["zhaojun wang", "duy nong", "amanda m. countryman", "james j. corbett", "travis warziniack"], "url": "https://arxiv.org/abs/2008.11334"}, {"title": "a spatial analysis of disposable income in ireland: a gwr approach", "id": "2008.11720", "abstract": "this paper examines the spatial distribution of income in ireland. median gross household disposable income data from the cso, available at the electoral division (ed) level, is used to explore the spatial variability in income. geary's c highlights the spatial dependence of income, highlighting that the distribution of income is not random across space and is influenced by location. given the presence of spatial autocorrelation, utilising a global ols regression will lead to biased results. geographically weighted regression (gwr) is used to examine the spatial heterogeneity of income and the impact of local demographic drivers on income. gwr results show the demographic drivers have varying levels of influence on income across locations. lone parent has a stronger negative impact in the cork commuter belt than it does in the dublin commuter belt. the relationship between household income and the demographic context of the area is a complicated one. this paper attempts to examine these relationships acknowledging the impact of space.", "categories": "econ.gn q-fin.ec", "created": "2020-08-26", "updated": "", "authors": ["paul kilgarriff", "martin charlton"], "url": "https://arxiv.org/abs/2008.11720"}, {"title": "changes in mobility and socioeconomic conditions in bogot\\'a city during   the covid-19 outbreak", "id": "2008.11850", "abstract": "we analyze mobility changes following the implementation of containment measures aimed at mitigating the spread of covid-19 in bogot\\'a, colombia. we characterize the mobility network before and during the pandemic and analyze its evolution and changes between january and july 2020. we then link the observed mobility changes to socioeconomic conditions, estimating a gravity model to assess the effect of socioeconomic conditions on mobility flows. we observe an overall reduction in mobility trends, but the overall connectivity between different areas of the city remains after the lockdown, reflecting the mobility network's resilience. we find that the responses to lockdown policies depend on socioeconomic conditions. before the pandemic, the population with better socioeconomic conditions shows higher mobility flows. since the lockdown, mobility presents a general decrease, but the population with worse socioeconomic conditions shows lower decreases in mobility flows. we conclude deriving policy implications.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-08-26", "updated": "", "authors": ["marco due\u00f1as", "mercedes campi", "luis olmos"], "url": "https://arxiv.org/abs/2008.11850"}, {"title": "a competitive search game with a moving target", "id": "2008.12032", "abstract": "we introduce a discrete-time search game, in which two players compete to find an object first. the object moves according to a time-varying markov chain on finitely many states. the players know the markov chain and the initial probability distribution of the object, but do not observe the current state of the object. the players are active in turns. the active player chooses a state, and this choice is observed by the other player. if the object is in the chosen state, this player wins and the game ends. otherwise, the object moves according to the markov chain and the game continues at the next period.   we show that this game admits a value, and for any error-term $\\veps>0$, each player has a pure (subgame-perfect) $\\veps$-optimal strategy. interestingly, a 0-optimal strategy does not always exist. the $\\veps$-optimal strategies are robust in the sense that they are $2\\veps$-optimal on all finite but sufficiently long horizons, and also $2\\veps$-optimal in the discounted version of the game provided that the discount factor is close to 1. we derive results on the analytic and structural properties of the value and the $\\veps$-optimal strategies. moreover, we examine the performance of the finite truncation strategies, which are easy to calculate and to implement. we devote special attention to the important time-homogeneous case, where additional results hold.", "categories": "cs.gt econ.th math.pr", "created": "2020-08-27", "updated": "", "authors": ["benoit duvocelle", "j\u00e1nos flesch", "mathias staudigl", "dries vermeulen"], "url": "https://arxiv.org/abs/2008.12032"}, {"title": "how much ad viewability is enough? the effect of display ad viewability   on advertising effectiveness", "id": "2008.12132", "abstract": "a large share of all online display advertisements (ads) are never seen by a human. for instance, an ad could appear below the page fold, where a user never scrolls. yet, an ad is essentially ineffective if it is not at least somewhat viewable. ad viewability - which refers to the pixel percentage-in-view and the exposure duration of an online display ad - has recently garnered great interest among digital advertisers and publishers. however, we know very little about the impact of ad viewability on advertising effectiveness. we work to close this gap by analyzing a large-scale observational data set with more than 350,000 ad impressions similar to the data sets that are typically available to digital advertisers and publishers. this analysis reveals that longer exposure durations (>10 seconds) and 100% visible pixels do not appear to be optimal in generating view-throughs. the highest view-through rates seem to be generated with relatively lower pixel/second-combinations of 50%/1, 50%/5, 75%/1, and 75%/5. however, this analysis does not account for user behavior that may be correlated with or even drive ad viewability and may therefore result in endogeneity issues. consequently, we manipulated ad viewability in a randomized online experiment for a major european news website, finding the highest ad recognition rates among relatively higher pixel/second-combinations of 75%/10, 100%/5 and 100%/10. everything below 75\\% or 5 seconds performs worse. yet, we find that it may be sufficient to have either a long exposure duration or high pixel percentage-in-view to reach high advertising effectiveness. our results provide guidance to advertisers enabling them to establish target viewability rates more appropriately and to publishers who wish to differentiate their viewability products.", "categories": "cs.cy econ.gn q-fin.ec", "created": "2020-08-26", "updated": "", "authors": ["christina uhl", "nadia abou nabout", "klaus miller"], "url": "https://arxiv.org/abs/2008.12132"}, {"title": "complexity science approach to economic crime", "id": "2008.12364", "abstract": "in this comment we discuss how complexity science and network science are particularly useful for identifying and describing the hidden traces of economic misbehaviour such as fraud and corruption.", "categories": "physics.soc-ph cs.si econ.gn q-fin.ec", "created": "2020-08-27", "updated": "", "authors": ["j\u00e1nos kert\u00e9sz", "johannes wachs"], "url": "https://arxiv.org/abs/2008.12364"}, {"title": "efficient closed-form estimation of large spatial autoregressions", "id": "2008.12395", "abstract": "newton-step approximations to pseudo maximum likelihood estimates of spatial autoregressive models with a large number of parameters are examined, in the sense that the parameter space grows slowly as a function of sample size. these have the same asymptotic efficiency properties as maximum likelihood under gaussianity but are of closed form. hence they are computationally simple and free from compactness assumptions, thereby avoiding two notorious pitfalls of implicitly defined estimates of large spatial autoregressions. for an initial least squares estimate, the newton step can also lead to weaker regularity conditions for a central limit theorem than those extant in the literature. a simulation study demonstrates excellent finite sample gains from newton iterations, especially in large multiparameter models for which grid search is costly. a small empirical illustration shows improvements in estimation precision with real data.", "categories": "econ.em stat.me", "created": "2020-08-27", "updated": "2020-09-11", "authors": ["abhimanyu gupta"], "url": "https://arxiv.org/abs/2008.12395"}, {"title": "layoffs, inequity and covid-19: a longitudinal study of the journalism   jobs crisis in australia from 2012 to 2020", "id": "2008.12459", "abstract": "in australia and beyond, journalism is reportedly an industry in crisis, a crisis exacerbated by covid-19. however, the evidence revealing the crisis is often anecdotal or limited in scope. in this unprecedented longitudinal research, we draw on data from the australian journalism jobs market from january 2012 until march 2020. using data science and machine learning techniques, we analyse two distinct data sets: job advertisements (ads) data comprising 3,698 journalist job ads from a corpus of over 6.7 million australian job ads; and official employment data from the australian bureau of statistics. having matched and analysed both sources, we address both the demand for and supply of journalists in australia over this critical period. the data show that the crisis is real, but there are also surprises. counter-intuitively, the number of journalism job ads in australia rose from 2012 until 2016, before falling into decline. less surprisingly, for the entire period studied the figures reveal extreme volatility, characterised by large and erratic fluctuations. the data also clearly show that covid-19 has significantly worsened the crisis. we can also tease out more granular findings, including: that there are now more women than men journalists in australia, but that gender inequity is worsening, with women journalists getting younger and worse-paid just as men journalists are, on average, getting older and better-paid; that, despite the crisis besetting the industry, the demand for journalism skills has increased; and that the skills sought by journalism job ads increasingly include social media and generalist communications.", "categories": "econ.gn q-fin.ec", "created": "2020-08-27", "updated": "", "authors": ["nik dawson", "sacha molitorisz", "marian-andrei rizoiu", "peter fray"], "url": "https://arxiv.org/abs/2008.12459"}, {"title": "how is machine learning useful for macroeconomic forecasting?", "id": "2008.12477", "abstract": "we move beyond \"is machine learning useful for macroeconomic forecasting?\" by adding the \"how\". the current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. in contrast, we study the usefulness of the underlying features driving ml gains over standard macroeconometric methods. we distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. to do so, we design experiments that allow to identify the \"treatment\" effects of interest. we conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) k-fold cross-validation is the best practice and (iv) the $l_2$ is preferred to the $\\bar \\epsilon$-insensitive in-sample loss. the forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. this suggests that machine learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions.", "categories": "econ.em stat.ap stat.ml", "created": "2020-08-28", "updated": "", "authors": ["philippe goulet coulombe", "maxime leroux", "dalibor stevanovic", "st\u00e9phane surprenant"], "url": "https://arxiv.org/abs/2008.12477"}, {"title": "nowcasting in a pandemic using non-parametric mixed frequency vars", "id": "2008.12706", "abstract": "this paper develops bayesian econometric methods for posterior and predictive inference in a non-parametric mixed frequency var using additive regression trees. we argue that regression tree models are ideally suited for macroeconomic nowcasting in the face of the extreme observations produced by the pandemic due to their flexibility and ability to model outliers. in a nowcasting application involving four major countries in the european union, we find substantial improvements in nowcasting performance relative to a linear mixed frequency var. a detailed examination of the predictive densities in the first six months of 2020 shows where these improvements are achieved.", "categories": "econ.em stat.ap stat.ml", "created": "2020-08-28", "updated": "2020-09-08", "authors": ["florian huber", "gary koop", "luca onorante", "michael pfarrhofer", "josef schreiner"], "url": "https://arxiv.org/abs/2008.12706"}, {"title": "better lee bounds", "id": "2008.12720", "abstract": "this paper develops methods for tightening lee (2009) bounds on average causal effects when the number of pre-randomization covariates is large, potentially exceeding the sample size. these better lee bounds are guaranteed to be sharp when few of the covariates affect the selection and the outcome. if this sparsity assumption fails, the bounds remain valid. i propose inference methods that enable hypothesis testing in either case. my results rely on a weakened monotonicity assumption that only needs to hold conditional on covariates. i show that the unconditional monotonicity assumption that motivates traditional lee bounds fails for the jobcorps training program. after imposing only conditional monotonicity, better lee bounds are found to be much more informative than standard lee bounds in a variety of settings.", "categories": "econ.em", "created": "2020-08-28", "updated": "", "authors": ["vira semenova"], "url": "https://arxiv.org/abs/2008.12720"}, {"title": "the identity fragmentation bias", "id": "2008.12849", "abstract": "consumers interact with firms across multiple devices, browsers, and machines; these interactions are often recorded with different identifiers for the same individual. the failure to correctly match different identities leads to a fragmented view of exposures and behaviors. this paper studies the identity fragmentation bias, referring to the estimation bias resulted from using fragmented data. using a formal framework, we decompose the contributing factors of the estimation bias caused by data fragmentation and discuss the direction of bias. contrary to conventional wisdom, this bias cannot be signed or bounded under standard assumptions. instead, upward biases and sign reversals can occur even in experimental settings. we then propose and compare several corrective measures, and demonstrate their performances using an empirical application.", "categories": "econ.em stat.ap", "created": "2020-08-28", "updated": "", "authors": ["tesary lin", "sanjog misra"], "url": "https://arxiv.org/abs/2008.12849"}, {"title": "implication of natal care and maternity leave on child morbidity:   evidence from ghana", "id": "2008.12910", "abstract": "failure to receive post-natal care within first week of delivery causes a 3% increase in the possibility of acute respiratory infection in children under five. mothers with unpaid maternity leave put their children at a risk of 3.9% increase in the possibility of ari compared to those with paid maternity leave.", "categories": "econ.gn q-fin.ec", "created": "2020-08-29", "updated": "", "authors": ["danny turkson", "joy kafui ahiabor"], "url": "https://arxiv.org/abs/2008.12910"}, {"title": "proportional participatory budgeting with cardinal utilities", "id": "2008.13276", "abstract": "we study voting rules for participatory budgeting, where a group of voters collectively decides which projects should be funded using a common budget. we allow the projects to have arbitrary costs, and the voters to have arbitrary additive valuations over the projects. we formulate two axioms that guarantee proportional representation to groups of voters with common interests. to the best of our knowledge, all known rules for participatory budgeting do not satisfy either of the two axioms; in addition we show that the most prominent proportional rules for committee elections (such as proportional approval voting) cannot be adapted to arbitrary costs nor to additive valuations so that they would satisfy our axioms of proportionality. we construct a simple and attractive voting rule that satisfies one of our axioms (for arbitrary costs and arbitrary additive valuations), and that can be evaluated in polynomial time. we prove that our other stronger axiom is also satisfiable, though by a computationally more expensive and less natural voting rule.", "categories": "cs.gt econ.th", "created": "2020-08-30", "updated": "", "authors": ["dominik peters", "grzegorz pierczy\u0144ski", "piotr skowron"], "url": "https://arxiv.org/abs/2008.13276"}, {"title": "a myopic adjustment process for mean field games with finite state and   action space", "id": "2008.13420", "abstract": "in this paper, we introduce a natural learning rule for mean field games with finite state and action space, the so-called myopic adjustment process. the main motivation for these considerations are the complex computations necessary to determine dynamic mean-field equilibria, which make it seem questionable whether agents are indeed able to play these equilibria. we prove that the myopic adjustment process converges locally towards stationary equilibria with deterministic equilibrium strategies under rather broad conditions. moreover, for a two-strategy setting, we also obtain a global convergence result under stronger, yet intuitive conditions.", "categories": "math.oc econ.th", "created": "2020-08-31", "updated": "", "authors": ["berenice anne neumann"], "url": "https://arxiv.org/abs/2008.13420"}, {"title": "optimal border control during the re-opening phase of the covid-19   pandemic", "id": "2008.13561", "abstract": "most of the existing literature on the current pandemic focuses on approaches to model the outbreak and spreading of covid-19. this paper proposes a generalized markov-switching approach, the suihr model, designed to study border control policies and contact tracing against covid-19 in a period where countries start to re-open. we offer the following contributions. first, the suihr model can include multiple entities, reflecting different government bodies with different containment measures. second, constraints as, for example, new case targets and medical resource limits can be imposed in a linear programming framework. third, in contrast to most sir models, we focus on the spreading of infectious people without symptoms instead of the spreading of people who are already showing symptoms. we find that even if a country has closed its borders completely, domestic contact tracing is not enough to go back to normal life. countries having successfully controlled the virus can keep it under check as long as imported risk is not growing, meaning they can lift travel restrictions with similar countries. however, opening borders towards countries with less controlled infection dynamics would require a mandatory quarantine or a strict test on arrival.", "categories": "q-bio.pe econ.gn q-fin.ec", "created": "2020-08-28", "updated": "2020-09-01", "authors": ["zhen zhu", "enzo weber", "till strohsal", "duaa serhan"], "url": "https://arxiv.org/abs/2008.13561"}, {"title": "causal inference in possibly nonlinear factor models", "id": "2008.13651", "abstract": "this paper develops a general causal inference method for treatment effects models under selection on unobservables. a large set of covariates that admits an unknown, possibly nonlinear factor structure is exploited to control for the latent confounders. the key building block is a local principal subspace approximation procedure that combines $k$-nearest neighbors matching and principal component analysis. estimators of many causal parameters, including average treatment effects and counterfactual distributions, are constructed based on doubly-robust score functions. large-sample properties of these estimators are established, which only require relatively mild conditions on the principal subspace approximation. the results are illustrated with an empirical application studying the effect of political connections on stock returns of financial firms, and a monte carlo experiment. the main technical and methodological results regarding the general local principal subspace approximation method may be of independent interest.", "categories": "econ.em stat.me stat.ml", "created": "2020-08-31", "updated": "", "authors": ["yingjie feng"], "url": "https://arxiv.org/abs/2008.13651"}, {"title": "reviewing climate change and agricultural market competitiveness", "id": "2008.13726", "abstract": "the paper is a collection of knowledge regarding the phenomenon of climate change, competitiveness, and literature linking the two phenomena to agricultural market competitiveness. the objective is to investigate the peer reviewed and grey literature on the subject to explore the link between climate change and agricultural market competitiveness and also explore an appropriate technique to validate the presumed relationship empirically. the paper concludes by identifying implications for developing an agricultural competitiveness index while incorporating the climate change impacts, to enhance the potential of agricultural markets for optimizing the agricultural sectors competitiveness.", "categories": "econ.gn q-fin.ec", "created": "2020-08-31", "updated": "", "authors": ["bakhtmina zia", "dr muhammad rafiq phd research scholar", "institute of management sciences", "n/a peshawar", "n/a pakistan", "associate professor", "institute of management sciences", "n/a peshawar", "n/a pakistan"], "url": "https://arxiv.org/abs/2008.13726"}, {"title": "robust semiparametric estimation in panel multinomial choice models", "id": "2009.00085", "abstract": "this paper proposes a robust method for semiparametric identification and estimation in panel multinomial choice models, where we allow for infinite-dimensional fixed effects that enter into consumer utilities in an additively nonseparable way, thus incorporating rich forms of unobserved heterogeneity. our identification strategy exploits multivariate monotonicity in parametric indexes, and uses the logical contraposition of an intertemporal inequality on choice probabilities to obtain identifying restrictions. we provide a consistent estimation procedure, and demonstrate the practical advantages of our method with simulations and an empirical illustration with the nielsen data.", "categories": "econ.em", "created": "2020-08-31", "updated": "", "authors": ["wayne yuan gao", "ming li"], "url": "https://arxiv.org/abs/2009.00085"}, {"title": "inclass nets: independent classifier networks for nonparametric   estimation of conditional independence mixture models and unsupervised   classification", "id": "2009.00131", "abstract": "we introduce a new machine-learning-based approach, which we call the independent classifier networks (inclass nets) technique, for the nonparameteric estimation of conditional independence mixture models (cimms). we approach the estimation of a cimm as a multi-class classification problem, since dividing the dataset into different categories naturally leads to the estimation of the mixture model. inclass nets consist of multiple independent classifier neural networks (nns), each of which handles one of the variates of the cimm. fitting the cimm to the data is performed by simultaneously training the individual nns using suitable cost functions. the ability of nns to approximate arbitrary functions makes our technique nonparametric. further leveraging the power of nns, we allow the conditionally independent variates of the model to be individually high-dimensional, which is the main advantage of our technique over existing non-machine-learning-based approaches. we derive some new results on the nonparametric identifiability of bivariate cimms, in the form of a necessary and a (different) sufficient condition for a bivariate cimm to be identifiable. we provide a public implementation of inclass nets as a python package called raindancesvi and validate our inclass nets technique with several worked out examples. our method also has applications in unsupervised and semi-supervised classification problems.", "categories": "stat.ml cs.lg econ.em hep-ph physics.data-an stat.me", "created": "2020-08-31", "updated": "", "authors": ["konstantin t. matchev", "prasanth shyamsundar"], "url": "https://arxiv.org/abs/2009.00131"}, {"title": "an optimal test for strategic interaction in social and economic network   formation between heterogeneous agents", "id": "2009.00212", "abstract": "we introduce a test for whether agents' preferences over network structure are interdependent. interdependent preferences induce strategic behavior since the optimal set of links directed by agent $i$ will vary with the configuration of links directed by other agents.   our model also incorporates agent-specific in- and out-degree heterogeneity and homophily on observable agent attributes. this introduces $2n+k^2$ nuisance parameters ($n$ is number of agents in the network and $k$ the number of possible agent attribute configurations).   under the null equilibrium is unique, but our hypothesis is nevertheless a composite one as the degree heterogeneity and homophily nuisance parameters may range freely across their parameter space. under the alternative our model is incomplete; there may be multiple equilibrium network configurations and our test is agnostic about which one is selected.   motivated by size control, and exploiting the exponential family structure of our model \\emph{under the null}, we restrict ourselves to conditional tests. we characterize the exact null distribution of a family of conditional tests and introduce a novel markov chain monte carlo (mcmc) algorithm for simulating this distribution.   we also characterize the locally best test. the form of this test depends upon the gradient of the likelihood with respect to the strategic interaction parameter in the neighborhood of the null. remarkably, this gradient, and consequently the form of the locally best test statistic, does not depend on how an equilibrium is selected. exploiting this lack of dependence, we outline a feasible version of the locally best test.", "categories": "econ.em", "created": "2020-08-31", "updated": "", "authors": ["andrin pelican", "bryan s. graham"], "url": "https://arxiv.org/abs/2009.00212"}, {"title": "time-varying parameters as ridge regressions", "id": "2009.00401", "abstract": "time-varying parameters (tvps) models are frequently used in economics to model structural change. i show that they are in fact ridge regressions. instantly, this makes computations, tuning, and implementation much easier than in the state-space paradigm. among other things, solving the equivalent dual ridge problem is computationally very fast even in high dimensions, and the crucial \"amount of time variation\" is tuned by cross-validation. evolving volatility is dealt with using a two-step ridge regression. i consider extensions that incorporate sparsity (the algorithm selects which parameters vary and which do not) and reduced-rank restrictions (variation is tied to a factor model). to demonstrate the usefulness of the approach, i use it to study the evolution of monetary policy in canada. the application requires the estimation of about 4600 tvps, a task well within the reach of the new method.", "categories": "econ.em stat.ap stat.ml", "created": "2020-09-01", "updated": "", "authors": ["philippe goulet coulombe"], "url": "https://arxiv.org/abs/2009.00401"}, {"title": "instrumental variable quantile regression", "id": "2009.00436", "abstract": "this chapter reviews the instrumental variable quantile regression model of chernozhukov and hansen (2005). we discuss the key conditions used for identification of structural quantile effects within this model which include the availability of instruments and a restriction on the ranks of structural disturbances. we outline several approaches to obtaining point estimates and performing statistical inference for model parameters. finally, we point to possible directions for future research.", "categories": "econ.em", "created": "2020-08-28", "updated": "", "authors": ["victor chernozhukov", "christian hansen", "kaspar wuthrich"], "url": "https://arxiv.org/abs/2009.00436"}, {"title": "incentives, lockdown, and testing: from thucydides's analysis to the   covid-19 pandemic", "id": "2009.00484", "abstract": "we consider the control of the covid-19 pandemic via incentives, through either stochastic sis or sir compartmental models. when the epidemic is ongoing, the population can reduce interactions between individuals in order to decrease the rate of transmission of the disease, and thus limit the epidemic. however, this effort comes at a cost for the population. therefore, the government can put into place incentive policies to encourage the lockdown of the population. in addition, the government may also implement a testing policy in order to know more precisely the spread of the epidemic within the country, and to isolate infected individuals. we provide numerical examples, as well as an extension to a stochastic seir compartmental model to account for the relatively long latency period of the covid-19 disease. the numerical results confirm the relevance of a tax and testing policy to improve the control of an epidemic. more precisely, if a tax policy is put into place, even in the absence of a specific testing policy, the population is encouraged to significantly reduce its interactions, thus limiting the spread of the disease. if the government also adjusts its testing policy, less effort is required on the population side, so individuals can interact almost as usual, and the epidemic is largely contained by the targeted isolation of positively-tested individuals.", "categories": "q-bio.pe econ.th math.oc math.pr physics.soc-ph", "created": "2020-09-01", "updated": "", "authors": ["emma hubert", "thibaut mastrolia", "dylan possama\u00ef", "xavier warin"], "url": "https://arxiv.org/abs/2009.00484"}, {"title": "finding core members of cooperative games using agent-based modeling", "id": "2009.00519", "abstract": "agent-based modeling (abm) is a powerful paradigm to gain insight into social phenomena. one area that abm has rarely been applied is coalition formation. traditionally, coalition formation is modeled using cooperative game theory. in this paper, a heuristic algorithm is developed that can be embedded into an abm to allow the agents to find coalition. the resultant coalition structures are comparable to those found by cooperative game theory solution approaches, specifically, the core. a heuristic approach is required due to the computational complexity of finding a cooperative game theory solution which limits its application to about only a score of agents. the abm paradigm provides a platform in which simple rules and interactions between agents can produce a macro-level effect without the large computational requirements. as such, it can be an effective means for approximating cooperative game solutions for large numbers of agents. our heuristic algorithm combines agent-based modeling and cooperative game theory to help find agent partitions that are members of a games' core solution. the accuracy of our heuristic algorithm can be determined by comparing its outcomes to the actual core solutions. this comparison achieved by developing an experiment that uses a specific example of a cooperative game called the glove game. the glove game is a type of exchange economy game. finding the traditional cooperative game theory solutions is computationally intensive for large numbers of players because each possible partition must be compared to each possible coalition to determine the core set; hence our experiment only considers games of up to nine players. the results indicate that our heuristic approach achieves a core solution over 90% of the time for the games considered in our experiment.", "categories": "cs.ma cs.ai econ.th", "created": "2020-08-30", "updated": "", "authors": ["daniele vernon-bido", "andrew j. collins"], "url": "https://arxiv.org/abs/2009.00519"}, {"title": "high-resolution poverty maps in sub-saharan africa", "id": "2009.00544", "abstract": "up-to-date poverty maps are an important tool for policy makers, but until now, have been prohibitively expensive to produce. we propose a generalizable prediction methodology to produce poverty maps at the village level using geospatial data and machine learning algorithms. we tested the proposed method for 25 sub-saharan african countries and validated them against survey data. the proposed method can increase the validity of both single country and cross-country estimations leading to higher precision in poverty maps of 44 sub-saharan african countries than previously available. more importantly, our cross-country estimation enables the creation of poverty maps when it is not practical or cost-effective to field new national household surveys, as is the case with many low- and middle-income countries.", "categories": "cs.cy cs.lg econ.gn q-fin.ec", "created": "2020-09-01", "updated": "2020-09-28", "authors": ["kamwoo lee", "jeanine braithwaite"], "url": "https://arxiv.org/abs/2009.00544"}, {"title": "a vector monotonicity assumption for multiple instruments", "id": "2009.00553", "abstract": "when a researcher wishes to use multiple instrumental variables for a single binary treatment, the familiar late monotonicity assumption can become restrictive: it requires that all units share a common direction of response even when different instruments are shifted in opposing directions. what i call vector monotonicity, by contrast, simply restricts treatment status to be monotonic in each instrument separately. this is a natural assumption in many contexts, capturing the intuitive notion of \"no defiers\" for each instrument. i show that in a setting with a binary treatment and multiple discrete instruments, a class of causal parameters is point identified under vector monotonicity, including the average treatment effect among units that are responsive to any particular subset of the instruments. i propose a simple \"2sls-like\" estimator for the family of identified treatment effect parameters. an empirical application revisits the labor market returns to college education.", "categories": "econ.em", "created": "2020-09-01", "updated": "", "authors": ["leonard goff"], "url": "https://arxiv.org/abs/2009.00553"}, {"title": "bear markets and recessions versus bull markets and expansions", "id": "2009.01343", "abstract": "this paper examines the dynamic interaction between falling and rising markets for both the real and the financial sectors of the largest economy in the world using asymmetric causality tests. these tests require that each underlying variable in the model be transformed into partial sums of the positive and negative components. the positive components represent the rising markets and the negative components embody the falling markets. the sample period covers some part of the covid19 pandemic. since the data is non normal and the volatility is time varying, the bootstrap simulations with leverage adjustments are used in order to create reliable critical values when causality tests are conducted. the results of the asymmetric causality tests disclose that the bear markets are causing the recessions as well as the bull markets are causing the economic expansions. the causal effect of bull markets on economic expansions is higher compared to the causal effect of bear markets on economic recessions. in addition, it is found that economic expansions cause bull markets but recessions do not cause bear markets. thus, the policies that remedy the falling financial markets can also help the economy when it is in a recession.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-08-20", "updated": "2020-09-07", "authors": ["abdulnasser hatemi-j"], "url": "https://arxiv.org/abs/2009.01343"}, {"title": "eliciting information from sensitive survey questions", "id": "2009.01430", "abstract": "this paper considers how to elicit information from sensitive survey questions. first we thoroughly evaluate list experiments (le), a leading method in the experimental literature on sensitive questions. our empirical results demonstrate that the assumptions required to identify sensitive information in le are violated for the majority of surveys. next we propose a novel survey method, called multiple response technique (mrt), for eliciting information from sensitive questions. we require all of the respondents to answer three questions related to the sensitive information. this technique recovers sensitive information at a disaggregated level while still allowing arbitrary misreporting in survey responses. an application of the mrt provides novel empirical evidence on sexual orientation and lesbian, gay, bisexual, and transgender (lgbt)-related sentiment.", "categories": "econ.gn q-fin.ec", "created": "2020-09-02", "updated": "", "authors": ["yonghong an", "pengfei liu"], "url": "https://arxiv.org/abs/2009.01430"}, {"title": "hidden group time profiles: heterogeneous drawdown behaviours in   retirement", "id": "2009.01505", "abstract": "this article investigates retirement decumulation behaviours using the grouped fixed-effects (gfe) estimator applied to australian panel data on drawdowns from phased withdrawal retirement income products. behaviours exhibited by the distinct latent groups identified suggest that retirees may adopt simple heuristics determining how they draw down their accumulated wealth. two extensions to the original gfe methodology are proposed: a latent group label-matching procedure which broadens bootstrap inference to include the time profile estimates, and a modified estimation procedure for models with time-invariant additive fixed effects estimated using unbalanced data.", "categories": "econ.em", "created": "2020-09-03", "updated": "", "authors": ["igor balnozan", "denzil g. fiebig", "anthony asher", "robert kohn", "scott a. sisson"], "url": "https://arxiv.org/abs/2009.01505"}, {"title": "a robust score-driven filter for multivariate time series", "id": "2009.01517", "abstract": "a novel multivariate score-driven model is proposed to extract signals from noisy vector processes. by assuming that the conditional location vector from a multivariate student's t distribution changes over time, we construct a robust filter which is able to overcome several issues that naturally arise when modeling heavy-tailed phenomena and, more in general, vectors of dependent non-gaussian time series. we derive conditions for stationarity and invertibility and estimate the unknown parameters by maximum likelihood. strong consistency and asymptotic normality of the estimator are proved and the finite sample properties are illustrated by a monte-carlo study. from a computational point of view, analytical formulae are derived, which consent to develop estimation procedures based on the fisher scoring method. the theory is supported by a novel empirical illustration that shows how the model can be effectively applied to estimate consumer prices from home scanner data.", "categories": "econ.em stat.me", "created": "2020-09-03", "updated": "", "authors": ["enzo d'innocenzo", "alessandra luati", "mario mazzocchi"], "url": "https://arxiv.org/abs/2009.01517"}, {"title": "deep learning in science", "id": "2009.01575", "abstract": "much of the recent success of artificial intelligence (ai) has been spurred on by impressive achievements within a broader family of machine learning methods, commonly referred to as deep learning (dl). this paper provides insights on the diffusion and impact of dl in science. through a natural language processing (nlp) approach on the arxiv.org publication corpus, we delineate the emerging dl technology and identify a list of relevant search terms. these search terms allow us to retrieve dl-related publications from web of science across all sciences. based on that sample, we document the dl diffusion process in the scientific system. we find i) an exponential growth in the adoption of dl as a research tool across all sciences and all over the world, ii) regional differentiation in dl application domains, and iii) a transition from interdisciplinary dl applications to disciplinary research within application domains. in a second step, we investigate how the adoption of dl methods affects scientific development. therefore, we empirically assess how dl adoption relates to re-combinatorial novelty and scientific impact in the health sciences. we find that dl adoption is negatively correlated with re-combinatorial novelty, but positively correlated with expectation as well as variance of citation performance. our findings suggest that dl does not (yet?) work as an autopilot to navigate complex knowledge landscapes and overthrow their structure. however, the 'dl principle' qualifies for its versatility as the nucleus of a general scientific method that advances science in a measurable way.", "categories": "cs.cy econ.em", "created": "2020-09-03", "updated": "2020-09-04", "authors": ["stefano bianchini", "moritz m\u00fcller", "pierre pelletier"], "url": "https://arxiv.org/abs/2009.01575"}, {"title": "using household grants to benchmark the cost effectiveness of a usaid   workforce readiness program", "id": "2009.01749", "abstract": "we use a randomized experiment to compare a workforce training program to cash transfers in rwanda. conducted in a sample of poor and underemployed youth, this study measures the impact of the training program not only relative to a control group but relative to the counterfactual of simply disbursing the cost of the program directly to beneficiaries. while the training program was successful in improving a number of core outcomes (productive hours, assets, savings, and subjective well-being), cost-equivalent cash transfers move all these outcomes as well as consumption, income, and wealth. in the head-to-head costing comparison cash proves superior across a number of economic outcomes, while training outperforms cash only in the production of business knowledge. we find little evidence of complementarity between human and physical capital interventions, and no signs of heterogeneity or spillover effects.", "categories": "econ.gn q-fin.ec", "created": "2020-09-02", "updated": "", "authors": ["craig mcintosh", "andrew zeitlin"], "url": "https://arxiv.org/abs/2009.01749"}, {"title": "the role of parallel trends in event study settings: an application to   environmental economics", "id": "2009.01963", "abstract": "difference-in-differences (did) research designs usually rely on variation of treatment timing such that, after making an appropriate parallel trends assumption, one can identify, estimate, and make inference about causal effects. in practice, however, different did procedures rely on different parallel trends assumptions (pta), and recover different causal parameters. in this paper, we focus on staggered did (also referred as event-studies) and discuss the role played by the pta in terms of identification and estimation of causal parameters. we document a ``robustness'' vs. ``efficiency'' trade-off in terms of the strength of the underlying pta, and argue that practitioners should be explicit about these trade-offs whenever using did procedures. we propose new did estimators that reflect these trade-offs and derived their large sample properties. we illustrate the practical relevance of these results by assessing whether the transition from federal to state management of the clean water act affects compliance rates.", "categories": "econ.em", "created": "2020-09-03", "updated": "", "authors": ["michelle marcus", "pedro h. c. sant'anna"], "url": "https://arxiv.org/abs/2009.01963"}, {"title": "instrument validity for heterogeneous causal effects", "id": "2009.01995", "abstract": "this paper provides a general framework for testing instrument validity in heterogeneous causal effect models. we first generalize the testable implications of the instrument validity assumption provided by balke and pearl (1997), imbens and rubin (1997), and heckman and vytlacil (2005). the generalization involves the cases where the treatment can be multivalued (and ordered) or unordered, and there can be conditioning covariates. based on these testable implications, we propose a nonparametric test which is proved to be asymptotically size controlled and consistent. because of the nonstandard nature of the problem in question, the test statistic is constructed based on a nonsmooth map, which causes technical complications. we provide an extended continuous mapping theorem and an extended delta method, which may be of independent interest, to establish the asymptotic distribution of the test statistic under null. we then extend the bootstrap method proposed by fang and santos (2018) to approximate this asymptotic distribution and construct a critical value for the test. compared to the test proposed by kitagawa (2015), our test can be applied in more general settings and may achieve power improvement. evidence that the test performs well on finite samples is provided via simulations. we revisit the empirical study of card (1993) and use their data to demonstrate application of the proposed test in practice. we show that a valid instrument for a multivalued treatment may not remain valid if the treatment is coarsened.", "categories": "econ.em math.st stat.th", "created": "2020-09-03", "updated": "", "authors": ["zhenting sun"], "url": "https://arxiv.org/abs/2009.01995"}, {"title": "unlucky number 13? manipulating evidence subject to snooping", "id": "2009.02198", "abstract": "questionable research practices like harking or p-hacking have generated considerable recent interest throughout and beyond the scientific community. we subsume such practices involving secret data snooping that influences subsequent statistical inference under the term messing (manipulating evidence subject to snooping) and discuss, illustrate and quantify the possibly dramatic effects of several forms of messing using an empirical and a simple theoretical example. the empirical example uses numbers from the most popular german lottery, which seem to suggest that 13 is an unlucky number.", "categories": "stat.ap econ.em", "created": "2020-09-04", "updated": "", "authors": ["uwe hassler", "marc-oliver pohle"], "url": "https://arxiv.org/abs/2009.02198"}, {"title": "cointegrating polynomial regressions with power law trends: a new angle   on the environmental kuznets curve", "id": "2009.02262", "abstract": "the environment kuznets curve (ekc) predicts an inverted u-shaped relationship between economic growth and environmental pollution. current analyses frequently employ models which restrict the nonlinearities in the data to be explained by the economic growth variable only. we propose a generalized cointegrating polynomial regression (gcpr) with flexible time trends to proxy time effects such as technological progress and/or environmental awareness. more specifically, a gcpr includes flexible powers of deterministic trends and integer powers of stochastic trends. we estimate the gcpr by nonlinear least squares and derive its asymptotic distribution. endogeneity of the regressors can introduce nuisance parameters into this limiting distribution but a simulated approach nevertheless enables us to conduct valid inference. moreover, a subsampling kpss test can be used to check the stationarity of the errors. a comprehensive simulation study shows good performance of the simulated inference approach and the subsampling kpss test. we illustrate the gcpr approach on a dataset of 18 industrialised countries containing gdp and co2 emissions. we conclude that: (1) the evidence for an ekc is significantly reduced when a nonlinear time trend is included, and (2) a linear cointegrating relation between gdp and co2 around a power law trend also provides an accurate description of the data.", "categories": "econ.em math.st stat.th", "created": "2020-09-04", "updated": "", "authors": ["yicong lin", "hanno reuvers"], "url": "https://arxiv.org/abs/2009.02262"}, {"title": "heterogeneous coefficients, control variables, and identification of   treatment effects", "id": "2009.02314", "abstract": "multidimensional heterogeneity and endogeneity are important features of models with multiple treatments. we consider a heterogeneous coefficients model where the outcome is a linear combination of dummy treatment variables, with each variable representing a different kind of treatment. we use control variables to give necessary and sufficient conditions for identification of average treatment effects. with mutually exclusive treatments we find that, provided the generalized propensity scores (imbens, 2000) are bounded away from zero with probability one, a simple identification condition is that their sum be bounded away from one with probability one. these results generalize the classical identification result of rosenbaum and rubin (1983) for binary treatments.", "categories": "econ.em stat.me", "created": "2020-09-04", "updated": "", "authors": ["whitney k. newey", "sami stouli"], "url": "https://arxiv.org/abs/2009.02314"}, {"title": "covid-19: tail risk and predictive regressions", "id": "2009.02486", "abstract": "reliable analysis and forecasting of the spread of covid-19 pandemic and its impacts on global finance and world's economies requires application of econometrically justified and robust methods. at the same time, statistical and econometric analysis of financial and economic markets and of the spread of covid-19 is complicated by the inherent potential non-stationarity, dependence, heterogeneity and heavy-tailedness in the data. this project focuses on econometrically justified robust analysis of the effects of the covid-19 pandemic on the world's financial markets in different countries across the world. among other results, the study focuses on robust inference in predictive regressions for different countries across the world. we also present a detailed study of persistence, heavy-tailedness and tail risk properties of the time series of the covid-19 death rates that motivate the necessity in applications of robust inference methods in the analysis. econometrically justified analysis is based on application of heteroskedasticity and autocorrelation consistent (hac) inference methods, related approaches using consistent standard errors, recently developed robust $t$-statistic inference procedures and robust tail index estimation approaches.", "categories": "econ.em", "created": "2020-09-05", "updated": "", "authors": ["walter distaso", "rustam ibragimov", "alexander semenov", "anton skrobotov"], "url": "https://arxiv.org/abs/2009.02486"}, {"title": "decomposing identification gains and evaluating instrument   identification power for partially identified average treatment effects", "id": "2009.02642", "abstract": "this paper studies the instrument identification power for the average treatment effect (ate) in partially identified binary outcome models with an endogenous binary treatment. we propose a novel approach to measure the instrument identification power by their ability to reduce the width of the ate bounds. we show that instrument strength, as determined by the extreme values of the conditional propensity score, and its interplays with the degree of endogeneity and the exogenous covariates all play a role in bounding the ate. we decompose the ate identification gains into a sequence of measurable components, and construct a standardized quantitative measure for the instrument identification power ($iip$). the decomposition and the $iip$ evaluation are illustrated with finite-sample simulation studies and an empirical example of childbearing and women's labor supply. our simulations show that the $iip$ is a useful tool for detecting irrelevant instruments.", "categories": "econ.em", "created": "2020-09-05", "updated": "", "authors": ["lina zhang", "david t. frazier", "d. s. poskitt", "xueyan zhao"], "url": "https://arxiv.org/abs/2009.02642"}, {"title": "do black and indigenous communities receive their fair share of vaccines   under the 2018 cdc guidelines", "id": "2009.02853", "abstract": "a major focus of debate about rationing guidelines for covid-19 vaccines is whether and how to prioritize access for minority populations that have been particularly affected by the pandemic, and been the subject of historical and structural disadvantage, particularly black and indigenous individuals. we simulate the 2018 cdc vaccine allocation guidelines using data from the american community survey under different assumptions on total vaccine supply. black and indigenous individuals combined receive a higher share of vaccines compared to their population share for all assumptions on total vaccine supply. however, their vaccine share under the 2018 cdc guidelines is considerably lower than their share of covid-19 deaths and age-adjusted deaths. we then simulate one method to incorporate disadvantage in vaccine allocation via a reserve system. in a reserve system, units are placed into categories and units reserved for a category give preferential treatment to individuals from that category. using the area deprivation index (adi) as a proxy for disadvantage, we show that a 40% high-adi reserve increases the number of vaccines allocated to black or indigenous individuals, with a share that approaches their covid-19 death share when there are about 75 million units. our findings illustrate that whether an allocation is equitable depends crucially on the benchmark and highlight the importance of considering the expected distribution of outcomes from implementing vaccine allocation guidelines.", "categories": "econ.gn q-fin.ec", "created": "2020-09-06", "updated": "", "authors": ["parag a. pathak", "harald schmidt", "adam solomon", "edwin song", "tayfun s\u00f6nmez", "m. utku \u00fcnver"], "url": "https://arxiv.org/abs/2009.02853"}, {"title": "two-stage maximum score estimator", "id": "2009.02854", "abstract": "this paper considers the asymptotic theory of a semiparametric m-estimator that is generally applicable to models that satisfy a monotonicity condition in one or several parametric indexes. we call the estimator two-stage maximum score (tsms) estimator since our estimator involves a first-stage nonparametric regression when applied to the binary choice model of manski (1975, 1985). we characterize the asymptotic distribution of the tsms estimator, which features phase transitions depending on the dimension and thus the convergence rate of the first-stage estimation. we show that the tsms estimator is asymptotically equivalent to the smoothed maximum-score estimator (horowitz, 1992) when the dimension of the first-step estimation is relatively low, while still achieving partial rate acceleration relative to the cubic-root rate when the dimension is not too high. effectively, the first-stage nonparametric estimator serves as an imperfect smoothing function on a non-smooth criterion function, leading to the pivotality of the first-stage estimation error with respect to the second-stage convergence rate and asymptotic distribution", "categories": "econ.em", "created": "2020-09-06", "updated": "2020-10-07", "authors": ["wayne yuan gao", "sheng xu"], "url": "https://arxiv.org/abs/2009.02854"}, {"title": "an analysis of random elections with large numbers of voters", "id": "2009.02979", "abstract": "in an election in which each voter ranks all of the candidates, we consider the head-to-head results between each pair of candidates and form a labeled directed graph, called the margin graph, which contains the margin of victory of each candidate over each of the other candidates. a central issue in developing voting methods is that there can be cycles in this graph, where candidate $\\mathsf{a}$ defeats candidate $\\mathsf{b}$, $\\mathsf{b}$ defeats $\\mathsf{c}$, and $\\mathsf{c}$ defeats $\\mathsf{a}$. in this paper we apply the central limit theorem, graph homology, and linear algebra to analyze how likely such situations are to occur for large numbers of voters. there is a large literature on analyzing the probability of having a majority winner; our analysis is more fine-grained. the result of our analysis is that in elections with the number of voters going to infinity, margin graphs that are more cyclic in a certain precise sense are less likely to occur.", "categories": "econ.th cs.ma", "created": "2020-09-07", "updated": "", "authors": ["matthew harrison-trainor"], "url": "https://arxiv.org/abs/2009.02979"}, {"title": "doubly robust semiparametric difference-in-differences estimators with   high-dimensional data", "id": "2009.03151", "abstract": "this paper proposes a doubly robust two-stage semiparametric difference-in-difference estimator for estimating heterogeneous treatment effects with high-dimensional data. our new estimator is robust to model miss-specifications and allows for, but does not require, many more regressors than observations. the first stage allows a general set of machine learning methods to be used to estimate the propensity score. in the second stage, we derive the rates of convergence for both the parametric parameter and the unknown function under a partially linear specification for the outcome equation. we also provide bias correction procedures to allow for valid inference for the heterogeneous treatment effects. we evaluate the finite sample performance with extensive simulation studies. additionally, a real data analysis on the effect of fair minimum wage act on the unemployment rate is performed as an illustration of our method. an r package for implementing the proposed method is available on github.", "categories": "econ.em stat.ml", "created": "2020-09-07", "updated": "", "authors": ["yang ning", "sida peng", "jing tao"], "url": "https://arxiv.org/abs/2009.03151"}, {"title": "dimension reduction for high dimensional vector autoregressive models", "id": "2009.03361", "abstract": "this paper aims to decompose a large dimensional vector autoregessive (var) model into two components, the first one being generated by a small-scale var and the second one being a white noise sequence. hence, a reduced number of common factors generates the entire dynamics of the large system through a var structure. this modelling extends the common feature approach to high dimensional systems, and it differs from the dynamic factor models in which the idiosyncratic components can also embed a dynamic pattern. we show the conditions under which this decomposition exists, and we provide statistical tools to detect its presence in the data and to estimate the parameters of the underlying small-scale var model. we evaluate the practical value of the proposed methodology by simulations as well as by empirical applications on both economic and financial time series.", "categories": "econ.em", "created": "2020-09-07", "updated": "", "authors": ["gianluca cubadda", "alain hecq"], "url": "https://arxiv.org/abs/2009.03361"}, {"title": "counterfactual and welfare analysis with an approximate model", "id": "2009.03379", "abstract": "we propose a conceptual framework for counterfactual and welfare analysis for approximate models. our key assumption is that model approximation error is the same magnitude at new choices as the observed data. applying the framework to quasilinear utility, we obtain bounds on quantities at new prices using an approximate law of demand. we then bound utility differences between bundles and welfare differences between prices. all bounds are computable as linear programs. we provide detailed analytical results describing how the data map to the bounds including shape restrictions that provide a foundation for plug-in estimation. an application to gasoline demand illustrates the methodology.", "categories": "econ.em econ.gn q-fin.ec", "created": "2020-09-07", "updated": "", "authors": ["roy allen", "john rehbeck"], "url": "https://arxiv.org/abs/2009.03379"}, {"title": "globalization? trade war? a counterbalance perspective", "id": "2009.03436", "abstract": "during the past few decades, globalization and protectionism have ebbed and flowed from time to time among economies. the movements demand formal analytics that can help countries make better trade policies. they also imply that the best trade policies could be time-varying and country-specific. economies and their imports and exports constitute a counterbalanced network where conflict and cooperation are two sides of the same coin. a country could improve its relative strength in the network by embracing globalization, protectionism, or trade wars. this paper provides necessary conditions for globalization and trade wars, evaluates their side effects, identifies the right targets for conflict or collaboration, and recommends fair resolutions for trade wars. data and events from the past twenty years support these conditions.", "categories": "econ.gn q-fin.ec", "created": "2020-09-04", "updated": "2020-09-14", "authors": ["xingwei hu"], "url": "https://arxiv.org/abs/2009.03436"}, {"title": "local composite quantile regression for regression discontinuity", "id": "2009.03716", "abstract": "we introduce the local composite quantile regression (lcqr) to causal inference in regression discontinuity (rd) designs. kai et al. (2010) study the efficiency property of lcqr, while we show that its nice boundary performance translates to accurate estimation of treatment effects in rd under a variety of data generating processes. moreover, we propose a bias-corrected and standard error-adjusted t-test for inference, which leads to confidence intervals with good coverage probabilities. a bandwidth selector is also discussed. for illustration, we conduct a simulation study and revisit a classic example from lee (2008). a companion r package rdcqr is developed.", "categories": "econ.em", "created": "2020-09-08", "updated": "", "authors": ["xiao huang", "zhaoguo zhan"], "url": "https://arxiv.org/abs/2009.03716"}, {"title": "sales policies for a virtual assistant", "id": "2009.03719", "abstract": "we study the implications of selling through a voice-based virtual assistant (va). the seller has a set of products available and the va decides which product to offer and at what price, seeking to maximize its revenue, consumer- or total-surplus. the consumer is impatient and rational, seeking to maximize her expected utility given the information available to her. the va selects products based on the consumer's request and other information available to it and then presents them sequentially. once a product is presented and priced, the consumer evaluates it and decides whether to make a purchase. the consumer's valuation of each product comprises a pre-evaluation value, which is common knowledge, and a post-evaluation component which is private to the consumer. we solve for the equilibria and develop efficient algorithms for implementing the solution. we examine the effects of information asymmetry on the outcomes and study how incentive misalignment depends on the distribution of private valuations. we find that monotone rankings are optimal in the cases of a highly patient or impatient consumer and provide a good approximation for other levels of patience. the relationship between products' expected valuations and prices depends on the consumer's patience level and is monotone increasing (decreasing) when the consumer is highly impatient (patient). also, the seller's share of total surplus decreases in the amount of private information. we compare the va to a traditional web-based interface, where multiple products are presented simultaneously on each page. we find that within a page, the higher-value products are priced lower than the lower-value products when the private valuations are exponentially distributed. finally, the web-based interface generally achieves higher profits for the seller than a va due to the greater commitment power inherent in its presentation.", "categories": "econ.th cs.gt", "created": "2020-09-02", "updated": "", "authors": ["wenjia ba", "haim mendelson", "mingxi zhu"], "url": "https://arxiv.org/abs/2009.03719"}, {"title": "electoral accountability and selection with personalized news   aggregation", "id": "2009.03761", "abstract": "we study a model of electoral accountability and selection (eas) in which voters with heterogeneous horizontal preferences pay limited attention to the incumbent's performance using personalized news aggregators. extreme voters' news aggregators exhibit an own-party bias, which hampers their abilities to discern good and bad performances. while this effect alone would undermine eas, there is a countervailing effect stemming from the disagreement between extreme voters, which makes the centrist voter pivotal and could potentially improve eas. thus increasing mass polarization and shrinking attention spans have ambiguous effects on eas, whereas nuanced regulations of news aggregators unambiguously improve eas and voter welfare.", "categories": "econ.th", "created": "2020-09-03", "updated": "", "authors": ["anqi li", "lin hu", "ilya segal"], "url": "https://arxiv.org/abs/2009.03761"}, {"title": "exact computation of maximum rank correlation estimator", "id": "2009.03844", "abstract": "in this paper we provide an exact computation algorithm for the maximum rank correlation estimator using the mixed integer programming (mip) approach. we construct a new constrained optimization problem by transforming all indicator functions into binary parameters to be estimated and show that it is equivalent to the original problem. using a modern mip solver, we apply the proposed method to an empirical example and monte carlo simulations. we also consider an application of the best subset rank prediction and show that the original optimization problem can be reformulated as mip. we derive the non-asymptotic bound for the tail probability of the predictive performance measure.", "categories": "econ.em", "created": "2020-09-08", "updated": "", "authors": ["youngki shin", "zvezdomir todorov"], "url": "https://arxiv.org/abs/2009.03844"}, {"title": "the impact of covid-19 and policy responses on australian income   distribution and poverty", "id": "2009.04037", "abstract": "this paper undertakes a near real-time analysis of the income distribution effects of the covid-19 crisis in australia to understand the ongoing changes in the income distribution as well as the impact of policy responses. by semi-parametrically combining incomplete observed data from three different sources, namely, the monthly longitudinal labour force survey, the survey of income and housing and the administrative payroll data, we estimate the impact of covid-19 and the associated policy responses on the australian income distribution between february and june 2020, covering the immediate periods before and after the initial outbreak. our results suggest that despite the growth in unemployment, the gini of the equalised disposable income inequality dropped by nearly 0.03 point since february. the reduction is because of the additional wage subsidies and welfare supports offered as part of the policy response, offsetting a potential surge in income inequality. additionally, the poverty rate, which could have been doubled in the absence of the government response, also reduced by 3 to 4 percentage points. the result shows the effectiveness of temporary policy measures in maintaining both the living standards and the level of income inequality. however, the heavy reliance on the support measures raises the possibility that the changes in the income distribution may be reversed and even substantially worsened off should the measures be withdrawn.", "categories": "econ.gn q-fin.ec", "created": "2020-09-08", "updated": "", "authors": ["jinjing li", "yogi vidyattama", "hai anh la", "riyana miranti", "denisa m sologon"], "url": "https://arxiv.org/abs/2009.04037"}, {"title": "inter-organisational patent opposition network: how companies form   adversarial relationships", "id": "2009.04113", "abstract": "much of the research on networks using patent data focuses on citations and the collaboration networks of inventors, hence regarding patents as a positive sign of invention. however, patenting is, most importantly, a strategic action used by companies to compete with each other. this study sheds light on inter-organisational adversarial relationships in patenting for the first time. we constructed and analysed the network of companies connected via patent opposition relationships that occurred between 1980 and 2018. a majority of the companies are directly or indirectly connected to each other and hence form the largest connected component. we found that in the network, many companies disapprove patents in various industrial sectors as well as those owned by foreign companies. the network exhibits heavy-tailed, power-law-like degree distribution and assortative mixing, making it an unusual type of topology. we further investigated the dynamics of the formation of this network by conducting a temporal network motif analysis, with patent co-ownership among the companies considered. by regarding opposition as a negative relationship and patent co-ownership as a positive relationship, we analysed where collaboration may occur in the opposition network and how such positive relationships would interact with negative relationships. the results identified the structurally imbalanced triadic motifs and the temporal patterns of the occurrence of triads formed by a mixture of positive and negative relationships. our findings suggest that the mechanisms of the emergence of the inter-organisational adversarial relationships may differ from those of other types of negative relationships hence necessitating further research.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-09-09", "updated": "", "authors": ["tomomi kito", "nagi moriya", "junichi yamanoi"], "url": "https://arxiv.org/abs/2009.04113"}, {"title": "a framework for crop price forecasting in emerging economies by   analyzing the quality of time-series data", "id": "2009.04171", "abstract": "accuracy of crop price forecasting techniques is important because it enables the supply chain planners and government bodies to take appropriate actions by estimating market factors such as demand and supply. in emerging economies such as india, the crop prices at marketplaces are manually entered every day, which can be prone to human-induced errors like the entry of incorrect data or entry of no data for many days. in addition to such human prone errors, the fluctuations in the prices itself make the creation of stable and robust forecasting solution a challenging task. considering such complexities in crop price forecasting, in this paper, we present techniques to build robust crop price prediction models considering various features such as (i) historical price and market arrival quantity of crops, (ii) historical weather data that influence crop production and transportation, (iii) data quality-related features obtained by performing statistical analysis. we additionally propose a framework for context-based model selection and retraining considering factors such as model stability, data quality metrics, and trend analysis of crop prices. to show the efficacy of the proposed approach, we show experimental results on two crops - tomato and maize for 14 marketplaces in india and demonstrate that the proposed approach not only improves accuracy metrics significantly when compared against the standard forecasting techniques but also provides robust models.", "categories": "stat.ap econ.em", "created": "2020-09-09", "updated": "", "authors": ["ayush jain", "smit marvaniya", "shantanu godbole", "vitobha munigala"], "url": "https://arxiv.org/abs/2009.04171"}, {"title": "random non-expected utility: non-uniqueness", "id": "2009.04173", "abstract": "in random expected utility (gul and pesendorfer, 2006), the distribution of preferences is uniquely recoverable from random choice. this paper shows through two examples that such uniqueness fails in general if risk preferences are random but do not conform to expected utility theory. in the first, non-uniqueness obtains even if all preferences are confined to the betweenness class (dekel, 1986) and are suitably monotone. the second example illustrates random choice behavior consistent with random expected utility that is also consistent with random non-expected utility. on the other hand, we find that if risk preferences conform to weighted utility theory (chew, 1983) and are monotone in first-order stochastic dominance, random choice again uniquely identifies the distribution of preferences. finally, we argue that, depending on the domain of risk preferences, uniqueness may be restored if joint distributions of choice across a limited number of feasible sets are available.", "categories": "econ.th", "created": "2020-09-09", "updated": "", "authors": ["yi-hsuan lin"], "url": "https://arxiv.org/abs/2009.04173"}, {"title": "a survey on data pricing: from economics to data science", "id": "2009.04462", "abstract": "it is well recognized that data are invaluable. how can we assess the value of data objectively, systematically and quantitatively? pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. in this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. we examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. we cover both digital products and data products. last, we discuss a series of challenges and directions for future work.", "categories": "econ.th cs.db cs.lg", "created": "2020-09-09", "updated": "", "authors": ["jian pei"], "url": "https://arxiv.org/abs/2009.04462"}, {"title": "using nudges to prevent student dropouts in the pandemic", "id": "2009.04767", "abstract": "the impacts of covid-19 reach far beyond the hundreds of lives lost to the disease; in particular, the pre-existing learning crisis is expected to be magnified during school shutdown. despite efforts to put distance learning strategies in place, the threat of student dropouts, especially among adolescents, looms as a major concern. are interventions to motivate adolescents to stay in school effective amidst the pandemic? here we show that, in brazil, nudges via text messages to high-school students, to motivate them to stay engaged with school activities, substantially reduced dropouts during school shutdown, and greatly increased their motivation to go back to school when classes resume. while such nudges had been shown to decrease dropouts during normal times, it is surprising that those impacts replicate in the absence of regular classes because their effects are typically mediated by teachers (whose effort in the classroom changes in response to the nudges). results show that insights from the science of adolescent psychology can be leveraged to shift developmental trajectories at a critical juncture. they also qualify those insights: effects increase with exposure and gradually fade out once communication stops, providing novel evidence that motivational interventions work by redirecting adolescents' attention.", "categories": "econ.gn q-fin.ec", "created": "2020-09-10", "updated": "", "authors": ["guilherme lichand", "julien christen"], "url": "https://arxiv.org/abs/2009.04767"}, {"title": "on the effectiveness of minisum approval voting in an open strategy   setting: an agent-based approach", "id": "2009.04912", "abstract": "this work researches the impact of including a wider range of participants in the strategy-making process on the performance of organizations which operate in either moderately or highly complex environments. agent-based simulation demonstrates that the increased number of ideas generated from larger and diverse crowds and subsequent preference aggregation lead to rapid discovery of higher peaks in the organization's performance landscape. however, this is not the case when the expansion in the number of participants is small. the results confirm the most frequently mentioned benefit in the open strategy literature: the discovery of better performing strategies.", "categories": "cs.ai econ.gn econ.th q-fin.ec", "created": "2020-09-07", "updated": "2020-09-25", "authors": ["joop van de heijning", "stephan leitner", "alexandra rausch"], "url": "https://arxiv.org/abs/2009.04912"}, {"title": "the 2020 sturgis motorcycle rally and covid-19", "id": "2009.04917", "abstract": "the sturgis motorcycle rally that took place from august 7-16 was one of the largest public gatherings since the start of the covid-19 outbreak. over 460,000 visitors from across the united states travelled to sturgis, south dakota to attend the ten day event. using anonymous cell phone tracking data we identify the home counties of visitors to the rally and examine the impact of the rally on the spread of covid-19. our baseline estimate suggests a one standard deviation increase in sturgis attendance increased covid-19 case growth by 1.1pp in the weeks after the rally.", "categories": "q-bio.pe cs.si econ.gn q-fin.ec", "created": "2020-09-05", "updated": "", "authors": ["yong cai", "grant goehring"], "url": "https://arxiv.org/abs/2009.04917"}, {"title": "forecasting financial markets with semantic network analysis in the   covid-19 crisis", "id": "2009.04975", "abstract": "this paper uses a new textual data index for predicting stock market data. the index is applied to a large set of news to evaluate the importance of one or more general economic related keywords appearing in the text. the index assesses the importance of the economic related keywords, based on their frequency of use and semantic network position. we apply it to the italian press and construct indices to predict italian stock and bond market returns and volatilities in a recent sample period, including the covid-19 crisis. the evidence shows that the index captures well the different phases of financial time series. moreover, results indicate strong evidence of predictability for bond market data, both returns and volatilities, short and long maturities, and stock market volatility.", "categories": "q-fin.gn cs.cl cs.si econ.gn q-fin.ec", "created": "2020-09-09", "updated": "", "authors": ["a. fronzetti colladon", "s. grassi", "f. ravazzolo", "f. violante"], "url": "https://arxiv.org/abs/2009.04975"}, {"title": "tiered random matching markets: rank is proportional to popularity", "id": "2009.05124", "abstract": "we study the stable marriage problem in two-sided markets with randomly generated preferences. we consider agents on each side divided into a constant number of \"soft tiers\", which intuitively indicate the quality of the agent. specifically, every agent within a tier has the same public score, and agents on each side have preferences independently generated proportionally to the public scores of the other side.   we compute the expected average rank which agents in each tier have for their partners in the men-optimal stable matching, and prove concentration results for the average rank in asymptotically large markets. furthermore, we show that despite having a significant effect on ranks, public scores do not strongly influence the probability of an agent matching to a given tier of the other side. this generalizes results of [pittel 1989] which correspond to uniform preferences. the results quantitatively demonstrate the effect of competition due to the heterogeneous attractiveness of agents in the market, and we give the first explicit calculations of rank beyond uniform markets.", "categories": "cs.gt econ.th", "created": "2020-09-10", "updated": "", "authors": ["itai ashlagi", "mark braverman", "clayton thomas", "geng zhao"], "url": "https://arxiv.org/abs/2009.05124"}, {"title": "inference for high-dimensional exchangeable arrays", "id": "2009.05150", "abstract": "we consider inference for high-dimensional exchangeable arrays where the dimension may be much larger than the cluster sizes. specifically, we consider separately and jointly exchangeable arrays that correspond to multiway clustered and polyadic data, respectively. such exchangeable arrays have seen a surge of applications in empirical economics. however, both exchangeability concepts induce highly complicated dependence structures, which poses a significant challenge for inference in high dimensions. in this paper, we first derive high-dimensional central limit theorems (clts) over the rectangles for the exchangeable arrays. building on the high-dimensional clts, we develop novel multiplier bootstraps for the exchangeable arrays and derive their finite sample error bounds in high dimensions. the derivations of these theoretical results rely on new technical tools such as hoeffding-type decomposition and maximal inequalities for the degenerate components in the hoeffiding-type decomposition for the exchangeable arrays. we illustrate applications of our bootstrap methods to robust inference in demand analysis, robust inference in extended gravity analysis, and penalty choice for $\\ell_1$-penalized regression under multiway cluster sampling.", "categories": "econ.em math.st stat.th", "created": "2020-09-10", "updated": "2020-09-13", "authors": ["harold d. chiang", "kengo kato", "yuya sasaki"], "url": "https://arxiv.org/abs/2009.05150"}, {"title": "reforms meet fairness concerns in school and college admissions", "id": "2009.05245", "abstract": "recently, many matching systems around the world have been reformed. these reforms responded to objections that the matching mechanisms in use were unfair and manipulable. surprisingly, the mechanisms remained unfair even after the reforms: the new mechanisms may induce an outcome with a blocking student who desires and deserves a school which she did not receive. however, as we show in this paper, the reforms introduced matching mechanisms which are more fair compared to the counterfactuals. first, most of the reforms introduced mechanisms that are more fair by stability: whenever the old mechanism does not have a blocking student, the new mechanism does not have a blocking student either. second, some reforms introduced mechanisms that are more fair by counting: the old mechanism always has at least as many blocking students as the new mechanism. these findings give a novel rationale to the reforms and complement the recent literature showing that the same reforms have introduced less manipulable matching mechanisms. we further show that the fairness and manipulability of the mechanisms are strongly logically related.", "categories": "econ.th", "created": "2020-09-11", "updated": "", "authors": ["somouaoga bonkoungou", "alexander nesterov"], "url": "https://arxiv.org/abs/2009.05245"}, {"title": "cognitive abilities in the wild: population-scale game-based cognitive   assessment", "id": "2009.05274", "abstract": "psychology and the social sciences are undergoing a revolution: it has become increasingly clear that traditional lab-based experiments fail to capture the full range of differences in cognitive abilities and behaviours across the general population. some progress has been made toward devising measures that can be applied at scale across individuals and populations. what has been missing is a broad battery of validated tasks that can be easily deployed, used across different age ranges and social backgrounds, and employed in practical, clinical, and research contexts. here, we present skill lab, a game-based approach allowing the efficient assessment of a suite of cognitive abilities. skill lab has been validated outside the lab in a crowdsourced population-size sample recruited in collaboration with the danish broadcast company (danmarks radio, dr). our game-based measures are five times faster to complete than the equivalent traditional measures and replicate previous findings on the decline of cognitive abilities with age in a large population sample. furthermore, by combining the game data with an in-game survey, we demonstrate that this unique dataset has implication for key questions in social science, challenging the jack-of-all-trades theory of entrepreneurship and provide evidence for risk preference being independent of executive functioning.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-09-11", "updated": "", "authors": ["mads kock pedersen", "carlos mauricio casta\u00f1o d\u00edaz", "mario alejandro alba-marrugo", "ali amidi", "rajiv vaid basaiawmoit", "carsten bergenholtz", "morten h. christiansen", "miroslav gajdacz", "ralph hertwig", "byurakn ishkhanyan", "kim klyver", "nicolai ladegaard", "kim mathiasen", "christine parsons", "michael bang petersen", "janet rafner", "anders ryom villadsen", "mikkel wallentin", "jacob friis sherson", "skill lab players"], "url": "https://arxiv.org/abs/2009.05274"}, {"title": "strategy-proof allocation with outside option", "id": "2009.05311", "abstract": "we consider strategy-proof mechanisms to solve allocation problems where agents can choose outside options if they wish. mechanisms could return an allocation or a randomization over allocations. we prove two useful theorems, relying on an invariance property of the allocations found by strategy-proof mechanisms when agents vary the ranking of outside options in their preferences. the first theorem proves that, among individually rational and strategy-proof mechanisms, pinning down every agent's probability of choosing outside option in every preference profile is equivalent to pinning down a mechanism. the second theorem provides a sufficient condition for proving equivalence between two strategy-proof mechanisms when the number of possible allocations is finite. the two theorems provide a unified foundation for existing results in several distinct models and imply new results in some models.", "categories": "econ.th", "created": "2020-09-11", "updated": "", "authors": ["jun zhang"], "url": "https://arxiv.org/abs/2009.05311"}, {"title": "inferring hidden potentials in analytical regions: uncovering crime   suspect communities in medell\\'in", "id": "2009.05360", "abstract": "this paper proposes a bayesian approach to perform inference regarding the size of hidden populations at analytical region using reported statistics. to do so, we propose a specification taking into account one-sided error components and spatial effects within a panel data structure. our simulation exercises suggest good finite sample performance. we analyze rates of crime suspects living per neighborhood in medell\\'in (colombia) associated with four crime activities. our proposal seems to identify hot spots or \"crime communities\", potential neighborhoods where under-reporting is more severe, and also drivers of crime schools. statistical evidence suggests a high level of interaction between homicides and drug dealing in one hand, and motorcycle and car thefts on the other hand.", "categories": "econ.em", "created": "2020-09-11", "updated": "", "authors": ["alejandro puerta", "andr\u00e9s ram\u00edrez-hassan"], "url": "https://arxiv.org/abs/2009.05360"}, {"title": "object recognition for economic development from daytime satellite   imagery", "id": "2009.05455", "abstract": "reliable data about the stock of physical capital and infrastructure in developing countries is typically very scarce. this is particular a problem for data at the subnational level where existing data is often outdated, not consistently measured or coverage is incomplete. traditional data collection methods are time and labor-intensive costly, which often prohibits developing countries from collecting this type of data. this paper proposes a novel method to extract infrastructure features from high-resolution satellite images. we collected high-resolution satellite images for 5 million 1km $\\times$ 1km grid cells covering 21 african countries. we contribute to the growing body of literature in this area by training our machine learning algorithm on ground-truth data. we show that our approach strongly improves the predictive accuracy. our methodology can build the foundation to then predict subnational indicators of economic development for areas where this data is either missing or unreliable.", "categories": "econ.gn cs.cv eess.iv q-fin.ec", "created": "2020-09-11", "updated": "", "authors": ["klaus ackermann", "alexey chernikov", "nandini anantharama", "miethy zaman", "paul a raschky"], "url": "https://arxiv.org/abs/2009.05455"}, {"title": "mechanisms for a no-regret agent: beyond the common prior", "id": "2009.05518", "abstract": "a rich class of mechanism design problems can be understood as incomplete-information games between a principal who commits to a policy and an agent who responds, with payoffs determined by an unknown state of the world. traditionally, these models require strong and often-impractical assumptions about beliefs (a common prior over the state). in this paper, we dispense with the common prior. instead, we consider a repeated interaction where both the principal and the agent may learn over time from the state history. we reformulate mechanism design as a reinforcement learning problem and develop mechanisms that attain natural benchmarks without any assumptions on the state-generating process. our results make use of novel behavioral assumptions for the agent -- centered around counterfactual internal regret -- that capture the spirit of rationality without relying on beliefs.", "categories": "econ.th cs.gt", "created": "2020-09-11", "updated": "", "authors": ["modibo camara", "jason hartline", "aleck johnsen"], "url": "https://arxiv.org/abs/2009.05518"}, {"title": "application of a system of indicatirs for assessing the socio-economic   situation of a subject based on digital shadows", "id": "2009.05771", "abstract": "the development of digital economy sets its own requirements for the formation and development of so-called digital doubles and digital shadows of real objects (subjects/regions). an integral element of their development and application is a multi-level matrix of targets and resource constraints (time, financial, technological, production, etc.). the volume of statistical information collected for a digital double must meet several criteria: be objective, characterize the real state of the managed object as accurately as possible, contain all the necessary information on all managed parameters, and at the same time avoid unnecessary and duplicate indicators (\"information garbage\"). the relevance of forming the profile of the \"digital shadow of the region\" in the context of multitasking and conflict of departmental and federal statistics predetermined the goal of the work-to form a system of indicators of the socio-economic situation of regions based on the harmonization of information resources. in this study, an inventory of the composition of indicators of statistical forms for their relevance and relevance was carried out on the example of assessing the economic health of the subject and the level of provision of banking services", "categories": "econ.gn q-fin.ec", "created": "2020-09-12", "updated": "", "authors": ["olga g. lebedinskaya"], "url": "https://arxiv.org/abs/2009.05771"}, {"title": "regularized solutions to linear rational expectations models", "id": "2009.05875", "abstract": "this paper proposes a computational method for obtaining regularized solutions to linear rational expectations models. the algorithm allows for regularization cross-sectionally as well as across frequencies. the algorithm is illustrated by a variety of examples.", "categories": "econ.em", "created": "2020-09-12", "updated": "2020-09-21", "authors": ["majid m. al-sadoon"], "url": "https://arxiv.org/abs/2009.05875"}, {"title": "bayesian modelling of time-varying conditional heteroscedasticity", "id": "2009.06007", "abstract": "conditional heteroscedastic (ch) models are routinely used to analyze financial datasets. the classical models such as arch-garch with time-invariant coefficients are often inadequate to describe frequent changes over time due to market variability. however we can achieve significantly better insight by considering the time-varying analogues of these models. in this paper, we propose a bayesian approach to the estimation of such models and develop computationally efficient mcmc algorithm based on hamiltonian monte carlo (hmc) sampling. we also established posterior contraction rates with increasing sample size in terms of the average hellinger metric. the performance of our method is compared with frequentist estimates and estimates from the time constant analogues. to conclude the paper we obtain time-varying parameter estimates for some popular forex (currency conversion rate) and stock market datasets.", "categories": "math.st econ.em stat.th", "created": "2020-09-13", "updated": "", "authors": ["sayar karmakar", "arkaprava roy"], "url": "https://arxiv.org/abs/2009.06007"}, {"title": "the platform design problem", "id": "2009.06117", "abstract": "on-line firms deploy suites of software platforms, where each platform is designed to interact with users during a certain activity, such as browsing, chatting, socializing, emailing, driving, etc. the economic and incentive structure of this exchange, as well as its algorithmic nature, have not been explored to our knowledge; we initiate their study in this paper. we model this interaction as a stackelberg game between a designer and one or more agents. we model an agent as a markov chain whose states are activities; we assume that the agent's utility is a linear function of the steady-state distribution of this chain. the designer may design a platform for each of these activities/states; if a platform is adopted by the agent, the transition probabilities of the markov chain are affected, and so is the objective of the agent. the designer's utility is a linear function of the steady state probabilities of the accessible states (that is, the ones for which the platform has been adopted), minus the development cost of the platforms. the underlying optimization problem of the agent -- that is, how to choose the states for which to adopt the platform -- is an mdp. if this mdp has a simple yet plausible structure (the transition probabilities from one state to another only depend on the target state and the recurrent probability of the current state) the agent's problem can be solved by a greedy algorithm. the designer's optimization problem (designing a custom suite for the agent so as to optimize, through the agent's optimum reaction, the designer's revenue), while np-hard, has an fptas. these results generalize, under mild additional assumptions, from a single agent to a distribution of agents with finite support. the designer's optimization problem has abysmal \"price of robustness\", suggesting that learning the parameters of the problem is crucial for the designer.", "categories": "cs.gt cs.cc cs.lg cs.ma econ.th", "created": "2020-09-13", "updated": "", "authors": ["christos papadimitriou", "kiran vodrahalli", "mihalis yannakakis"], "url": "https://arxiv.org/abs/2009.06117"}, {"title": "inversion-free leontief inverse: statistical regularities in   input-output analysis from partial information", "id": "2009.06350", "abstract": "we present a baseline stochastic framework for assessing inter-sectorial relationships in a generic economy. we show that - irrespective of the specific features of the technology matrix for a given country or a particular year - the leontief multipliers (and any upstreamness/downstreamness indicator computed from the leontief inverse matrix) follow a universal pattern, which we characterize analytically. we formulate a universal benchmark to assess the structural inter-dependence of sectors in a generic economy. several empirical results on world input-output database (wiod, 2013 release) are presented that corroborate our findings.", "categories": "physics.soc-ph econ.gn q-fin.ec q-fin.gn", "created": "2020-09-14", "updated": "", "authors": ["silvia bartolucci", "fabio caccioli", "francesco caravelli", "pierpaolo vivo"], "url": "https://arxiv.org/abs/2009.06350"}, {"title": "robust discrete choice models with t-distributed kernel errors", "id": "2009.06383", "abstract": "models that are robust to aberrant choice behaviour have received limited attention in discrete choice analysis. in this paper, we analyse two robust alternatives to the multinomial probit (mnp) model. both alternative models belong to the family of robit models, whose kernel error distributions are heavy-tailed t-distributions. the first model is the multinomial robit (mnr) model in which a generic degrees of freedom parameter controls the heavy-tailedness of the kernel error distribution. the second alternative, the generalised multinomial robit (gen-mnr) model, has not been studied in the literature before and is more flexible than mnr, as it allows for alternative-specific marginal heavy-tailedness of the kernel error distribution. for both models, we devise scalable and gradient-free bayes estimators. we compare mnp, mnr and gen-mnr in a simulation study and a case study on transport mode choice behaviour. we find that both mnr and gen-mnr deliver significantly better in-sample fit and out-of-sample predictive accuracy than mnp. gen-mnr outperforms mnr due to its more flexible kernel error distribution. also, gen-mnr gives more reasonable elasticity estimates than mnp and mnr, in particular regarding the demand for under-represented alternatives in a class-imbalanced dataset.", "categories": "econ.em stat.me", "created": "2020-09-14", "updated": "", "authors": ["rico krueger", "prateek bansal", "michel bierlaire", "thomas gasos"], "url": "https://arxiv.org/abs/2009.06383"}, {"title": "capital flows and the stabilizing role of macroprudential policies in   cesee", "id": "2009.06391", "abstract": "in line with the recent policy discussion on the use of macroprudential measures to respond to cross-border risks arising from capital flows, this paper tries to quantify to what extent macroprudential policies (mpps) have been able to stabilize capital flows in central, eastern and southeastern europe (cesee) -- a region that experienced a substantial boom-bust cycle in capital flows amid the global financial crisis and where policymakers had been quite active in adopting mpps already before that crisis. to study the dynamic responses of capital flows to mpp shocks, we propose a novel regime-switching factor-augmented vector autoregressive (favar) model. it allows to capture potential structural breaks in the policy regime and to control -- besides domestic macroeconomic quantities -- for the impact of global factors such as the global financial cycle. feeding into this model a novel intensity-adjusted macroprudential policy index, we find that tighter mpps may be effective in containing domestic private sector credit growth and the volumes of gross capital inflows in a majority of the countries analyzed. however, they do not seem to generally shield cesee countries from capital flow volatility.", "categories": "econ.em", "created": "2020-09-10", "updated": "", "authors": ["markus eller", "niko hauzenberger", "florian huber", "helene schuberth", "lukas vashold"], "url": "https://arxiv.org/abs/2009.06391"}, {"title": "supervised learning for the prediction of firm dynamics", "id": "2009.06413", "abstract": "thanks to the increasing availability of granular, yet high-dimensional, firm level data, machine learning (ml) algorithms have been successfully applied to address multiple research questions related to firm dynamics. especially supervised learning (sl), the branch of ml dealing with the prediction of labelled outcomes, has been used to better predict firms' performance. in this contribution, we will illustrate a series of sl approaches to be used for prediction tasks, relevant at different stages of the company life cycle. the stages we will focus on are (i) startup and innovation, (ii) growth and performance of companies, and (iii) firms exit from the market. first, we review sl implementations to predict successful startups and r&d projects. next, we describe how sl tools can be used to analyze company growth and performance. finally, we review sl applications to better forecast financial distress and company failure. in the concluding section, we extend the discussion of sl methods in the light of targeted policies, result interpretability, and causality.", "categories": "econ.gn cs.lg q-fin.ec", "created": "2020-09-11", "updated": "", "authors": ["falco j. bargagli-stoffi", "jan niederreiter", "massimo riccaboni"], "url": "https://arxiv.org/abs/2009.06413"}, {"title": "crime aggregation, deterrence, and witness credibility", "id": "2009.06470", "abstract": "we present a model for the equilibrium frequency of offenses and the informativeness of witness reports when potential offenders can commit multiple offenses and witnesses are subject to retaliation risk and idiosyncratic reporting preferences. we compare two ways of handling multiple accusations discussed in legal scholarship: (i) when convictions are based on the probability that the defendant committed at least one, unspecified offense and entail a severe punishment, potential offenders induce negative correlation in witnesses' private information, which leads to uninformative reports, information aggregation failures, and frequent offenses in equilibrium. moreover, lowering the punishment in case of conviction can improve deterrence and the informativeness of witnesses' reports. (ii) when accusations are treated separately to adjudicate guilt and conviction entails a severe punishment, witness reports are highly informative and offenses are infrequent in equilibrium.", "categories": "econ.gn q-fin.ec", "created": "2020-09-14", "updated": "", "authors": ["harry pei", "bruno strulovici"], "url": "https://arxiv.org/abs/2009.06470"}, {"title": "optimal market making under partial information and numerical methods   for impulse control games with applications", "id": "2009.06521", "abstract": "the topics treated in this thesis are inherently two-fold. the first part considers the problem of a market maker optimally setting bid/ask quotes over a finite time horizon, to maximize her expected utility. the intensities of the orders she receives depend not only on the spreads she quotes, but also on unobservable factors modelled by a hidden markov chain. this stochastic control problem under partial information is solved by means of stochastic filtering, control and pdmps theory. the value function is characterized as the unique continuous viscosity solution of its dynamic programming equation and numerically compared with its full information counterpart. the optimal full information spreads are shown to be biased when the exact market regime is unknown, as the market maker needs to adjust for additional regime uncertainty in terms of pnl sensitivity and observable order flow volatility.   the second part deals with numerically solving nonzero-sum stochastic impulse control games. these offer a realistic and far-reaching modelling framework, but the difficulty in solving such problems has hindered their proliferation. a policy-iteration-type solver is proposed to solve an underlying system of quasi-variational inequalities, and it is validated numerically with reassuring results.   eventually, the focus is put on games with a symmetric structure and an improved algorithm is put forward. a rigorous convergence analysis is undertaken with natural assumptions on the players strategies, which admit graph-theoretic interpretations in the context of weakly chained diagonally dominant matrices. the algorithm is used to compute with high precision equilibrium payoffs and nash equilibria of otherwise too challenging problems, and even some for which results go beyond the scope of the currently available theory.", "categories": "math.oc cs.na econ.gn math.na q-fin.ec q-fin.tr", "created": "2020-09-14", "updated": "", "authors": ["diego zabaljauregui"], "url": "https://arxiv.org/abs/2009.06521"}, {"title": "vector copulas and vector sklar theorem", "id": "2009.06558", "abstract": "this paper introduces vector copulas and establishes a vector version of sklar's theorem. the latter provides a theoretical justification for the use of vector copulas to characterize nonlinear or rank dependence between a finite number of random vectors (robust to within vector dependence), and to construct multivariate distributions with any given non-overlapping multivariate marginals. we construct elliptical, archimedean, and kendall families of vector copulas and present algorithms to generate data from them. we introduce a concordance ordering for two random vectors with given within-dependence structures and generalize spearman's rho to random vectors. finally, we construct empirical vector copulas and show their consistency under mild conditions.", "categories": "econ.em math.pr math.st stat.th", "created": "2020-09-14", "updated": "", "authors": ["yanqin fan", "marc henry"], "url": "https://arxiv.org/abs/2009.06558"}, {"title": "spatial differencing for sample selection models with unobserved   heterogeneity", "id": "2009.06570", "abstract": "this paper derives identification, estimation, and inference results using spatial differencing in sample selection models with unobserved heterogeneity. we show that under the assumption of smooth changes across space of the unobserved sub-location specific heterogeneities and inverse mills ratio, key parameters of a sample selection model are identified. the smoothness of the sub-location specific heterogeneities implies a correlation in the outcomes. we assume that the correlation is restricted within a location or cluster and derive asymptotic results showing that as the number of independent clusters increases, the estimators are consistent and asymptotically normal. we also propose a formula for standard error estimation. a monte-carlo experiment illustrates the small sample properties of our estimator. the application of our procedure to estimate the determinants of the municipality tax rate in finland shows the importance of accounting for unobserved heterogeneity.", "categories": "econ.em stat.me", "created": "2020-09-14", "updated": "", "authors": ["alexander klein", "guy tchuente"], "url": "https://arxiv.org/abs/2009.06570"}, {"title": "the frisch--waugh--lovell theorem for standard errors", "id": "2009.06621", "abstract": "the frisch--waugh--lovell theorem states the equivalence of the coefficients from the full and partial regressions. i further show the equivalence between various standard errors. applying the new result to stratified experiments reveals the discrepancy between model-based and design-based standard errors.", "categories": "math.st econ.em stat.me stat.th", "created": "2020-09-14", "updated": "", "authors": ["peng ding"], "url": "https://arxiv.org/abs/2009.06621"}, {"title": "the impact of supply-chain networks on interactions between the   anti-covid-19 lockdowns in different regions", "id": "2009.06894", "abstract": "to prevent the spread of covid-19, many cities, states, and countries have `locked down', restricting economic activities in non-essential sectors. such lockdowns have substantially shrunk production in most countries. this study examines how the economic effects of lockdowns in different regions interact through supply chains, a network of firms for production, simulating an agent-based model of production on supply-chain data for 1.6 million firms in japan. we further investigate how the complex network structure affects the interactions of lockdowns, emphasising the role of upstreamness and loops by decomposing supply-chain flows into potential and circular flow components. we find that a region's upstreamness, intensity of loops, and supplier substitutability in supply chains with other regions largely determine the economic effect of the lockdown in the region. in particular, when a region lifts its lockdown, its economic recovery substantially varies depending on whether it lifts lockdown alone or together with another region closely linked through supply chains. these results propose the need for inter-region policy coordination to reduce the economic loss from lockdowns.", "categories": "cs.si cs.ma econ.gn q-fin.ec", "created": "2020-09-15", "updated": "", "authors": ["hiroyasu inoue", "yohsuke murase", "yasuyuki todo"], "url": "https://arxiv.org/abs/2009.06894"}, {"title": "covid-19 impact on global maritime mobility", "id": "2009.06960", "abstract": "to prevent the outbreak of the coronavirus disease (covid-19), numerous countries around the world went into lockdown and imposed unprecedented containment measures. these restrictions progressively produced changes to social behavior and global mobility patterns, evidently disrupting social and economic activities. here, using maritime traffic data, collected via a global network of automatic identification system (ais) receivers, we analyze the effects that the covid-19 pandemic and the containment measures had on the shipping industry, which accounts alone for more than 80% of the world trade. we introduce the notion of a \"maritime mobility index,\" a synthetic composite index, to quantitatively assess ship mobility in a given unit of time. the mobility index calculation used in this study, has a worldwide extent and is based on the computation of cumulative navigated miles (cnm) of all ships reporting their position and navigational status via ais. we compare 2020 mobility levels to those of previous years assuming that an unchanged growth rate would have been achieved, if not for covid-19. following the outbreak, we find an unprecedented drop in maritime mobility, across all categories of commercial shipping. the reduced activity is observable from march to june, when the most severe restrictions were in force, producing a variation of mobility quantified between -5.62% and -13.77% for container ships, between +2.28% and -3.32% for dry bulk, between -0.22% and -9.27% for wet bulk, and between -19.57% and -42.77% for passenger shipping. the presented study is unprecedented for the uniqueness and completeness of the employed ais dataset, which comprises a trillion ais messages broadcast worldwide by 50000 ships, a figure that closely parallels the documented size of the world merchant fleet.", "categories": "econ.gn q-fin.ec", "created": "2020-09-15", "updated": "", "authors": ["leonardo m. millefiori", "paolo braca", "dimitris zissis", "giannis spiliopoulos", "stefano marano", "peter k. willett", "sandro carniel"], "url": "https://arxiv.org/abs/2009.06960"}, {"title": "an agent-based model of delegation relationships with hidden-action: on   the effects of heterogeneous memory on performance", "id": "2009.07124", "abstract": "we introduce an agent-based model of delegation relationships between a principal and an agent, which is based on the standard-hidden action model introduced by holmstr\\\"om and, by doing so, provide a model which can be used to further explore theoretical topics in managerial economics, such as the efficiency of incentive mechanisms. we employ the concept of agentization, i.e., we systematically transform the standard hidden-action model into an agent-based model. our modeling approach allows for a relaxation of some of the rather \"heroic\" assumptions included in the standard hidden-action model, whereby we particularly focus on assumptions related to the (i) availability of information about the environment and the (ii) principal's and agent's cognitive capabilities (with a particular focus on their learning capabilities and their memory). our analysis focuses on how close and how fast the incentive scheme, which endogenously emerges from the agent-based model, converges to the solution proposed by the standard hidden-action model. also, we investigate whether a stable solution can emerge from the agent-based model variant. the results show that in stable environments the emergent result can nearly reach the solution proposed by the standard hidden-action model. surprisingly, the results indicate that turbulence in the environment leads to stability in earlier time periods.", "categories": "cs.ma econ.gn physics.soc-ph q-fin.ec", "created": "2020-09-09", "updated": "2020-09-28", "authors": ["patrick reinwald", "stephan leitner", "friederike wall"], "url": "https://arxiv.org/abs/2009.07124"}, {"title": "what factors have caused japanese prefectures to attract a larger   population influx?", "id": "2009.07144", "abstract": "regional promotion and centralized correction in tokyo have long been the goals of the government of japan. furthermore, in the wake of the recent new coronavirus (covid-19) epidemic, the momentum for rural migration is increasing, to prevent the risk of infection with the help of penetration of remote work. however, there is not enough debate about what kind of land will attract the population. therefore, in this paper, we will consider this problem by performing correlation analysis and multiple regression analysis with the inflow rate and the excess inflow rate of the population as the dependent variables, using recent government statistics for each prefecture. as a result of the analysis, in addition to economic factor variables, variables of climatic, amenity, and human factors correlated with the inflow rate, and it was shown that the model has the greatest explanatory power when multiple factors were used in addition to specific factors. therefore, local prefectures are required to take regional promotion measures focusing on not only economic factors but also multifaceted factors to attract the outside population.", "categories": "econ.gn physics.soc-ph q-fin.ec", "created": "2020-09-15", "updated": "", "authors": ["keisuke kokubun"], "url": "https://arxiv.org/abs/2009.07144"}, {"title": "network structures of collective intelligence: the contingent benefits   of group discussion", "id": "2009.07202", "abstract": "research on belief formation has produced contradictory findings on whether and when communication between group members will improve the accuracy of numeric estimates such as economic forecasts, medical diagnoses, and job candidate assessments. while some evidence suggests that carefully mediated processes such as the \"delphi method\" produce more accurate beliefs than unstructured discussion, others argue that unstructured discussion outperforms mediated processes. still others argue that independent individuals produce the most accurate beliefs. this paper shows how network theories of belief formation can resolve these inconsistencies, even when groups lack apparent structure as in informal conversation. emergent network structures of influence interact with the pre-discussion belief distribution to moderate the effect of communication on belief formation. as a result, communication sometimes increases and sometimes decreases the accuracy of the average belief in a group. the effects differ for mediated processes and unstructured communication, such that the relative benefit of each communication format depends on both group dynamics as well as the statistical properties of pre-interaction beliefs. these results resolve contradictions in previous research and offer practical recommendations for teams and organizations.", "categories": "econ.gn cs.si q-fin.ec", "created": "2020-09-15", "updated": "", "authors": ["joshua becker", "abdullah almaatouq", "agnes horvat"], "url": "https://arxiv.org/abs/2009.07202"}, {"title": "encompassing tests for value at risk and expected shortfall multi-step   forecasts based on inference on the boundary", "id": "2009.07341", "abstract": "we propose forecast encompassing tests for the expected shortfall (es) jointly with the value at risk (var) based on flexible link (or combination) functions. our setup allows testing encompassing for convex forecast combinations and for link functions which preclude crossings of the combined var and es forecasts. as the tests based on these link functions involve parameters which are on the boundary of the parameter space under the null hypothesis, we derive and base our tests on nonstandard asymptotic theory on the boundary. our simulation study shows that the encompassing tests based on our new link functions outperform tests based on unrestricted linear link functions for one-step and multi-step forecasts. we further illustrate the potential of the proposed tests in a real data analysis for forecasting var and es of the s&p 500 index.", "categories": "econ.em math.st q-fin.rm stat.th", "created": "2020-09-15", "updated": "", "authors": ["timo dimitriadis", "xiaochun liu", "julie schnaitmann"], "url": "https://arxiv.org/abs/2009.07341"}, {"title": "manipulation-robust regression discontinuity design", "id": "2009.07551", "abstract": "regression discontinuity designs (rdds) may not deliver reliable results if units manipulate their running variables. it is commonly believed that imprecise manipulations are harmless and, diagnostic tests detect precise manipulations. however, we demonstrate that rdds may fail to point-identify in the presence of imprecise manipulation, and that not all harmful manipulations are detectable.   to formalize these claims, we propose a class of rdds with harmless or detectable manipulations over locally randomized running variables as manipulation-robust rdds. the conditions for the manipulation-robust rdds may be intuitively verified using the institutional background. we demonstrate its verification process in case studies of applications that use the mccrary (2008) density test. the restrictions of manipulation-robust rdds generate partial identification results that are robust to possible manipulation. we apply the partial identification result to a controversy regarding the incumbency margin study of the u.s. house of representatives elections. the results show the robustness of the original conclusion of lee (2008).", "categories": "econ.em stat.me", "created": "2020-09-16", "updated": "", "authors": ["takuya ishihara", "masayuki sawada"], "url": "https://arxiv.org/abs/2009.07551"}, {"title": "economic complexity and growth: can value-added exports better explain   the link?", "id": "2009.07599", "abstract": "in economic literature, economic complexity is typically approximated on the basis of an economy's gross export structure. however, in times of ever increasingly integrated global value chains, gross exports may convey an inaccurate image of a country's economic performance since they also incorporate foreign value-added and double-counted exports. thus, i introduce a new empirical approach approximating economic complexity based on a country's value-added export structure. this approach leads to substantially different complexity rankings compared to the established metrics. moreover, the explanatory power of gdp per capita growth rates for a sample of 40 lower-middle- to high-income countries is considerably higher, even if controlling for typical growth regression covariates.", "categories": "econ.gn q-fin.ec", "created": "2020-09-16", "updated": "", "authors": ["philipp koch"], "url": "https://arxiv.org/abs/2009.07599"}, {"title": "the direct and indirect effect of cap support on farm income   enhancement:a farm-based econometric analysis", "id": "2009.07684", "abstract": "we assess the correlation between cap support provided to farmers and their income and use of capital and labour in the first year of the new cap regime. this is done applying three regression models on the italian fadn farms controlling for other farm characteristics. cap annual payments are positively correlated with farm income and capital but are negatively correlated with labour use. farm investment support provided by rdp measures is positively correlated to the amount of capital. results suggest that cap is positively affecting farm income directly but also indirectly by supporting the substitution of labour with capital", "categories": "econ.gn q-fin.ec", "created": "2020-09-16", "updated": "", "authors": ["simone severini", "luigi biagini"], "url": "https://arxiv.org/abs/2009.07684"}, {"title": "latent dirichlet allocation models for world trade analysis", "id": "2009.07727", "abstract": "the international trade is one of the classic areas of study in economics. nowadays, given the availability of data, the tools used for the analysis can be complemented and enriched with new methodologies and techniques that go beyond the traditional approach. the present paper shows the application of the latent dirichlet allocation models, a well known technique from the area of natural language processing, to search for latent dimensions in the product space of international trade, and their distribution across countries over time. we apply this technique to a dataset of countries' exports of goods from 1962 to 2016. the findings show the possibility to generate higher level classifications of goods based on the empirical evidence, and also allow to study the distribution of those classifications within countries. the latter show interesting insights about countries' trade specialisation.", "categories": "physics.soc-ph cs.cy econ.gn q-fin.ec", "created": "2020-09-16", "updated": "", "authors": ["diego kozlowski", "viktoriya semeshenko", "andrea molinari"], "url": "https://arxiv.org/abs/2009.07727"}, {"title": "tail behavior of stopped l\\'evy processes with markov modulation", "id": "2009.08010", "abstract": "this article concerns the tail probabilities of a light-tailed markov-modulated l\\'evy process stopped at a state-dependent poisson rate. the tails are shown to decay exponentially at rates given by the unique positive and negative roots of the spectral abscissa of a certain matrix-valued function. we illustrate the use of our results with an application to the stationary distribution of wealth in a simple economic model in which agents with constant absolute risk aversion are subject to random mortality and income fluctuation.", "categories": "math.pr econ.th", "created": "2020-09-16", "updated": "", "authors": ["brendan k. beare", "won-ki seo", "alexis akira toda"], "url": "https://arxiv.org/abs/2009.08010"}, {"title": "identification and estimation of a rational inattention discrete choice   model with bayesian persuasion", "id": "2009.08045", "abstract": "this paper studies the semi-parametric identification and estimation of a rational inattention model with bayesian persuasion. the identification requires the observation of a cross-section of market-level outcomes. the empirical content of the model can be characterized by three moment conditions. a two-step estimation procedure is proposed to avoid computation complexity in the structural model. in the empirical application, i study the persuasion effect of fox news in the 2000 presidential election. welfare analysis shows that persuasion will not influence voters with high school education but will generate higher dispersion in the welfare of voters with a partial college education and decrease the dispersion in the welfare of voters with a bachelors degree.", "categories": "econ.em", "created": "2020-09-16", "updated": "", "authors": ["moyu liao"], "url": "https://arxiv.org/abs/2009.08045"}, {"title": "fixed effects binary choice models with three or more periods", "id": "2009.08108", "abstract": "we consider fixed effects binary choice models with a fixed number of periods t and without a large support condition on the regressors. if the time-varying unobserved terms are i.i.d. with known distribution f, chamberlain (2010) shows that the common slope parameter is point-identified if and only if f is logistic. however, he considers in his proof only t=2. we show that actually, the result does not generalize to t>2: the common slope parameter and some parameters of the distribution of the shocks can be identified when f belongs to a family including the logit distribution. identification is based on a conditional moment restriction. we give necessary and sufficient conditions on the covariates for this restriction to identify the parameters. in addition, we show that under mild conditions, the corresponding gmm estimator reaches the semiparametric efficiency bound when t=3.", "categories": "econ.em", "created": "2020-09-17", "updated": "", "authors": ["laurent davezies", "xavier d'haultfoeuille", "martin mugnier"], "url": "https://arxiv.org/abs/2009.08108"}, {"title": "semiparametric testing with highly persistent predictors", "id": "2009.08291", "abstract": "we address the issue of semiparametric efficiency in the bivariate regression problem with a highly persistent predictor, where the joint distribution of the innovations is regarded an infinite-dimensional nuisance parameter. using a structural representation of the limit experiment and exploiting invariance relationships therein, we construct invariant point-optimal tests for the regression coefficient of interest. this approach naturally leads to a family of feasible tests based on the component-wise ranks of the innovations that can gain considerable power relative to existing tests under non-gaussian innovation distributions, while behaving equivalently under gaussianity. when an i.i.d. assumption on the innovations is appropriate for the data at hand, our tests exploit the efficiency gains possible. moreover, we show by simulation that our test remains well behaved under some forms of conditional heteroskedasticity.", "categories": "econ.em", "created": "2020-09-17", "updated": "", "authors": ["bas werker", "bo zhou"], "url": "https://arxiv.org/abs/2009.08291"}, {"title": "inference for large-scale linear systems with known coefficients", "id": "2009.08568", "abstract": "this paper considers the problem of testing whether there exists a non-negative solution to a possibly under-determined system of linear equations with known coefficients. this hypothesis testing problem arises naturally in a number of settings, including random coefficient, treatment effect, and discrete choice models, as well as a class of linear programming problems. as a first contribution, we obtain a novel geometric characterization of the null hypothesis in terms of identified parameters satisfying an infinite set of inequality restrictions. using this characterization, we devise a test that requires solving only linear programs for its implementation, and thus remains computationally feasible in the high-dimensional applications that motivate our analysis. the asymptotic size of the proposed test is shown to equal at most the nominal level uniformly over a large class of distributions that permits the number of linear equations to grow with the sample size.", "categories": "econ.em", "created": "2020-09-17", "updated": "", "authors": ["zheng fang", "andres santos", "azeem m. shaikh", "alexander torgovitsky"], "url": "https://arxiv.org/abs/2009.08568"}, {"title": "a study into the impact of anti-extradition bill protests on bangladeshi   immigration into hong kong", "id": "2009.09165", "abstract": "consequences from the 2019 anti-extradition protests in hong kong have been studied in many facets, but one topic of interest that has not been explored is the impact on the immigration of bangladeshi immigrants into the city. this paper explores the value add of bangladeshis to the hong kong, how the protests affected their mentality and consequently their immigration, and potentially longer-term detrimental effects on the city.", "categories": "econ.gn cs.si q-fin.ec", "created": "2020-09-19", "updated": "", "authors": ["siddhartha datta"], "url": "https://arxiv.org/abs/2009.09165"}, {"title": "the case against the universal basic income", "id": "2009.09198", "abstract": "with the cost of implementation shrinking and robot-to-workers ratio skyrocketing, the effects of automation on our economy and society are more palpable than ever. according to various studies, over half of our jobs could be fully executed by machines over the next decade or two, with severe impacts concentrated disproportionately on manufacturing-focused developing countries. in response to the threat of mass displacement of labour due to automation, economists, politicians, and even the business community have come to see universal basic income (ubi) as the panacea. this paper argued against a ubi by addressing its implementation costs and efficiency in mitigating the impact of automation through quantitative evidence as well as results of failed ubi-comparable programs across the world. the author of this paper instead advocated for the continuation of existing means-tested welfare systems and further investment in education scheme for unskilled and low-skilled labour.   this paper was submitted to the \"young economist of the year 2019\" essay competition hosted by the financial times and the royal economic society, where it won a high commendation and was one of the 36 best papers shortlisted among 1,300 qualified submissions to be honoured on the website of the royal economic society (2.7% acceptance rate). due to the rules and policies of the royal economic society, the author could only make this paper available to the public at least one year after the original date of submission.", "categories": "econ.gn q-fin.ec q-fin.gn", "created": "2020-09-19", "updated": "", "authors": ["le dong hai nguyen"], "url": "https://arxiv.org/abs/2009.09198"}, {"title": "real-time tracking of covid-19 impacts across europe reveals that   seeking \"herd immunity\" provides no economic benefits", "id": "2009.09222", "abstract": "this paper develops a methodology for tracking in real time the impact of the covid-19 pandemic on economic activity by analyzing high-frequency electricity market data. the approach is validated by several robustness tests and by contrasting our estimates with the official statistics on the recession caused by covid-19 in different european countries during the first two quarters of 2020. compared with the standard indicators, our results are much more chronologically disaggregated and up-to-date and, therefore, can inform the current debate on the appropriate policy response to the pandemic. unsurprisingly, we find that nations that experienced the most severe initial outbreaks also grappled with the hardest economic recessions. however, we detect diffused signs of recovery, with economic activity in most european countries returning to its pre-pandemic level by august 2020. furthermore, we show how delaying intervention or pursuing 'herd immunity' are not successful strategies, since they increase both economic disruption and mortality. the most effective short-run strategy to minimize the impact of the pandemic appears to be the introduction of early and relatively less stringent non-pharmaceutical interventions.", "categories": "econ.gn q-fin.ec", "created": "2020-09-19", "updated": "2020-10-07", "authors": ["carlo fezzi", "valeria fanghella"], "url": "https://arxiv.org/abs/2009.09222"}, {"title": "sulfur emission reduction in cargo ship manufacturers and shipping   companies based on marpol annex vi", "id": "2009.09547", "abstract": "this article explores the challenges for the adoption of scrubbers and low sulfur fuels on ship manufacturers and shipping companies. results show that ship manufacturers, must finance their working capital and operating costs, which implies an increase in the prices of the ships employing these new technologies. on the other hand, shipping companies must adopt the most appropriate technology according to the areas where ships navigate, the scale economies of trade routes, and the cost-benefit analysis of ship modernization.", "categories": "econ.gn q-fin.ec", "created": "2020-09-20", "updated": "", "authors": ["abraham londono pineda", "jose alejandro cano", "lissett pulgarin"], "url": "https://arxiv.org/abs/2009.09547"}, {"title": "optimal probabilistic forecasts: when do they work?", "id": "2009.09592", "abstract": "proper scoring rules are used to assess the out-of-sample accuracy of probabilistic forecasts, with different scoring rules rewarding distinct aspects of forecast performance. herein, we re-investigate the practice of using proper scoring rules to produce probabilistic forecasts that are `optimal' according to a given score, and assess when their out-of-sample accuracy is superior to alternative forecasts, according to that score. particular attention is paid to relative predictive performance under misspecification of the predictive model. using numerical illustrations, we document several novel findings within this paradigm that highlight the important interplay between the true data generating process, the assumed predictive model and the scoring rule. notably, we show that only when a predictive model is sufficiently compatible with the true process to allow a particular score criterion to reward what it is designed to reward, will this approach to forecasting reap benefits. subject to this compatibility however, the superiority of the optimal forecast will be greater, the greater is the degree of misspecification. we explore these issues under a range of different scenarios, and using both artificially simulated and empirical data.", "categories": "econ.em stat.me", "created": "2020-09-20", "updated": "", "authors": ["gael m. martin", "rub\u00e9n loaiza-maya", "david t. frazier", "worapree maneesoonthorn", "andr\u00e9s ram\u00edrez hassan"], "url": "https://arxiv.org/abs/2009.09592"}, {"title": "spillovers of program benefits with mismeasured networks", "id": "2009.09614", "abstract": "in studies of program evaluation under network interference, correctly measuring spillovers of the intervention is crucial for making appropriate policy recommendations. however, increasing empirical evidence has shown that network links are often measured with errors. this paper explores the identification and estimation of treatment and spillover effects when the network is mismeasured. i propose a novel method to nonparametrically point-identify the treatment and spillover effects, when two network observations are available. the method can deal with a large network with missing or misreported links and possesses several attractive features: (i) it allows heterogeneous treatment and spillover effects; (ii) it does not rely on modelling network formation or its misclassification probabilities; and (iii) it accommodates samples that are correlated in overlapping ways. a semiparametric estimation approach is proposed, and the analysis is applied to study the spillover effects of an insurance information program on the insurance adoption decisions.", "categories": "econ.em", "created": "2020-09-21", "updated": "", "authors": ["lina zhang"], "url": "https://arxiv.org/abs/2009.09614"}, {"title": "industrial topics in urban labor system", "id": "2009.09799", "abstract": "categorization is an essential component for us to understand the world for ourselves and to communicate it collectively. it is therefore important to recognize that classification system are not necessarily static, especially for economic systems, and even more so in urban areas where most innovation takes place and is implemented. out-of-date classification systems would potentially limit further understanding of the current economy because things constantly change. here, we develop an occupation-based classification system for the us labor economy, called industrial topics, that satisfy adaptability and representability. by leveraging the distributions of occupations across the us urban areas, we identify industrial topics - clusters of occupations based on their co-existence pattern. industrial topics indicate the mechanisms under the systematic allocation of different occupations. considering the densely connected occupations as an industrial topic, our approach characterizes regional economies by their topical composition. unlike the existing survey-based top-down approach, our method provides timely information about the underlying structure of the regional economy, which is critical for policymakers and business leaders, especially in our fast-changing economy.", "categories": "cs.si cs.lg econ.gn q-fin.ec", "created": "2020-09-17", "updated": "", "authors": ["jaehyuk park", "morgan r. frank", "lijun sun", "hyejin youn"], "url": "https://arxiv.org/abs/2009.09799"}, {"title": "on the existence of conditional maximum likelihood estimates of the   binary logit model with fixed effects", "id": "2009.09998", "abstract": "by exploiting mcfadden (1974)'s results on conditional logit estimation, we show that there exists a one-to-one mapping between existence and uniqueness of conditional maximum likelihood estimates of the binary logit model with fixed effects and the configuration of data points. our results extend those in albert and anderson (1984) for the cross-sectional case and can be used to build a simple algorithm that detects spurious estimates in finite samples. as an illustration, we exhibit an artificial dataset for which the stata's command \\texttt{clogit} returns spurious estimates.", "categories": "econ.em", "created": "2020-09-21", "updated": "2020-09-29", "authors": ["martin mugnier"], "url": "https://arxiv.org/abs/2009.09998"}, {"title": "recent developments on factor models and its applications in econometric   learning", "id": "2009.10103", "abstract": "this paper makes a selective survey on the recent development of the factor model and its application on statistical learnings. we focus on the perspective of the low-rank structure of factor models, and particularly draws attentions to estimating the model from the low-rank recovery point of view. the survey mainly consists of three parts: the first part is a review on new factor estimations based on modern techniques on recovering low-rank structures of high-dimensional models. the second part discusses statistical inferences of several factor-augmented models and applications in econometric learning models. the final part summarizes new developments dealing with unbalanced panels from the matrix completion perspective.", "categories": "econ.em stat.ml", "created": "2020-09-21", "updated": "", "authors": ["jianqing fan", "kunpeng li", "yuan liao"], "url": "https://arxiv.org/abs/2009.10103"}, {"title": "an arrow-debreu extension of the hylland-zeckhauser scheme: equilibrium   existence and algorithms", "id": "2009.10320", "abstract": "the arrow-debreu extension of the classic hylland-zeckhauser scheme for a one-sided matching market -- called adhz in this paper -- has natural applications but has instances which do not admit equilibria. by introducing approximation, we define the $\\epsilon$-approximate adhz model. we give the following results.   * existence of equilibrium for the $\\epsilon$-approximate adhz model under linear utility functions. the equilibrium satisfies pareto optimality, approximate envy-freeness and incentive compatibility in the large.   * a combinatorial polynomial time algorithm for an $\\epsilon$-approximate adhz equilibrium for the case of dichotomous, and more generally bi-valued, utilities.   * an instance of adhz, with dichotomous utilities and a strongly connected demand graph, which does not admit an equilibrium.   * a rational convex program for hz under dichotomous utilities; a combinatorial polynomial time algorithm for this case was given by vazirani and yannakakis (2020).   the $\\epsilon$-approximate adhz model fills a void, described in the paper, in the space of general mechanisms for one-sided matching markets.", "categories": "cs.gt econ.th", "created": "2020-09-22", "updated": "2020-09-28", "authors": ["jugal garg", "thorben tr\u00f6bst", "vijay v. vazirani"], "url": "https://arxiv.org/abs/2009.10320"}, {"title": "the two growth rates of the economy", "id": "2009.10451", "abstract": "economic growth is measured as the rate of relative change in gross domestic product (gdp) per capita. yet, when incomes follow random multiplicative growth, the ensemble-average (gdp per capita) growth rate is higher than the time-average growth rate achieved by each individual in the long run. this mathematical fact is the starting point of ergodicity economics. using the atypically high ensemble-average growth rate as the principal growth measure creates an incomplete picture. policymaking would be better informed by reporting both ensemble-average and time-average growth rates. we analyse rigorously these growth rates and describe their evolution in the united states and france over the last fifty years. the difference between the two growth rates gives rise to a natural measure of income inequality, equal to the mean logarithmic deviation. despite being estimated as the average of individual income growth rates, the time-average growth rate is independent of income mobility.", "categories": "econ.gn q-fin.ec", "created": "2020-09-22", "updated": "", "authors": ["alexander adamou", "yonatan berman", "ole peters"], "url": "https://arxiv.org/abs/2009.10451"}, {"title": "analysis of the main factors for the configuration of green ports in   colombia", "id": "2009.10834", "abstract": "this study analyzes the factors affecting the configuration and consolidation of green ports in colombia. for this purpose a case stady of maritime cargo ports of cartagena, barranquilla and santa marta is performed addressing semiestructured interviews to identify the factors contributing to the consolidation of green ports and the factors guiding the sustainability management in the ports that have not yet been certified as green ports. the results show that environmental regulations are atarting point not the key factor to consolidate asgreen ports. as a conclusions, the conversion of colombian to green ports should not be limited to the attaiment of certifications, such as ecoport certification, but should ensure the contribution to sustainable development through economic, social and environmental dimensions and the achievement of the sdgs", "categories": "econ.gn q-fin.ec", "created": "2020-09-22", "updated": "", "authors": ["abraham londono pineda", "tatiana arias naranjo", "jose alejandro cano arenas"], "url": "https://arxiv.org/abs/2009.10834"}, {"title": "a deep learning approach for dynamic balance sheet stress testing", "id": "2009.11075", "abstract": "in the aftermath of the financial crisis, supervisory authorities have considerably improved their approaches in performing financial stress testing. however, they have received significant criticism by the market participants due to the methodological assumptions and simplifications employed, which are considered as not accurately reflecting real conditions. first and foremost, current stress testing methodologies attempt to simulate the risks underlying a financial institution's balance sheet by using several satellite models, making their integration a really challenging task with significant estimation errors. secondly, they still suffer from not employing advanced statistical techniques, like machine learning, which capture better the nonlinear nature of adverse shocks. finally, the static balance sheet assumption, that is often employed, implies that the management of a bank passively monitors the realization of the adverse scenario, but does nothing to mitigate its impact. to address the above mentioned criticism, we introduce in this study a novel approach utilizing deep learning approach for dynamic balance sheet stress testing. experimental results give strong evidence that deep learning applied in big financial/supervisory datasets create a state of the art paradigm, which is capable of simulating real world scenarios in a more efficient way.", "categories": "q-fin.cp econ.gn q-fin.ec", "created": "2020-09-23", "updated": "", "authors": ["anastasios petropoulos", "vassilis siakoulis", "konstantinos p. panousis", "theodoros christophides", "sotirios chatzis"], "url": "https://arxiv.org/abs/2009.11075"}, {"title": "a step-by-step guide to design, implement, and analyze a discrete choice   experiment", "id": "2009.11235", "abstract": "discrete choice experiments (dce) have been widely used in health economics, environmental valuation, and other disciplines. however, there is a lack of resources disclosing the whole procedure of carrying out a dce. this document aims to assist anyone wishing to use the power of dces to understand people's behavior by providing a comprehensive guide to the procedure. this guide contains all the code needed to design, implement, and analyze a dce using only free software.", "categories": "econ.em", "created": "2020-09-23", "updated": "", "authors": ["daniel p\u00e9rez-troncoso"], "url": "https://arxiv.org/abs/2009.11235"}, {"title": "selling two identical objects", "id": "2009.11545", "abstract": "it is well-known that optimal (i.e., revenue-maximizing) selling mechanisms in multidimensional type spaces may involve randomization. we study mechanisms for selling two identical, indivisible objects to a single buyer. we analyze two settings: (i) decreasing marginal values (dmv) and (ii) increasing marginal values (imv). thus, the two marginal values of the buyer are not independent. we obtain sufficient conditions on the distribution of buyer values for the existence of an optimal mechanism that is deterministic.   in the dmv model, we show that under a well-known condition, it is optimal to sell the first unit deterministically. under the same sufficient condition, a bundling mechanism (which is deterministic) is optimal in the imv model. under a stronger sufficient condition, a deterministic mechanism is optimal in the dmv model.   our results apply to heterogenous objects when there is a specified sequence in which the two objects must be sold.", "categories": "econ.th cs.gt", "created": "2020-09-24", "updated": "", "authors": ["sushil bikhchandani", "debasis mishra"], "url": "https://arxiv.org/abs/2009.11545"}, {"title": "bayesian learning in dynamic non-atomic routing games", "id": "2009.11580", "abstract": "we consider a discrete-time nonatomic routing game with variable demand and uncertain costs. given a fixed routing network with single origin and destination, the costs functions on edges depend on some uncertain persistent state parameter. every period, a variable traffic demand routes through the network. the experienced costs are publicly observed and the belief about the state parameter is bayesianly updated. this paper studies the dynamics of equilibrium and beliefs. we say that there is strong learning when beliefs converge to the truth and there is weak learning when equilibrium flows converge to those under complete information. our main result is a characterization of the networks for which learning occurs for all increasing cost functions, given highly variable demand. we prove that these networks have a series-parallel structure and provide a counterexample to prove that the condition is necessary.", "categories": "econ.th cs.gt", "created": "2020-09-24", "updated": "", "authors": ["emilien macault", "marco scarsini", "tristan tomala"], "url": "https://arxiv.org/abs/2009.11580"}, {"title": "human and financial cost of covid-19", "id": "2009.11660", "abstract": "this paper analyzes the human and financial costs of the covid-19 pandemic on 92 countries. we compare country-by-country equity market dynamics to cumulative covid-19 case and death counts and new case trajectories. first, we examine the multivariate time series of cumulative cases and deaths, particularly regarding their changing structure over time. we reveal similarities between the case and death time series, and key dates that the structure of the time series changed. next, we classify new case time series, demonstrate five characteristic classes of trajectories, and quantify discrepancy between them with respect to the behavior of waves of the disease. finally, we show there is no relationship between countries' equity market performance and their success in managing covid-19. each country's equity index has been unresponsive to the domestic or global state of the pandemic. instead, these indices have been highly uniform, with most movement in march.", "categories": "physics.soc-ph econ.gn q-fin.ec", "created": "2020-09-24", "updated": "", "authors": ["nick james", "max menzies"], "url": "https://arxiv.org/abs/2009.11660"}, {"title": "non-convergence to stability in coalition formation games", "id": "2009.11689", "abstract": "we study the problem of convergence to stability in coalition formation games in which the strategies of each agent are coalitions in which she can participate and outcomes are coalition structures. given a natural blocking dynamic, an absorbing set is a minimum set of coalition structures that once reached is never abandoned. the coexistence of single and non-single absorbing sets is what causes lack of convergence to stability. to characterize games in which both types of set are present, we first relate circularity among coalitions in preferences (rings) with circularity among coalition structures (cycles) and show that there is a ring in preferences if and only if there is a cycle in coalition structures. then we identify a special configuration of overlapping rings in preferences characterizing games that lack convergence to stability. finally, we apply our findings to the study of games induced by sharing rules.", "categories": "econ.th cs.gt", "created": "2020-09-24", "updated": "", "authors": ["agust\u00edn g. bonifacio", "elena inarra", "pablo neme"], "url": "https://arxiv.org/abs/2009.11689"}, {"title": "a test for heckscher-ohlin using value-added exports", "id": "2009.11743", "abstract": "empirical evidence for the heckscher-ohlin model has been inconclusive. we test whether the predictions of the heckscher-ohlin theorem with respect to labor and capital find support in value-added trade. defining labor-capital intensities and endowments as the ratio of hours worked to the nominal capital stock, we find evidence against heckscher-ohlin. however, taking the ratio of total factor compensations, and thus accounting for differences in technologies, we find strong support for it. that is, labor-abundant countries tend to export value-added in goods of labor-intensive industries. moreover, differentiating between broad industries, we find support for nine out of twelve industries.", "categories": "econ.gn q-fin.ec", "created": "2020-09-24", "updated": "", "authors": ["philipp koch", "clemens fessler"], "url": "https://arxiv.org/abs/2009.11743"}, {"title": "the affiliate matching problem: on labor markets where firms are also   interested in the placement of previous workers", "id": "2009.11867", "abstract": "in many labor markets, workers and firms are connected via affiliative relationships. a management consulting firm wishes to both accept the best new workers but also place its current affiliated workers at strong firms. similarly, a research university wishes to hire strong job market candidates while also placing its own candidates at strong peer universities. we model this affiliate matching problem in a generalization of the classic stable marriage setting by permitting firms to state preferences over not just which workers to whom they are matched, but also to which firms their affiliated workers are matched. based on results from a human survey, we find that participants (acting as firms) give preference to their own affiliate workers in surprising ways that violate some assumptions of the classical stable marriage problem. this motivates a nuanced discussion of how stability could be defined in affiliate matching problems; we give an example of a marketplace which admits a stable match under one natural definition of stability, and does not for that same marketplace under a different, but still natural, definition. we conclude by setting a research agenda toward the creation of a centralized clearing mechanism in this general setting.", "categories": "econ.gn cs.ai cs.cy cs.ds cs.gt q-fin.ec", "created": "2020-09-23", "updated": "", "authors": ["samuel dooley", "john p. dickerson"], "url": "https://arxiv.org/abs/2009.11867"}, {"title": "pareto efficient combinatorial auctions: dichotomous preferences without   quasilinearity", "id": "2009.12114", "abstract": "we consider a combinatorial auction model where preferences of agents over bundles of objects and payments need not be quasilinear. however, we restrict the preferences of agents to be dichotomous. an agent with dichotomous preference partitions the set of bundles of objects as acceptable} and unacceptable, and at the same payment level, she is indifferent between bundles in each class but strictly prefers acceptable to unacceptable bundles. we show that there is no pareto efficient, dominant strategy incentive compatible (dsic), individually rational (ir) mechanism satisfying no subsidy if the domain of preferences includes all dichotomous preferences. however, a generalization of the vcg mechanism is pareto efficient, dsic, ir and satisfies no subsidy if the domain of preferences contains only positive income effect dichotomous preferences. we show the tightness of this result: adding any non-dichotomous preference (satisfying some natural properties) to the domain of quasilinear dichotomous preferences brings back the impossibility result.", "categories": "econ.th cs.gt", "created": "2020-09-25", "updated": "", "authors": ["komal malik", "debasis mishra"], "url": "https://arxiv.org/abs/2009.12114"}, {"title": "latent causal socioeconomic health index", "id": "2009.12217", "abstract": "this research develops a model-based latent causal socioeconomic health (lacsh) index at the national level. we build upon the latent health factor index (lhfi) approach that has been used to assess the unobservable ecological/ecosystem health. this framework integratively models the relationship between metrics, the latent health, and the covariates that drive the notion of health. in this paper, the lhfi structure is integrated with spatial modeling and statistical causal modeling, so as to evaluate the impact of a continuous policy variable (mandatory maternity leave days and government's expenditure on healthcare, respectively) on a nation's socioeconomic health, while formally accounting for spatial dependency among the nations. a novel visualization technique for evaluating covariate balance is also introduced for the case of a continuous policy (treatment) variable. we apply our lacsh model to countries around the world using data on various metrics and potential covariates pertaining to different aspects of societal health. the approach is structured in a bayesian hierarchical framework and results are obtained by markov chain monte carlo techniques.", "categories": "stat.me econ.gn q-fin.ec stat.ap", "created": "2020-09-24", "updated": "", "authors": ["f. swen kuh", "grace s. chiu", "anton h. westveld"], "url": "https://arxiv.org/abs/2009.12217"}, {"title": "nonclassical measurement error in the outcome variable", "id": "2009.12665", "abstract": "we study a semi-/nonparametric regression model with a general form of nonclassical measurement error in the outcome variable. we show equivalence of this model to a generalized regression model. our main identifying assumptions are a special regressor type restriction and monotonicity in the nonlinear relationship between the observed and unobserved true outcome. nonparametric identification is then obtained under a normalization of the unknown link function, which is a natural extension of the classical measurement error case. we propose a novel sieve rank estimator for the regression function. we establish a rate of convergence of the estimator which depends on the strength of identification. in monte carlo simulations, we find that our estimator corrects for biases induced by measurement errors and provides numerically stable results. we apply our method to analyze belief formation of stock market expectations with survey data from the german socio-economic panel (soep) and find evidence for non-classical measurement error in subjective belief data.", "categories": "econ.em stat.me", "created": "2020-09-26", "updated": "", "authors": ["christoph breunig", "stephan martin"], "url": "https://arxiv.org/abs/2009.12665"}, {"title": "on the continuity of the feasible set mapping in optimal transport", "id": "2009.12838", "abstract": "consider the set of probability measures with given marginal distributions on the product of two complete, separable metric spaces, seen as a correspondence when the marginal distributions vary. in problems of optimal transport, continuity of this correspondence from marginal to joint distributions is often desired, in light of berge's maximum theorem, to establish continuity of the value function in the marginal distributions, as well as stability of the set of optimal transport plans. bergin (1999) established the continuity of this correspondence, and in this note, we present a novel and considerably shorter proof of this important result. we then examine an application to an assignment game (transferable utility matching problem) with unknown type distributions.", "categories": "q-fin.rm econ.th q-fin.mf", "created": "2020-09-27", "updated": "", "authors": ["mario ghossoub", "david saunders"], "url": "https://arxiv.org/abs/2009.12838"}, {"title": "data-driven models of selfish routing: why price of anarchy does depend   on network topology", "id": "2009.12871", "abstract": "we investigate traffic routing both from the perspective of real world data as well as theory. first, we reveal through data analytics a natural but previously uncaptured regularity of real world routing behavior. agents only consider, in their strategy sets, paths whose free-flow costs (informally their lengths) are within a small multiplicative $(1+\\theta)$ constant of the optimal free-flow cost path connecting their source and destination where $\\theta\\geq0$. in the case of singapore, $\\theta=1$ is a good estimate of agents' route (pre)selection mechanism. in contrast, in pigou networks the ratio of the free-flow costs of the routes and thus $\\theta$ is infinite, so although such worst case networks are mathematically simple they correspond to artificial routing scenarios with little resemblance to real world conditions, opening the possibility of proving much stronger price of anarchy guarantees by explicitly studying their dependency on $\\theta$. we provide an exhaustive analysis of this question by providing provably tight bounds on poa($\\theta$) for arbitrary classes of cost functions both in the case of general congestion/routing games as well as in the special case of path-disjoint networks. for example, in the case of the standard bureau of public roads (bpr) cost model, $c_e(x)= a_e x^4+b_e$ and more generally quartic cost functions, the standard poa bound for $\\theta=\\infty$ is $2.1505$ (roughgarden, 2003) and it is tight both for general networks as well as path-disjoint and even parallel-edge networks. in comparison, in the case of $\\theta=1$, the poa in the case of general networks is only $1.6994$, whereas for path-disjoint/parallel-edge networks is even smaller ($1.3652$), showing that both the route geometries as captured by the parameter $\\theta$ as well as the network topology have significant effects on poa (figure 1).", "categories": "cs.gt econ.th", "created": "2020-09-27", "updated": "", "authors": ["francisco benita", "vittorio bil\u00f2", "barnab\u00e9 monnot", "georgios piliouras", "cosimo vinci"], "url": "https://arxiv.org/abs/2009.12871"}, {"title": "stochastic stability of a recency weighted sampling dynamic", "id": "2009.12910", "abstract": "it is common to model learning in games so that either a deterministic process or a finite state markov chain describes the evolution of play. such processes can however produce undesired outputs, where the players' behavior is heavily influenced by the modeling. in simulations we see how the assumptions in (young, 1993), a well-studied model for stochastic stability, lead to unexpected behavior in games without strict equilibria, such as matching pennies. the behavior should be considered a modeling artifact. in this paper we propose a continuous-state space model for learning in games that can converge to mixed nash equilibria, the recency weighted sampler (rws). the rws is similar in spirit young's model, but introduces a notion of best response where the players sample from a recency weighted history of interactions. we derive properties of the rws which are known to hold for finite-state space models of adaptive play, such as the convergence to and existence of a unique invariant distribution of the process, and the concentration of that distribution on minimal curb blocks. then, we establish conditions under which the rws process concentrates on mixed nash equilibria inside minimal curb blocks. while deriving the results, we develop a methodology that is relevant for a larger class of continuous state space learning models.", "categories": "econ.th", "created": "2020-09-27", "updated": "", "authors": ["alexander aurell", "gustav karreskog"], "url": "https://arxiv.org/abs/2009.12910"}, {"title": "teacher turnover in rwanda", "id": "2009.13091", "abstract": "despite widely documented shortfalls of teacher skills and effort, there is little systematic evidence of rates of teacher turnover in low-income countries. i investigate the incidence and consequences of teacher turnover in rwandan public primary schools over the period from 2016-2019. to do so, i combine the universe of teacher placement records with student enrollment figures and school-average primary leaving exam scores in a nationally representative sample of 259 schools. results highlight five features of teacher turnover. first, rates of teacher turnover are high: annually, 20 percent of teachers separate from their jobs, of which 11 percent exit from the public-sector teaching workforce. second, the burden of teacher churn is higher in schools with low learning levels and, perhaps surprisingly, in low pupil-teacher-ratio schools. third, teacher turnover is concentrated among early-career teachers, male teachers, and those assigned to teach math. fourth, replacing teachers quickly after they exit is a challenge; 23 percent of exiting teachers are not replaced the following year. and fifth, teacher turnover is associated with subsequent declines in learning outcomes. on average, the loss of a teacher is associated with a reduction in learning levels of 0.05 standard deviations. in addition to class-size increases, a possible mechanism for these learning outcomes is the prevalence of teachers teaching outside of their areas of subject expertise: in any given year, at least 21 percent of teachers teach in subjects in which they have not been trained. taken together, these results suggest that the problem of teacher turnover is substantial in magnitude and consequential for learning outcomes in schools.", "categories": "econ.gn q-fin.ec", "created": "2020-09-28", "updated": "", "authors": ["andrew zeitlin"], "url": "https://arxiv.org/abs/2009.13091"}, {"title": "learning classifiers under delayed feedback with a time window   assumption", "id": "2009.13092", "abstract": "we consider training a binary classifier under delayed feedback (df learning). in df learning, we first receive negative samples; subsequently, some samples turn positive. this problem is conceivable in various real-world applications such as online advertisements, where the user action takes place long after the first click. owing to the delayed feedback, simply separating the positive and negative data causes a sample selection bias. one solution is to assume that a long time window after first observing a sample reduces the sample selection bias. however, existing studies report that only using a portion of all samples based on the time window assumption yields suboptimal performance, and the use of all samples along with the time window assumption improves empirical performance. extending these existing studies, we propose a method with an unbiased and convex empirical risk constructed from the whole samples under the time window assumption. we provide experimental results to demonstrate the effectiveness of the proposed method using a real traffic log dataset.", "categories": "cs.lg econ.em stat.ml", "created": "2020-09-28", "updated": "", "authors": ["masahiro kato", "shota yasui"], "url": "https://arxiv.org/abs/2009.13092"}, {"title": "predictors of social distancing and mask-wearing behavior: panel survey   in seven u.s. states", "id": "2009.13103", "abstract": "this paper presents preliminary summary results from a longitudinal study of participants in seven u.s. states during the covid-19 pandemic. in addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. we pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. we note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. these findings can help inform the design of policy interventions regarding targeting specific demographic groups, which can help reduce the transmission speed of the covid-19 virus.", "categories": "econ.gn q-fin.ec", "created": "2020-09-28", "updated": "", "authors": ["plamen nikolov", "andreas pape", "ozlem tonguc", "charlotte williams"], "url": "https://arxiv.org/abs/2009.13103"}, {"title": "ordinal bayesian incentive compatibility in random assignment model", "id": "2009.13104", "abstract": "we explore the consequences of weakening the notion of incentive compatibility from strategy-proofness to ordinal bayesian incentive compatibility (obic) in the random assignment model. if the common prior of the agents is a uniform prior, then a large class of random mechanisms are obic with respect to this prior -- this includes the probabilistic serial mechanism. we then introduce a robust version of obic: a mechanism is locally robust obic if it is obic with respect all independent priors in some neighborhood of a given independent prior. we show that every locally robust obic mechanism satisfying a mild property called elementary monotonicity is strategy-proof. this leads to a strengthening of the impossibility result in bogomolnaia and moulin (2001): if there are at least four agents, there is no locally robust obic and ordinally efficient mechanism satisfying equal treatment of equals.", "categories": "econ.th cs.gt", "created": "2020-09-28", "updated": "", "authors": ["sulagna dasgupta", "debasis mishra"], "url": "https://arxiv.org/abs/2009.13104"}, {"title": "the role of behavioural plasticity in finite vs infinite populations", "id": "2009.13160", "abstract": "evolutionary game theory has proven to be an elegant framework providing many fruitful insights in population dynamics and human behaviour. here, we focus on the aspect of behavioural plasticity and its effect on the evolution of populations. we consider games with only two strategies in both well-mixed infinite and finite populations settings. we assume that individuals might exhibit behavioural plasticity referred to as incompetence of players. we study the effect of such heterogeneity on the outcome of local interactions and, ultimately, on global competition. for instance, a strategy that was dominated before can become desirable from the selection perspective when behavioural plasticity is taken into account. furthermore, it can ease conditions for a successful fixation in infinite populations' invasions. we demonstrate our findings on the examples of prisoners' dilemma and snowdrift game, where we define conditions under which cooperation can be promoted.", "categories": "q-bio.pe cs.gt econ.th", "created": "2020-09-28", "updated": "", "authors": ["m. kleshnina", "k. kaveh", "k. chatterjee"], "url": "https://arxiv.org/abs/2009.13160"}, {"title": "transparency, auditability and explainability of machine learning models   in credit scoring", "id": "2009.13384", "abstract": "a major requirement for credit scoring models is to provide a maximally accurate risk prediction. additionally, regulators demand these models to be transparent and auditable. thus, in credit scoring, very simple predictive models such as logistic regression or decision trees are still widely used and the superior predictive power of modern machine learning algorithms cannot be fully leveraged. significant potential is therefore missed, leading to higher reserves or more credit defaults. this paper works out different dimensions that have to be considered for making credit scoring models understandable and presents a framework for making ``black box'' machine learning models transparent, auditable and explainable. following this framework, we present an overview of techniques, demonstrate how they can be applied in credit scoring and how results compare to the interpretability of score cards. a real world case study shows that a comparable degree of interpretability can be achieved while machine learning techniques keep their ability to improve predictive power.", "categories": "stat.ml cs.lg econ.gn q-fin.ec stat.ap stat.me", "created": "2020-09-28", "updated": "", "authors": ["michael b\u00fccker", "gero szepannek", "alicja gosiewska", "przemyslaw biecek"], "url": "https://arxiv.org/abs/2009.13384"}, {"title": "difference-in-differences for ordinal outcomes: application to the   effect of mass shootings on attitudes toward gun control", "id": "2009.13404", "abstract": "the difference-in-differences (did) design is widely used in observational studies to estimate the causal effect of a treatment when repeated observations over time are available. yet, almost all existing methods assume linearity in the potential outcome (parallel trends assumption) and target the additive effect. in social science research, however, many outcomes of interest are measured on an ordinal scale. this makes the linearity assumption inappropriate because the difference between two ordinal potential outcomes is not well defined. in this paper, i propose a method to draw causal inferences for ordinal outcomes under the did design. unlike existing methods, the proposed method utilizes the latent variable framework to handle the non-numeric nature of the outcome, enabling identification and estimation of causal effects based on the assumption on the quantile of the latent continuous variable. the paper also proposes an equivalence-based test to assess the plausibility of the key identification assumption when additional pre-treatment periods are available. the proposed method is applied to a study estimating the causal effect of mass shootings on the public's support for gun control. i find little evidence for a uniform shift toward pro-gun control policies as found in the previous study, but find that the effect is concentrated on left-leaning respondents who experienced the shooting for the first time in more than a decade.", "categories": "stat.ap econ.em stat.me", "created": "2020-09-28", "updated": "", "authors": ["soichiro yamauchi"], "url": "https://arxiv.org/abs/2009.13404"}, {"title": "necessity of hyperbolic absolute risk aversion for the concavity of   consumption functions", "id": "2009.13564", "abstract": "carroll and kimball (1996) have shown that, in the class of utility functions that are strictly increasing, strictly concave, and have nonnegative third derivatives, hyperbolic absolute risk aversion (hara) is sufficient for the concavity of consumption functions in general consumption-saving problems. this paper shows that hara is necessary, implying the concavity of consumption is not a robust prediction outside the hara class.", "categories": "econ.th", "created": "2020-09-28", "updated": "", "authors": ["alexis akira toda"], "url": "https://arxiv.org/abs/2009.13564"}, {"title": "expectations, networks, and conventions", "id": "2009.13802", "abstract": "in coordination games and speculative over-the-counter financial markets, solutions depend on higher-order average expectations: agents' expectations about what counterparties, on average, expect their counterparties to think, etc. we offer a unified analysis of these objects and their limits, for general information structures, priors, and networks of counterparty relationships. our key device is an interaction structure combining the network and agents' beliefs, which we analyze using markov methods. this device allows us to nest classical beauty contests and network games within one model and unify their results. two applications illustrate the techniques: the first characterizes when slight optimism about counterparties' average expectations leads to contagion of optimism and extreme asset prices. the second describes the tyranny of the least-informed: agents coordinating on the prior expectations of the one with the worst private information, despite all having nearly common certainty, based on precise private signals, of the ex post optimal action.", "categories": "econ.th cs.gt", "created": "2020-09-29", "updated": "", "authors": ["benjamin golub", "stephen morris"], "url": "https://arxiv.org/abs/2009.13802"}, {"title": "sharp bounds on treatment effects for policy evaluation", "id": "2009.13861", "abstract": "for counterfactual policy evaluation, it is important to ensure that treatment parameters are relevant to the policies in question. this is especially challenging under unobserved heterogeneity, as is well featured in the definition of the local average treatment effect (late). being intrinsically local, the late is known to lack external validity in counterfactual environments. this paper investigates the possibility of extrapolating local treatment effects to different counterfactual settings when instrumental variables are only binary. we propose a novel framework to systematically calculate sharp nonparametric bounds on various policy-relevant treatment parameters that are defined as weighted averages of the marginal treatment effect (mte). our framework is flexible enough to incorporate a large menu of identifying assumptions beyond the shape restrictions on the mte that have been considered in prior studies. we apply our method to understand the effects of medical insurance policies on the use of medical services.", "categories": "econ.em", "created": "2020-09-29", "updated": "", "authors": ["sukjin han", "shenshen yang"], "url": "https://arxiv.org/abs/2009.13861"}, {"title": "online action learning in high dimensions: a new exploration rule for   contextual $\\epsilon_t$-greedy heuristics", "id": "2009.13961", "abstract": "bandit problems are pervasive in various fields of research and are also present in several practical applications. examples, including dynamic pricing and assortment and the design of auctions and incentives, permeate a large number of sequential treatment experiments. different applications impose distinct levels of restrictions on viable actions. some favor diversity of outcomes, while others require harmful actions to be closely monitored or mainly avoided. in this paper, we extend one of the most popular bandit solutions, the original $\\epsilon_t$-greedy heuristics, to high-dimensional contexts. moreover, we introduce a competing exploration mechanism that counts with searching sets based on order statistics. we view our proposals as alternatives for cases where pluralism is valued or, in the opposite direction, cases where the end-user should carefully tune the range of exploration of new actions. we find reasonable bounds for the cumulative regret of a decaying $\\epsilon_t$-greedy heuristic in both cases and we provide an upper bound for the initialization phase that implies the regret bounds when order statistics are considered to be at most equal but mostly better than the case when random searching is the sole exploration mechanism. additionally, we show that end-users have sufficient flexibility to avoid harmful actions since any cardinality for the higher-order statistics can be used to achieve an stricter upper bound. in a simulation exercise, we show that the algorithms proposed in this paper outperform simple and adapted counterparts.", "categories": "stat.ml cs.lg econ.em", "created": "2020-09-29", "updated": "", "authors": ["claudio c. flores", "marcelo c. medeiros"], "url": "https://arxiv.org/abs/2009.13961"}, {"title": "when local governments' stay-at-home orders meet the white house's   \"opening up america again\"", "id": "2009.14097", "abstract": "on april 16th, the white house launched \"opening up america again\" (ouaa) campaign while many u.s. counties had stay-at-home orders in place. we created a panel data set of 1,563 u.s. counties to study the impact of u.s. counties' stay-at-home orders on community mobility before and after the white house's campaign to reopen the country. our results suggest that before the ouaa campaign stay-at-home orders were effective in decreasing time spent at retail & recreation places and in increasing time spent at home. these stay-at-home orders were less effective in more conservative counties. we further find that the ouaa campaign significantly increased time spent at retail & recreation places and decreased time spent at home particularly in conservative counties. however, in conservative counties with stay-at-home orders in place, ouaa campaign was less effective when compared to conservative counties without stay-at-home orders. these findings signal promising news for local (county and state) authorities. that is, even when the federal government is reopening the country, the local authorities that enforced stay-at-home restrictions were to some extent effective.", "categories": "econ.gn q-fin.ec", "created": "2020-09-29", "updated": "", "authors": ["reza mousavi", "bin gu"], "url": "https://arxiv.org/abs/2009.14097"}, {"title": "price, volatility and the second-order economic theory", "id": "2009.14278", "abstract": "this paper considers price volatility as the reason for description of the second-degree economic variables, trades and expectations aggregated during certain time interval {\\delta}. we call it - the second-order economic theory. the n-th degree products of costs and volumes of trades, performed by economic agents during interval {\\delta} determine price n-th statistical moments. first two price statistical moments define volatility. to model volatility one needs description of the squares of trades aggregated during interval {\\delta}. to describe price probability one needs all n-th statistical moments of price but that is almost impossible. we define squares of agent's trades and macro expectations those approve the second-degree trades aggregated during interval {\\delta}. we believe that agents perform trades under action of multiple expectations. we derive equations on the second-degree trades and expectations in economic space. as economic space we regard numerical continuous risk grades. numerical risk grades are discussed at least for 80 years. we propose that econometrics permit accomplish risk assessment for almost all economic agents. agents risk ratings distribute agents by economic space and define densities of macro second-degree trades and expectations. in the linear approximation we derive mean square price and volatility disturbances as functions of the first and second-degree trades disturbances. in simple approximation numerous expectations and their perturbations can cause small harmonic oscillations of the second-degree trades disturbances and induce harmonic oscillations of price and volatility perturbations.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-09-06", "updated": "", "authors": ["victor olkhov"], "url": "https://arxiv.org/abs/2009.14278"}, {"title": "local regression distribution estimators", "id": "2009.14367", "abstract": "this paper investigates the large sample properties of local regression distribution estimators, which include a class of boundary adaptive density estimators as a prime example. first, we establish a pointwise gaussian large sample distributional approximation in a unified way, allowing for both boundary and interior evaluation points simultaneously. using this result, we study the asymptotic efficiency of the estimators, and show that a carefully crafted minimum distance implementation based on \"redundant\" regressors can lead to efficiency gains. second, we establish uniform linearizations and strong approximations for the estimators, and employ these results to construct valid confidence bands. third, we develop extensions to weighted distributions with estimated weights and to local $l^{2}$ least squares estimation. finally, we illustrate our methods with two applications in program evaluation: counterfactual density testing, and iv specification and heterogeneity density analysis. companion software packages in stata and r are available.", "categories": "econ.em math.st stat.me stat.th", "created": "2020-09-29", "updated": "", "authors": ["matias d. cattaneo", "michael jansson", "xinwei ma"], "url": "https://arxiv.org/abs/2009.14367"}, {"title": "on the quest for economic prosperity: a higher education strategic   perspective for the mena region", "id": "2009.14408", "abstract": "in a fast-changing technology-driven era, drafting an implementable strategic roadmap to achieve economic prosperity becomes a real challenge. although the national and international strategic development plans may vary, they usually target the improvement of the quality of living standards through boosting the national gdp per capita and the creation of decent jobs. there is no doubt that human capacity building, through higher education, is vital to the availability of highly qualified workforce supporting the implementation of the aforementioned strategies. in other words, fulfillment of most strategic development plan goals becomes dependent on the drafting and implementation of successful higher education strategies. for mena region countries, this is particularly crucial due to many specific challenges, some of which are different from those facing developed nations. more details on the mena region higher education strategic planning challenges as well as the proposed higher education strategic requirements to support national economic prosperity and fulfill the 2030 un sdgs are given in the paper.", "categories": "econ.gn q-fin.ec", "created": "2020-09-29", "updated": "", "authors": ["amr a. adly"], "url": "https://arxiv.org/abs/2009.14408"}, {"title": "debunking rumors in networks", "id": "2010.01018", "abstract": "we study the diffusion of a true and a false message (the rumor) in a social network. upon hearing a message, individuals may believe it, disbelieve it, or debunk it through costly verification. whenever the truth survives in steady state, so does the rumor. online social communication exacerbates relative rumor prevalence as long as it increases homophily or verification costs. our model highlights that successful policies in the fight against rumors increase individuals' incentives to verify.", "categories": "econ.th physics.soc-ph", "created": "2020-10-02", "updated": "", "authors": ["luca p. merlino", "paolo pin", "nicole tabasso"], "url": "https://arxiv.org/abs/2010.01018"}, {"title": "covid-19, economic policy uncertainty and stock market crash risk", "id": "2010.01043", "abstract": "this paper investigates the impact of economic policy uncertainty (epu) on the crash risk of us stock market during the covid-19 pandemic. to this end, we use the garch-s (garch with skewness) model to estimate daily skewness as a proxy for the stock market crash risk. the empirical results show the significantly negative correlation between epu and stock market crash risk, indicating the aggravation of epu increase the crash risk. moreover, the negative correlation gets stronger after the global covid-19 outbreak, which shows the crash risk of the us stock market will be more affected by epu during the epidemic.", "categories": "q-fin.gn econ.gn q-fin.ec", "created": "2020-10-02", "updated": "", "authors": ["zhifeng liu", "toan luu duc huynh", "jianjun sun", "peng-fei dai"], "url": "https://arxiv.org/abs/2010.01043"}, {"title": "heterogeneous effects of waste pricing policies", "id": "2010.01105", "abstract": "using machine learning methods in a quasi-experimental setting, i study the heterogeneous effects of waste prices -- unit prices on household unsorted waste disposal -- on waste demands and social welfare. first, using a unique panel of italian municipalities with large variation in prices and observables, i show that waste demands are nonlinear. i find evidence of nudge effects at low prices, and increasing elasticities at high prices driven by income effects and waste habits before policy. second, i combine municipal level price effects on unsorted and recycling waste with their impacts on municipal and pollution costs. i estimate overall welfare benefits after three years of adoption, when waste prices cause significant waste avoidance. as waste avoidance is highest at low prices, this implies that even low prices can substantially change waste behaviors and improve welfare.", "categories": "econ.gn q-fin.ec", "created": "2020-10-02", "updated": "", "authors": ["marica valente"], "url": "https://arxiv.org/abs/2010.01105"}, {"title": "a note on quadratic funding under constrained matching funds", "id": "2010.01193", "abstract": "in this note i show that quadratic funding achieves decentralized social efficiency in the extent there are enough (donor) matching funds to cover the quadratic funding objective. if individual backers internalize that matching funds will not be sufficient to reach the quadratic level, allocation will be biased towards the capitalist allocation, the more so, the less matching funds are available. this result emerges even when individual contributors are not required to finance the deficit (i.e., the difference between total contributions and available matching funds). i also show properties of the level of required matching fund, in order to better understand under which conditions social efficiency will most likely be compromised.", "categories": "econ.gn q-fin.ec", "created": "2020-10-02", "updated": "", "authors": ["ricardo a. pasquini"], "url": "https://arxiv.org/abs/2010.01193"}, {"title": "optimal echo chambers", "id": "2010.01249", "abstract": "this paper studies some benefits of ignoring those who disagree with you. we model a decision maker who draws a signal about the (real-valued) state of the world from a collection of unbiased sources of heterogeneous quality. exclusively sampling signals close to the prior expectation can be beneficial, as they are more likely high quality. since echo chambers are a rational response to uncertain information quality, eliminating them can backfire. signals close to the prior expectation can move beliefs further than more contrary views; limiting the ability to ignore opposing views can make beliefs less accurate and reduce the extent to which signals are heeded.", "categories": "econ.th", "created": "2020-10-02", "updated": "2020-10-07", "authors": ["gabriel martinez", "nicholas h. tenev"], "url": "https://arxiv.org/abs/2010.01249"}, {"title": "wealth and poverty: the effect of poverty on communities", "id": "2010.01335", "abstract": "this paper analyzes the differences in poverty in high wealth communities and low wealth communities. we first discuss methods of measuring poverty and analyze the causes of individual poverty and poverty in the bay area. three cases are considered regarding relative poverty. the first two cases involve neighborhoods in the bay area while the third case evaluates two neighborhoods within the city of san jose, ca. we find that low wealth communities have more crime, more teen births, and more cost-burdened renters because of high concentrations of temporary and seasonal workers, extensive regulations on greenhouse gas emissions, minimum wage laws, and limited housing supply. in the conclusion, we review past attempts to alleviate the effects of poverty and give suggestions on how future policy can be influenced to eventually create a future free of poverty.", "categories": "econ.gn q-fin.ec", "created": "2020-10-03", "updated": "", "authors": ["merrick wang", "robert johnston"], "url": "https://arxiv.org/abs/2010.01335"}, {"title": "bitcoin and its impact on the economy", "id": "2010.01337", "abstract": "the purpose of this paper is to review the concept of cryptocurrencies in our economy. first, bitcoin and alternative cryptocurrencies' histories are analyzed. we then study the implementation of bitcoin in the airline and real estate industries. our study finds that many bitcoin companies partner with airlines in order to decrease processing times, to provide ease of access for spending in international airports, and to reduce fees on foreign exchanges for fuel expenses, maintenance, and flight operations. bitcoin transactions have occurred in the real estate industry, but many businesses are concerned with bitcoin's potential interference with the u.s. government and its high volatility. as bitcoin's price has been growing rapidly, we assessed bitcoin's real value; bitcoin derives value from its scarcity, utility, and public trust. in the conclusion, we discuss bitcoin's future and conclude that bitcoin may change from a short-term profit investment to a more steady industry as we identify bitcoin with the \"greater fool theory\", and as the number of available bitcoins to be mined dwindles and technology becomes more expensive.", "categories": "econ.gn q-fin.ec", "created": "2020-10-03", "updated": "", "authors": ["merrick wang"], "url": "https://arxiv.org/abs/2010.01337"}, {"title": "a class of time-varying vector moving average models: nonparametric   kernel estimation and application", "id": "2010.01492", "abstract": "multivariate dynamic time series models are widely encountered in practical studies, e.g., modelling policy transmission mechanism and measuring connectedness between economic agents. to better capture the dynamics, this paper proposes a wide class of multivariate dynamic models with time-varying coefficients, which have a general time-varying vector moving average (vma) representation, and nest, for instance, time-varying vector autoregression (var), time-varying vector autoregression moving-average (varma), and so forth as special cases. the paper then develops a unified estimation method for the unknown quantities before an asymptotic theory for the proposed estimators is established. in the empirical study, we investigate the transmission mechanism of monetary policy using u.s. data, and uncover a fall in the volatilities of exogenous shocks. in addition, we find that (i) monetary policy shocks have less influence on inflation before and during the so-called great moderation, (ii) inflation is more anchored recently, and (iii) the long-run level of inflation is below, but quite close to the federal reserve's target of two percent after the beginning of the great moderation period.", "categories": "econ.em stat.me", "created": "2020-10-04", "updated": "", "authors": ["yayi yan", "jiti gao", "bin peng"], "url": "https://arxiv.org/abs/2010.01492"}, {"title": "strikingly suspicious overnight and intraday returns", "id": "2010.01727", "abstract": "the world's stock markets display a strikingly suspicious pattern of overnight and intraday returns. overnight returns to major stock market indices over the past few decades have been wildly positive, while intraday returns have been disturbingly negative. the cause of these astonishingly consistent return patterns is unknown. we highlight the features of these extraordinary patterns that have hindered the construction of any plausible innocuous explanation. we then use those same features to deduce the only plausible explanation so far advanced for these strikingly suspicious returns.", "categories": "q-fin.gn econ.gn q-fin.ec q-fin.tr", "created": "2020-10-04", "updated": "", "authors": ["bruce knuteson"], "url": "https://arxiv.org/abs/2010.01727"}, {"title": "robust and efficient estimation of potential outcome means under random   assignment", "id": "2010.01800", "abstract": "we study efficiency improvements in estimating a vector of potential outcome means using linear regression adjustment when there are more than two treatment levels. we show that using separate regression adjustments for each assignment level is never worse, asymptotically, than using the subsample averages. we also show that separate regression adjustment improves over pooled regression adjustment except in the obvious case where slope parameters in the linear projections are identical across the different assignment levels. we also characterize the class of nonlinear regression adjustment methods that preserve consistency of the potential outcome means despite arbitrary misspecification of the conditional mean functions. finally, we apply this general potential outcomes framework to a contingent valuation study for estimating lower bound mean willingness to pay for an oil spill prevention program in california.", "categories": "econ.em", "created": "2020-10-05", "updated": "", "authors": ["akanksha negi", "jeffrey m. wooldridge"], "url": "https://arxiv.org/abs/2010.01800"}, {"title": "deep distributional time series models and the probabilistic forecasting   of intraday electricity prices", "id": "2010.01844", "abstract": "recurrent neural networks (rnns) with rich feature vectors of past values can provide accurate point forecasts for series that exhibit complex serial dependence. we propose two approaches to constructing deep time series probabilistic models based on a variant of rnn called an echo state network (esn). the first is where the output layer of the esn has stochastic disturbances and a shrinkage prior for additional regularization. the second approach employs the implicit copula of an esn with gaussian disturbances, which is a deep copula process on the feature space. combining this copula with a non-parametrically estimated marginal distribution produces a deep distributional time series model. the resulting probabilistic forecasts are deep functions of the feature vector and also marginally calibrated. in both approaches, bayesian markov chain monte carlo methods are used to estimate the models and compute forecasts. the proposed deep time series models are suitable for the complex task of forecasting intraday electricity prices. using data from the australian national electricity market, we show that our models provide accurate probabilistic price forecasts. moreover, the models provide a flexible framework for incorporating probabilistic forecasts of electricity demand as additional features. we demonstrate that doing so in the deep distributional time series model in particular, increases price forecast accuracy substantially.", "categories": "stat.me econ.em stat.ap stat.co stat.ml", "created": "2020-10-05", "updated": "", "authors": ["nadja klein", "michael stanley smith", "david j. nott"], "url": "https://arxiv.org/abs/2010.01844"}, {"title": "information thermodynamics of financial markets: the glosten-milgrom   model", "id": "2010.01905", "abstract": "the glosten-milgrom model describes a single asset market, where informed traders interact with a market maker, in the presence of noise traders. we derive an analogy between this financial model and a szil\\'ard information engine by {\\em i)} showing that the optimal work extraction protocol in the latter coincides with the pricing strategy of the market maker in the former and {\\em ii)} defining a market analogue of the physical temperature from the analysis of the distribution of market orders. then we show that the expected gain of informed traders is bounded above by the product of this market temperature with the amount of information that informed traders have, in exact analogy with the corresponding formula for the maximal expected amount of work that can be extracted from a cycle of the information engine. in this way, recent ideas from information thermodynamics may shed light on financial markets, and lead to generalised inequalities, in the spirit of the extended second law of thermodynamics.", "categories": "cond-mat.stat-mech econ.th q-fin.tr", "created": "2020-10-05", "updated": "", "authors": ["l\u00e9o touzo", "matteo marsili", "don zagier"], "url": "https://arxiv.org/abs/2010.01905"}, {"title": "testing homogeneity in dynamic discrete games in finite samples", "id": "2010.02297", "abstract": "the literature on dynamic discrete games often assumes that the conditional choice probabilities and the state transition probabilities are homogeneous across markets and over time. we refer to this as the \"homogeneity assumption\" in dynamic discrete games. this homogeneity assumption enables empirical studies to estimate the game's structural parameters by pooling data from multiple markets and from many time periods. in this paper, we propose a hypothesis test to evaluate whether the homogeneity assumption holds in the data. our hypothesis is the result of an approximate randomization test, implemented via a markov chain monte carlo (mcmc) algorithm. we show that our hypothesis test becomes valid as the (user-defined) number of mcmc draws diverges, for any fixed number of markets, time-periods, and players. we apply our test to the empirical study of the u.s. portland cement industry in ryan (2012).", "categories": "econ.em", "created": "2020-10-05", "updated": "", "authors": ["federico a. bugni", "jackson bunting", "takuya ura"], "url": "https://arxiv.org/abs/2010.02297"}, {"title": "protectionism and economic growth: causal evidence from the first era of   globalization", "id": "2010.02378", "abstract": "we investigate how protectionist policies influence economic growth. our empirical strategy exploits an extraordinary tax scandal that gave rise to an unexpected change of government in sweden. a free-trade majority in parliament was overturned by a comfortable protectionist majority in the fall of 1887. we employ the synthetic control method to select control countries against which economic growth in sweden can be compared. we do not find evidence suggesting that protectionist policies influenced economic growth and examine channels why. tariffs increased government revenue. however, the results do not suggest that the protectionist government stimulated the economy in the short-run by increasing government expenditure.", "categories": "econ.gn q-fin.ec", "created": "2020-10-05", "updated": "", "authors": ["niklas potrafke", "fabian ruthardt", "kaspar w\u00fcthrich"], "url": "https://arxiv.org/abs/2010.02378"}, {"title": "a recursive logit model with choice aversion and its application to   route choice analysis", "id": "2010.02398", "abstract": "we introduce a route choice model that incorporates the notion of choice aversion in transportation networks. formally, we propose a recursive logit model which incorporates a penalty term that accounts for the dimension of the choice set at each node of the network. we make three contributions. first, we show that our model overcomes the correlation problem between routes, a common pitfall of traditional logit models. in particular, our approach can be seen as an alternative to the class of models known as path size logit (psl). second, we show how our model can generate violations of regularity in the path choice probabilities. in particular, we show that removing edges in the network can decrease the probability of some existing paths. finally, we show that under the presence of choice aversion, adding edges to the network can increase the total cost of the system. in other words, a type of braess's paradox can emerge even in the case of uncongested networks. we show that these phenomena can be characterized in terms of a parameter that measures users' degree of choice aversion.", "categories": "econ.em cs.gt", "created": "2020-10-05", "updated": "", "authors": ["austin knies", "emerson melo"], "url": "https://arxiv.org/abs/2010.02398"}, {"title": "heterogeneity in food expenditure amongst us families: evidence from   longitudinal quantile regression", "id": "2010.02614", "abstract": "empirical studies on food expenditure are largely based on cross-section data and for a few studies based on longitudinal (or panel) data the focus has been on the conditional mean. while the former, by construction, cannot model the dependencies between observations across time, the latter cannot look at the relationship between food expenditure and covariates (such as income, education, etc.) at lower (or upper) quantiles, which are of interest to policymakers. this paper analyzes expenditures on total food (tf), food at home (fah), and food away from home (fafh) using mean regression and quantile regression models for longitudinal data to examine the impact of economic recession and various demographic, socioeconomic, and geographic factors. the data is taken from the panel study of income dynamics (psid) and comprises of 2174 families in the united states (us) observed between 2001-2015. results indicate that age and education of the head, family income, female headed family, marital status, and economic recession are important determinants for all three types of food expenditure. spouse education, family size, and some regional indicators are important for expenditures on tf and fah, but not for fafh. quantile analysis reveals considerable heterogeneity in the covariate effects for all types of food expenditure, which cannot be captured by models focused on conditional mean. the study ends by showing that modeling conditional dependence between observations across time for the same family unit is crucial to reducing/avoiding heterogeneity bias and better model fitting.", "categories": "econ.gn q-fin.ec", "created": "2020-10-06", "updated": "", "authors": ["arjun gupta", "soudeh mirghasemi", "mohammad arshad rahman"], "url": "https://arxiv.org/abs/2010.02614"}, {"title": "extending social resource exchange to events of abundance and   sufficiency", "id": "2010.02658", "abstract": "this article identifies how scarcity, abundance, and sufficiency influence exchange behavior. analyzing the mechanisms governing exchange of resources constitutes the foundation of several social-science perspectives. neoclassical economics provides one of the most well-known perspectives of how rational individuals allocate and exchange resources. using rational choice theory (rct), neoclassical economics assumes that exchange between two individuals will occur when resources are scarce and that these individuals interact rationally to satisfy their requirements (i.e., preferences). while rct is useful to characterize interaction in closed and stylized systems, it proves insufficient to capture social and psychological reality where culture, emotions, and habits play an integral part in resource exchange. social resource theory (srt) improves on rct in several respects by making the social nature of resources the object of study. srt shows how human interaction is driven by an array of psychological mechanisms, from emotions to heuristics. thus, srt provides a more realistic foundation for analyzing and explaining social exchange than the stylized instrumental rationality of rct. yet srt has no clear place for events of abundance and sufficiency as additional motivations to exchange resources. this article synthesize and formalize a foundation for srt using not only scarcity but also abundance and sufficiency.", "categories": "econ.gn q-fin.ec", "created": "2020-10-06", "updated": "", "authors": ["jonas b\u00e5\u00e5th", "adel daoud"], "url": "https://arxiv.org/abs/2010.02658"}, {"title": "state of the art survey of deep learning and machine learning models for   smart cities and urban sustainability", "id": "2010.02670", "abstract": "deep learning (dl) and machine learning (ml) methods have recently contributed to the advancement of models in the various aspects of prediction, planning, and uncertainty analysis of smart cities and urban development. this paper presents the state of the art of dl and ml methods used in this realm. through a novel taxonomy, the advances in model development and new application domains in urban sustainability and smart cities are presented. findings reveal that five dl and ml methods have been most applied to address the different aspects of smart cities. these are artificial neural networks; support vector machines; decision trees; ensembles, bayesians, hybrids, and neuro-fuzzy; and deep learning. it is also disclosed that energy, health, and urban transport are the main domains of smart cities that dl and ml methods contributed in to address their problems.", "categories": "econ.gn q-fin.ec", "created": "2020-10-06", "updated": "", "authors": ["saeed nosratabadi", "amir mosavi", "ramin keivani", "sina ardabili", "farshid aram"], "url": "https://arxiv.org/abs/2010.02670"}, {"title": "modelling temperature variation of mushroom growing hall using   artificial neural networks", "id": "2010.02673", "abstract": "the recent developments of computer and electronic systems have made the use of intelligent systems for the automation of agricultural industries. in this study, the temperature variation of the mushroom growing room was modeled by multi-layered perceptron and radial basis function networks based on independent parameters including ambient temperature, water temperature, fresh air and circulation air dampers, and water tap. according to the obtained results from the networks, the best network for mlp was in the second repetition with 12 neurons in the hidden layer and in 20 neurons in the hidden layer for radial basis function network. the obtained results from comparative parameters for two networks showed the highest correlation coefficient (0.966), the lowest root mean square error (rmse) (0.787) and the lowest mean absolute error (mae) (0.02746) for radial basis function. therefore, the neural network with radial basis function was selected as a predictor of the behavior of the system for the temperature of mushroom growing halls controlling system.", "categories": "econ.gn q-fin.ec", "created": "2020-10-06", "updated": "", "authors": ["sina ardabili", "amir mosavi", "asghar mahmoudi", "tarahom mesri gundoshmian", "saeed nosratabadi", "annamaria r. varkonyi-koczy"], "url": "https://arxiv.org/abs/2010.02673"}, {"title": "leader cultural intelligence and organizational performance", "id": "2010.02678", "abstract": "one of the challenges for international companies is to manage multicultural environments effectively. cultural intelligence (cq) is a soft skill required of the leaders of organizations working in cross-cultural contexts to be able to communicate effectively in such environments. on the other hand, organizational structure plays an active role in developing and promoting such skills in an organization. therefore, this study aimed to investigate the effect of leader cq on organizational performance mediated by organizational structure. to achieve the objective of this research, first, conceptual models and hypotheses of this research were formed based on the literature. then, a quantitative empirical research design using a questionnaire, as a tool for data collection, and structural equation modeling, as a tool for data analysis, was employed among executives of knowledge-based companies in the science and technology park, bushehr, iran. the results disclosed that leader cq directly and indirectly (i.e., through the organizational structure) has a positive and significant effect on organizational performance. in other words, in organizations that operate in a multicultural environment, the higher the level of leader cq, the higher the performance of that organization. accordingly, such companies are encouraged to invest in improving the cultural intelligence of their leaders to improve their performance in cross-cultural environments, and to design appropriate organizational structures for the development of their intellectual capital.", "categories": "econ.gn q-fin.ec", "created": "2020-10-06", "updated": "", "authors": ["saeed nosratabadi", "parvaneh bahrami", "khodayar palouzian", "amir mosavi"], "url": "https://arxiv.org/abs/2010.02678"}, {"title": "comment on gouri\\'eroux, monfort, renne (2019): identification and   estimation in non-fundamental structural varma models", "id": "2010.02711", "abstract": "this comment points out a serious flaw in the article \"gouri\\'eroux, monfort, renne (2019): identification and estimation in non-fundamental structural varma models\" with regard to mirroring complex-valued roots with blaschke polynomial matrices. moreover, the (non-) feasibility of the proposed method (if the handling of blaschke transformation were not prohibitive) for cross-sectional dimensions greater than two and vector moving average (vma) polynomial matrices of degree greater than one is discussed.", "categories": "econ.em", "created": "2020-10-06", "updated": "", "authors": ["bernd funovits"], "url": "https://arxiv.org/abs/2010.02711"}, {"title": "infinite-dimensional fisher markets: equilibrium, duality and   optimization", "id": "2010.03025", "abstract": "this paper considers a linear fisher market with $n$ buyers and a continuum of items. in order to compute market equilibria, we introduce (infinite-dimensional) convex programs over banach spaces, thereby generalizing the eisenberg-gale convex program and its dual. regarding the new convex programs, we establish existence of optimal solutions, the existence of kkt-type conditions, as well as strong duality. all these properties are established via non-standard arguments, which circumvent the limitations of duality theory in optimization over infinite-dimensional vector spaces. furthermore, we show that there exists a pure equilibrium allocation, i.e., a division of the item space. similar to the finite-dimensional case, a market equilibrium under the infinite-dimensional fisher market is pareto optimal, envy-free and proportional. we also show how to obtain the (a.e. unique) equilibrium price vector and a pure equilibrium allocation from the (unique) $n$-dimensional equilibrium bang-per-buck vector. when the item space is the unit interval $[0,1]$ and buyers have piecewise linear utilities, we show that $\\epsilon$-approximate equilibrium prices can be computed in time polynomial in the market size and $\\log \\frac{1}{\\epsilon}$. this is achieved by solving a finite-dimensional convex program using the ellipsoid method. to this end, we give nontrivial and efficient subgradient and separation oracles. for general buyer valuations, we propose computing market equilibrium using stochastic dual averaging, which finds an approximate equilibrium price vector with high probability.", "categories": "cs.gt econ.th math.oc", "created": "2020-10-06", "updated": "2020-10-07", "authors": ["yuan gao", "christian kroer"], "url": "https://arxiv.org/abs/2010.03025"}, {"title": "dynamic coalitions in complex task environments: to change or not to   change a winning team?", "id": "2010.03371", "abstract": "decision makers are often confronted with complex tasks which cannot be solved by an individual alone, but require collaboration in the form of a coalition. previous literature argues that instability, in terms of the re-organization of a coalition with respect to its members over time, is detrimental to performance. other lines of research, such as the dynamic capabilities framework, challenge this view. our objective is to understand the effects of instability on the performance of coalitions which are formed to solve complex tasks. in order to do so, we adapt the nk-model to the context of human decision-making in coalitions, and introduce an auction-based mechanism for autonomous coalition formation and a learning mechanism for human agents. preliminary results suggest that re-organizing innovative and well-performing teams is beneficial, but that this is true only in certain situations.", "categories": "econ.gn q-fin.ec", "created": "2020-10-07", "updated": "", "authors": ["dario blanco-fernandez", "stephan leitner", "alexandra rausch"], "url": "https://arxiv.org/abs/2010.03371"}, {"title": "further results on the estimation of dynamic panel logit models with   fixed effects", "id": "2010.03382", "abstract": "kitazawa (2013, 2016) showed that the common parameters in the panel logit ar(1) model with strictly exogenous covariates and fixed effects are estimable at the root-n rate using the generalized method of moments. honor\\'e and weidner (2020) extended his results in various directions: they found additional moment conditions for the logit ar(1) model and also considered estimation of logit ar(p) models with p>1. in this note we prove a conjecture in their paper and show that 2^{t}-2t of their moment functions for the logit ar(1) model are linearly independent and span the set of valid moment functions, which is a 2^{t}-2t -dimensional linear subspace of the 2^{t} -dimensional vector space of real valued functions over the outcomes y element of {0,1}^{t}.", "categories": "econ.em math.st stat.th", "created": "2020-10-07", "updated": "", "authors": ["hugo kruiniger"], "url": "https://arxiv.org/abs/2010.03382"}, {"title": "no data? no problem! a search-based recommendation system with cold   starts", "id": "2010.03455", "abstract": "recommendation systems are essential ingredients in producing matches between products and buyers. despite their ubiquity, they face two important challenges. first, they are data-intensive, a feature that precludes sophisticated recommendations by some types of sellers, including those selling durable goods. second, they often focus on estimating fixed evaluations of products by consumers while ignoring state-dependent behaviors identified in the marketing literature.   we propose a recommendation system based on consumer browsing behaviors, which bypasses the \"cold start\" problem described above, and takes into account the fact that consumers act as \"moving targets,\" behaving differently depending on the recommendations suggested to them along their search journey. first, we recover the consumers' search policy function via machine learning methods. second, we include that policy into the recommendation system's dynamic problem via a bellman equation framework.   when compared with the seller's own recommendations, our system produces a profit increase of 33%. our counterfactual analyses indicate that browsing history along with past recommendations feature strong complementary effects in value creation. moreover, managing customer churn effectively is a big part of value creation, whereas recommending alternatives in a forward-looking way produces moderate effects.", "categories": "econ.gn q-fin.ec", "created": "2020-10-07", "updated": "", "authors": ["pedro m. gardete", "carlos d. santos"], "url": "https://arxiv.org/abs/2010.03455"}, {"title": "to act or not to act? political competition in the presence of a threat", "id": "2010.03464", "abstract": "we present a model of political competition in which an incumbent politician, may implement a costly policy to prevent a possible threat to, for example, national security or a natural disaster.", "categories": "econ.gn q-fin.ec", "created": "2020-10-07", "updated": "", "authors": ["arthur fishman", "doron klunover"], "url": "https://arxiv.org/abs/2010.03464"}, {"title": "sentiment of tweets and socio-economic characteristics as the   determinants of voting behavior at the regional level. case study of 2019   polish parliamentary election", "id": "2010.03493", "abstract": "this work is dedicated to finding the determinants of voting behavior in poland at the poviat level. 2019 parliamentary election has been analyzed and an attempt to explain vote share for the winning party (law and justice) has been made. sentiment analysis of tweets in polish (original) and english (machine-translations), collected in the period around the election, has been applied. amid multiple machine learning approaches tested, the best classification accuracy has been achieved by huggingface bert on machine-translated tweets. ols regression, with sentiment of tweets and selected socio-economic features as independent variables, has been utilized to explain law and justice vote share in poviats. sentiment of tweets has been found to be a significant predictor, as stipulated by the literature of the field.", "categories": "econ.gn q-fin.ec", "created": "2020-10-07", "updated": "", "authors": ["grzegorz krochmal"], "url": "https://arxiv.org/abs/2010.03493"}, {"title": "interpreting unconditional quantile regression with conditional   independence", "id": "2010.03606", "abstract": "this note provides additional interpretation for the counterfactual outcome distribution and corresponding unconditional quantile \"effects\" defined and estimated by firpo, fortin, and lemieux (2009) and chernozhukov, fern\\'andez-val, and melly (2013). with conditional independence of the policy variable of interest, these methods estimate the policy effect for certain types of policies, but not others. in particular, they estimate the effect of a policy change that itself satisfies conditional independence.", "categories": "econ.em", "created": "2020-10-07", "updated": "", "authors": ["david m. kaplan"], "url": "https://arxiv.org/abs/2010.03606"}, {"title": "agent based computational model aided approach to improvise the   inequality-adjusted human development index (ihdi) for greater parity in real   scenario assessments", "id": "2010.03677", "abstract": "to design, evaluate and tune policies for all-inclusive human development, the primary requisite is to assess the true state of affairs of the society. statistical indices like gdp, gini coefficients have been developed to accomplish the evaluation of the socio-economic systems. they have remained prevalent in the conventional economic theories but little do they have in the offing regarding true well-being and development of humans. human development index (hdi) and thereafter inequality-adjusted human development index (ihdi) has been the path changing composite-index having the focus on human development. however, even though its fundamental philosophy has an all-inclusive human development focus, the composite-indices appear to be unable to grasp the actual assessment in several scenarios. this happens due to the dynamic non-linearity of social-systems where superposition principle cannot be applied between all of its inputs and outputs of the system as the system's own attributes get altered upon each input. we would discuss the apparent shortcomings and probable refinement of the existing index using an agent based computational system model approach.", "categories": "cs.cy cs.si econ.gn q-fin.ec", "created": "2020-10-07", "updated": "", "authors": ["pradipta banerjee", "subhrabrata choudhury"], "url": "https://arxiv.org/abs/2010.03677"}, {"title": "adaptive doubly robust estimator", "id": "2010.03792", "abstract": "we propose a doubly robust (dr) estimator for off-policy evaluation (ope) from data obtained via multi-armed bandit (mab) algorithms. the goal of ope is to evaluate a new policy using historical data. because the mab algorithms sequentially updates the policy based on past observations, the generated samples are not independent and identically distributed (i.i.d.). to conduct ope from dependent samples, we propose an ope estimator with asymptotic normality even under the dependency. in particular, we focus on a dr estimator, which consists of an inverse probability weighting (ipw) component and an estimator of the conditionally expected outcome. the proposed adaptive dr estimator only requires the convergence rate conditions of the nuisance estimators and the other mild regularity conditions; that is, we do not impose a specific time-series structure and donsker's condition. we investigate the effectiveness by using benchmark datasets compared to a past proposed dr estimator with double/debiased machine learning and an adaptive version of an augmented ipw estimator.", "categories": "cs.lg econ.em stat.me stat.ml", "created": "2020-10-08", "updated": "", "authors": ["masahiro kato"], "url": "https://arxiv.org/abs/2010.03792"}, {"title": "consistent specification test of the quantile autoregression", "id": "2010.03898", "abstract": "this paper proposes a test for the joint hypothesis of correct dynamic specification and no omitted latent factors for the quantile autoregression. if the composite null is rejected we proceed to disentangle the cause of rejection, i.e., dynamic misspecification or an omitted variable. we establish the asymptotic distribution of the test statistics under fairly weak conditions and show that factor estimation error is negligible. a monte carlo study shows that the suggested tests have good finite sample properties. finally, we undertake an empirical illustration of modelling gdp growth and cpi inflation in the united kingdom, where we find evidence that factor augmented models are correctly specified in contrast with their non-augmented counterparts when it comes to gdp growth, while also exploring the asymmetric behaviour of the growth and inflation distributions.", "categories": "econ.em", "created": "2020-10-08", "updated": "", "authors": ["anthoulla phella"], "url": "https://arxiv.org/abs/2010.03898"}, {"title": "prediction intervals for deep neural networks", "id": "2010.04044", "abstract": "the aim of this paper is to propose a suitable method for constructing prediction intervals for the output of neural network models. to do this, we adapt the extremely randomized trees method originally developed for random forests to construct ensembles of neural networks. the extra-randomness introduced in the ensemble reduces the variance of the predictions and yields gains in out-of-sample accuracy. an extensive monte carlo simulation exercise shows the good performance of this novel method for constructing prediction intervals in terms of coverage probability and mean square prediction error. this approach is superior to state-of-the-art methods extant in the literature such as the widely used mc dropout and bootstrap procedures. the out-of-sample accuracy of the novel algorithm is further evaluated using experimental settings already adopted in the literature.", "categories": "stat.ml cs.lg econ.em stat.me", "created": "2020-10-08", "updated": "", "authors": ["tullio mancini", "hector calvo-pardo", "jose olmo"], "url": "https://arxiv.org/abs/2010.04044"}, {"title": "inference with a single treated cluster", "id": "2010.04076", "abstract": "i introduce a generic method for inference about a scalar parameter in research designs with a finite number of heterogeneous clusters where only a single cluster received treatment. this situation is commonplace in difference-in-differences estimation but the test developed here applies more generally. i show that the test controls size and has power under asymptotics where the number of observations within each cluster is large but the number of clusters is fixed. the test combines weighted, approximately gaussian parameter estimates with a rearrangement procedure to obtain its critical values. the weights needed for most empirically relevant situations are tabulated in the paper. calculation of the critical values is computationally simple and does not require simulation or resampling. the rearrangement test is highly robust to situations where some clusters are much more variable than others. examples and an empirical application are provided.", "categories": "econ.em stat.me", "created": "2020-10-08", "updated": "", "authors": ["andreas hagemann"], "url": "https://arxiv.org/abs/2010.04076"}, {"title": "the english patient: evaluating local lockdowns using real-time covid-19   & consumption data", "id": "2010.04129", "abstract": "we find uk 'local lockdowns' of cities and sub-regions, focused on limiting contact between households in homes, turn the tide on rising positive covid-19 cases without the large declines in consumption accompanying the march 2020 national lockdown, which limited all social contact. our study harnesses a new source of real-time, transaction-level consumption data that we show to be highly correlated with official statistics. the effectiveness of local lockdowns are evaluated applying a difference-in-difference approach which exploits nearby localities not subject to local lockdowns as comparison groups. our findings indicate that policymakers may be able to contain virus outbreaks without killing local economies. however, the ultimate effectiveness of local lockdowns is expected to be highly dependent on co-ordination between regions and testing.", "categories": "econ.gn q-fin.ec", "created": "2020-10-08", "updated": "", "authors": ["john gathergood", "benedict guttman-kenney"], "url": "https://arxiv.org/abs/2010.04129"}, {"title": "extremal quantile regression", "id": "math/0505639", "abstract": "quantile regression is an important tool for estimation of conditional quantiles of a response y given a vector of covariates x. it can be used to measure the effect of covariates not only in the center of a distribution, but also in the upper and lower tails. this paper develops a theory of quantile regression in the tails. specifically, it obtains the large sample properties of extremal (extreme order and intermediate order) quantile regression estimators for the linear quantile regression model with the tails restricted to the domain of minimum attraction and closed under tail equivalence across regressor values. this modeling setup combines restrictions of extreme value theory with leading homoscedastic and heteroscedastic linear specifications of regression analysis. in large samples, extreme order regression quantiles converge weakly to \\argmin functionals of stochastic integrals of poisson processes that depend on regressors, while intermediate regression quantiles and their functionals converge to normal vectors with variance matrices dependent on the tail parameters and the regressor design.", "categories": "math.st econ.em stat.th", "created": "2005-05-30", "updated": "", "authors": ["victor chernozhukov"], "url": "https://arxiv.org/abs/math/0505639"}]